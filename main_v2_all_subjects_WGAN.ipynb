{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92746a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_ import *\n",
    "from train_evaluate import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7aae3",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85a7dd",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0824322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ''\n",
    "X_train_valid, y_train_valid, X_test, y_test = load_data(data_dir, subjects=[1,2,3,4,5,6,7,8,9]) # default subjects=[1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb1db",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68646e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (6768, 250, 1, 22)\n",
      "Shape of x_valid: (1692, 250, 1, 22)\n",
      "Shape of x_test: (1772, 250, 1, 22)\n",
      "Shape of y_train: torch.Size([6768, 4])\n",
      "Shape of y_valid: torch.Size([1692, 4])\n",
      "Shape of y_test: torch.Size([1772, 4])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = main_prep(X_train_valid,y_train_valid,X_test, y_test,2,2,True)\n",
    "_, y_train =y_train.max(dim=1) \n",
    "_, y_valid =y_valid.max(dim=1) \n",
    "_, y_test =y_test.max(dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6fc9c",
   "metadata": {},
   "source": [
    "## PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders  = dataloader_setup(x_train, y_train, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af55041",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe828a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR      = 0.0005\n",
    "BETAS   = (0.9, 0.999)\n",
    "EPS     = 1e-08\n",
    "DECAY   = 0.0005\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7eae4",
   "metadata": {},
   "source": [
    "# Modeling (CNN, LSTM, GRU, CNN+LSTM, CNN+GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0a35c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2932018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the CNN model using sequential class\n",
    "class CNN(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(800,4)\n",
    "        \n",
    "    # forward method\n",
    "    def forward(self, input):  #input(22,250,1)\n",
    "        x = input.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "        x = self.flatten(x)\n",
    "        out = self.dense(x) \n",
    "        return out\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "class CNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.gru = nn.GRU(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bab66",
   "metadata": {},
   "source": [
    "## Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21af93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate CNN model\n",
    "cnn = CNN()\n",
    "# create your cnn optimizer\n",
    "cnn_optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate LSTM model\n",
    "lstm = LSTM()\n",
    "# create your lstm optimizer\n",
    "lstm_optimizer = optim.Adam(lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate GRU model\n",
    "gru = GRU()\n",
    "# create your gru optimizer\n",
    "gru_optimizer = optim.Adam(gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate CNN_LSTM model\n",
    "cnn_lstm = CNN_LSTM()\n",
    "# create your cnn_lstm optimizer\n",
    "cnn_lstm_optimizer = optim.Adam(cnn_lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate CNN_GRU model\n",
    "cnn_gru = CNN_GRU()\n",
    "# create your cnn_gru optimizer\n",
    "cnn_gru_optimizer = optim.Adam(cnn_gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897c535",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn, cnn_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fe233",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6a6b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(lstm, lstm_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95517be3",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(gru, gru_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c17b8f",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088397f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_lstm, cnn_lstm_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9c135",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8419f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_gru, cnn_gru_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc453a",
   "metadata": {},
   "source": [
    "# GAN Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e02cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_WGAN = np.load('WGAN_generate_X.npy')\n",
    "fake_label = np.load('generate_label.npy')\n",
    "\n",
    "fake_WGAN = np.swapaxes(fake_WGAN, 1,2)\n",
    "fake_WGAN = np.swapaxes(fake_WGAN,2,3)\n",
    "\n",
    "fake_label = torch.tensor(fake_label).to(torch.int64)\n",
    "fake_label = torch.argmax(fake_label, -1)\n",
    "fake_label = fake_label.numpy()\n",
    "fake_label = torch.LongTensor(fake_label).to(torch.int64)\n",
    "\n",
    "\n",
    "#Add 8460/4 fake data\n",
    "x_train_plus_WGAN = np.vstack((x_train, fake_WGAN[0:fake_WGAN.shape[0]//4]))\n",
    "y_train_plus_WGAN = np.hstack((y_train, fake_label[0:fake_WGAN.shape[0]//4]))\n",
    "p = np.random.permutation(x_train_plus_WGAN.shape[0])\n",
    "x_train_plus_WGAN, y_train_plus_WGAN = x_train_plus_WGAN[p], y_train_plus_WGAN[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e77b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_wgan = dataloader_setup(x_train_plus_WGAN, y_train_plus_WGAN, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3d10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ACGAN = np.load('ACGAN_generate_X.npy')\n",
    "fake_label = np.load('generate_label.npy')\n",
    "\n",
    "fake_ACGAN = np.swapaxes(fake_ACGAN, 1,2)\n",
    "fake_ACGAN = np.swapaxes(fake_ACGAN,2,3)\n",
    "\n",
    "fake_label = torch.tensor(fake_label).to(torch.int64)\n",
    "fake_label = torch.argmax(fake_label, -1)\n",
    "fake_label = fake_label.numpy()\n",
    "fake_label = torch.LongTensor(fake_label).to(torch.int64) \n",
    "\n",
    "#Add 8460/4 fake data\n",
    "x_train_plus_ACGAN = np.vstack((x_train, fake_WGAN[0:fake_ACGAN.shape[0]//4]))\n",
    "y_train_plus_ACGAN = np.hstack((y_train, fake_label[0:fake_ACGAN.shape[0]//4]))\n",
    "p = np.random.permutation(x_train_plus_ACGAN.shape[0])\n",
    "x_train_plus_ACGAN, y_train_plus_ACGAN = x_train_plus_ACGAN[p], y_train_plus_ACGAN[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51525ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_acgan = dataloader_setup(x_train_plus_ACGAN, y_train_plus_ACGAN, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a0437",
   "metadata": {},
   "source": [
    "# ACGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f4983",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn, cnn_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e52289",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(lstm, lstm_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c719bc4",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97398b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(gru, gru_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d1b5c",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c552b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_lstm, cnn_lstm_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7620c8",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf210aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_gru, cnn_gru_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a2681",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658728a2",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a9abc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/8883 (0.00%)]\t\tLoss: 1.65029\n",
      "Training Progress: \tEpoch 1 [320/8883 (3.60%)]\t\tLoss: 1.68599\n",
      "Training Progress: \tEpoch 1 [640/8883 (7.19%)]\t\tLoss: 1.46664\n",
      "Training Progress: \tEpoch 1 [960/8883 (10.79%)]\t\tLoss: 1.56269\n",
      "Training Progress: \tEpoch 1 [1280/8883 (14.39%)]\t\tLoss: 1.50588\n",
      "Training Progress: \tEpoch 1 [1600/8883 (17.99%)]\t\tLoss: 1.67049\n",
      "Training Progress: \tEpoch 1 [1920/8883 (21.58%)]\t\tLoss: 1.44089\n",
      "Training Progress: \tEpoch 1 [2240/8883 (25.18%)]\t\tLoss: 1.57890\n",
      "Training Progress: \tEpoch 1 [2560/8883 (28.78%)]\t\tLoss: 1.49102\n",
      "Training Progress: \tEpoch 1 [2880/8883 (32.37%)]\t\tLoss: 1.59067\n",
      "Training Progress: \tEpoch 1 [3200/8883 (35.97%)]\t\tLoss: 1.52617\n",
      "Training Progress: \tEpoch 1 [3520/8883 (39.57%)]\t\tLoss: 1.43644\n",
      "Training Progress: \tEpoch 1 [3840/8883 (43.17%)]\t\tLoss: 1.48197\n",
      "Training Progress: \tEpoch 1 [4160/8883 (46.76%)]\t\tLoss: 1.60653\n",
      "Training Progress: \tEpoch 1 [4480/8883 (50.36%)]\t\tLoss: 1.43721\n",
      "Training Progress: \tEpoch 1 [4800/8883 (53.96%)]\t\tLoss: 1.42475\n",
      "Training Progress: \tEpoch 1 [5120/8883 (57.55%)]\t\tLoss: 1.32257\n",
      "Training Progress: \tEpoch 1 [5440/8883 (61.15%)]\t\tLoss: 1.41085\n",
      "Training Progress: \tEpoch 1 [5760/8883 (64.75%)]\t\tLoss: 1.31634\n",
      "Training Progress: \tEpoch 1 [6080/8883 (68.35%)]\t\tLoss: 1.66319\n",
      "Training Progress: \tEpoch 1 [6400/8883 (71.94%)]\t\tLoss: 1.54915\n",
      "Training Progress: \tEpoch 1 [6720/8883 (75.54%)]\t\tLoss: 1.43624\n",
      "Training Progress: \tEpoch 1 [7040/8883 (79.14%)]\t\tLoss: 1.38012\n",
      "Training Progress: \tEpoch 1 [7360/8883 (82.73%)]\t\tLoss: 1.37746\n",
      "Training Progress: \tEpoch 1 [7680/8883 (86.33%)]\t\tLoss: 1.39229\n",
      "Training Progress: \tEpoch 1 [8000/8883 (89.93%)]\t\tLoss: 1.24197\n",
      "Training Progress: \tEpoch 1 [8320/8883 (93.53%)]\t\tLoss: 1.25074\n",
      "Training Progress: \tEpoch 1 [8640/8883 (97.12%)]\t\tLoss: 1.28239\n",
      "\tTrain loss: 0.03945, Accuracy: 3797/8883 (42.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 825/1692 (48.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/8883 (0.00%)]\t\tLoss: 1.29285\n",
      "Training Progress: \tEpoch 2 [320/8883 (3.60%)]\t\tLoss: 1.27398\n",
      "Training Progress: \tEpoch 2 [640/8883 (7.19%)]\t\tLoss: 1.36208\n",
      "Training Progress: \tEpoch 2 [960/8883 (10.79%)]\t\tLoss: 1.22837\n",
      "Training Progress: \tEpoch 2 [1280/8883 (14.39%)]\t\tLoss: 1.26533\n",
      "Training Progress: \tEpoch 2 [1600/8883 (17.99%)]\t\tLoss: 1.14450\n",
      "Training Progress: \tEpoch 2 [1920/8883 (21.58%)]\t\tLoss: 1.25836\n",
      "Training Progress: \tEpoch 2 [2240/8883 (25.18%)]\t\tLoss: 1.16746\n",
      "Training Progress: \tEpoch 2 [2560/8883 (28.78%)]\t\tLoss: 1.42504\n",
      "Training Progress: \tEpoch 2 [2880/8883 (32.37%)]\t\tLoss: 1.23225\n",
      "Training Progress: \tEpoch 2 [3200/8883 (35.97%)]\t\tLoss: 1.40114\n",
      "Training Progress: \tEpoch 2 [3520/8883 (39.57%)]\t\tLoss: 1.27325\n",
      "Training Progress: \tEpoch 2 [3840/8883 (43.17%)]\t\tLoss: 1.38411\n",
      "Training Progress: \tEpoch 2 [4160/8883 (46.76%)]\t\tLoss: 1.53744\n",
      "Training Progress: \tEpoch 2 [4480/8883 (50.36%)]\t\tLoss: 1.22818\n",
      "Training Progress: \tEpoch 2 [4800/8883 (53.96%)]\t\tLoss: 1.23718\n",
      "Training Progress: \tEpoch 2 [5120/8883 (57.55%)]\t\tLoss: 1.34197\n",
      "Training Progress: \tEpoch 2 [5440/8883 (61.15%)]\t\tLoss: 1.33467\n",
      "Training Progress: \tEpoch 2 [5760/8883 (64.75%)]\t\tLoss: 1.10748\n",
      "Training Progress: \tEpoch 2 [6080/8883 (68.35%)]\t\tLoss: 1.33007\n",
      "Training Progress: \tEpoch 2 [6400/8883 (71.94%)]\t\tLoss: 1.34182\n",
      "Training Progress: \tEpoch 2 [6720/8883 (75.54%)]\t\tLoss: 1.30479\n",
      "Training Progress: \tEpoch 2 [7040/8883 (79.14%)]\t\tLoss: 1.24327\n",
      "Training Progress: \tEpoch 2 [7360/8883 (82.73%)]\t\tLoss: 1.43526\n",
      "Training Progress: \tEpoch 2 [7680/8883 (86.33%)]\t\tLoss: 1.20421\n",
      "Training Progress: \tEpoch 2 [8000/8883 (89.93%)]\t\tLoss: 1.26216\n",
      "Training Progress: \tEpoch 2 [8320/8883 (93.53%)]\t\tLoss: 1.05700\n",
      "Training Progress: \tEpoch 2 [8640/8883 (97.12%)]\t\tLoss: 1.31543\n",
      "\tTrain loss: 0.03684, Accuracy: 4132/8883 (46.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 873/1692 (51.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/8883 (0.00%)]\t\tLoss: 1.20024\n",
      "Training Progress: \tEpoch 3 [320/8883 (3.60%)]\t\tLoss: 1.23043\n",
      "Training Progress: \tEpoch 3 [640/8883 (7.19%)]\t\tLoss: 1.20684\n",
      "Training Progress: \tEpoch 3 [960/8883 (10.79%)]\t\tLoss: 1.15843\n",
      "Training Progress: \tEpoch 3 [1280/8883 (14.39%)]\t\tLoss: 1.15203\n",
      "Training Progress: \tEpoch 3 [1600/8883 (17.99%)]\t\tLoss: 1.23057\n",
      "Training Progress: \tEpoch 3 [1920/8883 (21.58%)]\t\tLoss: 1.04047\n",
      "Training Progress: \tEpoch 3 [2240/8883 (25.18%)]\t\tLoss: 1.18495\n",
      "Training Progress: \tEpoch 3 [2560/8883 (28.78%)]\t\tLoss: 1.33986\n",
      "Training Progress: \tEpoch 3 [2880/8883 (32.37%)]\t\tLoss: 1.26532\n",
      "Training Progress: \tEpoch 3 [3200/8883 (35.97%)]\t\tLoss: 1.30697\n",
      "Training Progress: \tEpoch 3 [3520/8883 (39.57%)]\t\tLoss: 1.26743\n",
      "Training Progress: \tEpoch 3 [3840/8883 (43.17%)]\t\tLoss: 1.35495\n",
      "Training Progress: \tEpoch 3 [4160/8883 (46.76%)]\t\tLoss: 1.34847\n",
      "Training Progress: \tEpoch 3 [4480/8883 (50.36%)]\t\tLoss: 1.14874\n",
      "Training Progress: \tEpoch 3 [4800/8883 (53.96%)]\t\tLoss: 1.28225\n",
      "Training Progress: \tEpoch 3 [5120/8883 (57.55%)]\t\tLoss: 1.25997\n",
      "Training Progress: \tEpoch 3 [5440/8883 (61.15%)]\t\tLoss: 1.31254\n",
      "Training Progress: \tEpoch 3 [5760/8883 (64.75%)]\t\tLoss: 1.13482\n",
      "Training Progress: \tEpoch 3 [6080/8883 (68.35%)]\t\tLoss: 1.22735\n",
      "Training Progress: \tEpoch 3 [6400/8883 (71.94%)]\t\tLoss: 1.10357\n",
      "Training Progress: \tEpoch 3 [6720/8883 (75.54%)]\t\tLoss: 1.28012\n",
      "Training Progress: \tEpoch 3 [7040/8883 (79.14%)]\t\tLoss: 1.19300\n",
      "Training Progress: \tEpoch 3 [7360/8883 (82.73%)]\t\tLoss: 1.31598\n",
      "Training Progress: \tEpoch 3 [7680/8883 (86.33%)]\t\tLoss: 1.09879\n",
      "Training Progress: \tEpoch 3 [8000/8883 (89.93%)]\t\tLoss: 1.32168\n",
      "Training Progress: \tEpoch 3 [8320/8883 (93.53%)]\t\tLoss: 1.09036\n",
      "Training Progress: \tEpoch 3 [8640/8883 (97.12%)]\t\tLoss: 1.22051\n",
      "\tTrain loss: 0.03533, Accuracy: 4442/8883 (50.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 947/1692 (55.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 874/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/8883 (0.00%)]\t\tLoss: 1.24132\n",
      "Training Progress: \tEpoch 4 [320/8883 (3.60%)]\t\tLoss: 1.15709\n",
      "Training Progress: \tEpoch 4 [640/8883 (7.19%)]\t\tLoss: 1.13303\n",
      "Training Progress: \tEpoch 4 [960/8883 (10.79%)]\t\tLoss: 1.17214\n",
      "Training Progress: \tEpoch 4 [1280/8883 (14.39%)]\t\tLoss: 1.19746\n",
      "Training Progress: \tEpoch 4 [1600/8883 (17.99%)]\t\tLoss: 1.07572\n",
      "Training Progress: \tEpoch 4 [1920/8883 (21.58%)]\t\tLoss: 1.10195\n",
      "Training Progress: \tEpoch 4 [2240/8883 (25.18%)]\t\tLoss: 1.01024\n",
      "Training Progress: \tEpoch 4 [2560/8883 (28.78%)]\t\tLoss: 1.29132\n",
      "Training Progress: \tEpoch 4 [2880/8883 (32.37%)]\t\tLoss: 1.21148\n",
      "Training Progress: \tEpoch 4 [3200/8883 (35.97%)]\t\tLoss: 1.38416\n",
      "Training Progress: \tEpoch 4 [3520/8883 (39.57%)]\t\tLoss: 1.32873\n",
      "Training Progress: \tEpoch 4 [3840/8883 (43.17%)]\t\tLoss: 1.25840\n",
      "Training Progress: \tEpoch 4 [4160/8883 (46.76%)]\t\tLoss: 1.07635\n",
      "Training Progress: \tEpoch 4 [4480/8883 (50.36%)]\t\tLoss: 1.03552\n",
      "Training Progress: \tEpoch 4 [4800/8883 (53.96%)]\t\tLoss: 1.23394\n",
      "Training Progress: \tEpoch 4 [5120/8883 (57.55%)]\t\tLoss: 1.35385\n",
      "Training Progress: \tEpoch 4 [5440/8883 (61.15%)]\t\tLoss: 1.14962\n",
      "Training Progress: \tEpoch 4 [5760/8883 (64.75%)]\t\tLoss: 1.06472\n",
      "Training Progress: \tEpoch 4 [6080/8883 (68.35%)]\t\tLoss: 1.10785\n",
      "Training Progress: \tEpoch 4 [6400/8883 (71.94%)]\t\tLoss: 1.06250\n",
      "Training Progress: \tEpoch 4 [6720/8883 (75.54%)]\t\tLoss: 1.13591\n",
      "Training Progress: \tEpoch 4 [7040/8883 (79.14%)]\t\tLoss: 1.00499\n",
      "Training Progress: \tEpoch 4 [7360/8883 (82.73%)]\t\tLoss: 1.15068\n",
      "Training Progress: \tEpoch 4 [7680/8883 (86.33%)]\t\tLoss: 0.96870\n",
      "Training Progress: \tEpoch 4 [8000/8883 (89.93%)]\t\tLoss: 1.04665\n",
      "Training Progress: \tEpoch 4 [8320/8883 (93.53%)]\t\tLoss: 1.05191\n",
      "Training Progress: \tEpoch 4 [8640/8883 (97.12%)]\t\tLoss: 1.37728\n",
      "\tTrain loss: 0.03348, Accuracy: 4754/8883 (53.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1020/1692 (60.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 931/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/8883 (0.00%)]\t\tLoss: 1.05357\n",
      "Training Progress: \tEpoch 5 [320/8883 (3.60%)]\t\tLoss: 1.07102\n",
      "Training Progress: \tEpoch 5 [640/8883 (7.19%)]\t\tLoss: 1.02085\n",
      "Training Progress: \tEpoch 5 [960/8883 (10.79%)]\t\tLoss: 1.33584\n",
      "Training Progress: \tEpoch 5 [1280/8883 (14.39%)]\t\tLoss: 1.04826\n",
      "Training Progress: \tEpoch 5 [1600/8883 (17.99%)]\t\tLoss: 1.00970\n",
      "Training Progress: \tEpoch 5 [1920/8883 (21.58%)]\t\tLoss: 1.02223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 5 [2240/8883 (25.18%)]\t\tLoss: 1.18928\n",
      "Training Progress: \tEpoch 5 [2560/8883 (28.78%)]\t\tLoss: 1.16394\n",
      "Training Progress: \tEpoch 5 [2880/8883 (32.37%)]\t\tLoss: 1.10341\n",
      "Training Progress: \tEpoch 5 [3200/8883 (35.97%)]\t\tLoss: 1.33387\n",
      "Training Progress: \tEpoch 5 [3520/8883 (39.57%)]\t\tLoss: 1.18658\n",
      "Training Progress: \tEpoch 5 [3840/8883 (43.17%)]\t\tLoss: 1.29155\n",
      "Training Progress: \tEpoch 5 [4160/8883 (46.76%)]\t\tLoss: 1.19504\n",
      "Training Progress: \tEpoch 5 [4480/8883 (50.36%)]\t\tLoss: 1.02439\n",
      "Training Progress: \tEpoch 5 [4800/8883 (53.96%)]\t\tLoss: 1.16354\n",
      "Training Progress: \tEpoch 5 [5120/8883 (57.55%)]\t\tLoss: 1.31042\n",
      "Training Progress: \tEpoch 5 [5440/8883 (61.15%)]\t\tLoss: 1.25251\n",
      "Training Progress: \tEpoch 5 [5760/8883 (64.75%)]\t\tLoss: 1.22860\n",
      "Training Progress: \tEpoch 5 [6080/8883 (68.35%)]\t\tLoss: 1.05840\n",
      "Training Progress: \tEpoch 5 [6400/8883 (71.94%)]\t\tLoss: 1.06054\n",
      "Training Progress: \tEpoch 5 [6720/8883 (75.54%)]\t\tLoss: 1.03263\n",
      "Training Progress: \tEpoch 5 [7040/8883 (79.14%)]\t\tLoss: 1.09937\n",
      "Training Progress: \tEpoch 5 [7360/8883 (82.73%)]\t\tLoss: 1.08119\n",
      "Training Progress: \tEpoch 5 [7680/8883 (86.33%)]\t\tLoss: 0.90160\n",
      "Training Progress: \tEpoch 5 [8000/8883 (89.93%)]\t\tLoss: 1.16731\n",
      "Training Progress: \tEpoch 5 [8320/8883 (93.53%)]\t\tLoss: 1.07722\n",
      "Training Progress: \tEpoch 5 [8640/8883 (97.12%)]\t\tLoss: 1.19078\n",
      "\tTrain loss: 0.03285, Accuracy: 4744/8883 (53.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 993/1692 (58.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 923/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/8883 (0.00%)]\t\tLoss: 1.18072\n",
      "Training Progress: \tEpoch 6 [320/8883 (3.60%)]\t\tLoss: 1.06104\n",
      "Training Progress: \tEpoch 6 [640/8883 (7.19%)]\t\tLoss: 0.98414\n",
      "Training Progress: \tEpoch 6 [960/8883 (10.79%)]\t\tLoss: 1.18059\n",
      "Training Progress: \tEpoch 6 [1280/8883 (14.39%)]\t\tLoss: 1.20902\n",
      "Training Progress: \tEpoch 6 [1600/8883 (17.99%)]\t\tLoss: 1.12808\n",
      "Training Progress: \tEpoch 6 [1920/8883 (21.58%)]\t\tLoss: 1.12005\n",
      "Training Progress: \tEpoch 6 [2240/8883 (25.18%)]\t\tLoss: 1.14894\n",
      "Training Progress: \tEpoch 6 [2560/8883 (28.78%)]\t\tLoss: 0.95833\n",
      "Training Progress: \tEpoch 6 [2880/8883 (32.37%)]\t\tLoss: 1.05964\n",
      "Training Progress: \tEpoch 6 [3200/8883 (35.97%)]\t\tLoss: 1.32947\n",
      "Training Progress: \tEpoch 6 [3520/8883 (39.57%)]\t\tLoss: 1.30314\n",
      "Training Progress: \tEpoch 6 [3840/8883 (43.17%)]\t\tLoss: 1.01618\n",
      "Training Progress: \tEpoch 6 [4160/8883 (46.76%)]\t\tLoss: 1.26946\n",
      "Training Progress: \tEpoch 6 [4480/8883 (50.36%)]\t\tLoss: 0.93200\n",
      "Training Progress: \tEpoch 6 [4800/8883 (53.96%)]\t\tLoss: 1.31983\n",
      "Training Progress: \tEpoch 6 [5120/8883 (57.55%)]\t\tLoss: 1.25848\n",
      "Training Progress: \tEpoch 6 [5440/8883 (61.15%)]\t\tLoss: 1.26379\n",
      "Training Progress: \tEpoch 6 [5760/8883 (64.75%)]\t\tLoss: 1.07651\n",
      "Training Progress: \tEpoch 6 [6080/8883 (68.35%)]\t\tLoss: 1.04764\n",
      "Training Progress: \tEpoch 6 [6400/8883 (71.94%)]\t\tLoss: 0.98409\n",
      "Training Progress: \tEpoch 6 [6720/8883 (75.54%)]\t\tLoss: 1.15887\n",
      "Training Progress: \tEpoch 6 [7040/8883 (79.14%)]\t\tLoss: 1.19808\n",
      "Training Progress: \tEpoch 6 [7360/8883 (82.73%)]\t\tLoss: 1.11033\n",
      "Training Progress: \tEpoch 6 [7680/8883 (86.33%)]\t\tLoss: 0.83643\n",
      "Training Progress: \tEpoch 6 [8000/8883 (89.93%)]\t\tLoss: 1.06089\n",
      "Training Progress: \tEpoch 6 [8320/8883 (93.53%)]\t\tLoss: 0.93617\n",
      "Training Progress: \tEpoch 6 [8640/8883 (97.12%)]\t\tLoss: 1.12006\n",
      "\tTrain loss: 0.03114, Accuracy: 5082/8883 (57.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1082/1692 (63.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 979/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/8883 (0.00%)]\t\tLoss: 0.92691\n",
      "Training Progress: \tEpoch 7 [320/8883 (3.60%)]\t\tLoss: 0.97251\n",
      "Training Progress: \tEpoch 7 [640/8883 (7.19%)]\t\tLoss: 1.05227\n",
      "Training Progress: \tEpoch 7 [960/8883 (10.79%)]\t\tLoss: 1.19267\n",
      "Training Progress: \tEpoch 7 [1280/8883 (14.39%)]\t\tLoss: 1.07167\n",
      "Training Progress: \tEpoch 7 [1600/8883 (17.99%)]\t\tLoss: 1.05426\n",
      "Training Progress: \tEpoch 7 [1920/8883 (21.58%)]\t\tLoss: 0.87980\n",
      "Training Progress: \tEpoch 7 [2240/8883 (25.18%)]\t\tLoss: 1.02094\n",
      "Training Progress: \tEpoch 7 [2560/8883 (28.78%)]\t\tLoss: 1.11978\n",
      "Training Progress: \tEpoch 7 [2880/8883 (32.37%)]\t\tLoss: 1.14722\n",
      "Training Progress: \tEpoch 7 [3200/8883 (35.97%)]\t\tLoss: 1.27127\n",
      "Training Progress: \tEpoch 7 [3520/8883 (39.57%)]\t\tLoss: 1.28734\n",
      "Training Progress: \tEpoch 7 [3840/8883 (43.17%)]\t\tLoss: 1.35956\n",
      "Training Progress: \tEpoch 7 [4160/8883 (46.76%)]\t\tLoss: 1.16473\n",
      "Training Progress: \tEpoch 7 [4480/8883 (50.36%)]\t\tLoss: 0.98905\n",
      "Training Progress: \tEpoch 7 [4800/8883 (53.96%)]\t\tLoss: 0.96217\n",
      "Training Progress: \tEpoch 7 [5120/8883 (57.55%)]\t\tLoss: 1.19371\n",
      "Training Progress: \tEpoch 7 [5440/8883 (61.15%)]\t\tLoss: 1.07116\n",
      "Training Progress: \tEpoch 7 [5760/8883 (64.75%)]\t\tLoss: 0.96043\n",
      "Training Progress: \tEpoch 7 [6080/8883 (68.35%)]\t\tLoss: 1.01333\n",
      "Training Progress: \tEpoch 7 [6400/8883 (71.94%)]\t\tLoss: 1.06405\n",
      "Training Progress: \tEpoch 7 [6720/8883 (75.54%)]\t\tLoss: 1.06008\n",
      "Training Progress: \tEpoch 7 [7040/8883 (79.14%)]\t\tLoss: 1.22583\n",
      "Training Progress: \tEpoch 7 [7360/8883 (82.73%)]\t\tLoss: 1.09883\n",
      "Training Progress: \tEpoch 7 [7680/8883 (86.33%)]\t\tLoss: 0.95374\n",
      "Training Progress: \tEpoch 7 [8000/8883 (89.93%)]\t\tLoss: 1.07485\n",
      "Training Progress: \tEpoch 7 [8320/8883 (93.53%)]\t\tLoss: 1.11285\n",
      "Training Progress: \tEpoch 7 [8640/8883 (97.12%)]\t\tLoss: 1.02676\n",
      "\tTrain loss: 0.03059, Accuracy: 5042/8883 (56.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1093/1692 (64.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 964/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/8883 (0.00%)]\t\tLoss: 1.00187\n",
      "Training Progress: \tEpoch 8 [320/8883 (3.60%)]\t\tLoss: 0.98550\n",
      "Training Progress: \tEpoch 8 [640/8883 (7.19%)]\t\tLoss: 1.00607\n",
      "Training Progress: \tEpoch 8 [960/8883 (10.79%)]\t\tLoss: 1.17871\n",
      "Training Progress: \tEpoch 8 [1280/8883 (14.39%)]\t\tLoss: 0.98774\n",
      "Training Progress: \tEpoch 8 [1600/8883 (17.99%)]\t\tLoss: 0.88242\n",
      "Training Progress: \tEpoch 8 [1920/8883 (21.58%)]\t\tLoss: 1.00325\n",
      "Training Progress: \tEpoch 8 [2240/8883 (25.18%)]\t\tLoss: 0.82835\n",
      "Training Progress: \tEpoch 8 [2560/8883 (28.78%)]\t\tLoss: 1.05999\n",
      "Training Progress: \tEpoch 8 [2880/8883 (32.37%)]\t\tLoss: 1.03210\n",
      "Training Progress: \tEpoch 8 [3200/8883 (35.97%)]\t\tLoss: 1.05345\n",
      "Training Progress: \tEpoch 8 [3520/8883 (39.57%)]\t\tLoss: 1.32458\n",
      "Training Progress: \tEpoch 8 [3840/8883 (43.17%)]\t\tLoss: 1.02470\n",
      "Training Progress: \tEpoch 8 [4160/8883 (46.76%)]\t\tLoss: 1.12603\n",
      "Training Progress: \tEpoch 8 [4480/8883 (50.36%)]\t\tLoss: 0.84147\n",
      "Training Progress: \tEpoch 8 [4800/8883 (53.96%)]\t\tLoss: 1.08869\n",
      "Training Progress: \tEpoch 8 [5120/8883 (57.55%)]\t\tLoss: 1.17850\n",
      "Training Progress: \tEpoch 8 [5440/8883 (61.15%)]\t\tLoss: 1.26689\n",
      "Training Progress: \tEpoch 8 [5760/8883 (64.75%)]\t\tLoss: 1.09162\n",
      "Training Progress: \tEpoch 8 [6080/8883 (68.35%)]\t\tLoss: 0.96728\n",
      "Training Progress: \tEpoch 8 [6400/8883 (71.94%)]\t\tLoss: 1.07394\n",
      "Training Progress: \tEpoch 8 [6720/8883 (75.54%)]\t\tLoss: 1.15755\n",
      "Training Progress: \tEpoch 8 [7040/8883 (79.14%)]\t\tLoss: 0.99261\n",
      "Training Progress: \tEpoch 8 [7360/8883 (82.73%)]\t\tLoss: 1.02758\n",
      "Training Progress: \tEpoch 8 [7680/8883 (86.33%)]\t\tLoss: 0.99525\n",
      "Training Progress: \tEpoch 8 [8000/8883 (89.93%)]\t\tLoss: 1.12431\n",
      "Training Progress: \tEpoch 8 [8320/8883 (93.53%)]\t\tLoss: 1.13432\n",
      "Training Progress: \tEpoch 8 [8640/8883 (97.12%)]\t\tLoss: 1.25441\n",
      "\tTrain loss: 0.03006, Accuracy: 5161/8883 (58.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1094/1692 (64.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 987/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/8883 (0.00%)]\t\tLoss: 1.08398\n",
      "Training Progress: \tEpoch 9 [320/8883 (3.60%)]\t\tLoss: 0.85158\n",
      "Training Progress: \tEpoch 9 [640/8883 (7.19%)]\t\tLoss: 0.87309\n",
      "Training Progress: \tEpoch 9 [960/8883 (10.79%)]\t\tLoss: 1.08777\n",
      "Training Progress: \tEpoch 9 [1280/8883 (14.39%)]\t\tLoss: 1.09677\n",
      "Training Progress: \tEpoch 9 [1600/8883 (17.99%)]\t\tLoss: 0.84044\n",
      "Training Progress: \tEpoch 9 [1920/8883 (21.58%)]\t\tLoss: 1.01573\n",
      "Training Progress: \tEpoch 9 [2240/8883 (25.18%)]\t\tLoss: 1.05182\n",
      "Training Progress: \tEpoch 9 [2560/8883 (28.78%)]\t\tLoss: 1.06742\n",
      "Training Progress: \tEpoch 9 [2880/8883 (32.37%)]\t\tLoss: 0.98380\n",
      "Training Progress: \tEpoch 9 [3200/8883 (35.97%)]\t\tLoss: 0.97803\n",
      "Training Progress: \tEpoch 9 [3520/8883 (39.57%)]\t\tLoss: 1.28849\n",
      "Training Progress: \tEpoch 9 [3840/8883 (43.17%)]\t\tLoss: 1.15965\n",
      "Training Progress: \tEpoch 9 [4160/8883 (46.76%)]\t\tLoss: 1.09672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 9 [4480/8883 (50.36%)]\t\tLoss: 1.05040\n",
      "Training Progress: \tEpoch 9 [4800/8883 (53.96%)]\t\tLoss: 1.24077\n",
      "Training Progress: \tEpoch 9 [5120/8883 (57.55%)]\t\tLoss: 1.11539\n",
      "Training Progress: \tEpoch 9 [5440/8883 (61.15%)]\t\tLoss: 1.04403\n",
      "Training Progress: \tEpoch 9 [5760/8883 (64.75%)]\t\tLoss: 0.99431\n",
      "Training Progress: \tEpoch 9 [6080/8883 (68.35%)]\t\tLoss: 0.78399\n",
      "Training Progress: \tEpoch 9 [6400/8883 (71.94%)]\t\tLoss: 1.02036\n",
      "Training Progress: \tEpoch 9 [6720/8883 (75.54%)]\t\tLoss: 1.18321\n",
      "Training Progress: \tEpoch 9 [7040/8883 (79.14%)]\t\tLoss: 1.05007\n",
      "Training Progress: \tEpoch 9 [7360/8883 (82.73%)]\t\tLoss: 0.95355\n",
      "Training Progress: \tEpoch 9 [7680/8883 (86.33%)]\t\tLoss: 0.87961\n",
      "Training Progress: \tEpoch 9 [8000/8883 (89.93%)]\t\tLoss: 1.11242\n",
      "Training Progress: \tEpoch 9 [8320/8883 (93.53%)]\t\tLoss: 0.93612\n",
      "Training Progress: \tEpoch 9 [8640/8883 (97.12%)]\t\tLoss: 1.11541\n",
      "\tTrain loss: 0.02891, Accuracy: 5308/8883 (59.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1140/1692 (67.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 972/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/8883 (0.00%)]\t\tLoss: 1.16415\n",
      "Training Progress: \tEpoch 10 [320/8883 (3.60%)]\t\tLoss: 1.01888\n",
      "Training Progress: \tEpoch 10 [640/8883 (7.19%)]\t\tLoss: 0.84423\n",
      "Training Progress: \tEpoch 10 [960/8883 (10.79%)]\t\tLoss: 1.22730\n",
      "Training Progress: \tEpoch 10 [1280/8883 (14.39%)]\t\tLoss: 1.14357\n",
      "Training Progress: \tEpoch 10 [1600/8883 (17.99%)]\t\tLoss: 0.98608\n",
      "Training Progress: \tEpoch 10 [1920/8883 (21.58%)]\t\tLoss: 0.91530\n",
      "Training Progress: \tEpoch 10 [2240/8883 (25.18%)]\t\tLoss: 0.95585\n",
      "Training Progress: \tEpoch 10 [2560/8883 (28.78%)]\t\tLoss: 1.04581\n",
      "Training Progress: \tEpoch 10 [2880/8883 (32.37%)]\t\tLoss: 1.16481\n",
      "Training Progress: \tEpoch 10 [3200/8883 (35.97%)]\t\tLoss: 1.10790\n",
      "Training Progress: \tEpoch 10 [3520/8883 (39.57%)]\t\tLoss: 1.23086\n",
      "Training Progress: \tEpoch 10 [3840/8883 (43.17%)]\t\tLoss: 0.92700\n",
      "Training Progress: \tEpoch 10 [4160/8883 (46.76%)]\t\tLoss: 1.01668\n",
      "Training Progress: \tEpoch 10 [4480/8883 (50.36%)]\t\tLoss: 1.00384\n",
      "Training Progress: \tEpoch 10 [4800/8883 (53.96%)]\t\tLoss: 1.25714\n",
      "Training Progress: \tEpoch 10 [5120/8883 (57.55%)]\t\tLoss: 1.03256\n",
      "Training Progress: \tEpoch 10 [5440/8883 (61.15%)]\t\tLoss: 1.18901\n",
      "Training Progress: \tEpoch 10 [5760/8883 (64.75%)]\t\tLoss: 0.89479\n",
      "Training Progress: \tEpoch 10 [6080/8883 (68.35%)]\t\tLoss: 0.96414\n",
      "Training Progress: \tEpoch 10 [6400/8883 (71.94%)]\t\tLoss: 0.92337\n",
      "Training Progress: \tEpoch 10 [6720/8883 (75.54%)]\t\tLoss: 1.10740\n",
      "Training Progress: \tEpoch 10 [7040/8883 (79.14%)]\t\tLoss: 1.01660\n",
      "Training Progress: \tEpoch 10 [7360/8883 (82.73%)]\t\tLoss: 0.92573\n",
      "Training Progress: \tEpoch 10 [7680/8883 (86.33%)]\t\tLoss: 0.88806\n",
      "Training Progress: \tEpoch 10 [8000/8883 (89.93%)]\t\tLoss: 1.05318\n",
      "Training Progress: \tEpoch 10 [8320/8883 (93.53%)]\t\tLoss: 0.96554\n",
      "Training Progress: \tEpoch 10 [8640/8883 (97.12%)]\t\tLoss: 1.01947\n",
      "\tTrain loss: 0.02765, Accuracy: 5524/8883 (62.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1209/1692 (71.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/8883 (0.00%)]\t\tLoss: 1.16158\n",
      "Training Progress: \tEpoch 11 [320/8883 (3.60%)]\t\tLoss: 0.96112\n",
      "Training Progress: \tEpoch 11 [640/8883 (7.19%)]\t\tLoss: 0.92060\n",
      "Training Progress: \tEpoch 11 [960/8883 (10.79%)]\t\tLoss: 1.05057\n",
      "Training Progress: \tEpoch 11 [1280/8883 (14.39%)]\t\tLoss: 1.09649\n",
      "Training Progress: \tEpoch 11 [1600/8883 (17.99%)]\t\tLoss: 0.89756\n",
      "Training Progress: \tEpoch 11 [1920/8883 (21.58%)]\t\tLoss: 1.00493\n",
      "Training Progress: \tEpoch 11 [2240/8883 (25.18%)]\t\tLoss: 0.92991\n",
      "Training Progress: \tEpoch 11 [2560/8883 (28.78%)]\t\tLoss: 1.00247\n",
      "Training Progress: \tEpoch 11 [2880/8883 (32.37%)]\t\tLoss: 1.12900\n",
      "Training Progress: \tEpoch 11 [3200/8883 (35.97%)]\t\tLoss: 0.99628\n",
      "Training Progress: \tEpoch 11 [3520/8883 (39.57%)]\t\tLoss: 1.13015\n",
      "Training Progress: \tEpoch 11 [3840/8883 (43.17%)]\t\tLoss: 1.13576\n",
      "Training Progress: \tEpoch 11 [4160/8883 (46.76%)]\t\tLoss: 0.86511\n",
      "Training Progress: \tEpoch 11 [4480/8883 (50.36%)]\t\tLoss: 0.88278\n",
      "Training Progress: \tEpoch 11 [4800/8883 (53.96%)]\t\tLoss: 0.86930\n",
      "Training Progress: \tEpoch 11 [5120/8883 (57.55%)]\t\tLoss: 1.19086\n",
      "Training Progress: \tEpoch 11 [5440/8883 (61.15%)]\t\tLoss: 0.99328\n",
      "Training Progress: \tEpoch 11 [5760/8883 (64.75%)]\t\tLoss: 0.92272\n",
      "Training Progress: \tEpoch 11 [6080/8883 (68.35%)]\t\tLoss: 0.95693\n",
      "Training Progress: \tEpoch 11 [6400/8883 (71.94%)]\t\tLoss: 1.10535\n",
      "Training Progress: \tEpoch 11 [6720/8883 (75.54%)]\t\tLoss: 1.14313\n",
      "Training Progress: \tEpoch 11 [7040/8883 (79.14%)]\t\tLoss: 1.02132\n",
      "Training Progress: \tEpoch 11 [7360/8883 (82.73%)]\t\tLoss: 1.02733\n",
      "Training Progress: \tEpoch 11 [7680/8883 (86.33%)]\t\tLoss: 0.73188\n",
      "Training Progress: \tEpoch 11 [8000/8883 (89.93%)]\t\tLoss: 0.90213\n",
      "Training Progress: \tEpoch 11 [8320/8883 (93.53%)]\t\tLoss: 0.87669\n",
      "Training Progress: \tEpoch 11 [8640/8883 (97.12%)]\t\tLoss: 1.26562\n",
      "\tTrain loss: 0.02610, Accuracy: 5670/8883 (63.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/8883 (0.00%)]\t\tLoss: 0.99327\n",
      "Training Progress: \tEpoch 12 [320/8883 (3.60%)]\t\tLoss: 0.90423\n",
      "Training Progress: \tEpoch 12 [640/8883 (7.19%)]\t\tLoss: 0.85664\n",
      "Training Progress: \tEpoch 12 [960/8883 (10.79%)]\t\tLoss: 1.24090\n",
      "Training Progress: \tEpoch 12 [1280/8883 (14.39%)]\t\tLoss: 1.07197\n",
      "Training Progress: \tEpoch 12 [1600/8883 (17.99%)]\t\tLoss: 0.73200\n",
      "Training Progress: \tEpoch 12 [1920/8883 (21.58%)]\t\tLoss: 0.78279\n",
      "Training Progress: \tEpoch 12 [2240/8883 (25.18%)]\t\tLoss: 1.00808\n",
      "Training Progress: \tEpoch 12 [2560/8883 (28.78%)]\t\tLoss: 0.92882\n",
      "Training Progress: \tEpoch 12 [2880/8883 (32.37%)]\t\tLoss: 0.93157\n",
      "Training Progress: \tEpoch 12 [3200/8883 (35.97%)]\t\tLoss: 1.14209\n",
      "Training Progress: \tEpoch 12 [3520/8883 (39.57%)]\t\tLoss: 1.42745\n",
      "Training Progress: \tEpoch 12 [3840/8883 (43.17%)]\t\tLoss: 1.16478\n",
      "Training Progress: \tEpoch 12 [4160/8883 (46.76%)]\t\tLoss: 0.97033\n",
      "Training Progress: \tEpoch 12 [4480/8883 (50.36%)]\t\tLoss: 0.84109\n",
      "Training Progress: \tEpoch 12 [4800/8883 (53.96%)]\t\tLoss: 1.06892\n",
      "Training Progress: \tEpoch 12 [5120/8883 (57.55%)]\t\tLoss: 0.99556\n",
      "Training Progress: \tEpoch 12 [5440/8883 (61.15%)]\t\tLoss: 1.17830\n",
      "Training Progress: \tEpoch 12 [5760/8883 (64.75%)]\t\tLoss: 0.97726\n",
      "Training Progress: \tEpoch 12 [6080/8883 (68.35%)]\t\tLoss: 0.87123\n",
      "Training Progress: \tEpoch 12 [6400/8883 (71.94%)]\t\tLoss: 0.94984\n",
      "Training Progress: \tEpoch 12 [6720/8883 (75.54%)]\t\tLoss: 1.17382\n",
      "Training Progress: \tEpoch 12 [7040/8883 (79.14%)]\t\tLoss: 0.82132\n",
      "Training Progress: \tEpoch 12 [7360/8883 (82.73%)]\t\tLoss: 0.98457\n",
      "Training Progress: \tEpoch 12 [7680/8883 (86.33%)]\t\tLoss: 0.78142\n",
      "Training Progress: \tEpoch 12 [8000/8883 (89.93%)]\t\tLoss: 1.21593\n",
      "Training Progress: \tEpoch 12 [8320/8883 (93.53%)]\t\tLoss: 0.93252\n",
      "Training Progress: \tEpoch 12 [8640/8883 (97.12%)]\t\tLoss: 1.20497\n",
      "\tTrain loss: 0.02583, Accuracy: 5784/8883 (65.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1232/1692 (72.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1092/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/8883 (0.00%)]\t\tLoss: 0.85764\n",
      "Training Progress: \tEpoch 13 [320/8883 (3.60%)]\t\tLoss: 1.17890\n",
      "Training Progress: \tEpoch 13 [640/8883 (7.19%)]\t\tLoss: 0.91792\n",
      "Training Progress: \tEpoch 13 [960/8883 (10.79%)]\t\tLoss: 0.99516\n",
      "Training Progress: \tEpoch 13 [1280/8883 (14.39%)]\t\tLoss: 1.05879\n",
      "Training Progress: \tEpoch 13 [1600/8883 (17.99%)]\t\tLoss: 0.83442\n",
      "Training Progress: \tEpoch 13 [1920/8883 (21.58%)]\t\tLoss: 0.97338\n",
      "Training Progress: \tEpoch 13 [2240/8883 (25.18%)]\t\tLoss: 1.02906\n",
      "Training Progress: \tEpoch 13 [2560/8883 (28.78%)]\t\tLoss: 0.99940\n",
      "Training Progress: \tEpoch 13 [2880/8883 (32.37%)]\t\tLoss: 0.89020\n",
      "Training Progress: \tEpoch 13 [3200/8883 (35.97%)]\t\tLoss: 1.21205\n",
      "Training Progress: \tEpoch 13 [3520/8883 (39.57%)]\t\tLoss: 1.06851\n",
      "Training Progress: \tEpoch 13 [3840/8883 (43.17%)]\t\tLoss: 1.14903\n",
      "Training Progress: \tEpoch 13 [4160/8883 (46.76%)]\t\tLoss: 0.89405\n",
      "Training Progress: \tEpoch 13 [4480/8883 (50.36%)]\t\tLoss: 0.80696\n",
      "Training Progress: \tEpoch 13 [4800/8883 (53.96%)]\t\tLoss: 0.89771\n",
      "Training Progress: \tEpoch 13 [5120/8883 (57.55%)]\t\tLoss: 0.96193\n",
      "Training Progress: \tEpoch 13 [5440/8883 (61.15%)]\t\tLoss: 0.86142\n",
      "Training Progress: \tEpoch 13 [5760/8883 (64.75%)]\t\tLoss: 0.94501\n",
      "Training Progress: \tEpoch 13 [6080/8883 (68.35%)]\t\tLoss: 0.73824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 13 [6400/8883 (71.94%)]\t\tLoss: 0.90076\n",
      "Training Progress: \tEpoch 13 [6720/8883 (75.54%)]\t\tLoss: 1.12802\n",
      "Training Progress: \tEpoch 13 [7040/8883 (79.14%)]\t\tLoss: 1.05834\n",
      "Training Progress: \tEpoch 13 [7360/8883 (82.73%)]\t\tLoss: 0.96369\n",
      "Training Progress: \tEpoch 13 [7680/8883 (86.33%)]\t\tLoss: 0.60401\n",
      "Training Progress: \tEpoch 13 [8000/8883 (89.93%)]\t\tLoss: 1.02245\n",
      "Training Progress: \tEpoch 13 [8320/8883 (93.53%)]\t\tLoss: 0.87289\n",
      "Training Progress: \tEpoch 13 [8640/8883 (97.12%)]\t\tLoss: 1.02550\n",
      "\tTrain loss: 0.02398, Accuracy: 6037/8883 (67.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1307/1692 (77.00%)\n",
      "\tTest loss: 0.00050, Accuracy: 1156/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/8883 (0.00%)]\t\tLoss: 1.05018\n",
      "Training Progress: \tEpoch 14 [320/8883 (3.60%)]\t\tLoss: 1.06244\n",
      "Training Progress: \tEpoch 14 [640/8883 (7.19%)]\t\tLoss: 0.89847\n",
      "Training Progress: \tEpoch 14 [960/8883 (10.79%)]\t\tLoss: 1.09929\n",
      "Training Progress: \tEpoch 14 [1280/8883 (14.39%)]\t\tLoss: 1.05187\n",
      "Training Progress: \tEpoch 14 [1600/8883 (17.99%)]\t\tLoss: 0.81516\n",
      "Training Progress: \tEpoch 14 [1920/8883 (21.58%)]\t\tLoss: 0.89523\n",
      "Training Progress: \tEpoch 14 [2240/8883 (25.18%)]\t\tLoss: 0.79763\n",
      "Training Progress: \tEpoch 14 [2560/8883 (28.78%)]\t\tLoss: 0.94938\n",
      "Training Progress: \tEpoch 14 [2880/8883 (32.37%)]\t\tLoss: 0.98403\n",
      "Training Progress: \tEpoch 14 [3200/8883 (35.97%)]\t\tLoss: 1.10443\n",
      "Training Progress: \tEpoch 14 [3520/8883 (39.57%)]\t\tLoss: 1.23819\n",
      "Training Progress: \tEpoch 14 [3840/8883 (43.17%)]\t\tLoss: 1.05404\n",
      "Training Progress: \tEpoch 14 [4160/8883 (46.76%)]\t\tLoss: 1.13118\n",
      "Training Progress: \tEpoch 14 [4480/8883 (50.36%)]\t\tLoss: 0.78814\n",
      "Training Progress: \tEpoch 14 [4800/8883 (53.96%)]\t\tLoss: 0.83686\n",
      "Training Progress: \tEpoch 14 [5120/8883 (57.55%)]\t\tLoss: 0.91190\n",
      "Training Progress: \tEpoch 14 [5440/8883 (61.15%)]\t\tLoss: 0.77568\n",
      "Training Progress: \tEpoch 14 [5760/8883 (64.75%)]\t\tLoss: 0.87428\n",
      "Training Progress: \tEpoch 14 [6080/8883 (68.35%)]\t\tLoss: 0.67594\n",
      "Training Progress: \tEpoch 14 [6400/8883 (71.94%)]\t\tLoss: 0.92945\n",
      "Training Progress: \tEpoch 14 [6720/8883 (75.54%)]\t\tLoss: 1.16032\n",
      "Training Progress: \tEpoch 14 [7040/8883 (79.14%)]\t\tLoss: 0.98706\n",
      "Training Progress: \tEpoch 14 [7360/8883 (82.73%)]\t\tLoss: 0.88405\n",
      "Training Progress: \tEpoch 14 [7680/8883 (86.33%)]\t\tLoss: 0.72458\n",
      "Training Progress: \tEpoch 14 [8000/8883 (89.93%)]\t\tLoss: 0.92611\n",
      "Training Progress: \tEpoch 14 [8320/8883 (93.53%)]\t\tLoss: 0.85378\n",
      "Training Progress: \tEpoch 14 [8640/8883 (97.12%)]\t\tLoss: 0.96844\n",
      "\tTrain loss: 0.02398, Accuracy: 5969/8883 (67.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1296/1692 (76.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1101/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/8883 (0.00%)]\t\tLoss: 1.02617\n",
      "Training Progress: \tEpoch 15 [320/8883 (3.60%)]\t\tLoss: 0.99475\n",
      "Training Progress: \tEpoch 15 [640/8883 (7.19%)]\t\tLoss: 0.60219\n",
      "Training Progress: \tEpoch 15 [960/8883 (10.79%)]\t\tLoss: 0.96935\n",
      "Training Progress: \tEpoch 15 [1280/8883 (14.39%)]\t\tLoss: 0.90675\n",
      "Training Progress: \tEpoch 15 [1600/8883 (17.99%)]\t\tLoss: 0.67626\n",
      "Training Progress: \tEpoch 15 [1920/8883 (21.58%)]\t\tLoss: 0.88138\n",
      "Training Progress: \tEpoch 15 [2240/8883 (25.18%)]\t\tLoss: 0.76790\n",
      "Training Progress: \tEpoch 15 [2560/8883 (28.78%)]\t\tLoss: 1.04448\n",
      "Training Progress: \tEpoch 15 [2880/8883 (32.37%)]\t\tLoss: 0.90541\n",
      "Training Progress: \tEpoch 15 [3200/8883 (35.97%)]\t\tLoss: 0.88826\n",
      "Training Progress: \tEpoch 15 [3520/8883 (39.57%)]\t\tLoss: 1.19383\n",
      "Training Progress: \tEpoch 15 [3840/8883 (43.17%)]\t\tLoss: 0.96300\n",
      "Training Progress: \tEpoch 15 [4160/8883 (46.76%)]\t\tLoss: 0.94237\n",
      "Training Progress: \tEpoch 15 [4480/8883 (50.36%)]\t\tLoss: 0.83796\n",
      "Training Progress: \tEpoch 15 [4800/8883 (53.96%)]\t\tLoss: 0.96195\n",
      "Training Progress: \tEpoch 15 [5120/8883 (57.55%)]\t\tLoss: 1.14696\n",
      "Training Progress: \tEpoch 15 [5440/8883 (61.15%)]\t\tLoss: 1.14915\n",
      "Training Progress: \tEpoch 15 [5760/8883 (64.75%)]\t\tLoss: 0.78309\n",
      "Training Progress: \tEpoch 15 [6080/8883 (68.35%)]\t\tLoss: 0.69986\n",
      "Training Progress: \tEpoch 15 [6400/8883 (71.94%)]\t\tLoss: 0.79103\n",
      "Training Progress: \tEpoch 15 [6720/8883 (75.54%)]\t\tLoss: 0.99764\n",
      "Training Progress: \tEpoch 15 [7040/8883 (79.14%)]\t\tLoss: 0.85260\n",
      "Training Progress: \tEpoch 15 [7360/8883 (82.73%)]\t\tLoss: 0.91721\n",
      "Training Progress: \tEpoch 15 [7680/8883 (86.33%)]\t\tLoss: 0.74683\n",
      "Training Progress: \tEpoch 15 [8000/8883 (89.93%)]\t\tLoss: 0.91992\n",
      "Training Progress: \tEpoch 15 [8320/8883 (93.53%)]\t\tLoss: 1.08582\n",
      "Training Progress: \tEpoch 15 [8640/8883 (97.12%)]\t\tLoss: 1.17550\n",
      "\tTrain loss: 0.02323, Accuracy: 6100/8883 (68.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1308/1692 (77.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1107/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/8883 (0.00%)]\t\tLoss: 0.90383\n",
      "Training Progress: \tEpoch 16 [320/8883 (3.60%)]\t\tLoss: 1.04039\n",
      "Training Progress: \tEpoch 16 [640/8883 (7.19%)]\t\tLoss: 0.76314\n",
      "Training Progress: \tEpoch 16 [960/8883 (10.79%)]\t\tLoss: 1.02385\n",
      "Training Progress: \tEpoch 16 [1280/8883 (14.39%)]\t\tLoss: 0.81634\n",
      "Training Progress: \tEpoch 16 [1600/8883 (17.99%)]\t\tLoss: 0.70755\n",
      "Training Progress: \tEpoch 16 [1920/8883 (21.58%)]\t\tLoss: 0.78043\n",
      "Training Progress: \tEpoch 16 [2240/8883 (25.18%)]\t\tLoss: 0.82828\n",
      "Training Progress: \tEpoch 16 [2560/8883 (28.78%)]\t\tLoss: 0.87208\n",
      "Training Progress: \tEpoch 16 [2880/8883 (32.37%)]\t\tLoss: 0.96163\n",
      "Training Progress: \tEpoch 16 [3200/8883 (35.97%)]\t\tLoss: 0.92570\n",
      "Training Progress: \tEpoch 16 [3520/8883 (39.57%)]\t\tLoss: 1.20090\n",
      "Training Progress: \tEpoch 16 [3840/8883 (43.17%)]\t\tLoss: 1.07178\n",
      "Training Progress: \tEpoch 16 [4160/8883 (46.76%)]\t\tLoss: 1.01039\n",
      "Training Progress: \tEpoch 16 [4480/8883 (50.36%)]\t\tLoss: 0.71219\n",
      "Training Progress: \tEpoch 16 [4800/8883 (53.96%)]\t\tLoss: 0.76689\n",
      "Training Progress: \tEpoch 16 [5120/8883 (57.55%)]\t\tLoss: 1.02617\n",
      "Training Progress: \tEpoch 16 [5440/8883 (61.15%)]\t\tLoss: 0.98213\n",
      "Training Progress: \tEpoch 16 [5760/8883 (64.75%)]\t\tLoss: 0.82922\n",
      "Training Progress: \tEpoch 16 [6080/8883 (68.35%)]\t\tLoss: 0.73304\n",
      "Training Progress: \tEpoch 16 [6400/8883 (71.94%)]\t\tLoss: 0.81237\n",
      "Training Progress: \tEpoch 16 [6720/8883 (75.54%)]\t\tLoss: 0.92792\n",
      "Training Progress: \tEpoch 16 [7040/8883 (79.14%)]\t\tLoss: 0.82967\n",
      "Training Progress: \tEpoch 16 [7360/8883 (82.73%)]\t\tLoss: 0.85155\n",
      "Training Progress: \tEpoch 16 [7680/8883 (86.33%)]\t\tLoss: 0.78709\n",
      "Training Progress: \tEpoch 16 [8000/8883 (89.93%)]\t\tLoss: 0.80620\n",
      "Training Progress: \tEpoch 16 [8320/8883 (93.53%)]\t\tLoss: 1.01006\n",
      "Training Progress: \tEpoch 16 [8640/8883 (97.12%)]\t\tLoss: 1.05144\n",
      "\tTrain loss: 0.02261, Accuracy: 6154/8883 (69.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1322/1692 (78.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1144/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/8883 (0.00%)]\t\tLoss: 0.80769\n",
      "Training Progress: \tEpoch 17 [320/8883 (3.60%)]\t\tLoss: 0.97266\n",
      "Training Progress: \tEpoch 17 [640/8883 (7.19%)]\t\tLoss: 0.90235\n",
      "Training Progress: \tEpoch 17 [960/8883 (10.79%)]\t\tLoss: 1.03064\n",
      "Training Progress: \tEpoch 17 [1280/8883 (14.39%)]\t\tLoss: 0.90660\n",
      "Training Progress: \tEpoch 17 [1600/8883 (17.99%)]\t\tLoss: 0.97242\n",
      "Training Progress: \tEpoch 17 [1920/8883 (21.58%)]\t\tLoss: 0.83981\n",
      "Training Progress: \tEpoch 17 [2240/8883 (25.18%)]\t\tLoss: 0.77807\n",
      "Training Progress: \tEpoch 17 [2560/8883 (28.78%)]\t\tLoss: 0.92913\n",
      "Training Progress: \tEpoch 17 [2880/8883 (32.37%)]\t\tLoss: 0.95654\n",
      "Training Progress: \tEpoch 17 [3200/8883 (35.97%)]\t\tLoss: 0.97571\n",
      "Training Progress: \tEpoch 17 [3520/8883 (39.57%)]\t\tLoss: 1.24192\n",
      "Training Progress: \tEpoch 17 [3840/8883 (43.17%)]\t\tLoss: 0.98230\n",
      "Training Progress: \tEpoch 17 [4160/8883 (46.76%)]\t\tLoss: 0.90509\n",
      "Training Progress: \tEpoch 17 [4480/8883 (50.36%)]\t\tLoss: 0.74098\n",
      "Training Progress: \tEpoch 17 [4800/8883 (53.96%)]\t\tLoss: 0.86000\n",
      "Training Progress: \tEpoch 17 [5120/8883 (57.55%)]\t\tLoss: 1.10809\n",
      "Training Progress: \tEpoch 17 [5440/8883 (61.15%)]\t\tLoss: 0.98163\n",
      "Training Progress: \tEpoch 17 [5760/8883 (64.75%)]\t\tLoss: 0.79350\n",
      "Training Progress: \tEpoch 17 [6080/8883 (68.35%)]\t\tLoss: 0.72681\n",
      "Training Progress: \tEpoch 17 [6400/8883 (71.94%)]\t\tLoss: 0.85262\n",
      "Training Progress: \tEpoch 17 [6720/8883 (75.54%)]\t\tLoss: 1.10614\n",
      "Training Progress: \tEpoch 17 [7040/8883 (79.14%)]\t\tLoss: 0.93559\n",
      "Training Progress: \tEpoch 17 [7360/8883 (82.73%)]\t\tLoss: 0.98827\n",
      "Training Progress: \tEpoch 17 [7680/8883 (86.33%)]\t\tLoss: 0.74535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 17 [8000/8883 (89.93%)]\t\tLoss: 0.86771\n",
      "Training Progress: \tEpoch 17 [8320/8883 (93.53%)]\t\tLoss: 0.89622\n",
      "Training Progress: \tEpoch 17 [8640/8883 (97.12%)]\t\tLoss: 0.94149\n",
      "\tTrain loss: 0.02166, Accuracy: 6347/8883 (71.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1387/1692 (81.00%)\n",
      "\tTest loss: 0.00050, Accuracy: 1152/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/8883 (0.00%)]\t\tLoss: 1.14399\n",
      "Training Progress: \tEpoch 18 [320/8883 (3.60%)]\t\tLoss: 0.85151\n",
      "Training Progress: \tEpoch 18 [640/8883 (7.19%)]\t\tLoss: 0.75566\n",
      "Training Progress: \tEpoch 18 [960/8883 (10.79%)]\t\tLoss: 0.93781\n",
      "Training Progress: \tEpoch 18 [1280/8883 (14.39%)]\t\tLoss: 1.02634\n",
      "Training Progress: \tEpoch 18 [1600/8883 (17.99%)]\t\tLoss: 0.64440\n",
      "Training Progress: \tEpoch 18 [1920/8883 (21.58%)]\t\tLoss: 0.77451\n",
      "Training Progress: \tEpoch 18 [2240/8883 (25.18%)]\t\tLoss: 0.74677\n",
      "Training Progress: \tEpoch 18 [2560/8883 (28.78%)]\t\tLoss: 1.01443\n",
      "Training Progress: \tEpoch 18 [2880/8883 (32.37%)]\t\tLoss: 0.92104\n",
      "Training Progress: \tEpoch 18 [3200/8883 (35.97%)]\t\tLoss: 1.14423\n",
      "Training Progress: \tEpoch 18 [3520/8883 (39.57%)]\t\tLoss: 1.11159\n",
      "Training Progress: \tEpoch 18 [3840/8883 (43.17%)]\t\tLoss: 1.07873\n",
      "Training Progress: \tEpoch 18 [4160/8883 (46.76%)]\t\tLoss: 1.06462\n",
      "Training Progress: \tEpoch 18 [4480/8883 (50.36%)]\t\tLoss: 0.89914\n",
      "Training Progress: \tEpoch 18 [4800/8883 (53.96%)]\t\tLoss: 0.77904\n",
      "Training Progress: \tEpoch 18 [5120/8883 (57.55%)]\t\tLoss: 0.84983\n",
      "Training Progress: \tEpoch 18 [5440/8883 (61.15%)]\t\tLoss: 1.20429\n",
      "Training Progress: \tEpoch 18 [5760/8883 (64.75%)]\t\tLoss: 0.83337\n",
      "Training Progress: \tEpoch 18 [6080/8883 (68.35%)]\t\tLoss: 0.71056\n",
      "Training Progress: \tEpoch 18 [6400/8883 (71.94%)]\t\tLoss: 0.90109\n",
      "Training Progress: \tEpoch 18 [6720/8883 (75.54%)]\t\tLoss: 0.89658\n",
      "Training Progress: \tEpoch 18 [7040/8883 (79.14%)]\t\tLoss: 0.93951\n",
      "Training Progress: \tEpoch 18 [7360/8883 (82.73%)]\t\tLoss: 0.82041\n",
      "Training Progress: \tEpoch 18 [7680/8883 (86.33%)]\t\tLoss: 0.87960\n",
      "Training Progress: \tEpoch 18 [8000/8883 (89.93%)]\t\tLoss: 0.98792\n",
      "Training Progress: \tEpoch 18 [8320/8883 (93.53%)]\t\tLoss: 0.91134\n",
      "Training Progress: \tEpoch 18 [8640/8883 (97.12%)]\t\tLoss: 0.98455\n",
      "\tTrain loss: 0.02120, Accuracy: 6308/8883 (71.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1359/1692 (80.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1165/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/8883 (0.00%)]\t\tLoss: 0.70798\n",
      "Training Progress: \tEpoch 19 [320/8883 (3.60%)]\t\tLoss: 0.91243\n",
      "Training Progress: \tEpoch 19 [640/8883 (7.19%)]\t\tLoss: 0.81178\n",
      "Training Progress: \tEpoch 19 [960/8883 (10.79%)]\t\tLoss: 0.97598\n",
      "Training Progress: \tEpoch 19 [1280/8883 (14.39%)]\t\tLoss: 0.92164\n",
      "Training Progress: \tEpoch 19 [1600/8883 (17.99%)]\t\tLoss: 1.00500\n",
      "Training Progress: \tEpoch 19 [1920/8883 (21.58%)]\t\tLoss: 0.74311\n",
      "Training Progress: \tEpoch 19 [2240/8883 (25.18%)]\t\tLoss: 0.84030\n",
      "Training Progress: \tEpoch 19 [2560/8883 (28.78%)]\t\tLoss: 0.86573\n",
      "Training Progress: \tEpoch 19 [2880/8883 (32.37%)]\t\tLoss: 0.83136\n",
      "Training Progress: \tEpoch 19 [3200/8883 (35.97%)]\t\tLoss: 0.99590\n",
      "Training Progress: \tEpoch 19 [3520/8883 (39.57%)]\t\tLoss: 1.00784\n",
      "Training Progress: \tEpoch 19 [3840/8883 (43.17%)]\t\tLoss: 0.88930\n",
      "Training Progress: \tEpoch 19 [4160/8883 (46.76%)]\t\tLoss: 1.13334\n",
      "Training Progress: \tEpoch 19 [4480/8883 (50.36%)]\t\tLoss: 0.61679\n",
      "Training Progress: \tEpoch 19 [4800/8883 (53.96%)]\t\tLoss: 0.90519\n",
      "Training Progress: \tEpoch 19 [5120/8883 (57.55%)]\t\tLoss: 0.92010\n",
      "Training Progress: \tEpoch 19 [5440/8883 (61.15%)]\t\tLoss: 0.99042\n",
      "Training Progress: \tEpoch 19 [5760/8883 (64.75%)]\t\tLoss: 0.81785\n",
      "Training Progress: \tEpoch 19 [6080/8883 (68.35%)]\t\tLoss: 0.76408\n",
      "Training Progress: \tEpoch 19 [6400/8883 (71.94%)]\t\tLoss: 0.86108\n",
      "Training Progress: \tEpoch 19 [6720/8883 (75.54%)]\t\tLoss: 0.94593\n",
      "Training Progress: \tEpoch 19 [7040/8883 (79.14%)]\t\tLoss: 0.79056\n",
      "Training Progress: \tEpoch 19 [7360/8883 (82.73%)]\t\tLoss: 0.89459\n",
      "Training Progress: \tEpoch 19 [7680/8883 (86.33%)]\t\tLoss: 0.67951\n",
      "Training Progress: \tEpoch 19 [8000/8883 (89.93%)]\t\tLoss: 0.90778\n",
      "Training Progress: \tEpoch 19 [8320/8883 (93.53%)]\t\tLoss: 0.86646\n",
      "Training Progress: \tEpoch 19 [8640/8883 (97.12%)]\t\tLoss: 0.97597\n",
      "\tTrain loss: 0.02019, Accuracy: 6549/8883 (73.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1422/1692 (84.00%)\n",
      "\tTest loss: 0.00050, Accuracy: 1166/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/8883 (0.00%)]\t\tLoss: 1.03093\n",
      "Training Progress: \tEpoch 20 [320/8883 (3.60%)]\t\tLoss: 0.87969\n",
      "Training Progress: \tEpoch 20 [640/8883 (7.19%)]\t\tLoss: 0.67986\n",
      "Training Progress: \tEpoch 20 [960/8883 (10.79%)]\t\tLoss: 1.03905\n",
      "Training Progress: \tEpoch 20 [1280/8883 (14.39%)]\t\tLoss: 0.87425\n",
      "Training Progress: \tEpoch 20 [1600/8883 (17.99%)]\t\tLoss: 0.85156\n",
      "Training Progress: \tEpoch 20 [1920/8883 (21.58%)]\t\tLoss: 0.76866\n",
      "Training Progress: \tEpoch 20 [2240/8883 (25.18%)]\t\tLoss: 0.71761\n",
      "Training Progress: \tEpoch 20 [2560/8883 (28.78%)]\t\tLoss: 0.99135\n",
      "Training Progress: \tEpoch 20 [2880/8883 (32.37%)]\t\tLoss: 0.96914\n",
      "Training Progress: \tEpoch 20 [3200/8883 (35.97%)]\t\tLoss: 0.94963\n",
      "Training Progress: \tEpoch 20 [3520/8883 (39.57%)]\t\tLoss: 1.09346\n",
      "Training Progress: \tEpoch 20 [3840/8883 (43.17%)]\t\tLoss: 1.09157\n",
      "Training Progress: \tEpoch 20 [4160/8883 (46.76%)]\t\tLoss: 0.62730\n",
      "Training Progress: \tEpoch 20 [4480/8883 (50.36%)]\t\tLoss: 0.65009\n",
      "Training Progress: \tEpoch 20 [4800/8883 (53.96%)]\t\tLoss: 0.76388\n",
      "Training Progress: \tEpoch 20 [5120/8883 (57.55%)]\t\tLoss: 0.98086\n",
      "Training Progress: \tEpoch 20 [5440/8883 (61.15%)]\t\tLoss: 1.04313\n",
      "Training Progress: \tEpoch 20 [5760/8883 (64.75%)]\t\tLoss: 0.89520\n",
      "Training Progress: \tEpoch 20 [6080/8883 (68.35%)]\t\tLoss: 0.70728\n",
      "Training Progress: \tEpoch 20 [6400/8883 (71.94%)]\t\tLoss: 0.90448\n",
      "Training Progress: \tEpoch 20 [6720/8883 (75.54%)]\t\tLoss: 0.95628\n",
      "Training Progress: \tEpoch 20 [7040/8883 (79.14%)]\t\tLoss: 0.97730\n",
      "Training Progress: \tEpoch 20 [7360/8883 (82.73%)]\t\tLoss: 0.93654\n",
      "Training Progress: \tEpoch 20 [7680/8883 (86.33%)]\t\tLoss: 0.68184\n",
      "Training Progress: \tEpoch 20 [8000/8883 (89.93%)]\t\tLoss: 1.15410\n",
      "Training Progress: \tEpoch 20 [8320/8883 (93.53%)]\t\tLoss: 0.72477\n",
      "Training Progress: \tEpoch 20 [8640/8883 (97.12%)]\t\tLoss: 0.92478\n",
      "\tTrain loss: 0.02054, Accuracy: 6424/8883 (72.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1391/1692 (82.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1111/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/8883 (0.00%)]\t\tLoss: 0.92609\n",
      "Training Progress: \tEpoch 21 [320/8883 (3.60%)]\t\tLoss: 0.77240\n",
      "Training Progress: \tEpoch 21 [640/8883 (7.19%)]\t\tLoss: 0.83420\n",
      "Training Progress: \tEpoch 21 [960/8883 (10.79%)]\t\tLoss: 1.05915\n",
      "Training Progress: \tEpoch 21 [1280/8883 (14.39%)]\t\tLoss: 0.90602\n",
      "Training Progress: \tEpoch 21 [1600/8883 (17.99%)]\t\tLoss: 0.74378\n",
      "Training Progress: \tEpoch 21 [1920/8883 (21.58%)]\t\tLoss: 0.52758\n",
      "Training Progress: \tEpoch 21 [2240/8883 (25.18%)]\t\tLoss: 0.61734\n",
      "Training Progress: \tEpoch 21 [2560/8883 (28.78%)]\t\tLoss: 0.93572\n",
      "Training Progress: \tEpoch 21 [2880/8883 (32.37%)]\t\tLoss: 0.86481\n",
      "Training Progress: \tEpoch 21 [3200/8883 (35.97%)]\t\tLoss: 1.09590\n",
      "Training Progress: \tEpoch 21 [3520/8883 (39.57%)]\t\tLoss: 0.94120\n",
      "Training Progress: \tEpoch 21 [3840/8883 (43.17%)]\t\tLoss: 0.89240\n",
      "Training Progress: \tEpoch 21 [4160/8883 (46.76%)]\t\tLoss: 1.02596\n",
      "Training Progress: \tEpoch 21 [4480/8883 (50.36%)]\t\tLoss: 0.80684\n",
      "Training Progress: \tEpoch 21 [4800/8883 (53.96%)]\t\tLoss: 0.94240\n",
      "Training Progress: \tEpoch 21 [5120/8883 (57.55%)]\t\tLoss: 0.81716\n",
      "Training Progress: \tEpoch 21 [5440/8883 (61.15%)]\t\tLoss: 0.89980\n",
      "Training Progress: \tEpoch 21 [5760/8883 (64.75%)]\t\tLoss: 0.85431\n",
      "Training Progress: \tEpoch 21 [6080/8883 (68.35%)]\t\tLoss: 0.54816\n",
      "Training Progress: \tEpoch 21 [6400/8883 (71.94%)]\t\tLoss: 0.78509\n",
      "Training Progress: \tEpoch 21 [6720/8883 (75.54%)]\t\tLoss: 0.89508\n",
      "Training Progress: \tEpoch 21 [7040/8883 (79.14%)]\t\tLoss: 0.86391\n",
      "Training Progress: \tEpoch 21 [7360/8883 (82.73%)]\t\tLoss: 0.87872\n",
      "Training Progress: \tEpoch 21 [7680/8883 (86.33%)]\t\tLoss: 0.68170\n",
      "Training Progress: \tEpoch 21 [8000/8883 (89.93%)]\t\tLoss: 0.90197\n",
      "Training Progress: \tEpoch 21 [8320/8883 (93.53%)]\t\tLoss: 0.65994\n",
      "Training Progress: \tEpoch 21 [8640/8883 (97.12%)]\t\tLoss: 1.02651\n",
      "\tTrain loss: 0.01984, Accuracy: 6541/8883 (73.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1400/1692 (82.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/8883 (0.00%)]\t\tLoss: 0.88232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 22 [320/8883 (3.60%)]\t\tLoss: 0.79987\n",
      "Training Progress: \tEpoch 22 [640/8883 (7.19%)]\t\tLoss: 0.69944\n",
      "Training Progress: \tEpoch 22 [960/8883 (10.79%)]\t\tLoss: 0.85637\n",
      "Training Progress: \tEpoch 22 [1280/8883 (14.39%)]\t\tLoss: 0.89874\n",
      "Training Progress: \tEpoch 22 [1600/8883 (17.99%)]\t\tLoss: 0.66763\n",
      "Training Progress: \tEpoch 22 [1920/8883 (21.58%)]\t\tLoss: 0.73059\n",
      "Training Progress: \tEpoch 22 [2240/8883 (25.18%)]\t\tLoss: 0.71039\n",
      "Training Progress: \tEpoch 22 [2560/8883 (28.78%)]\t\tLoss: 0.73954\n",
      "Training Progress: \tEpoch 22 [2880/8883 (32.37%)]\t\tLoss: 0.95304\n",
      "Training Progress: \tEpoch 22 [3200/8883 (35.97%)]\t\tLoss: 0.95588\n",
      "Training Progress: \tEpoch 22 [3520/8883 (39.57%)]\t\tLoss: 1.07958\n",
      "Training Progress: \tEpoch 22 [3840/8883 (43.17%)]\t\tLoss: 1.01787\n",
      "Training Progress: \tEpoch 22 [4160/8883 (46.76%)]\t\tLoss: 0.97541\n",
      "Training Progress: \tEpoch 22 [4480/8883 (50.36%)]\t\tLoss: 0.77154\n",
      "Training Progress: \tEpoch 22 [4800/8883 (53.96%)]\t\tLoss: 0.74000\n",
      "Training Progress: \tEpoch 22 [5120/8883 (57.55%)]\t\tLoss: 0.88312\n",
      "Training Progress: \tEpoch 22 [5440/8883 (61.15%)]\t\tLoss: 0.85193\n",
      "Training Progress: \tEpoch 22 [5760/8883 (64.75%)]\t\tLoss: 0.87728\n",
      "Training Progress: \tEpoch 22 [6080/8883 (68.35%)]\t\tLoss: 0.78448\n",
      "Training Progress: \tEpoch 22 [6400/8883 (71.94%)]\t\tLoss: 0.71501\n",
      "Training Progress: \tEpoch 22 [6720/8883 (75.54%)]\t\tLoss: 0.92847\n",
      "Training Progress: \tEpoch 22 [7040/8883 (79.14%)]\t\tLoss: 0.87938\n",
      "Training Progress: \tEpoch 22 [7360/8883 (82.73%)]\t\tLoss: 0.80102\n",
      "Training Progress: \tEpoch 22 [7680/8883 (86.33%)]\t\tLoss: 0.67918\n",
      "Training Progress: \tEpoch 22 [8000/8883 (89.93%)]\t\tLoss: 0.79656\n",
      "Training Progress: \tEpoch 22 [8320/8883 (93.53%)]\t\tLoss: 0.85549\n",
      "Training Progress: \tEpoch 22 [8640/8883 (97.12%)]\t\tLoss: 0.89306\n",
      "\tTrain loss: 0.01877, Accuracy: 6701/8883 (75.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1451/1692 (85.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1145/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/8883 (0.00%)]\t\tLoss: 0.90635\n",
      "Training Progress: \tEpoch 23 [320/8883 (3.60%)]\t\tLoss: 0.65726\n",
      "Training Progress: \tEpoch 23 [640/8883 (7.19%)]\t\tLoss: 0.62699\n",
      "Training Progress: \tEpoch 23 [960/8883 (10.79%)]\t\tLoss: 0.88810\n",
      "Training Progress: \tEpoch 23 [1280/8883 (14.39%)]\t\tLoss: 1.02637\n",
      "Training Progress: \tEpoch 23 [1600/8883 (17.99%)]\t\tLoss: 0.57090\n",
      "Training Progress: \tEpoch 23 [1920/8883 (21.58%)]\t\tLoss: 0.67801\n",
      "Training Progress: \tEpoch 23 [2240/8883 (25.18%)]\t\tLoss: 0.63415\n",
      "Training Progress: \tEpoch 23 [2560/8883 (28.78%)]\t\tLoss: 0.76142\n",
      "Training Progress: \tEpoch 23 [2880/8883 (32.37%)]\t\tLoss: 0.84430\n",
      "Training Progress: \tEpoch 23 [3200/8883 (35.97%)]\t\tLoss: 0.80728\n",
      "Training Progress: \tEpoch 23 [3520/8883 (39.57%)]\t\tLoss: 1.18072\n",
      "Training Progress: \tEpoch 23 [3840/8883 (43.17%)]\t\tLoss: 1.02621\n",
      "Training Progress: \tEpoch 23 [4160/8883 (46.76%)]\t\tLoss: 0.81476\n",
      "Training Progress: \tEpoch 23 [4480/8883 (50.36%)]\t\tLoss: 0.65354\n",
      "Training Progress: \tEpoch 23 [4800/8883 (53.96%)]\t\tLoss: 0.85539\n",
      "Training Progress: \tEpoch 23 [5120/8883 (57.55%)]\t\tLoss: 0.93713\n",
      "Training Progress: \tEpoch 23 [5440/8883 (61.15%)]\t\tLoss: 0.87090\n",
      "Training Progress: \tEpoch 23 [5760/8883 (64.75%)]\t\tLoss: 0.73033\n",
      "Training Progress: \tEpoch 23 [6080/8883 (68.35%)]\t\tLoss: 0.67940\n",
      "Training Progress: \tEpoch 23 [6400/8883 (71.94%)]\t\tLoss: 0.75642\n",
      "Training Progress: \tEpoch 23 [6720/8883 (75.54%)]\t\tLoss: 0.87060\n",
      "Training Progress: \tEpoch 23 [7040/8883 (79.14%)]\t\tLoss: 1.04114\n",
      "Training Progress: \tEpoch 23 [7360/8883 (82.73%)]\t\tLoss: 0.66623\n",
      "Training Progress: \tEpoch 23 [7680/8883 (86.33%)]\t\tLoss: 0.65832\n",
      "Training Progress: \tEpoch 23 [8000/8883 (89.93%)]\t\tLoss: 1.17474\n",
      "Training Progress: \tEpoch 23 [8320/8883 (93.53%)]\t\tLoss: 0.95018\n",
      "Training Progress: \tEpoch 23 [8640/8883 (97.12%)]\t\tLoss: 1.10877\n",
      "\tTrain loss: 0.01843, Accuracy: 6761/8883 (76.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1167/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/8883 (0.00%)]\t\tLoss: 0.77735\n",
      "Training Progress: \tEpoch 24 [320/8883 (3.60%)]\t\tLoss: 0.88608\n",
      "Training Progress: \tEpoch 24 [640/8883 (7.19%)]\t\tLoss: 0.77619\n",
      "Training Progress: \tEpoch 24 [960/8883 (10.79%)]\t\tLoss: 0.99460\n",
      "Training Progress: \tEpoch 24 [1280/8883 (14.39%)]\t\tLoss: 0.82101\n",
      "Training Progress: \tEpoch 24 [1600/8883 (17.99%)]\t\tLoss: 0.63708\n",
      "Training Progress: \tEpoch 24 [1920/8883 (21.58%)]\t\tLoss: 0.66450\n",
      "Training Progress: \tEpoch 24 [2240/8883 (25.18%)]\t\tLoss: 0.69511\n",
      "Training Progress: \tEpoch 24 [2560/8883 (28.78%)]\t\tLoss: 1.07112\n",
      "Training Progress: \tEpoch 24 [2880/8883 (32.37%)]\t\tLoss: 0.73975\n",
      "Training Progress: \tEpoch 24 [3200/8883 (35.97%)]\t\tLoss: 0.73151\n",
      "Training Progress: \tEpoch 24 [3520/8883 (39.57%)]\t\tLoss: 1.14433\n",
      "Training Progress: \tEpoch 24 [3840/8883 (43.17%)]\t\tLoss: 1.11886\n",
      "Training Progress: \tEpoch 24 [4160/8883 (46.76%)]\t\tLoss: 0.73318\n",
      "Training Progress: \tEpoch 24 [4480/8883 (50.36%)]\t\tLoss: 0.67044\n",
      "Training Progress: \tEpoch 24 [4800/8883 (53.96%)]\t\tLoss: 0.72148\n",
      "Training Progress: \tEpoch 24 [5120/8883 (57.55%)]\t\tLoss: 0.97077\n",
      "Training Progress: \tEpoch 24 [5440/8883 (61.15%)]\t\tLoss: 0.84077\n",
      "Training Progress: \tEpoch 24 [5760/8883 (64.75%)]\t\tLoss: 0.72146\n",
      "Training Progress: \tEpoch 24 [6080/8883 (68.35%)]\t\tLoss: 0.60437\n",
      "Training Progress: \tEpoch 24 [6400/8883 (71.94%)]\t\tLoss: 0.66986\n",
      "Training Progress: \tEpoch 24 [6720/8883 (75.54%)]\t\tLoss: 0.83828\n",
      "Training Progress: \tEpoch 24 [7040/8883 (79.14%)]\t\tLoss: 0.88070\n",
      "Training Progress: \tEpoch 24 [7360/8883 (82.73%)]\t\tLoss: 0.70294\n",
      "Training Progress: \tEpoch 24 [7680/8883 (86.33%)]\t\tLoss: 0.95663\n",
      "Training Progress: \tEpoch 24 [8000/8883 (89.93%)]\t\tLoss: 0.83361\n",
      "Training Progress: \tEpoch 24 [8320/8883 (93.53%)]\t\tLoss: 0.71756\n",
      "Training Progress: \tEpoch 24 [8640/8883 (97.12%)]\t\tLoss: 0.93144\n",
      "\tTrain loss: 0.01804, Accuracy: 6778/8883 (76.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1463/1692 (86.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1150/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/8883 (0.00%)]\t\tLoss: 0.82333\n",
      "Training Progress: \tEpoch 25 [320/8883 (3.60%)]\t\tLoss: 0.84976\n",
      "Training Progress: \tEpoch 25 [640/8883 (7.19%)]\t\tLoss: 0.67920\n",
      "Training Progress: \tEpoch 25 [960/8883 (10.79%)]\t\tLoss: 0.86347\n",
      "Training Progress: \tEpoch 25 [1280/8883 (14.39%)]\t\tLoss: 0.81448\n",
      "Training Progress: \tEpoch 25 [1600/8883 (17.99%)]\t\tLoss: 0.82791\n",
      "Training Progress: \tEpoch 25 [1920/8883 (21.58%)]\t\tLoss: 0.61610\n",
      "Training Progress: \tEpoch 25 [2240/8883 (25.18%)]\t\tLoss: 0.77533\n",
      "Training Progress: \tEpoch 25 [2560/8883 (28.78%)]\t\tLoss: 0.85951\n",
      "Training Progress: \tEpoch 25 [2880/8883 (32.37%)]\t\tLoss: 0.84332\n",
      "Training Progress: \tEpoch 25 [3200/8883 (35.97%)]\t\tLoss: 0.96930\n",
      "Training Progress: \tEpoch 25 [3520/8883 (39.57%)]\t\tLoss: 0.81051\n",
      "Training Progress: \tEpoch 25 [3840/8883 (43.17%)]\t\tLoss: 0.95425\n",
      "Training Progress: \tEpoch 25 [4160/8883 (46.76%)]\t\tLoss: 0.92679\n",
      "Training Progress: \tEpoch 25 [4480/8883 (50.36%)]\t\tLoss: 0.67980\n",
      "Training Progress: \tEpoch 25 [4800/8883 (53.96%)]\t\tLoss: 0.92731\n",
      "Training Progress: \tEpoch 25 [5120/8883 (57.55%)]\t\tLoss: 0.96431\n",
      "Training Progress: \tEpoch 25 [5440/8883 (61.15%)]\t\tLoss: 0.75417\n",
      "Training Progress: \tEpoch 25 [5760/8883 (64.75%)]\t\tLoss: 0.86392\n",
      "Training Progress: \tEpoch 25 [6080/8883 (68.35%)]\t\tLoss: 0.62980\n",
      "Training Progress: \tEpoch 25 [6400/8883 (71.94%)]\t\tLoss: 0.85814\n",
      "Training Progress: \tEpoch 25 [6720/8883 (75.54%)]\t\tLoss: 0.73386\n",
      "Training Progress: \tEpoch 25 [7040/8883 (79.14%)]\t\tLoss: 0.83391\n",
      "Training Progress: \tEpoch 25 [7360/8883 (82.73%)]\t\tLoss: 0.89915\n",
      "Training Progress: \tEpoch 25 [7680/8883 (86.33%)]\t\tLoss: 0.52893\n",
      "Training Progress: \tEpoch 25 [8000/8883 (89.93%)]\t\tLoss: 0.94081\n",
      "Training Progress: \tEpoch 25 [8320/8883 (93.53%)]\t\tLoss: 0.84770\n",
      "Training Progress: \tEpoch 25 [8640/8883 (97.12%)]\t\tLoss: 0.91525\n",
      "\tTrain loss: 0.01766, Accuracy: 6808/8883 (76.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1468/1692 (86.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1151/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/8883 (0.00%)]\t\tLoss: 0.62194\n",
      "Training Progress: \tEpoch 26 [320/8883 (3.60%)]\t\tLoss: 0.79992\n",
      "Training Progress: \tEpoch 26 [640/8883 (7.19%)]\t\tLoss: 0.71379\n",
      "Training Progress: \tEpoch 26 [960/8883 (10.79%)]\t\tLoss: 0.90023\n",
      "Training Progress: \tEpoch 26 [1280/8883 (14.39%)]\t\tLoss: 0.62233\n",
      "Training Progress: \tEpoch 26 [1600/8883 (17.99%)]\t\tLoss: 0.56480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 26 [1920/8883 (21.58%)]\t\tLoss: 0.57113\n",
      "Training Progress: \tEpoch 26 [2240/8883 (25.18%)]\t\tLoss: 0.62379\n",
      "Training Progress: \tEpoch 26 [2560/8883 (28.78%)]\t\tLoss: 0.83838\n",
      "Training Progress: \tEpoch 26 [2880/8883 (32.37%)]\t\tLoss: 0.90652\n",
      "Training Progress: \tEpoch 26 [3200/8883 (35.97%)]\t\tLoss: 0.70324\n",
      "Training Progress: \tEpoch 26 [3520/8883 (39.57%)]\t\tLoss: 1.00560\n",
      "Training Progress: \tEpoch 26 [3840/8883 (43.17%)]\t\tLoss: 0.97121\n",
      "Training Progress: \tEpoch 26 [4160/8883 (46.76%)]\t\tLoss: 0.65909\n",
      "Training Progress: \tEpoch 26 [4480/8883 (50.36%)]\t\tLoss: 0.63335\n",
      "Training Progress: \tEpoch 26 [4800/8883 (53.96%)]\t\tLoss: 0.70535\n",
      "Training Progress: \tEpoch 26 [5120/8883 (57.55%)]\t\tLoss: 0.71117\n",
      "Training Progress: \tEpoch 26 [5440/8883 (61.15%)]\t\tLoss: 0.76331\n",
      "Training Progress: \tEpoch 26 [5760/8883 (64.75%)]\t\tLoss: 0.69438\n",
      "Training Progress: \tEpoch 26 [6080/8883 (68.35%)]\t\tLoss: 0.86239\n",
      "Training Progress: \tEpoch 26 [6400/8883 (71.94%)]\t\tLoss: 0.82817\n",
      "Training Progress: \tEpoch 26 [6720/8883 (75.54%)]\t\tLoss: 0.78857\n",
      "Training Progress: \tEpoch 26 [7040/8883 (79.14%)]\t\tLoss: 0.67836\n",
      "Training Progress: \tEpoch 26 [7360/8883 (82.73%)]\t\tLoss: 0.99126\n",
      "Training Progress: \tEpoch 26 [7680/8883 (86.33%)]\t\tLoss: 0.70273\n",
      "Training Progress: \tEpoch 26 [8000/8883 (89.93%)]\t\tLoss: 0.61546\n",
      "Training Progress: \tEpoch 26 [8320/8883 (93.53%)]\t\tLoss: 0.90493\n",
      "Training Progress: \tEpoch 26 [8640/8883 (97.12%)]\t\tLoss: 0.88058\n",
      "\tTrain loss: 0.01780, Accuracy: 6702/8883 (75.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/8883 (0.00%)]\t\tLoss: 0.70056\n",
      "Training Progress: \tEpoch 27 [320/8883 (3.60%)]\t\tLoss: 0.73112\n",
      "Training Progress: \tEpoch 27 [640/8883 (7.19%)]\t\tLoss: 0.80635\n",
      "Training Progress: \tEpoch 27 [960/8883 (10.79%)]\t\tLoss: 1.06519\n",
      "Training Progress: \tEpoch 27 [1280/8883 (14.39%)]\t\tLoss: 0.76767\n",
      "Training Progress: \tEpoch 27 [1600/8883 (17.99%)]\t\tLoss: 0.57631\n",
      "Training Progress: \tEpoch 27 [1920/8883 (21.58%)]\t\tLoss: 0.67097\n",
      "Training Progress: \tEpoch 27 [2240/8883 (25.18%)]\t\tLoss: 0.65671\n",
      "Training Progress: \tEpoch 27 [2560/8883 (28.78%)]\t\tLoss: 0.76925\n",
      "Training Progress: \tEpoch 27 [2880/8883 (32.37%)]\t\tLoss: 0.78445\n",
      "Training Progress: \tEpoch 27 [3200/8883 (35.97%)]\t\tLoss: 0.99856\n",
      "Training Progress: \tEpoch 27 [3520/8883 (39.57%)]\t\tLoss: 1.11644\n",
      "Training Progress: \tEpoch 27 [3840/8883 (43.17%)]\t\tLoss: 0.94377\n",
      "Training Progress: \tEpoch 27 [4160/8883 (46.76%)]\t\tLoss: 0.85188\n",
      "Training Progress: \tEpoch 27 [4480/8883 (50.36%)]\t\tLoss: 0.75945\n",
      "Training Progress: \tEpoch 27 [4800/8883 (53.96%)]\t\tLoss: 0.64699\n",
      "Training Progress: \tEpoch 27 [5120/8883 (57.55%)]\t\tLoss: 1.19053\n",
      "Training Progress: \tEpoch 27 [5440/8883 (61.15%)]\t\tLoss: 0.75965\n",
      "Training Progress: \tEpoch 27 [5760/8883 (64.75%)]\t\tLoss: 0.71231\n",
      "Training Progress: \tEpoch 27 [6080/8883 (68.35%)]\t\tLoss: 0.83601\n",
      "Training Progress: \tEpoch 27 [6400/8883 (71.94%)]\t\tLoss: 0.64422\n",
      "Training Progress: \tEpoch 27 [6720/8883 (75.54%)]\t\tLoss: 0.67133\n",
      "Training Progress: \tEpoch 27 [7040/8883 (79.14%)]\t\tLoss: 0.80000\n",
      "Training Progress: \tEpoch 27 [7360/8883 (82.73%)]\t\tLoss: 0.72752\n",
      "Training Progress: \tEpoch 27 [7680/8883 (86.33%)]\t\tLoss: 0.55357\n",
      "Training Progress: \tEpoch 27 [8000/8883 (89.93%)]\t\tLoss: 0.76900\n",
      "Training Progress: \tEpoch 27 [8320/8883 (93.53%)]\t\tLoss: 0.72525\n",
      "Training Progress: \tEpoch 27 [8640/8883 (97.12%)]\t\tLoss: 1.18333\n",
      "\tTrain loss: 0.01688, Accuracy: 6905/8883 (77.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1503/1692 (88.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1164/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/8883 (0.00%)]\t\tLoss: 0.87988\n",
      "Training Progress: \tEpoch 28 [320/8883 (3.60%)]\t\tLoss: 0.80183\n",
      "Training Progress: \tEpoch 28 [640/8883 (7.19%)]\t\tLoss: 0.83983\n",
      "Training Progress: \tEpoch 28 [960/8883 (10.79%)]\t\tLoss: 0.84808\n",
      "Training Progress: \tEpoch 28 [1280/8883 (14.39%)]\t\tLoss: 0.84625\n",
      "Training Progress: \tEpoch 28 [1600/8883 (17.99%)]\t\tLoss: 0.74730\n",
      "Training Progress: \tEpoch 28 [1920/8883 (21.58%)]\t\tLoss: 0.74319\n",
      "Training Progress: \tEpoch 28 [2240/8883 (25.18%)]\t\tLoss: 0.66793\n",
      "Training Progress: \tEpoch 28 [2560/8883 (28.78%)]\t\tLoss: 0.75027\n",
      "Training Progress: \tEpoch 28 [2880/8883 (32.37%)]\t\tLoss: 0.79054\n",
      "Training Progress: \tEpoch 28 [3200/8883 (35.97%)]\t\tLoss: 0.67874\n",
      "Training Progress: \tEpoch 28 [3520/8883 (39.57%)]\t\tLoss: 0.99760\n",
      "Training Progress: \tEpoch 28 [3840/8883 (43.17%)]\t\tLoss: 0.82306\n",
      "Training Progress: \tEpoch 28 [4160/8883 (46.76%)]\t\tLoss: 0.80362\n",
      "Training Progress: \tEpoch 28 [4480/8883 (50.36%)]\t\tLoss: 0.68398\n",
      "Training Progress: \tEpoch 28 [4800/8883 (53.96%)]\t\tLoss: 0.77055\n",
      "Training Progress: \tEpoch 28 [5120/8883 (57.55%)]\t\tLoss: 0.66522\n",
      "Training Progress: \tEpoch 28 [5440/8883 (61.15%)]\t\tLoss: 0.73655\n",
      "Training Progress: \tEpoch 28 [5760/8883 (64.75%)]\t\tLoss: 0.70606\n",
      "Training Progress: \tEpoch 28 [6080/8883 (68.35%)]\t\tLoss: 0.62381\n",
      "Training Progress: \tEpoch 28 [6400/8883 (71.94%)]\t\tLoss: 0.62912\n",
      "Training Progress: \tEpoch 28 [6720/8883 (75.54%)]\t\tLoss: 0.86999\n",
      "Training Progress: \tEpoch 28 [7040/8883 (79.14%)]\t\tLoss: 0.71357\n",
      "Training Progress: \tEpoch 28 [7360/8883 (82.73%)]\t\tLoss: 0.70255\n",
      "Training Progress: \tEpoch 28 [7680/8883 (86.33%)]\t\tLoss: 0.55974\n",
      "Training Progress: \tEpoch 28 [8000/8883 (89.93%)]\t\tLoss: 0.82889\n",
      "Training Progress: \tEpoch 28 [8320/8883 (93.53%)]\t\tLoss: 0.69684\n",
      "Training Progress: \tEpoch 28 [8640/8883 (97.12%)]\t\tLoss: 0.70302\n",
      "\tTrain loss: 0.01673, Accuracy: 6907/8883 (77.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1506/1692 (89.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1103/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/8883 (0.00%)]\t\tLoss: 0.71189\n",
      "Training Progress: \tEpoch 29 [320/8883 (3.60%)]\t\tLoss: 0.69536\n",
      "Training Progress: \tEpoch 29 [640/8883 (7.19%)]\t\tLoss: 0.70710\n",
      "Training Progress: \tEpoch 29 [960/8883 (10.79%)]\t\tLoss: 0.76528\n",
      "Training Progress: \tEpoch 29 [1280/8883 (14.39%)]\t\tLoss: 0.60388\n",
      "Training Progress: \tEpoch 29 [1600/8883 (17.99%)]\t\tLoss: 0.53870\n",
      "Training Progress: \tEpoch 29 [1920/8883 (21.58%)]\t\tLoss: 0.88702\n",
      "Training Progress: \tEpoch 29 [2240/8883 (25.18%)]\t\tLoss: 0.79981\n",
      "Training Progress: \tEpoch 29 [2560/8883 (28.78%)]\t\tLoss: 0.63965\n",
      "Training Progress: \tEpoch 29 [2880/8883 (32.37%)]\t\tLoss: 0.88312\n",
      "Training Progress: \tEpoch 29 [3200/8883 (35.97%)]\t\tLoss: 0.95946\n",
      "Training Progress: \tEpoch 29 [3520/8883 (39.57%)]\t\tLoss: 0.97245\n",
      "Training Progress: \tEpoch 29 [3840/8883 (43.17%)]\t\tLoss: 0.84166\n",
      "Training Progress: \tEpoch 29 [4160/8883 (46.76%)]\t\tLoss: 0.80178\n",
      "Training Progress: \tEpoch 29 [4480/8883 (50.36%)]\t\tLoss: 0.63729\n",
      "Training Progress: \tEpoch 29 [4800/8883 (53.96%)]\t\tLoss: 0.88647\n",
      "Training Progress: \tEpoch 29 [5120/8883 (57.55%)]\t\tLoss: 0.69040\n",
      "Training Progress: \tEpoch 29 [5440/8883 (61.15%)]\t\tLoss: 0.88354\n",
      "Training Progress: \tEpoch 29 [5760/8883 (64.75%)]\t\tLoss: 0.64548\n",
      "Training Progress: \tEpoch 29 [6080/8883 (68.35%)]\t\tLoss: 0.55149\n",
      "Training Progress: \tEpoch 29 [6400/8883 (71.94%)]\t\tLoss: 0.79613\n",
      "Training Progress: \tEpoch 29 [6720/8883 (75.54%)]\t\tLoss: 1.09630\n",
      "Training Progress: \tEpoch 29 [7040/8883 (79.14%)]\t\tLoss: 0.72405\n",
      "Training Progress: \tEpoch 29 [7360/8883 (82.73%)]\t\tLoss: 0.83102\n",
      "Training Progress: \tEpoch 29 [7680/8883 (86.33%)]\t\tLoss: 0.54725\n",
      "Training Progress: \tEpoch 29 [8000/8883 (89.93%)]\t\tLoss: 0.72803\n",
      "Training Progress: \tEpoch 29 [8320/8883 (93.53%)]\t\tLoss: 0.79608\n",
      "Training Progress: \tEpoch 29 [8640/8883 (97.12%)]\t\tLoss: 1.01609\n",
      "\tTrain loss: 0.01595, Accuracy: 7022/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1535/1692 (90.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1177/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/8883 (0.00%)]\t\tLoss: 0.73128\n",
      "Training Progress: \tEpoch 30 [320/8883 (3.60%)]\t\tLoss: 0.84693\n",
      "Training Progress: \tEpoch 30 [640/8883 (7.19%)]\t\tLoss: 0.65373\n",
      "Training Progress: \tEpoch 30 [960/8883 (10.79%)]\t\tLoss: 0.80101\n",
      "Training Progress: \tEpoch 30 [1280/8883 (14.39%)]\t\tLoss: 0.82986\n",
      "Training Progress: \tEpoch 30 [1600/8883 (17.99%)]\t\tLoss: 0.43046\n",
      "Training Progress: \tEpoch 30 [1920/8883 (21.58%)]\t\tLoss: 0.73281\n",
      "Training Progress: \tEpoch 30 [2240/8883 (25.18%)]\t\tLoss: 0.75004\n",
      "Training Progress: \tEpoch 30 [2560/8883 (28.78%)]\t\tLoss: 0.57120\n",
      "Training Progress: \tEpoch 30 [2880/8883 (32.37%)]\t\tLoss: 0.73821\n",
      "Training Progress: \tEpoch 30 [3200/8883 (35.97%)]\t\tLoss: 0.82372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 30 [3520/8883 (39.57%)]\t\tLoss: 0.98650\n",
      "Training Progress: \tEpoch 30 [3840/8883 (43.17%)]\t\tLoss: 0.73376\n",
      "Training Progress: \tEpoch 30 [4160/8883 (46.76%)]\t\tLoss: 0.64017\n",
      "Training Progress: \tEpoch 30 [4480/8883 (50.36%)]\t\tLoss: 0.69275\n",
      "Training Progress: \tEpoch 30 [4800/8883 (53.96%)]\t\tLoss: 0.63717\n",
      "Training Progress: \tEpoch 30 [5120/8883 (57.55%)]\t\tLoss: 0.94647\n",
      "Training Progress: \tEpoch 30 [5440/8883 (61.15%)]\t\tLoss: 0.73380\n",
      "Training Progress: \tEpoch 30 [5760/8883 (64.75%)]\t\tLoss: 0.72188\n",
      "Training Progress: \tEpoch 30 [6080/8883 (68.35%)]\t\tLoss: 0.71820\n",
      "Training Progress: \tEpoch 30 [6400/8883 (71.94%)]\t\tLoss: 0.59096\n",
      "Training Progress: \tEpoch 30 [6720/8883 (75.54%)]\t\tLoss: 0.81874\n",
      "Training Progress: \tEpoch 30 [7040/8883 (79.14%)]\t\tLoss: 1.02567\n",
      "Training Progress: \tEpoch 30 [7360/8883 (82.73%)]\t\tLoss: 0.78435\n",
      "Training Progress: \tEpoch 30 [7680/8883 (86.33%)]\t\tLoss: 0.58855\n",
      "Training Progress: \tEpoch 30 [8000/8883 (89.93%)]\t\tLoss: 0.82672\n",
      "Training Progress: \tEpoch 30 [8320/8883 (93.53%)]\t\tLoss: 0.64468\n",
      "Training Progress: \tEpoch 30 [8640/8883 (97.12%)]\t\tLoss: 0.78460\n",
      "\tTrain loss: 0.01581, Accuracy: 7027/8883 (79.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1557/1692 (92.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1170/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/8883 (0.00%)]\t\tLoss: 0.63571\n",
      "Training Progress: \tEpoch 31 [320/8883 (3.60%)]\t\tLoss: 0.72307\n",
      "Training Progress: \tEpoch 31 [640/8883 (7.19%)]\t\tLoss: 0.64802\n",
      "Training Progress: \tEpoch 31 [960/8883 (10.79%)]\t\tLoss: 0.84252\n",
      "Training Progress: \tEpoch 31 [1280/8883 (14.39%)]\t\tLoss: 0.76669\n",
      "Training Progress: \tEpoch 31 [1600/8883 (17.99%)]\t\tLoss: 0.62327\n",
      "Training Progress: \tEpoch 31 [1920/8883 (21.58%)]\t\tLoss: 0.50804\n",
      "Training Progress: \tEpoch 31 [2240/8883 (25.18%)]\t\tLoss: 0.72402\n",
      "Training Progress: \tEpoch 31 [2560/8883 (28.78%)]\t\tLoss: 0.89913\n",
      "Training Progress: \tEpoch 31 [2880/8883 (32.37%)]\t\tLoss: 0.70339\n",
      "Training Progress: \tEpoch 31 [3200/8883 (35.97%)]\t\tLoss: 0.89528\n",
      "Training Progress: \tEpoch 31 [3520/8883 (39.57%)]\t\tLoss: 0.79619\n",
      "Training Progress: \tEpoch 31 [3840/8883 (43.17%)]\t\tLoss: 0.90661\n",
      "Training Progress: \tEpoch 31 [4160/8883 (46.76%)]\t\tLoss: 0.77946\n",
      "Training Progress: \tEpoch 31 [4480/8883 (50.36%)]\t\tLoss: 0.66981\n",
      "Training Progress: \tEpoch 31 [4800/8883 (53.96%)]\t\tLoss: 0.74215\n",
      "Training Progress: \tEpoch 31 [5120/8883 (57.55%)]\t\tLoss: 0.99355\n",
      "Training Progress: \tEpoch 31 [5440/8883 (61.15%)]\t\tLoss: 0.80659\n",
      "Training Progress: \tEpoch 31 [5760/8883 (64.75%)]\t\tLoss: 0.79599\n",
      "Training Progress: \tEpoch 31 [6080/8883 (68.35%)]\t\tLoss: 0.66155\n",
      "Training Progress: \tEpoch 31 [6400/8883 (71.94%)]\t\tLoss: 0.85024\n",
      "Training Progress: \tEpoch 31 [6720/8883 (75.54%)]\t\tLoss: 0.74319\n",
      "Training Progress: \tEpoch 31 [7040/8883 (79.14%)]\t\tLoss: 0.86665\n",
      "Training Progress: \tEpoch 31 [7360/8883 (82.73%)]\t\tLoss: 0.74525\n",
      "Training Progress: \tEpoch 31 [7680/8883 (86.33%)]\t\tLoss: 0.66933\n",
      "Training Progress: \tEpoch 31 [8000/8883 (89.93%)]\t\tLoss: 0.83243\n",
      "Training Progress: \tEpoch 31 [8320/8883 (93.53%)]\t\tLoss: 0.88132\n",
      "Training Progress: \tEpoch 31 [8640/8883 (97.12%)]\t\tLoss: 0.65701\n",
      "\tTrain loss: 0.01576, Accuracy: 6999/8883 (78.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1551/1692 (91.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1152/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/8883 (0.00%)]\t\tLoss: 1.01412\n",
      "Training Progress: \tEpoch 32 [320/8883 (3.60%)]\t\tLoss: 0.71584\n",
      "Training Progress: \tEpoch 32 [640/8883 (7.19%)]\t\tLoss: 0.64103\n",
      "Training Progress: \tEpoch 32 [960/8883 (10.79%)]\t\tLoss: 0.81582\n",
      "Training Progress: \tEpoch 32 [1280/8883 (14.39%)]\t\tLoss: 0.54462\n",
      "Training Progress: \tEpoch 32 [1600/8883 (17.99%)]\t\tLoss: 0.38831\n",
      "Training Progress: \tEpoch 32 [1920/8883 (21.58%)]\t\tLoss: 0.53317\n",
      "Training Progress: \tEpoch 32 [2240/8883 (25.18%)]\t\tLoss: 0.53889\n",
      "Training Progress: \tEpoch 32 [2560/8883 (28.78%)]\t\tLoss: 0.67771\n",
      "Training Progress: \tEpoch 32 [2880/8883 (32.37%)]\t\tLoss: 0.84031\n",
      "Training Progress: \tEpoch 32 [3200/8883 (35.97%)]\t\tLoss: 0.65878\n",
      "Training Progress: \tEpoch 32 [3520/8883 (39.57%)]\t\tLoss: 0.80286\n",
      "Training Progress: \tEpoch 32 [3840/8883 (43.17%)]\t\tLoss: 0.93604\n",
      "Training Progress: \tEpoch 32 [4160/8883 (46.76%)]\t\tLoss: 0.91062\n",
      "Training Progress: \tEpoch 32 [4480/8883 (50.36%)]\t\tLoss: 0.71339\n",
      "Training Progress: \tEpoch 32 [4800/8883 (53.96%)]\t\tLoss: 0.50726\n",
      "Training Progress: \tEpoch 32 [5120/8883 (57.55%)]\t\tLoss: 0.89949\n",
      "Training Progress: \tEpoch 32 [5440/8883 (61.15%)]\t\tLoss: 0.71268\n",
      "Training Progress: \tEpoch 32 [5760/8883 (64.75%)]\t\tLoss: 0.74638\n",
      "Training Progress: \tEpoch 32 [6080/8883 (68.35%)]\t\tLoss: 0.59521\n",
      "Training Progress: \tEpoch 32 [6400/8883 (71.94%)]\t\tLoss: 0.75538\n",
      "Training Progress: \tEpoch 32 [6720/8883 (75.54%)]\t\tLoss: 0.94615\n",
      "Training Progress: \tEpoch 32 [7040/8883 (79.14%)]\t\tLoss: 0.92419\n",
      "Training Progress: \tEpoch 32 [7360/8883 (82.73%)]\t\tLoss: 0.80038\n",
      "Training Progress: \tEpoch 32 [7680/8883 (86.33%)]\t\tLoss: 0.56993\n",
      "Training Progress: \tEpoch 32 [8000/8883 (89.93%)]\t\tLoss: 0.85637\n",
      "Training Progress: \tEpoch 32 [8320/8883 (93.53%)]\t\tLoss: 0.74664\n",
      "Training Progress: \tEpoch 32 [8640/8883 (97.12%)]\t\tLoss: 0.96018\n",
      "\tTrain loss: 0.01528, Accuracy: 7041/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1568/1692 (92.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1123/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/8883 (0.00%)]\t\tLoss: 0.79271\n",
      "Training Progress: \tEpoch 33 [320/8883 (3.60%)]\t\tLoss: 0.83131\n",
      "Training Progress: \tEpoch 33 [640/8883 (7.19%)]\t\tLoss: 0.52264\n",
      "Training Progress: \tEpoch 33 [960/8883 (10.79%)]\t\tLoss: 0.80507\n",
      "Training Progress: \tEpoch 33 [1280/8883 (14.39%)]\t\tLoss: 0.70072\n",
      "Training Progress: \tEpoch 33 [1600/8883 (17.99%)]\t\tLoss: 0.55140\n",
      "Training Progress: \tEpoch 33 [1920/8883 (21.58%)]\t\tLoss: 0.48233\n",
      "Training Progress: \tEpoch 33 [2240/8883 (25.18%)]\t\tLoss: 0.68952\n",
      "Training Progress: \tEpoch 33 [2560/8883 (28.78%)]\t\tLoss: 0.76212\n",
      "Training Progress: \tEpoch 33 [2880/8883 (32.37%)]\t\tLoss: 0.80440\n",
      "Training Progress: \tEpoch 33 [3200/8883 (35.97%)]\t\tLoss: 0.73424\n",
      "Training Progress: \tEpoch 33 [3520/8883 (39.57%)]\t\tLoss: 1.03531\n",
      "Training Progress: \tEpoch 33 [3840/8883 (43.17%)]\t\tLoss: 0.91308\n",
      "Training Progress: \tEpoch 33 [4160/8883 (46.76%)]\t\tLoss: 0.76742\n",
      "Training Progress: \tEpoch 33 [4480/8883 (50.36%)]\t\tLoss: 0.54743\n",
      "Training Progress: \tEpoch 33 [4800/8883 (53.96%)]\t\tLoss: 0.84805\n",
      "Training Progress: \tEpoch 33 [5120/8883 (57.55%)]\t\tLoss: 0.94004\n",
      "Training Progress: \tEpoch 33 [5440/8883 (61.15%)]\t\tLoss: 0.85619\n",
      "Training Progress: \tEpoch 33 [5760/8883 (64.75%)]\t\tLoss: 0.62873\n",
      "Training Progress: \tEpoch 33 [6080/8883 (68.35%)]\t\tLoss: 0.52790\n",
      "Training Progress: \tEpoch 33 [6400/8883 (71.94%)]\t\tLoss: 0.48051\n",
      "Training Progress: \tEpoch 33 [6720/8883 (75.54%)]\t\tLoss: 0.78817\n",
      "Training Progress: \tEpoch 33 [7040/8883 (79.14%)]\t\tLoss: 0.75851\n",
      "Training Progress: \tEpoch 33 [7360/8883 (82.73%)]\t\tLoss: 0.81938\n",
      "Training Progress: \tEpoch 33 [7680/8883 (86.33%)]\t\tLoss: 0.58279\n",
      "Training Progress: \tEpoch 33 [8000/8883 (89.93%)]\t\tLoss: 0.76161\n",
      "Training Progress: \tEpoch 33 [8320/8883 (93.53%)]\t\tLoss: 0.70396\n",
      "Training Progress: \tEpoch 33 [8640/8883 (97.12%)]\t\tLoss: 0.69496\n",
      "\tTrain loss: 0.01525, Accuracy: 7063/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1575/1692 (93.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1130/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/8883 (0.00%)]\t\tLoss: 0.58014\n",
      "Training Progress: \tEpoch 34 [320/8883 (3.60%)]\t\tLoss: 0.57998\n",
      "Training Progress: \tEpoch 34 [640/8883 (7.19%)]\t\tLoss: 0.52339\n",
      "Training Progress: \tEpoch 34 [960/8883 (10.79%)]\t\tLoss: 0.64370\n",
      "Training Progress: \tEpoch 34 [1280/8883 (14.39%)]\t\tLoss: 0.74219\n",
      "Training Progress: \tEpoch 34 [1600/8883 (17.99%)]\t\tLoss: 0.39295\n",
      "Training Progress: \tEpoch 34 [1920/8883 (21.58%)]\t\tLoss: 0.46127\n",
      "Training Progress: \tEpoch 34 [2240/8883 (25.18%)]\t\tLoss: 0.81276\n",
      "Training Progress: \tEpoch 34 [2560/8883 (28.78%)]\t\tLoss: 0.65488\n",
      "Training Progress: \tEpoch 34 [2880/8883 (32.37%)]\t\tLoss: 0.78680\n",
      "Training Progress: \tEpoch 34 [3200/8883 (35.97%)]\t\tLoss: 0.89096\n",
      "Training Progress: \tEpoch 34 [3520/8883 (39.57%)]\t\tLoss: 0.75867\n",
      "Training Progress: \tEpoch 34 [3840/8883 (43.17%)]\t\tLoss: 0.90021\n",
      "Training Progress: \tEpoch 34 [4160/8883 (46.76%)]\t\tLoss: 0.98301\n",
      "Training Progress: \tEpoch 34 [4480/8883 (50.36%)]\t\tLoss: 0.49589\n",
      "Training Progress: \tEpoch 34 [4800/8883 (53.96%)]\t\tLoss: 0.75982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 34 [5120/8883 (57.55%)]\t\tLoss: 0.76504\n",
      "Training Progress: \tEpoch 34 [5440/8883 (61.15%)]\t\tLoss: 0.75829\n",
      "Training Progress: \tEpoch 34 [5760/8883 (64.75%)]\t\tLoss: 0.62360\n",
      "Training Progress: \tEpoch 34 [6080/8883 (68.35%)]\t\tLoss: 0.81703\n",
      "Training Progress: \tEpoch 34 [6400/8883 (71.94%)]\t\tLoss: 0.78781\n",
      "Training Progress: \tEpoch 34 [6720/8883 (75.54%)]\t\tLoss: 0.90745\n",
      "Training Progress: \tEpoch 34 [7040/8883 (79.14%)]\t\tLoss: 0.86245\n",
      "Training Progress: \tEpoch 34 [7360/8883 (82.73%)]\t\tLoss: 0.74063\n",
      "Training Progress: \tEpoch 34 [7680/8883 (86.33%)]\t\tLoss: 0.57103\n",
      "Training Progress: \tEpoch 34 [8000/8883 (89.93%)]\t\tLoss: 0.69908\n",
      "Training Progress: \tEpoch 34 [8320/8883 (93.53%)]\t\tLoss: 0.73008\n",
      "Training Progress: \tEpoch 34 [8640/8883 (97.12%)]\t\tLoss: 0.86860\n",
      "\tTrain loss: 0.01497, Accuracy: 7073/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1567/1692 (92.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1133/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/8883 (0.00%)]\t\tLoss: 0.92629\n",
      "Training Progress: \tEpoch 35 [320/8883 (3.60%)]\t\tLoss: 0.63335\n",
      "Training Progress: \tEpoch 35 [640/8883 (7.19%)]\t\tLoss: 0.55948\n",
      "Training Progress: \tEpoch 35 [960/8883 (10.79%)]\t\tLoss: 0.86565\n",
      "Training Progress: \tEpoch 35 [1280/8883 (14.39%)]\t\tLoss: 0.70314\n",
      "Training Progress: \tEpoch 35 [1600/8883 (17.99%)]\t\tLoss: 0.52961\n",
      "Training Progress: \tEpoch 35 [1920/8883 (21.58%)]\t\tLoss: 0.58891\n",
      "Training Progress: \tEpoch 35 [2240/8883 (25.18%)]\t\tLoss: 0.70895\n",
      "Training Progress: \tEpoch 35 [2560/8883 (28.78%)]\t\tLoss: 0.94570\n",
      "Training Progress: \tEpoch 35 [2880/8883 (32.37%)]\t\tLoss: 0.75278\n",
      "Training Progress: \tEpoch 35 [3200/8883 (35.97%)]\t\tLoss: 0.62475\n",
      "Training Progress: \tEpoch 35 [3520/8883 (39.57%)]\t\tLoss: 0.88533\n",
      "Training Progress: \tEpoch 35 [3840/8883 (43.17%)]\t\tLoss: 0.89221\n",
      "Training Progress: \tEpoch 35 [4160/8883 (46.76%)]\t\tLoss: 0.68214\n",
      "Training Progress: \tEpoch 35 [4480/8883 (50.36%)]\t\tLoss: 0.57090\n",
      "Training Progress: \tEpoch 35 [4800/8883 (53.96%)]\t\tLoss: 0.58174\n",
      "Training Progress: \tEpoch 35 [5120/8883 (57.55%)]\t\tLoss: 0.79013\n",
      "Training Progress: \tEpoch 35 [5440/8883 (61.15%)]\t\tLoss: 0.85397\n",
      "Training Progress: \tEpoch 35 [5760/8883 (64.75%)]\t\tLoss: 0.92650\n",
      "Training Progress: \tEpoch 35 [6080/8883 (68.35%)]\t\tLoss: 0.64946\n",
      "Training Progress: \tEpoch 35 [6400/8883 (71.94%)]\t\tLoss: 0.61155\n",
      "Training Progress: \tEpoch 35 [6720/8883 (75.54%)]\t\tLoss: 0.88176\n",
      "Training Progress: \tEpoch 35 [7040/8883 (79.14%)]\t\tLoss: 0.77468\n",
      "Training Progress: \tEpoch 35 [7360/8883 (82.73%)]\t\tLoss: 0.70674\n",
      "Training Progress: \tEpoch 35 [7680/8883 (86.33%)]\t\tLoss: 0.80580\n",
      "Training Progress: \tEpoch 35 [8000/8883 (89.93%)]\t\tLoss: 0.88070\n",
      "Training Progress: \tEpoch 35 [8320/8883 (93.53%)]\t\tLoss: 0.61234\n",
      "Training Progress: \tEpoch 35 [8640/8883 (97.12%)]\t\tLoss: 0.84957\n",
      "\tTrain loss: 0.01483, Accuracy: 7066/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1571/1692 (92.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/8883 (0.00%)]\t\tLoss: 0.80943\n",
      "Training Progress: \tEpoch 36 [320/8883 (3.60%)]\t\tLoss: 0.77232\n",
      "Training Progress: \tEpoch 36 [640/8883 (7.19%)]\t\tLoss: 0.52300\n",
      "Training Progress: \tEpoch 36 [960/8883 (10.79%)]\t\tLoss: 0.68911\n",
      "Training Progress: \tEpoch 36 [1280/8883 (14.39%)]\t\tLoss: 0.65133\n",
      "Training Progress: \tEpoch 36 [1600/8883 (17.99%)]\t\tLoss: 0.44741\n",
      "Training Progress: \tEpoch 36 [1920/8883 (21.58%)]\t\tLoss: 0.57546\n",
      "Training Progress: \tEpoch 36 [2240/8883 (25.18%)]\t\tLoss: 0.61991\n",
      "Training Progress: \tEpoch 36 [2560/8883 (28.78%)]\t\tLoss: 0.72333\n",
      "Training Progress: \tEpoch 36 [2880/8883 (32.37%)]\t\tLoss: 0.77718\n",
      "Training Progress: \tEpoch 36 [3200/8883 (35.97%)]\t\tLoss: 0.59463\n",
      "Training Progress: \tEpoch 36 [3520/8883 (39.57%)]\t\tLoss: 1.09584\n",
      "Training Progress: \tEpoch 36 [3840/8883 (43.17%)]\t\tLoss: 0.87206\n",
      "Training Progress: \tEpoch 36 [4160/8883 (46.76%)]\t\tLoss: 0.80705\n",
      "Training Progress: \tEpoch 36 [4480/8883 (50.36%)]\t\tLoss: 0.63203\n",
      "Training Progress: \tEpoch 36 [4800/8883 (53.96%)]\t\tLoss: 0.74205\n",
      "Training Progress: \tEpoch 36 [5120/8883 (57.55%)]\t\tLoss: 0.97425\n",
      "Training Progress: \tEpoch 36 [5440/8883 (61.15%)]\t\tLoss: 0.73540\n",
      "Training Progress: \tEpoch 36 [5760/8883 (64.75%)]\t\tLoss: 0.67342\n",
      "Training Progress: \tEpoch 36 [6080/8883 (68.35%)]\t\tLoss: 0.71636\n",
      "Training Progress: \tEpoch 36 [6400/8883 (71.94%)]\t\tLoss: 0.58157\n",
      "Training Progress: \tEpoch 36 [6720/8883 (75.54%)]\t\tLoss: 1.21815\n",
      "Training Progress: \tEpoch 36 [7040/8883 (79.14%)]\t\tLoss: 0.83519\n",
      "Training Progress: \tEpoch 36 [7360/8883 (82.73%)]\t\tLoss: 0.99249\n",
      "Training Progress: \tEpoch 36 [7680/8883 (86.33%)]\t\tLoss: 0.64311\n",
      "Training Progress: \tEpoch 36 [8000/8883 (89.93%)]\t\tLoss: 0.72585\n",
      "Training Progress: \tEpoch 36 [8320/8883 (93.53%)]\t\tLoss: 0.66934\n",
      "Training Progress: \tEpoch 36 [8640/8883 (97.12%)]\t\tLoss: 0.68795\n",
      "\tTrain loss: 0.01458, Accuracy: 7110/8883 (80.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1586/1692 (93.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1143/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/8883 (0.00%)]\t\tLoss: 0.74639\n",
      "Training Progress: \tEpoch 37 [320/8883 (3.60%)]\t\tLoss: 0.67864\n",
      "Training Progress: \tEpoch 37 [640/8883 (7.19%)]\t\tLoss: 0.65707\n",
      "Training Progress: \tEpoch 37 [960/8883 (10.79%)]\t\tLoss: 0.78997\n",
      "Training Progress: \tEpoch 37 [1280/8883 (14.39%)]\t\tLoss: 0.82567\n",
      "Training Progress: \tEpoch 37 [1600/8883 (17.99%)]\t\tLoss: 0.56670\n",
      "Training Progress: \tEpoch 37 [1920/8883 (21.58%)]\t\tLoss: 0.55051\n",
      "Training Progress: \tEpoch 37 [2240/8883 (25.18%)]\t\tLoss: 0.66747\n",
      "Training Progress: \tEpoch 37 [2560/8883 (28.78%)]\t\tLoss: 0.69810\n",
      "Training Progress: \tEpoch 37 [2880/8883 (32.37%)]\t\tLoss: 0.57080\n",
      "Training Progress: \tEpoch 37 [3200/8883 (35.97%)]\t\tLoss: 0.72742\n",
      "Training Progress: \tEpoch 37 [3520/8883 (39.57%)]\t\tLoss: 0.99648\n",
      "Training Progress: \tEpoch 37 [3840/8883 (43.17%)]\t\tLoss: 0.82302\n",
      "Training Progress: \tEpoch 37 [4160/8883 (46.76%)]\t\tLoss: 0.62860\n",
      "Training Progress: \tEpoch 37 [4480/8883 (50.36%)]\t\tLoss: 0.66701\n",
      "Training Progress: \tEpoch 37 [4800/8883 (53.96%)]\t\tLoss: 0.79285\n",
      "Training Progress: \tEpoch 37 [5120/8883 (57.55%)]\t\tLoss: 0.78981\n",
      "Training Progress: \tEpoch 37 [5440/8883 (61.15%)]\t\tLoss: 0.77987\n",
      "Training Progress: \tEpoch 37 [5760/8883 (64.75%)]\t\tLoss: 0.65305\n",
      "Training Progress: \tEpoch 37 [6080/8883 (68.35%)]\t\tLoss: 0.57350\n",
      "Training Progress: \tEpoch 37 [6400/8883 (71.94%)]\t\tLoss: 0.50287\n",
      "Training Progress: \tEpoch 37 [6720/8883 (75.54%)]\t\tLoss: 0.69105\n",
      "Training Progress: \tEpoch 37 [7040/8883 (79.14%)]\t\tLoss: 0.69680\n",
      "Training Progress: \tEpoch 37 [7360/8883 (82.73%)]\t\tLoss: 0.79733\n",
      "Training Progress: \tEpoch 37 [7680/8883 (86.33%)]\t\tLoss: 0.39763\n",
      "Training Progress: \tEpoch 37 [8000/8883 (89.93%)]\t\tLoss: 0.78057\n",
      "Training Progress: \tEpoch 37 [8320/8883 (93.53%)]\t\tLoss: 0.56039\n",
      "Training Progress: \tEpoch 37 [8640/8883 (97.12%)]\t\tLoss: 0.70327\n",
      "\tTrain loss: 0.01405, Accuracy: 7127/8883 (80.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1604/1692 (94.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1138/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/8883 (0.00%)]\t\tLoss: 0.71445\n",
      "Training Progress: \tEpoch 38 [320/8883 (3.60%)]\t\tLoss: 0.70561\n",
      "Training Progress: \tEpoch 38 [640/8883 (7.19%)]\t\tLoss: 0.66866\n",
      "Training Progress: \tEpoch 38 [960/8883 (10.79%)]\t\tLoss: 0.65896\n",
      "Training Progress: \tEpoch 38 [1280/8883 (14.39%)]\t\tLoss: 0.74717\n",
      "Training Progress: \tEpoch 38 [1600/8883 (17.99%)]\t\tLoss: 0.47331\n",
      "Training Progress: \tEpoch 38 [1920/8883 (21.58%)]\t\tLoss: 0.54028\n",
      "Training Progress: \tEpoch 38 [2240/8883 (25.18%)]\t\tLoss: 0.90020\n",
      "Training Progress: \tEpoch 38 [2560/8883 (28.78%)]\t\tLoss: 0.62331\n",
      "Training Progress: \tEpoch 38 [2880/8883 (32.37%)]\t\tLoss: 0.64063\n",
      "Training Progress: \tEpoch 38 [3200/8883 (35.97%)]\t\tLoss: 0.65263\n",
      "Training Progress: \tEpoch 38 [3520/8883 (39.57%)]\t\tLoss: 1.00337\n",
      "Training Progress: \tEpoch 38 [3840/8883 (43.17%)]\t\tLoss: 0.82060\n",
      "Training Progress: \tEpoch 38 [4160/8883 (46.76%)]\t\tLoss: 0.78185\n",
      "Training Progress: \tEpoch 38 [4480/8883 (50.36%)]\t\tLoss: 0.48934\n",
      "Training Progress: \tEpoch 38 [4800/8883 (53.96%)]\t\tLoss: 0.51170\n",
      "Training Progress: \tEpoch 38 [5120/8883 (57.55%)]\t\tLoss: 0.82017\n",
      "Training Progress: \tEpoch 38 [5440/8883 (61.15%)]\t\tLoss: 1.07259\n",
      "Training Progress: \tEpoch 38 [5760/8883 (64.75%)]\t\tLoss: 0.75064\n",
      "Training Progress: \tEpoch 38 [6080/8883 (68.35%)]\t\tLoss: 0.56705\n",
      "Training Progress: \tEpoch 38 [6400/8883 (71.94%)]\t\tLoss: 0.58129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 38 [6720/8883 (75.54%)]\t\tLoss: 0.81770\n",
      "Training Progress: \tEpoch 38 [7040/8883 (79.14%)]\t\tLoss: 0.79504\n",
      "Training Progress: \tEpoch 38 [7360/8883 (82.73%)]\t\tLoss: 0.76175\n",
      "Training Progress: \tEpoch 38 [7680/8883 (86.33%)]\t\tLoss: 0.44014\n",
      "Training Progress: \tEpoch 38 [8000/8883 (89.93%)]\t\tLoss: 0.71078\n",
      "Training Progress: \tEpoch 38 [8320/8883 (93.53%)]\t\tLoss: 0.76292\n",
      "Training Progress: \tEpoch 38 [8640/8883 (97.12%)]\t\tLoss: 0.93437\n",
      "\tTrain loss: 0.01427, Accuracy: 7114/8883 (80.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1585/1692 (93.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/8883 (0.00%)]\t\tLoss: 0.46575\n",
      "Training Progress: \tEpoch 39 [320/8883 (3.60%)]\t\tLoss: 0.72826\n",
      "Training Progress: \tEpoch 39 [640/8883 (7.19%)]\t\tLoss: 0.61659\n",
      "Training Progress: \tEpoch 39 [960/8883 (10.79%)]\t\tLoss: 0.72965\n",
      "Training Progress: \tEpoch 39 [1280/8883 (14.39%)]\t\tLoss: 0.63626\n",
      "Training Progress: \tEpoch 39 [1600/8883 (17.99%)]\t\tLoss: 0.42278\n",
      "Training Progress: \tEpoch 39 [1920/8883 (21.58%)]\t\tLoss: 0.55046\n",
      "Training Progress: \tEpoch 39 [2240/8883 (25.18%)]\t\tLoss: 0.66961\n",
      "Training Progress: \tEpoch 39 [2560/8883 (28.78%)]\t\tLoss: 0.65125\n",
      "Training Progress: \tEpoch 39 [2880/8883 (32.37%)]\t\tLoss: 0.83884\n",
      "Training Progress: \tEpoch 39 [3200/8883 (35.97%)]\t\tLoss: 0.75106\n",
      "Training Progress: \tEpoch 39 [3520/8883 (39.57%)]\t\tLoss: 0.93659\n",
      "Training Progress: \tEpoch 39 [3840/8883 (43.17%)]\t\tLoss: 0.92087\n",
      "Training Progress: \tEpoch 39 [4160/8883 (46.76%)]\t\tLoss: 0.71206\n",
      "Training Progress: \tEpoch 39 [4480/8883 (50.36%)]\t\tLoss: 0.49117\n",
      "Training Progress: \tEpoch 39 [4800/8883 (53.96%)]\t\tLoss: 0.60448\n",
      "Training Progress: \tEpoch 39 [5120/8883 (57.55%)]\t\tLoss: 0.80797\n",
      "Training Progress: \tEpoch 39 [5440/8883 (61.15%)]\t\tLoss: 0.77610\n",
      "Training Progress: \tEpoch 39 [5760/8883 (64.75%)]\t\tLoss: 0.62243\n",
      "Training Progress: \tEpoch 39 [6080/8883 (68.35%)]\t\tLoss: 0.75928\n",
      "Training Progress: \tEpoch 39 [6400/8883 (71.94%)]\t\tLoss: 0.86239\n",
      "Training Progress: \tEpoch 39 [6720/8883 (75.54%)]\t\tLoss: 0.91706\n",
      "Training Progress: \tEpoch 39 [7040/8883 (79.14%)]\t\tLoss: 0.72102\n",
      "Training Progress: \tEpoch 39 [7360/8883 (82.73%)]\t\tLoss: 0.74879\n",
      "Training Progress: \tEpoch 39 [7680/8883 (86.33%)]\t\tLoss: 0.52660\n",
      "Training Progress: \tEpoch 39 [8000/8883 (89.93%)]\t\tLoss: 0.67431\n",
      "Training Progress: \tEpoch 39 [8320/8883 (93.53%)]\t\tLoss: 0.67746\n",
      "Training Progress: \tEpoch 39 [8640/8883 (97.12%)]\t\tLoss: 0.92578\n",
      "\tTrain loss: 0.01375, Accuracy: 7215/8883 (81.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1611/1692 (95.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1126/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/8883 (0.00%)]\t\tLoss: 0.60282\n",
      "Training Progress: \tEpoch 40 [320/8883 (3.60%)]\t\tLoss: 0.87241\n",
      "Training Progress: \tEpoch 40 [640/8883 (7.19%)]\t\tLoss: 0.56629\n",
      "Training Progress: \tEpoch 40 [960/8883 (10.79%)]\t\tLoss: 0.79360\n",
      "Training Progress: \tEpoch 40 [1280/8883 (14.39%)]\t\tLoss: 0.71685\n",
      "Training Progress: \tEpoch 40 [1600/8883 (17.99%)]\t\tLoss: 0.39271\n",
      "Training Progress: \tEpoch 40 [1920/8883 (21.58%)]\t\tLoss: 0.64057\n",
      "Training Progress: \tEpoch 40 [2240/8883 (25.18%)]\t\tLoss: 0.53913\n",
      "Training Progress: \tEpoch 40 [2560/8883 (28.78%)]\t\tLoss: 0.77355\n",
      "Training Progress: \tEpoch 40 [2880/8883 (32.37%)]\t\tLoss: 0.65725\n",
      "Training Progress: \tEpoch 40 [3200/8883 (35.97%)]\t\tLoss: 0.77261\n",
      "Training Progress: \tEpoch 40 [3520/8883 (39.57%)]\t\tLoss: 1.05831\n",
      "Training Progress: \tEpoch 40 [3840/8883 (43.17%)]\t\tLoss: 0.70526\n",
      "Training Progress: \tEpoch 40 [4160/8883 (46.76%)]\t\tLoss: 0.77099\n",
      "Training Progress: \tEpoch 40 [4480/8883 (50.36%)]\t\tLoss: 0.49907\n",
      "Training Progress: \tEpoch 40 [4800/8883 (53.96%)]\t\tLoss: 0.76076\n",
      "Training Progress: \tEpoch 40 [5120/8883 (57.55%)]\t\tLoss: 0.75181\n",
      "Training Progress: \tEpoch 40 [5440/8883 (61.15%)]\t\tLoss: 0.63479\n",
      "Training Progress: \tEpoch 40 [5760/8883 (64.75%)]\t\tLoss: 0.95368\n",
      "Training Progress: \tEpoch 40 [6080/8883 (68.35%)]\t\tLoss: 0.48703\n",
      "Training Progress: \tEpoch 40 [6400/8883 (71.94%)]\t\tLoss: 0.65542\n",
      "Training Progress: \tEpoch 40 [6720/8883 (75.54%)]\t\tLoss: 0.83052\n",
      "Training Progress: \tEpoch 40 [7040/8883 (79.14%)]\t\tLoss: 0.76771\n",
      "Training Progress: \tEpoch 40 [7360/8883 (82.73%)]\t\tLoss: 0.72061\n",
      "Training Progress: \tEpoch 40 [7680/8883 (86.33%)]\t\tLoss: 0.52986\n",
      "Training Progress: \tEpoch 40 [8000/8883 (89.93%)]\t\tLoss: 0.73719\n",
      "Training Progress: \tEpoch 40 [8320/8883 (93.53%)]\t\tLoss: 0.73151\n",
      "Training Progress: \tEpoch 40 [8640/8883 (97.12%)]\t\tLoss: 0.66386\n",
      "\tTrain loss: 0.01369, Accuracy: 7200/8883 (81.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1609/1692 (95.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1147/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/8883 (0.00%)]\t\tLoss: 0.72780\n",
      "Training Progress: \tEpoch 41 [320/8883 (3.60%)]\t\tLoss: 0.61069\n",
      "Training Progress: \tEpoch 41 [640/8883 (7.19%)]\t\tLoss: 0.53805\n",
      "Training Progress: \tEpoch 41 [960/8883 (10.79%)]\t\tLoss: 0.87363\n",
      "Training Progress: \tEpoch 41 [1280/8883 (14.39%)]\t\tLoss: 0.65219\n",
      "Training Progress: \tEpoch 41 [1600/8883 (17.99%)]\t\tLoss: 0.72839\n",
      "Training Progress: \tEpoch 41 [1920/8883 (21.58%)]\t\tLoss: 0.58724\n",
      "Training Progress: \tEpoch 41 [2240/8883 (25.18%)]\t\tLoss: 0.75721\n",
      "Training Progress: \tEpoch 41 [2560/8883 (28.78%)]\t\tLoss: 0.61411\n",
      "Training Progress: \tEpoch 41 [2880/8883 (32.37%)]\t\tLoss: 0.64461\n",
      "Training Progress: \tEpoch 41 [3200/8883 (35.97%)]\t\tLoss: 0.58974\n",
      "Training Progress: \tEpoch 41 [3520/8883 (39.57%)]\t\tLoss: 0.73814\n",
      "Training Progress: \tEpoch 41 [3840/8883 (43.17%)]\t\tLoss: 0.80572\n",
      "Training Progress: \tEpoch 41 [4160/8883 (46.76%)]\t\tLoss: 0.64892\n",
      "Training Progress: \tEpoch 41 [4480/8883 (50.36%)]\t\tLoss: 0.69586\n",
      "Training Progress: \tEpoch 41 [4800/8883 (53.96%)]\t\tLoss: 0.63063\n",
      "Training Progress: \tEpoch 41 [5120/8883 (57.55%)]\t\tLoss: 1.01012\n",
      "Training Progress: \tEpoch 41 [5440/8883 (61.15%)]\t\tLoss: 0.75602\n",
      "Training Progress: \tEpoch 41 [5760/8883 (64.75%)]\t\tLoss: 0.62392\n",
      "Training Progress: \tEpoch 41 [6080/8883 (68.35%)]\t\tLoss: 0.52280\n",
      "Training Progress: \tEpoch 41 [6400/8883 (71.94%)]\t\tLoss: 0.78480\n",
      "Training Progress: \tEpoch 41 [6720/8883 (75.54%)]\t\tLoss: 0.87870\n",
      "Training Progress: \tEpoch 41 [7040/8883 (79.14%)]\t\tLoss: 0.88195\n",
      "Training Progress: \tEpoch 41 [7360/8883 (82.73%)]\t\tLoss: 0.69337\n",
      "Training Progress: \tEpoch 41 [7680/8883 (86.33%)]\t\tLoss: 0.69257\n",
      "Training Progress: \tEpoch 41 [8000/8883 (89.93%)]\t\tLoss: 0.78787\n",
      "Training Progress: \tEpoch 41 [8320/8883 (93.53%)]\t\tLoss: 0.73375\n",
      "Training Progress: \tEpoch 41 [8640/8883 (97.12%)]\t\tLoss: 0.78687\n",
      "\tTrain loss: 0.01363, Accuracy: 7194/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1610/1692 (95.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1127/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/8883 (0.00%)]\t\tLoss: 0.75059\n",
      "Training Progress: \tEpoch 42 [320/8883 (3.60%)]\t\tLoss: 0.65489\n",
      "Training Progress: \tEpoch 42 [640/8883 (7.19%)]\t\tLoss: 0.54631\n",
      "Training Progress: \tEpoch 42 [960/8883 (10.79%)]\t\tLoss: 0.71081\n",
      "Training Progress: \tEpoch 42 [1280/8883 (14.39%)]\t\tLoss: 0.76945\n",
      "Training Progress: \tEpoch 42 [1600/8883 (17.99%)]\t\tLoss: 0.39973\n",
      "Training Progress: \tEpoch 42 [1920/8883 (21.58%)]\t\tLoss: 0.56765\n",
      "Training Progress: \tEpoch 42 [2240/8883 (25.18%)]\t\tLoss: 0.65418\n",
      "Training Progress: \tEpoch 42 [2560/8883 (28.78%)]\t\tLoss: 0.71719\n",
      "Training Progress: \tEpoch 42 [2880/8883 (32.37%)]\t\tLoss: 0.67628\n",
      "Training Progress: \tEpoch 42 [3200/8883 (35.97%)]\t\tLoss: 0.62786\n",
      "Training Progress: \tEpoch 42 [3520/8883 (39.57%)]\t\tLoss: 0.76837\n",
      "Training Progress: \tEpoch 42 [3840/8883 (43.17%)]\t\tLoss: 0.68802\n",
      "Training Progress: \tEpoch 42 [4160/8883 (46.76%)]\t\tLoss: 0.62256\n",
      "Training Progress: \tEpoch 42 [4480/8883 (50.36%)]\t\tLoss: 0.56344\n",
      "Training Progress: \tEpoch 42 [4800/8883 (53.96%)]\t\tLoss: 0.61839\n",
      "Training Progress: \tEpoch 42 [5120/8883 (57.55%)]\t\tLoss: 0.71327\n",
      "Training Progress: \tEpoch 42 [5440/8883 (61.15%)]\t\tLoss: 0.67069\n",
      "Training Progress: \tEpoch 42 [5760/8883 (64.75%)]\t\tLoss: 0.86281\n",
      "Training Progress: \tEpoch 42 [6080/8883 (68.35%)]\t\tLoss: 0.53710\n",
      "Training Progress: \tEpoch 42 [6400/8883 (71.94%)]\t\tLoss: 0.91235\n",
      "Training Progress: \tEpoch 42 [6720/8883 (75.54%)]\t\tLoss: 0.75646\n",
      "Training Progress: \tEpoch 42 [7040/8883 (79.14%)]\t\tLoss: 0.82007\n",
      "Training Progress: \tEpoch 42 [7360/8883 (82.73%)]\t\tLoss: 0.62025\n",
      "Training Progress: \tEpoch 42 [7680/8883 (86.33%)]\t\tLoss: 0.67680\n",
      "Training Progress: \tEpoch 42 [8000/8883 (89.93%)]\t\tLoss: 0.69967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [8320/8883 (93.53%)]\t\tLoss: 0.71996\n",
      "Training Progress: \tEpoch 42 [8640/8883 (97.12%)]\t\tLoss: 0.79826\n",
      "\tTrain loss: 0.01391, Accuracy: 7145/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1604/1692 (94.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1098/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/8883 (0.00%)]\t\tLoss: 0.67003\n",
      "Training Progress: \tEpoch 43 [320/8883 (3.60%)]\t\tLoss: 0.69657\n",
      "Training Progress: \tEpoch 43 [640/8883 (7.19%)]\t\tLoss: 0.67195\n",
      "Training Progress: \tEpoch 43 [960/8883 (10.79%)]\t\tLoss: 0.89104\n",
      "Training Progress: \tEpoch 43 [1280/8883 (14.39%)]\t\tLoss: 0.55395\n",
      "Training Progress: \tEpoch 43 [1600/8883 (17.99%)]\t\tLoss: 0.46098\n",
      "Training Progress: \tEpoch 43 [1920/8883 (21.58%)]\t\tLoss: 0.68635\n",
      "Training Progress: \tEpoch 43 [2240/8883 (25.18%)]\t\tLoss: 0.71420\n",
      "Training Progress: \tEpoch 43 [2560/8883 (28.78%)]\t\tLoss: 0.69589\n",
      "Training Progress: \tEpoch 43 [2880/8883 (32.37%)]\t\tLoss: 0.55570\n",
      "Training Progress: \tEpoch 43 [3200/8883 (35.97%)]\t\tLoss: 0.73104\n",
      "Training Progress: \tEpoch 43 [3520/8883 (39.57%)]\t\tLoss: 0.98148\n",
      "Training Progress: \tEpoch 43 [3840/8883 (43.17%)]\t\tLoss: 0.84549\n",
      "Training Progress: \tEpoch 43 [4160/8883 (46.76%)]\t\tLoss: 0.49163\n",
      "Training Progress: \tEpoch 43 [4480/8883 (50.36%)]\t\tLoss: 0.58391\n",
      "Training Progress: \tEpoch 43 [4800/8883 (53.96%)]\t\tLoss: 0.58485\n",
      "Training Progress: \tEpoch 43 [5120/8883 (57.55%)]\t\tLoss: 0.63111\n",
      "Training Progress: \tEpoch 43 [5440/8883 (61.15%)]\t\tLoss: 0.95331\n",
      "Training Progress: \tEpoch 43 [5760/8883 (64.75%)]\t\tLoss: 0.71132\n",
      "Training Progress: \tEpoch 43 [6080/8883 (68.35%)]\t\tLoss: 0.52485\n",
      "Training Progress: \tEpoch 43 [6400/8883 (71.94%)]\t\tLoss: 0.62999\n",
      "Training Progress: \tEpoch 43 [6720/8883 (75.54%)]\t\tLoss: 1.00342\n",
      "Training Progress: \tEpoch 43 [7040/8883 (79.14%)]\t\tLoss: 0.66153\n",
      "Training Progress: \tEpoch 43 [7360/8883 (82.73%)]\t\tLoss: 0.68658\n",
      "Training Progress: \tEpoch 43 [7680/8883 (86.33%)]\t\tLoss: 0.53076\n",
      "Training Progress: \tEpoch 43 [8000/8883 (89.93%)]\t\tLoss: 0.66555\n",
      "Training Progress: \tEpoch 43 [8320/8883 (93.53%)]\t\tLoss: 0.61379\n",
      "Training Progress: \tEpoch 43 [8640/8883 (97.12%)]\t\tLoss: 0.72142\n",
      "\tTrain loss: 0.01336, Accuracy: 7160/8883 (80.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1605/1692 (94.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1104/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/8883 (0.00%)]\t\tLoss: 0.64362\n",
      "Training Progress: \tEpoch 44 [320/8883 (3.60%)]\t\tLoss: 0.72359\n",
      "Training Progress: \tEpoch 44 [640/8883 (7.19%)]\t\tLoss: 0.48329\n",
      "Training Progress: \tEpoch 44 [960/8883 (10.79%)]\t\tLoss: 0.91063\n",
      "Training Progress: \tEpoch 44 [1280/8883 (14.39%)]\t\tLoss: 0.55806\n",
      "Training Progress: \tEpoch 44 [1600/8883 (17.99%)]\t\tLoss: 0.54334\n",
      "Training Progress: \tEpoch 44 [1920/8883 (21.58%)]\t\tLoss: 0.53778\n",
      "Training Progress: \tEpoch 44 [2240/8883 (25.18%)]\t\tLoss: 0.52760\n",
      "Training Progress: \tEpoch 44 [2560/8883 (28.78%)]\t\tLoss: 0.70093\n",
      "Training Progress: \tEpoch 44 [2880/8883 (32.37%)]\t\tLoss: 0.60617\n",
      "Training Progress: \tEpoch 44 [3200/8883 (35.97%)]\t\tLoss: 0.88984\n",
      "Training Progress: \tEpoch 44 [3520/8883 (39.57%)]\t\tLoss: 0.86313\n",
      "Training Progress: \tEpoch 44 [3840/8883 (43.17%)]\t\tLoss: 0.90873\n",
      "Training Progress: \tEpoch 44 [4160/8883 (46.76%)]\t\tLoss: 0.63761\n",
      "Training Progress: \tEpoch 44 [4480/8883 (50.36%)]\t\tLoss: 0.74245\n",
      "Training Progress: \tEpoch 44 [4800/8883 (53.96%)]\t\tLoss: 0.61193\n",
      "Training Progress: \tEpoch 44 [5120/8883 (57.55%)]\t\tLoss: 0.73143\n",
      "Training Progress: \tEpoch 44 [5440/8883 (61.15%)]\t\tLoss: 0.67276\n",
      "Training Progress: \tEpoch 44 [5760/8883 (64.75%)]\t\tLoss: 0.67049\n",
      "Training Progress: \tEpoch 44 [6080/8883 (68.35%)]\t\tLoss: 0.52560\n",
      "Training Progress: \tEpoch 44 [6400/8883 (71.94%)]\t\tLoss: 0.68338\n",
      "Training Progress: \tEpoch 44 [6720/8883 (75.54%)]\t\tLoss: 0.74721\n",
      "Training Progress: \tEpoch 44 [7040/8883 (79.14%)]\t\tLoss: 0.76361\n",
      "Training Progress: \tEpoch 44 [7360/8883 (82.73%)]\t\tLoss: 0.61129\n",
      "Training Progress: \tEpoch 44 [7680/8883 (86.33%)]\t\tLoss: 0.82545\n",
      "Training Progress: \tEpoch 44 [8000/8883 (89.93%)]\t\tLoss: 0.58311\n",
      "Training Progress: \tEpoch 44 [8320/8883 (93.53%)]\t\tLoss: 0.74813\n",
      "Training Progress: \tEpoch 44 [8640/8883 (97.12%)]\t\tLoss: 0.84823\n",
      "\tTrain loss: 0.01319, Accuracy: 7205/8883 (81.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1610/1692 (95.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1148/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/8883 (0.00%)]\t\tLoss: 0.80067\n",
      "Training Progress: \tEpoch 45 [320/8883 (3.60%)]\t\tLoss: 0.88337\n",
      "Training Progress: \tEpoch 45 [640/8883 (7.19%)]\t\tLoss: 0.54409\n",
      "Training Progress: \tEpoch 45 [960/8883 (10.79%)]\t\tLoss: 0.73643\n",
      "Training Progress: \tEpoch 45 [1280/8883 (14.39%)]\t\tLoss: 0.56489\n",
      "Training Progress: \tEpoch 45 [1600/8883 (17.99%)]\t\tLoss: 0.49766\n",
      "Training Progress: \tEpoch 45 [1920/8883 (21.58%)]\t\tLoss: 0.45171\n",
      "Training Progress: \tEpoch 45 [2240/8883 (25.18%)]\t\tLoss: 0.75908\n",
      "Training Progress: \tEpoch 45 [2560/8883 (28.78%)]\t\tLoss: 0.84110\n",
      "Training Progress: \tEpoch 45 [2880/8883 (32.37%)]\t\tLoss: 0.71315\n",
      "Training Progress: \tEpoch 45 [3200/8883 (35.97%)]\t\tLoss: 0.73980\n",
      "Training Progress: \tEpoch 45 [3520/8883 (39.57%)]\t\tLoss: 0.96137\n",
      "Training Progress: \tEpoch 45 [3840/8883 (43.17%)]\t\tLoss: 0.66449\n",
      "Training Progress: \tEpoch 45 [4160/8883 (46.76%)]\t\tLoss: 0.72897\n",
      "Training Progress: \tEpoch 45 [4480/8883 (50.36%)]\t\tLoss: 0.40012\n",
      "Training Progress: \tEpoch 45 [4800/8883 (53.96%)]\t\tLoss: 0.80307\n",
      "Training Progress: \tEpoch 45 [5120/8883 (57.55%)]\t\tLoss: 0.69351\n",
      "Training Progress: \tEpoch 45 [5440/8883 (61.15%)]\t\tLoss: 0.85347\n",
      "Training Progress: \tEpoch 45 [5760/8883 (64.75%)]\t\tLoss: 0.61287\n",
      "Training Progress: \tEpoch 45 [6080/8883 (68.35%)]\t\tLoss: 0.56755\n",
      "Training Progress: \tEpoch 45 [6400/8883 (71.94%)]\t\tLoss: 0.57479\n",
      "Training Progress: \tEpoch 45 [6720/8883 (75.54%)]\t\tLoss: 0.73473\n",
      "Training Progress: \tEpoch 45 [7040/8883 (79.14%)]\t\tLoss: 0.88198\n",
      "Training Progress: \tEpoch 45 [7360/8883 (82.73%)]\t\tLoss: 0.71593\n",
      "Training Progress: \tEpoch 45 [7680/8883 (86.33%)]\t\tLoss: 0.54365\n",
      "Training Progress: \tEpoch 45 [8000/8883 (89.93%)]\t\tLoss: 0.64262\n",
      "Training Progress: \tEpoch 45 [8320/8883 (93.53%)]\t\tLoss: 0.63858\n",
      "Training Progress: \tEpoch 45 [8640/8883 (97.12%)]\t\tLoss: 0.61730\n",
      "\tTrain loss: 0.01288, Accuracy: 7243/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1621/1692 (95.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1135/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/8883 (0.00%)]\t\tLoss: 0.47157\n",
      "Training Progress: \tEpoch 46 [320/8883 (3.60%)]\t\tLoss: 0.67571\n",
      "Training Progress: \tEpoch 46 [640/8883 (7.19%)]\t\tLoss: 0.53943\n",
      "Training Progress: \tEpoch 46 [960/8883 (10.79%)]\t\tLoss: 0.98531\n",
      "Training Progress: \tEpoch 46 [1280/8883 (14.39%)]\t\tLoss: 0.67816\n",
      "Training Progress: \tEpoch 46 [1600/8883 (17.99%)]\t\tLoss: 0.66628\n",
      "Training Progress: \tEpoch 46 [1920/8883 (21.58%)]\t\tLoss: 0.58170\n",
      "Training Progress: \tEpoch 46 [2240/8883 (25.18%)]\t\tLoss: 0.50938\n",
      "Training Progress: \tEpoch 46 [2560/8883 (28.78%)]\t\tLoss: 0.58222\n",
      "Training Progress: \tEpoch 46 [2880/8883 (32.37%)]\t\tLoss: 0.54754\n",
      "Training Progress: \tEpoch 46 [3200/8883 (35.97%)]\t\tLoss: 0.68087\n",
      "Training Progress: \tEpoch 46 [3520/8883 (39.57%)]\t\tLoss: 1.01462\n",
      "Training Progress: \tEpoch 46 [3840/8883 (43.17%)]\t\tLoss: 0.74143\n",
      "Training Progress: \tEpoch 46 [4160/8883 (46.76%)]\t\tLoss: 0.63690\n",
      "Training Progress: \tEpoch 46 [4480/8883 (50.36%)]\t\tLoss: 0.56924\n",
      "Training Progress: \tEpoch 46 [4800/8883 (53.96%)]\t\tLoss: 0.65387\n",
      "Training Progress: \tEpoch 46 [5120/8883 (57.55%)]\t\tLoss: 0.79412\n",
      "Training Progress: \tEpoch 46 [5440/8883 (61.15%)]\t\tLoss: 0.71375\n",
      "Training Progress: \tEpoch 46 [5760/8883 (64.75%)]\t\tLoss: 0.80984\n",
      "Training Progress: \tEpoch 46 [6080/8883 (68.35%)]\t\tLoss: 0.53477\n",
      "Training Progress: \tEpoch 46 [6400/8883 (71.94%)]\t\tLoss: 0.54838\n",
      "Training Progress: \tEpoch 46 [6720/8883 (75.54%)]\t\tLoss: 0.66829\n",
      "Training Progress: \tEpoch 46 [7040/8883 (79.14%)]\t\tLoss: 0.77777\n",
      "Training Progress: \tEpoch 46 [7360/8883 (82.73%)]\t\tLoss: 0.91527\n",
      "Training Progress: \tEpoch 46 [7680/8883 (86.33%)]\t\tLoss: 0.67413\n",
      "Training Progress: \tEpoch 46 [8000/8883 (89.93%)]\t\tLoss: 0.71973\n",
      "Training Progress: \tEpoch 46 [8320/8883 (93.53%)]\t\tLoss: 0.66707\n",
      "Training Progress: \tEpoch 46 [8640/8883 (97.12%)]\t\tLoss: 0.66925\n",
      "\tTrain loss: 0.01313, Accuracy: 7216/8883 (81.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1624/1692 (95.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1129/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/8883 (0.00%)]\t\tLoss: 0.74405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 47 [320/8883 (3.60%)]\t\tLoss: 0.64505\n",
      "Training Progress: \tEpoch 47 [640/8883 (7.19%)]\t\tLoss: 0.40635\n",
      "Training Progress: \tEpoch 47 [960/8883 (10.79%)]\t\tLoss: 0.87863\n",
      "Training Progress: \tEpoch 47 [1280/8883 (14.39%)]\t\tLoss: 0.90130\n",
      "Training Progress: \tEpoch 47 [1600/8883 (17.99%)]\t\tLoss: 0.42168\n",
      "Training Progress: \tEpoch 47 [1920/8883 (21.58%)]\t\tLoss: 0.45186\n",
      "Training Progress: \tEpoch 47 [2240/8883 (25.18%)]\t\tLoss: 0.47823\n",
      "Training Progress: \tEpoch 47 [2560/8883 (28.78%)]\t\tLoss: 0.52274\n",
      "Training Progress: \tEpoch 47 [2880/8883 (32.37%)]\t\tLoss: 0.57463\n",
      "Training Progress: \tEpoch 47 [3200/8883 (35.97%)]\t\tLoss: 0.65886\n",
      "Training Progress: \tEpoch 47 [3520/8883 (39.57%)]\t\tLoss: 0.73390\n",
      "Training Progress: \tEpoch 47 [3840/8883 (43.17%)]\t\tLoss: 0.78182\n",
      "Training Progress: \tEpoch 47 [4160/8883 (46.76%)]\t\tLoss: 0.56890\n",
      "Training Progress: \tEpoch 47 [4480/8883 (50.36%)]\t\tLoss: 0.52192\n",
      "Training Progress: \tEpoch 47 [4800/8883 (53.96%)]\t\tLoss: 0.70596\n",
      "Training Progress: \tEpoch 47 [5120/8883 (57.55%)]\t\tLoss: 0.85787\n",
      "Training Progress: \tEpoch 47 [5440/8883 (61.15%)]\t\tLoss: 0.80464\n",
      "Training Progress: \tEpoch 47 [5760/8883 (64.75%)]\t\tLoss: 0.60797\n",
      "Training Progress: \tEpoch 47 [6080/8883 (68.35%)]\t\tLoss: 0.58228\n",
      "Training Progress: \tEpoch 47 [6400/8883 (71.94%)]\t\tLoss: 0.53450\n",
      "Training Progress: \tEpoch 47 [6720/8883 (75.54%)]\t\tLoss: 0.73265\n",
      "Training Progress: \tEpoch 47 [7040/8883 (79.14%)]\t\tLoss: 0.77479\n",
      "Training Progress: \tEpoch 47 [7360/8883 (82.73%)]\t\tLoss: 0.64428\n",
      "Training Progress: \tEpoch 47 [7680/8883 (86.33%)]\t\tLoss: 0.50874\n",
      "Training Progress: \tEpoch 47 [8000/8883 (89.93%)]\t\tLoss: 0.67981\n",
      "Training Progress: \tEpoch 47 [8320/8883 (93.53%)]\t\tLoss: 0.61476\n",
      "Training Progress: \tEpoch 47 [8640/8883 (97.12%)]\t\tLoss: 0.85251\n",
      "\tTrain loss: 0.01291, Accuracy: 7207/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1622/1692 (95.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1142/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/8883 (0.00%)]\t\tLoss: 0.51343\n",
      "Training Progress: \tEpoch 48 [320/8883 (3.60%)]\t\tLoss: 0.78050\n",
      "Training Progress: \tEpoch 48 [640/8883 (7.19%)]\t\tLoss: 0.77288\n",
      "Training Progress: \tEpoch 48 [960/8883 (10.79%)]\t\tLoss: 0.75565\n",
      "Training Progress: \tEpoch 48 [1280/8883 (14.39%)]\t\tLoss: 0.89687\n",
      "Training Progress: \tEpoch 48 [1600/8883 (17.99%)]\t\tLoss: 0.51454\n",
      "Training Progress: \tEpoch 48 [1920/8883 (21.58%)]\t\tLoss: 0.43400\n",
      "Training Progress: \tEpoch 48 [2240/8883 (25.18%)]\t\tLoss: 0.53058\n",
      "Training Progress: \tEpoch 48 [2560/8883 (28.78%)]\t\tLoss: 0.77065\n",
      "Training Progress: \tEpoch 48 [2880/8883 (32.37%)]\t\tLoss: 0.48650\n",
      "Training Progress: \tEpoch 48 [3200/8883 (35.97%)]\t\tLoss: 0.89975\n",
      "Training Progress: \tEpoch 48 [3520/8883 (39.57%)]\t\tLoss: 0.91682\n",
      "Training Progress: \tEpoch 48 [3840/8883 (43.17%)]\t\tLoss: 0.81786\n",
      "Training Progress: \tEpoch 48 [4160/8883 (46.76%)]\t\tLoss: 0.71588\n",
      "Training Progress: \tEpoch 48 [4480/8883 (50.36%)]\t\tLoss: 0.54443\n",
      "Training Progress: \tEpoch 48 [4800/8883 (53.96%)]\t\tLoss: 0.85576\n",
      "Training Progress: \tEpoch 48 [5120/8883 (57.55%)]\t\tLoss: 0.90537\n",
      "Training Progress: \tEpoch 48 [5440/8883 (61.15%)]\t\tLoss: 0.67043\n",
      "Training Progress: \tEpoch 48 [5760/8883 (64.75%)]\t\tLoss: 0.65811\n",
      "Training Progress: \tEpoch 48 [6080/8883 (68.35%)]\t\tLoss: 0.63838\n",
      "Training Progress: \tEpoch 48 [6400/8883 (71.94%)]\t\tLoss: 0.68263\n",
      "Training Progress: \tEpoch 48 [6720/8883 (75.54%)]\t\tLoss: 0.79180\n",
      "Training Progress: \tEpoch 48 [7040/8883 (79.14%)]\t\tLoss: 0.64061\n",
      "Training Progress: \tEpoch 48 [7360/8883 (82.73%)]\t\tLoss: 0.74712\n",
      "Training Progress: \tEpoch 48 [7680/8883 (86.33%)]\t\tLoss: 0.45230\n",
      "Training Progress: \tEpoch 48 [8000/8883 (89.93%)]\t\tLoss: 0.67523\n",
      "Training Progress: \tEpoch 48 [8320/8883 (93.53%)]\t\tLoss: 0.80630\n",
      "Training Progress: \tEpoch 48 [8640/8883 (97.12%)]\t\tLoss: 0.82570\n",
      "\tTrain loss: 0.01306, Accuracy: 7214/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1626/1692 (96.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1127/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/8883 (0.00%)]\t\tLoss: 0.74106\n",
      "Training Progress: \tEpoch 49 [320/8883 (3.60%)]\t\tLoss: 0.81342\n",
      "Training Progress: \tEpoch 49 [640/8883 (7.19%)]\t\tLoss: 0.49039\n",
      "Training Progress: \tEpoch 49 [960/8883 (10.79%)]\t\tLoss: 0.54565\n",
      "Training Progress: \tEpoch 49 [1280/8883 (14.39%)]\t\tLoss: 0.90817\n",
      "Training Progress: \tEpoch 49 [1600/8883 (17.99%)]\t\tLoss: 0.64122\n",
      "Training Progress: \tEpoch 49 [1920/8883 (21.58%)]\t\tLoss: 0.60451\n",
      "Training Progress: \tEpoch 49 [2240/8883 (25.18%)]\t\tLoss: 0.68245\n",
      "Training Progress: \tEpoch 49 [2560/8883 (28.78%)]\t\tLoss: 0.68547\n",
      "Training Progress: \tEpoch 49 [2880/8883 (32.37%)]\t\tLoss: 0.73786\n",
      "Training Progress: \tEpoch 49 [3200/8883 (35.97%)]\t\tLoss: 0.61972\n",
      "Training Progress: \tEpoch 49 [3520/8883 (39.57%)]\t\tLoss: 0.87739\n",
      "Training Progress: \tEpoch 49 [3840/8883 (43.17%)]\t\tLoss: 0.55339\n",
      "Training Progress: \tEpoch 49 [4160/8883 (46.76%)]\t\tLoss: 0.89114\n",
      "Training Progress: \tEpoch 49 [4480/8883 (50.36%)]\t\tLoss: 0.53343\n",
      "Training Progress: \tEpoch 49 [4800/8883 (53.96%)]\t\tLoss: 0.62283\n",
      "Training Progress: \tEpoch 49 [5120/8883 (57.55%)]\t\tLoss: 0.78192\n",
      "Training Progress: \tEpoch 49 [5440/8883 (61.15%)]\t\tLoss: 0.65551\n",
      "Training Progress: \tEpoch 49 [5760/8883 (64.75%)]\t\tLoss: 0.73808\n",
      "Training Progress: \tEpoch 49 [6080/8883 (68.35%)]\t\tLoss: 0.39331\n",
      "Training Progress: \tEpoch 49 [6400/8883 (71.94%)]\t\tLoss: 0.54784\n",
      "Training Progress: \tEpoch 49 [6720/8883 (75.54%)]\t\tLoss: 0.87294\n",
      "Training Progress: \tEpoch 49 [7040/8883 (79.14%)]\t\tLoss: 0.69171\n",
      "Training Progress: \tEpoch 49 [7360/8883 (82.73%)]\t\tLoss: 0.67362\n",
      "Training Progress: \tEpoch 49 [7680/8883 (86.33%)]\t\tLoss: 0.53496\n",
      "Training Progress: \tEpoch 49 [8000/8883 (89.93%)]\t\tLoss: 0.63693\n",
      "Training Progress: \tEpoch 49 [8320/8883 (93.53%)]\t\tLoss: 1.06399\n",
      "Training Progress: \tEpoch 49 [8640/8883 (97.12%)]\t\tLoss: 0.79550\n",
      "\tTrain loss: 0.01285, Accuracy: 7261/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1639/1692 (96.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1117/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/8883 (0.00%)]\t\tLoss: 0.75505\n",
      "Training Progress: \tEpoch 50 [320/8883 (3.60%)]\t\tLoss: 0.72619\n",
      "Training Progress: \tEpoch 50 [640/8883 (7.19%)]\t\tLoss: 0.57299\n",
      "Training Progress: \tEpoch 50 [960/8883 (10.79%)]\t\tLoss: 0.81679\n",
      "Training Progress: \tEpoch 50 [1280/8883 (14.39%)]\t\tLoss: 0.67694\n",
      "Training Progress: \tEpoch 50 [1600/8883 (17.99%)]\t\tLoss: 0.50019\n",
      "Training Progress: \tEpoch 50 [1920/8883 (21.58%)]\t\tLoss: 0.48484\n",
      "Training Progress: \tEpoch 50 [2240/8883 (25.18%)]\t\tLoss: 0.47893\n",
      "Training Progress: \tEpoch 50 [2560/8883 (28.78%)]\t\tLoss: 0.56876\n",
      "Training Progress: \tEpoch 50 [2880/8883 (32.37%)]\t\tLoss: 0.55939\n",
      "Training Progress: \tEpoch 50 [3200/8883 (35.97%)]\t\tLoss: 0.82962\n",
      "Training Progress: \tEpoch 50 [3520/8883 (39.57%)]\t\tLoss: 0.77828\n",
      "Training Progress: \tEpoch 50 [3840/8883 (43.17%)]\t\tLoss: 0.85282\n",
      "Training Progress: \tEpoch 50 [4160/8883 (46.76%)]\t\tLoss: 0.69265\n",
      "Training Progress: \tEpoch 50 [4480/8883 (50.36%)]\t\tLoss: 0.52964\n",
      "Training Progress: \tEpoch 50 [4800/8883 (53.96%)]\t\tLoss: 0.57187\n",
      "Training Progress: \tEpoch 50 [5120/8883 (57.55%)]\t\tLoss: 0.92870\n",
      "Training Progress: \tEpoch 50 [5440/8883 (61.15%)]\t\tLoss: 0.65518\n",
      "Training Progress: \tEpoch 50 [5760/8883 (64.75%)]\t\tLoss: 0.75619\n",
      "Training Progress: \tEpoch 50 [6080/8883 (68.35%)]\t\tLoss: 0.43945\n",
      "Training Progress: \tEpoch 50 [6400/8883 (71.94%)]\t\tLoss: 0.62242\n",
      "Training Progress: \tEpoch 50 [6720/8883 (75.54%)]\t\tLoss: 0.86493\n",
      "Training Progress: \tEpoch 50 [7040/8883 (79.14%)]\t\tLoss: 0.69247\n",
      "Training Progress: \tEpoch 50 [7360/8883 (82.73%)]\t\tLoss: 0.70712\n",
      "Training Progress: \tEpoch 50 [7680/8883 (86.33%)]\t\tLoss: 0.49199\n",
      "Training Progress: \tEpoch 50 [8000/8883 (89.93%)]\t\tLoss: 0.65093\n",
      "Training Progress: \tEpoch 50 [8320/8883 (93.53%)]\t\tLoss: 0.63918\n",
      "Training Progress: \tEpoch 50 [8640/8883 (97.12%)]\t\tLoss: 0.75631\n",
      "\tTrain loss: 0.01271, Accuracy: 7215/8883 (81.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1636/1692 (96.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1092/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/8883 (0.00%)]\t\tLoss: 0.62348\n",
      "Training Progress: \tEpoch 51 [320/8883 (3.60%)]\t\tLoss: 0.80256\n",
      "Training Progress: \tEpoch 51 [640/8883 (7.19%)]\t\tLoss: 0.28942\n",
      "Training Progress: \tEpoch 51 [960/8883 (10.79%)]\t\tLoss: 0.64947\n",
      "Training Progress: \tEpoch 51 [1280/8883 (14.39%)]\t\tLoss: 0.66888\n",
      "Training Progress: \tEpoch 51 [1600/8883 (17.99%)]\t\tLoss: 0.48314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 51 [1920/8883 (21.58%)]\t\tLoss: 0.57636\n",
      "Training Progress: \tEpoch 51 [2240/8883 (25.18%)]\t\tLoss: 0.71163\n",
      "Training Progress: \tEpoch 51 [2560/8883 (28.78%)]\t\tLoss: 0.67725\n",
      "Training Progress: \tEpoch 51 [2880/8883 (32.37%)]\t\tLoss: 0.67802\n",
      "Training Progress: \tEpoch 51 [3200/8883 (35.97%)]\t\tLoss: 0.90945\n",
      "Training Progress: \tEpoch 51 [3520/8883 (39.57%)]\t\tLoss: 0.65356\n",
      "Training Progress: \tEpoch 51 [3840/8883 (43.17%)]\t\tLoss: 0.78765\n",
      "Training Progress: \tEpoch 51 [4160/8883 (46.76%)]\t\tLoss: 0.72024\n",
      "Training Progress: \tEpoch 51 [4480/8883 (50.36%)]\t\tLoss: 0.58471\n",
      "Training Progress: \tEpoch 51 [4800/8883 (53.96%)]\t\tLoss: 0.65351\n",
      "Training Progress: \tEpoch 51 [5120/8883 (57.55%)]\t\tLoss: 0.70050\n",
      "Training Progress: \tEpoch 51 [5440/8883 (61.15%)]\t\tLoss: 0.61169\n",
      "Training Progress: \tEpoch 51 [5760/8883 (64.75%)]\t\tLoss: 0.75657\n",
      "Training Progress: \tEpoch 51 [6080/8883 (68.35%)]\t\tLoss: 0.51303\n",
      "Training Progress: \tEpoch 51 [6400/8883 (71.94%)]\t\tLoss: 0.55522\n",
      "Training Progress: \tEpoch 51 [6720/8883 (75.54%)]\t\tLoss: 0.72664\n",
      "Training Progress: \tEpoch 51 [7040/8883 (79.14%)]\t\tLoss: 0.70594\n",
      "Training Progress: \tEpoch 51 [7360/8883 (82.73%)]\t\tLoss: 0.77582\n",
      "Training Progress: \tEpoch 51 [7680/8883 (86.33%)]\t\tLoss: 0.60101\n",
      "Training Progress: \tEpoch 51 [8000/8883 (89.93%)]\t\tLoss: 0.78799\n",
      "Training Progress: \tEpoch 51 [8320/8883 (93.53%)]\t\tLoss: 0.56227\n",
      "Training Progress: \tEpoch 51 [8640/8883 (97.12%)]\t\tLoss: 0.61620\n",
      "\tTrain loss: 0.01264, Accuracy: 7238/8883 (81.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1122/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/8883 (0.00%)]\t\tLoss: 0.64232\n",
      "Training Progress: \tEpoch 52 [320/8883 (3.60%)]\t\tLoss: 0.61243\n",
      "Training Progress: \tEpoch 52 [640/8883 (7.19%)]\t\tLoss: 0.78392\n",
      "Training Progress: \tEpoch 52 [960/8883 (10.79%)]\t\tLoss: 0.85382\n",
      "Training Progress: \tEpoch 52 [1280/8883 (14.39%)]\t\tLoss: 0.71787\n",
      "Training Progress: \tEpoch 52 [1600/8883 (17.99%)]\t\tLoss: 0.47057\n",
      "Training Progress: \tEpoch 52 [1920/8883 (21.58%)]\t\tLoss: 0.37921\n",
      "Training Progress: \tEpoch 52 [2240/8883 (25.18%)]\t\tLoss: 0.67151\n",
      "Training Progress: \tEpoch 52 [2560/8883 (28.78%)]\t\tLoss: 0.76821\n",
      "Training Progress: \tEpoch 52 [2880/8883 (32.37%)]\t\tLoss: 0.63254\n",
      "Training Progress: \tEpoch 52 [3200/8883 (35.97%)]\t\tLoss: 0.51718\n",
      "Training Progress: \tEpoch 52 [3520/8883 (39.57%)]\t\tLoss: 0.85379\n",
      "Training Progress: \tEpoch 52 [3840/8883 (43.17%)]\t\tLoss: 1.05008\n",
      "Training Progress: \tEpoch 52 [4160/8883 (46.76%)]\t\tLoss: 0.73082\n",
      "Training Progress: \tEpoch 52 [4480/8883 (50.36%)]\t\tLoss: 0.62225\n",
      "Training Progress: \tEpoch 52 [4800/8883 (53.96%)]\t\tLoss: 0.71553\n",
      "Training Progress: \tEpoch 52 [5120/8883 (57.55%)]\t\tLoss: 0.78919\n",
      "Training Progress: \tEpoch 52 [5440/8883 (61.15%)]\t\tLoss: 0.97842\n",
      "Training Progress: \tEpoch 52 [5760/8883 (64.75%)]\t\tLoss: 0.79507\n",
      "Training Progress: \tEpoch 52 [6080/8883 (68.35%)]\t\tLoss: 0.58313\n",
      "Training Progress: \tEpoch 52 [6400/8883 (71.94%)]\t\tLoss: 0.64074\n",
      "Training Progress: \tEpoch 52 [6720/8883 (75.54%)]\t\tLoss: 0.77729\n",
      "Training Progress: \tEpoch 52 [7040/8883 (79.14%)]\t\tLoss: 1.00873\n",
      "Training Progress: \tEpoch 52 [7360/8883 (82.73%)]\t\tLoss: 0.84764\n",
      "Training Progress: \tEpoch 52 [7680/8883 (86.33%)]\t\tLoss: 0.63378\n",
      "Training Progress: \tEpoch 52 [8000/8883 (89.93%)]\t\tLoss: 0.74766\n",
      "Training Progress: \tEpoch 52 [8320/8883 (93.53%)]\t\tLoss: 0.74093\n",
      "Training Progress: \tEpoch 52 [8640/8883 (97.12%)]\t\tLoss: 0.80857\n",
      "\tTrain loss: 0.01279, Accuracy: 7227/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1619/1692 (95.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1151/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/8883 (0.00%)]\t\tLoss: 0.58668\n",
      "Training Progress: \tEpoch 53 [320/8883 (3.60%)]\t\tLoss: 0.59396\n",
      "Training Progress: \tEpoch 53 [640/8883 (7.19%)]\t\tLoss: 0.55661\n",
      "Training Progress: \tEpoch 53 [960/8883 (10.79%)]\t\tLoss: 0.65784\n",
      "Training Progress: \tEpoch 53 [1280/8883 (14.39%)]\t\tLoss: 0.63371\n",
      "Training Progress: \tEpoch 53 [1600/8883 (17.99%)]\t\tLoss: 0.29141\n",
      "Training Progress: \tEpoch 53 [1920/8883 (21.58%)]\t\tLoss: 0.56266\n",
      "Training Progress: \tEpoch 53 [2240/8883 (25.18%)]\t\tLoss: 0.57494\n",
      "Training Progress: \tEpoch 53 [2560/8883 (28.78%)]\t\tLoss: 0.73248\n",
      "Training Progress: \tEpoch 53 [2880/8883 (32.37%)]\t\tLoss: 0.58544\n",
      "Training Progress: \tEpoch 53 [3200/8883 (35.97%)]\t\tLoss: 0.69008\n",
      "Training Progress: \tEpoch 53 [3520/8883 (39.57%)]\t\tLoss: 0.72567\n",
      "Training Progress: \tEpoch 53 [3840/8883 (43.17%)]\t\tLoss: 0.60523\n",
      "Training Progress: \tEpoch 53 [4160/8883 (46.76%)]\t\tLoss: 0.45825\n",
      "Training Progress: \tEpoch 53 [4480/8883 (50.36%)]\t\tLoss: 0.62736\n",
      "Training Progress: \tEpoch 53 [4800/8883 (53.96%)]\t\tLoss: 0.64158\n",
      "Training Progress: \tEpoch 53 [5120/8883 (57.55%)]\t\tLoss: 0.94536\n",
      "Training Progress: \tEpoch 53 [5440/8883 (61.15%)]\t\tLoss: 0.80082\n",
      "Training Progress: \tEpoch 53 [5760/8883 (64.75%)]\t\tLoss: 0.52608\n",
      "Training Progress: \tEpoch 53 [6080/8883 (68.35%)]\t\tLoss: 0.41176\n",
      "Training Progress: \tEpoch 53 [6400/8883 (71.94%)]\t\tLoss: 0.59810\n",
      "Training Progress: \tEpoch 53 [6720/8883 (75.54%)]\t\tLoss: 0.68340\n",
      "Training Progress: \tEpoch 53 [7040/8883 (79.14%)]\t\tLoss: 0.73520\n",
      "Training Progress: \tEpoch 53 [7360/8883 (82.73%)]\t\tLoss: 0.66656\n",
      "Training Progress: \tEpoch 53 [7680/8883 (86.33%)]\t\tLoss: 0.54409\n",
      "Training Progress: \tEpoch 53 [8000/8883 (89.93%)]\t\tLoss: 0.58519\n",
      "Training Progress: \tEpoch 53 [8320/8883 (93.53%)]\t\tLoss: 0.85751\n",
      "Training Progress: \tEpoch 53 [8640/8883 (97.12%)]\t\tLoss: 0.80910\n",
      "\tTrain loss: 0.01243, Accuracy: 7269/8883 (81.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1122/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/8883 (0.00%)]\t\tLoss: 0.64131\n",
      "Training Progress: \tEpoch 54 [320/8883 (3.60%)]\t\tLoss: 0.56726\n",
      "Training Progress: \tEpoch 54 [640/8883 (7.19%)]\t\tLoss: 0.40138\n",
      "Training Progress: \tEpoch 54 [960/8883 (10.79%)]\t\tLoss: 0.65113\n",
      "Training Progress: \tEpoch 54 [1280/8883 (14.39%)]\t\tLoss: 0.57371\n",
      "Training Progress: \tEpoch 54 [1600/8883 (17.99%)]\t\tLoss: 0.57542\n",
      "Training Progress: \tEpoch 54 [1920/8883 (21.58%)]\t\tLoss: 0.48050\n",
      "Training Progress: \tEpoch 54 [2240/8883 (25.18%)]\t\tLoss: 0.49601\n",
      "Training Progress: \tEpoch 54 [2560/8883 (28.78%)]\t\tLoss: 0.54668\n",
      "Training Progress: \tEpoch 54 [2880/8883 (32.37%)]\t\tLoss: 0.74797\n",
      "Training Progress: \tEpoch 54 [3200/8883 (35.97%)]\t\tLoss: 0.59776\n",
      "Training Progress: \tEpoch 54 [3520/8883 (39.57%)]\t\tLoss: 0.63921\n",
      "Training Progress: \tEpoch 54 [3840/8883 (43.17%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 54 [4160/8883 (46.76%)]\t\tLoss: 0.55644\n",
      "Training Progress: \tEpoch 54 [4480/8883 (50.36%)]\t\tLoss: 0.50741\n",
      "Training Progress: \tEpoch 54 [4800/8883 (53.96%)]\t\tLoss: 0.65934\n",
      "Training Progress: \tEpoch 54 [5120/8883 (57.55%)]\t\tLoss: 0.64966\n",
      "Training Progress: \tEpoch 54 [5440/8883 (61.15%)]\t\tLoss: 0.74703\n",
      "Training Progress: \tEpoch 54 [5760/8883 (64.75%)]\t\tLoss: 0.66855\n",
      "Training Progress: \tEpoch 54 [6080/8883 (68.35%)]\t\tLoss: 0.58194\n",
      "Training Progress: \tEpoch 54 [6400/8883 (71.94%)]\t\tLoss: 0.63982\n",
      "Training Progress: \tEpoch 54 [6720/8883 (75.54%)]\t\tLoss: 0.64715\n",
      "Training Progress: \tEpoch 54 [7040/8883 (79.14%)]\t\tLoss: 0.76980\n",
      "Training Progress: \tEpoch 54 [7360/8883 (82.73%)]\t\tLoss: 0.61165\n",
      "Training Progress: \tEpoch 54 [7680/8883 (86.33%)]\t\tLoss: 0.66316\n",
      "Training Progress: \tEpoch 54 [8000/8883 (89.93%)]\t\tLoss: 0.74549\n",
      "Training Progress: \tEpoch 54 [8320/8883 (93.53%)]\t\tLoss: 0.74274\n",
      "Training Progress: \tEpoch 54 [8640/8883 (97.12%)]\t\tLoss: 0.73837\n",
      "\tTrain loss: 0.01238, Accuracy: 7265/8883 (81.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1650/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1150/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/8883 (0.00%)]\t\tLoss: 0.49961\n",
      "Training Progress: \tEpoch 55 [320/8883 (3.60%)]\t\tLoss: 0.60084\n",
      "Training Progress: \tEpoch 55 [640/8883 (7.19%)]\t\tLoss: 0.50812\n",
      "Training Progress: \tEpoch 55 [960/8883 (10.79%)]\t\tLoss: 0.58969\n",
      "Training Progress: \tEpoch 55 [1280/8883 (14.39%)]\t\tLoss: 0.63395\n",
      "Training Progress: \tEpoch 55 [1600/8883 (17.99%)]\t\tLoss: 0.43145\n",
      "Training Progress: \tEpoch 55 [1920/8883 (21.58%)]\t\tLoss: 0.46430\n",
      "Training Progress: \tEpoch 55 [2240/8883 (25.18%)]\t\tLoss: 0.69441\n",
      "Training Progress: \tEpoch 55 [2560/8883 (28.78%)]\t\tLoss: 0.73779\n",
      "Training Progress: \tEpoch 55 [2880/8883 (32.37%)]\t\tLoss: 0.67773\n",
      "Training Progress: \tEpoch 55 [3200/8883 (35.97%)]\t\tLoss: 0.55410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 55 [3520/8883 (39.57%)]\t\tLoss: 0.70545\n",
      "Training Progress: \tEpoch 55 [3840/8883 (43.17%)]\t\tLoss: 0.84217\n",
      "Training Progress: \tEpoch 55 [4160/8883 (46.76%)]\t\tLoss: 0.43795\n",
      "Training Progress: \tEpoch 55 [4480/8883 (50.36%)]\t\tLoss: 0.42842\n",
      "Training Progress: \tEpoch 55 [4800/8883 (53.96%)]\t\tLoss: 0.79525\n",
      "Training Progress: \tEpoch 55 [5120/8883 (57.55%)]\t\tLoss: 0.60960\n",
      "Training Progress: \tEpoch 55 [5440/8883 (61.15%)]\t\tLoss: 0.63651\n",
      "Training Progress: \tEpoch 55 [5760/8883 (64.75%)]\t\tLoss: 0.55919\n",
      "Training Progress: \tEpoch 55 [6080/8883 (68.35%)]\t\tLoss: 0.58876\n",
      "Training Progress: \tEpoch 55 [6400/8883 (71.94%)]\t\tLoss: 0.38489\n",
      "Training Progress: \tEpoch 55 [6720/8883 (75.54%)]\t\tLoss: 0.71555\n",
      "Training Progress: \tEpoch 55 [7040/8883 (79.14%)]\t\tLoss: 0.71854\n",
      "Training Progress: \tEpoch 55 [7360/8883 (82.73%)]\t\tLoss: 0.69290\n",
      "Training Progress: \tEpoch 55 [7680/8883 (86.33%)]\t\tLoss: 0.65035\n",
      "Training Progress: \tEpoch 55 [8000/8883 (89.93%)]\t\tLoss: 0.51446\n",
      "Training Progress: \tEpoch 55 [8320/8883 (93.53%)]\t\tLoss: 0.70202\n",
      "Training Progress: \tEpoch 55 [8640/8883 (97.12%)]\t\tLoss: 0.84153\n",
      "\tTrain loss: 0.01216, Accuracy: 7239/8883 (81.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1645/1692 (97.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1117/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/8883 (0.00%)]\t\tLoss: 0.41577\n",
      "Training Progress: \tEpoch 56 [320/8883 (3.60%)]\t\tLoss: 0.75011\n",
      "Training Progress: \tEpoch 56 [640/8883 (7.19%)]\t\tLoss: 0.38968\n",
      "Training Progress: \tEpoch 56 [960/8883 (10.79%)]\t\tLoss: 0.59496\n",
      "Training Progress: \tEpoch 56 [1280/8883 (14.39%)]\t\tLoss: 0.78591\n",
      "Training Progress: \tEpoch 56 [1600/8883 (17.99%)]\t\tLoss: 0.45083\n",
      "Training Progress: \tEpoch 56 [1920/8883 (21.58%)]\t\tLoss: 0.56610\n",
      "Training Progress: \tEpoch 56 [2240/8883 (25.18%)]\t\tLoss: 0.47771\n",
      "Training Progress: \tEpoch 56 [2560/8883 (28.78%)]\t\tLoss: 0.74404\n",
      "Training Progress: \tEpoch 56 [2880/8883 (32.37%)]\t\tLoss: 0.66241\n",
      "Training Progress: \tEpoch 56 [3200/8883 (35.97%)]\t\tLoss: 0.61940\n",
      "Training Progress: \tEpoch 56 [3520/8883 (39.57%)]\t\tLoss: 0.86201\n",
      "Training Progress: \tEpoch 56 [3840/8883 (43.17%)]\t\tLoss: 0.87935\n",
      "Training Progress: \tEpoch 56 [4160/8883 (46.76%)]\t\tLoss: 0.49873\n",
      "Training Progress: \tEpoch 56 [4480/8883 (50.36%)]\t\tLoss: 0.55419\n",
      "Training Progress: \tEpoch 56 [4800/8883 (53.96%)]\t\tLoss: 0.67458\n",
      "Training Progress: \tEpoch 56 [5120/8883 (57.55%)]\t\tLoss: 0.72413\n",
      "Training Progress: \tEpoch 56 [5440/8883 (61.15%)]\t\tLoss: 0.68880\n",
      "Training Progress: \tEpoch 56 [5760/8883 (64.75%)]\t\tLoss: 0.62639\n",
      "Training Progress: \tEpoch 56 [6080/8883 (68.35%)]\t\tLoss: 0.40852\n",
      "Training Progress: \tEpoch 56 [6400/8883 (71.94%)]\t\tLoss: 0.61858\n",
      "Training Progress: \tEpoch 56 [6720/8883 (75.54%)]\t\tLoss: 0.66189\n",
      "Training Progress: \tEpoch 56 [7040/8883 (79.14%)]\t\tLoss: 0.66251\n",
      "Training Progress: \tEpoch 56 [7360/8883 (82.73%)]\t\tLoss: 0.98424\n",
      "Training Progress: \tEpoch 56 [7680/8883 (86.33%)]\t\tLoss: 0.52055\n",
      "Training Progress: \tEpoch 56 [8000/8883 (89.93%)]\t\tLoss: 0.71173\n",
      "Training Progress: \tEpoch 56 [8320/8883 (93.53%)]\t\tLoss: 0.54177\n",
      "Training Progress: \tEpoch 56 [8640/8883 (97.12%)]\t\tLoss: 0.78534\n",
      "\tTrain loss: 0.01228, Accuracy: 7272/8883 (81.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1650/1692 (97.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1142/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/8883 (0.00%)]\t\tLoss: 0.67249\n",
      "Training Progress: \tEpoch 57 [320/8883 (3.60%)]\t\tLoss: 0.75163\n",
      "Training Progress: \tEpoch 57 [640/8883 (7.19%)]\t\tLoss: 0.35847\n",
      "Training Progress: \tEpoch 57 [960/8883 (10.79%)]\t\tLoss: 0.73354\n",
      "Training Progress: \tEpoch 57 [1280/8883 (14.39%)]\t\tLoss: 0.56232\n",
      "Training Progress: \tEpoch 57 [1600/8883 (17.99%)]\t\tLoss: 0.54001\n",
      "Training Progress: \tEpoch 57 [1920/8883 (21.58%)]\t\tLoss: 0.51598\n",
      "Training Progress: \tEpoch 57 [2240/8883 (25.18%)]\t\tLoss: 0.53540\n",
      "Training Progress: \tEpoch 57 [2560/8883 (28.78%)]\t\tLoss: 0.63568\n",
      "Training Progress: \tEpoch 57 [2880/8883 (32.37%)]\t\tLoss: 0.90631\n",
      "Training Progress: \tEpoch 57 [3200/8883 (35.97%)]\t\tLoss: 0.56812\n",
      "Training Progress: \tEpoch 57 [3520/8883 (39.57%)]\t\tLoss: 0.99334\n",
      "Training Progress: \tEpoch 57 [3840/8883 (43.17%)]\t\tLoss: 0.67380\n",
      "Training Progress: \tEpoch 57 [4160/8883 (46.76%)]\t\tLoss: 0.53682\n",
      "Training Progress: \tEpoch 57 [4480/8883 (50.36%)]\t\tLoss: 0.55097\n",
      "Training Progress: \tEpoch 57 [4800/8883 (53.96%)]\t\tLoss: 0.59285\n",
      "Training Progress: \tEpoch 57 [5120/8883 (57.55%)]\t\tLoss: 0.82691\n",
      "Training Progress: \tEpoch 57 [5440/8883 (61.15%)]\t\tLoss: 0.90984\n",
      "Training Progress: \tEpoch 57 [5760/8883 (64.75%)]\t\tLoss: 0.79487\n",
      "Training Progress: \tEpoch 57 [6080/8883 (68.35%)]\t\tLoss: 0.48655\n",
      "Training Progress: \tEpoch 57 [6400/8883 (71.94%)]\t\tLoss: 0.42476\n",
      "Training Progress: \tEpoch 57 [6720/8883 (75.54%)]\t\tLoss: 0.71451\n",
      "Training Progress: \tEpoch 57 [7040/8883 (79.14%)]\t\tLoss: 0.79539\n",
      "Training Progress: \tEpoch 57 [7360/8883 (82.73%)]\t\tLoss: 0.61764\n",
      "Training Progress: \tEpoch 57 [7680/8883 (86.33%)]\t\tLoss: 0.73516\n",
      "Training Progress: \tEpoch 57 [8000/8883 (89.93%)]\t\tLoss: 0.65322\n",
      "Training Progress: \tEpoch 57 [8320/8883 (93.53%)]\t\tLoss: 0.84398\n",
      "Training Progress: \tEpoch 57 [8640/8883 (97.12%)]\t\tLoss: 0.90787\n",
      "\tTrain loss: 0.01219, Accuracy: 7280/8883 (81.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1160/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/8883 (0.00%)]\t\tLoss: 0.64627\n",
      "Training Progress: \tEpoch 58 [320/8883 (3.60%)]\t\tLoss: 0.92453\n",
      "Training Progress: \tEpoch 58 [640/8883 (7.19%)]\t\tLoss: 0.52328\n",
      "Training Progress: \tEpoch 58 [960/8883 (10.79%)]\t\tLoss: 0.78066\n",
      "Training Progress: \tEpoch 58 [1280/8883 (14.39%)]\t\tLoss: 0.79698\n",
      "Training Progress: \tEpoch 58 [1600/8883 (17.99%)]\t\tLoss: 0.55032\n",
      "Training Progress: \tEpoch 58 [1920/8883 (21.58%)]\t\tLoss: 0.50216\n",
      "Training Progress: \tEpoch 58 [2240/8883 (25.18%)]\t\tLoss: 0.54272\n",
      "Training Progress: \tEpoch 58 [2560/8883 (28.78%)]\t\tLoss: 0.62188\n",
      "Training Progress: \tEpoch 58 [2880/8883 (32.37%)]\t\tLoss: 0.64265\n",
      "Training Progress: \tEpoch 58 [3200/8883 (35.97%)]\t\tLoss: 0.63504\n",
      "Training Progress: \tEpoch 58 [3520/8883 (39.57%)]\t\tLoss: 0.92804\n",
      "Training Progress: \tEpoch 58 [3840/8883 (43.17%)]\t\tLoss: 0.75479\n",
      "Training Progress: \tEpoch 58 [4160/8883 (46.76%)]\t\tLoss: 0.46735\n",
      "Training Progress: \tEpoch 58 [4480/8883 (50.36%)]\t\tLoss: 0.76198\n",
      "Training Progress: \tEpoch 58 [4800/8883 (53.96%)]\t\tLoss: 0.56885\n",
      "Training Progress: \tEpoch 58 [5120/8883 (57.55%)]\t\tLoss: 0.67499\n",
      "Training Progress: \tEpoch 58 [5440/8883 (61.15%)]\t\tLoss: 0.59582\n",
      "Training Progress: \tEpoch 58 [5760/8883 (64.75%)]\t\tLoss: 0.82884\n",
      "Training Progress: \tEpoch 58 [6080/8883 (68.35%)]\t\tLoss: 0.50319\n",
      "Training Progress: \tEpoch 58 [6400/8883 (71.94%)]\t\tLoss: 0.69524\n",
      "Training Progress: \tEpoch 58 [6720/8883 (75.54%)]\t\tLoss: 0.84292\n",
      "Training Progress: \tEpoch 58 [7040/8883 (79.14%)]\t\tLoss: 0.63813\n",
      "Training Progress: \tEpoch 58 [7360/8883 (82.73%)]\t\tLoss: 0.56851\n",
      "Training Progress: \tEpoch 58 [7680/8883 (86.33%)]\t\tLoss: 0.47010\n",
      "Training Progress: \tEpoch 58 [8000/8883 (89.93%)]\t\tLoss: 0.59166\n",
      "Training Progress: \tEpoch 58 [8320/8883 (93.53%)]\t\tLoss: 0.70553\n",
      "Training Progress: \tEpoch 58 [8640/8883 (97.12%)]\t\tLoss: 0.74686\n",
      "\tTrain loss: 0.01215, Accuracy: 7269/8883 (81.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1148/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/8883 (0.00%)]\t\tLoss: 0.78490\n",
      "Training Progress: \tEpoch 59 [320/8883 (3.60%)]\t\tLoss: 0.55033\n",
      "Training Progress: \tEpoch 59 [640/8883 (7.19%)]\t\tLoss: 0.42375\n",
      "Training Progress: \tEpoch 59 [960/8883 (10.79%)]\t\tLoss: 0.84997\n",
      "Training Progress: \tEpoch 59 [1280/8883 (14.39%)]\t\tLoss: 0.55472\n",
      "Training Progress: \tEpoch 59 [1600/8883 (17.99%)]\t\tLoss: 0.47808\n",
      "Training Progress: \tEpoch 59 [1920/8883 (21.58%)]\t\tLoss: 0.50007\n",
      "Training Progress: \tEpoch 59 [2240/8883 (25.18%)]\t\tLoss: 0.51409\n",
      "Training Progress: \tEpoch 59 [2560/8883 (28.78%)]\t\tLoss: 0.61128\n",
      "Training Progress: \tEpoch 59 [2880/8883 (32.37%)]\t\tLoss: 0.64661\n",
      "Training Progress: \tEpoch 59 [3200/8883 (35.97%)]\t\tLoss: 0.76207\n",
      "Training Progress: \tEpoch 59 [3520/8883 (39.57%)]\t\tLoss: 0.95108\n",
      "Training Progress: \tEpoch 59 [3840/8883 (43.17%)]\t\tLoss: 0.83538\n",
      "Training Progress: \tEpoch 59 [4160/8883 (46.76%)]\t\tLoss: 0.55430\n",
      "Training Progress: \tEpoch 59 [4480/8883 (50.36%)]\t\tLoss: 0.62692\n",
      "Training Progress: \tEpoch 59 [4800/8883 (53.96%)]\t\tLoss: 0.63806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 59 [5120/8883 (57.55%)]\t\tLoss: 0.47940\n",
      "Training Progress: \tEpoch 59 [5440/8883 (61.15%)]\t\tLoss: 0.61368\n",
      "Training Progress: \tEpoch 59 [5760/8883 (64.75%)]\t\tLoss: 0.80674\n",
      "Training Progress: \tEpoch 59 [6080/8883 (68.35%)]\t\tLoss: 0.67311\n",
      "Training Progress: \tEpoch 59 [6400/8883 (71.94%)]\t\tLoss: 0.64065\n",
      "Training Progress: \tEpoch 59 [6720/8883 (75.54%)]\t\tLoss: 0.78538\n",
      "Training Progress: \tEpoch 59 [7040/8883 (79.14%)]\t\tLoss: 0.70993\n",
      "Training Progress: \tEpoch 59 [7360/8883 (82.73%)]\t\tLoss: 0.77384\n",
      "Training Progress: \tEpoch 59 [7680/8883 (86.33%)]\t\tLoss: 0.50488\n",
      "Training Progress: \tEpoch 59 [8000/8883 (89.93%)]\t\tLoss: 0.58619\n",
      "Training Progress: \tEpoch 59 [8320/8883 (93.53%)]\t\tLoss: 0.48256\n",
      "Training Progress: \tEpoch 59 [8640/8883 (97.12%)]\t\tLoss: 0.89566\n",
      "\tTrain loss: 0.01208, Accuracy: 7257/8883 (81.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1649/1692 (97.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/8883 (0.00%)]\t\tLoss: 0.58049\n",
      "Training Progress: \tEpoch 60 [320/8883 (3.60%)]\t\tLoss: 0.58179\n",
      "Training Progress: \tEpoch 60 [640/8883 (7.19%)]\t\tLoss: 0.41656\n",
      "Training Progress: \tEpoch 60 [960/8883 (10.79%)]\t\tLoss: 0.73397\n",
      "Training Progress: \tEpoch 60 [1280/8883 (14.39%)]\t\tLoss: 0.71570\n",
      "Training Progress: \tEpoch 60 [1600/8883 (17.99%)]\t\tLoss: 0.40009\n",
      "Training Progress: \tEpoch 60 [1920/8883 (21.58%)]\t\tLoss: 0.71682\n",
      "Training Progress: \tEpoch 60 [2240/8883 (25.18%)]\t\tLoss: 0.59674\n",
      "Training Progress: \tEpoch 60 [2560/8883 (28.78%)]\t\tLoss: 0.62175\n",
      "Training Progress: \tEpoch 60 [2880/8883 (32.37%)]\t\tLoss: 0.67045\n",
      "Training Progress: \tEpoch 60 [3200/8883 (35.97%)]\t\tLoss: 0.66309\n",
      "Training Progress: \tEpoch 60 [3520/8883 (39.57%)]\t\tLoss: 0.82019\n",
      "Training Progress: \tEpoch 60 [3840/8883 (43.17%)]\t\tLoss: 0.87723\n",
      "Training Progress: \tEpoch 60 [4160/8883 (46.76%)]\t\tLoss: 0.63218\n",
      "Training Progress: \tEpoch 60 [4480/8883 (50.36%)]\t\tLoss: 0.49591\n",
      "Training Progress: \tEpoch 60 [4800/8883 (53.96%)]\t\tLoss: 0.57986\n",
      "Training Progress: \tEpoch 60 [5120/8883 (57.55%)]\t\tLoss: 0.65664\n",
      "Training Progress: \tEpoch 60 [5440/8883 (61.15%)]\t\tLoss: 0.56902\n",
      "Training Progress: \tEpoch 60 [5760/8883 (64.75%)]\t\tLoss: 0.66841\n",
      "Training Progress: \tEpoch 60 [6080/8883 (68.35%)]\t\tLoss: 0.46612\n",
      "Training Progress: \tEpoch 60 [6400/8883 (71.94%)]\t\tLoss: 0.63664\n",
      "Training Progress: \tEpoch 60 [6720/8883 (75.54%)]\t\tLoss: 0.81315\n",
      "Training Progress: \tEpoch 60 [7040/8883 (79.14%)]\t\tLoss: 0.79198\n",
      "Training Progress: \tEpoch 60 [7360/8883 (82.73%)]\t\tLoss: 0.83312\n",
      "Training Progress: \tEpoch 60 [7680/8883 (86.33%)]\t\tLoss: 0.45018\n",
      "Training Progress: \tEpoch 60 [8000/8883 (89.93%)]\t\tLoss: 0.63247\n",
      "Training Progress: \tEpoch 60 [8320/8883 (93.53%)]\t\tLoss: 0.50848\n",
      "Training Progress: \tEpoch 60 [8640/8883 (97.12%)]\t\tLoss: 0.80861\n",
      "\tTrain loss: 0.01221, Accuracy: 7212/8883 (81.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1648/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1146/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/8883 (0.00%)]\t\tLoss: 0.39763\n",
      "Training Progress: \tEpoch 61 [320/8883 (3.60%)]\t\tLoss: 0.63322\n",
      "Training Progress: \tEpoch 61 [640/8883 (7.19%)]\t\tLoss: 0.49161\n",
      "Training Progress: \tEpoch 61 [960/8883 (10.79%)]\t\tLoss: 0.80932\n",
      "Training Progress: \tEpoch 61 [1280/8883 (14.39%)]\t\tLoss: 0.73485\n",
      "Training Progress: \tEpoch 61 [1600/8883 (17.99%)]\t\tLoss: 0.45686\n",
      "Training Progress: \tEpoch 61 [1920/8883 (21.58%)]\t\tLoss: 0.56215\n",
      "Training Progress: \tEpoch 61 [2240/8883 (25.18%)]\t\tLoss: 0.81272\n",
      "Training Progress: \tEpoch 61 [2560/8883 (28.78%)]\t\tLoss: 0.75346\n",
      "Training Progress: \tEpoch 61 [2880/8883 (32.37%)]\t\tLoss: 0.54728\n",
      "Training Progress: \tEpoch 61 [3200/8883 (35.97%)]\t\tLoss: 0.79600\n",
      "Training Progress: \tEpoch 61 [3520/8883 (39.57%)]\t\tLoss: 0.76429\n",
      "Training Progress: \tEpoch 61 [3840/8883 (43.17%)]\t\tLoss: 0.79338\n",
      "Training Progress: \tEpoch 61 [4160/8883 (46.76%)]\t\tLoss: 0.81146\n",
      "Training Progress: \tEpoch 61 [4480/8883 (50.36%)]\t\tLoss: 0.44837\n",
      "Training Progress: \tEpoch 61 [4800/8883 (53.96%)]\t\tLoss: 0.62856\n",
      "Training Progress: \tEpoch 61 [5120/8883 (57.55%)]\t\tLoss: 0.70551\n",
      "Training Progress: \tEpoch 61 [5440/8883 (61.15%)]\t\tLoss: 0.83603\n",
      "Training Progress: \tEpoch 61 [5760/8883 (64.75%)]\t\tLoss: 0.65293\n",
      "Training Progress: \tEpoch 61 [6080/8883 (68.35%)]\t\tLoss: 0.55995\n",
      "Training Progress: \tEpoch 61 [6400/8883 (71.94%)]\t\tLoss: 0.48561\n",
      "Training Progress: \tEpoch 61 [6720/8883 (75.54%)]\t\tLoss: 0.68055\n",
      "Training Progress: \tEpoch 61 [7040/8883 (79.14%)]\t\tLoss: 0.82326\n",
      "Training Progress: \tEpoch 61 [7360/8883 (82.73%)]\t\tLoss: 0.85526\n",
      "Training Progress: \tEpoch 61 [7680/8883 (86.33%)]\t\tLoss: 0.54275\n",
      "Training Progress: \tEpoch 61 [8000/8883 (89.93%)]\t\tLoss: 0.66108\n",
      "Training Progress: \tEpoch 61 [8320/8883 (93.53%)]\t\tLoss: 0.75793\n",
      "Training Progress: \tEpoch 61 [8640/8883 (97.12%)]\t\tLoss: 0.67440\n",
      "\tTrain loss: 0.01193, Accuracy: 7280/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1652/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1157/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/8883 (0.00%)]\t\tLoss: 0.67349\n",
      "Training Progress: \tEpoch 62 [320/8883 (3.60%)]\t\tLoss: 0.81327\n",
      "Training Progress: \tEpoch 62 [640/8883 (7.19%)]\t\tLoss: 0.42855\n",
      "Training Progress: \tEpoch 62 [960/8883 (10.79%)]\t\tLoss: 0.60372\n",
      "Training Progress: \tEpoch 62 [1280/8883 (14.39%)]\t\tLoss: 0.59121\n",
      "Training Progress: \tEpoch 62 [1600/8883 (17.99%)]\t\tLoss: 0.42131\n",
      "Training Progress: \tEpoch 62 [1920/8883 (21.58%)]\t\tLoss: 0.66000\n",
      "Training Progress: \tEpoch 62 [2240/8883 (25.18%)]\t\tLoss: 0.68061\n",
      "Training Progress: \tEpoch 62 [2560/8883 (28.78%)]\t\tLoss: 0.59943\n",
      "Training Progress: \tEpoch 62 [2880/8883 (32.37%)]\t\tLoss: 0.51903\n",
      "Training Progress: \tEpoch 62 [3200/8883 (35.97%)]\t\tLoss: 0.75899\n",
      "Training Progress: \tEpoch 62 [3520/8883 (39.57%)]\t\tLoss: 0.80452\n",
      "Training Progress: \tEpoch 62 [3840/8883 (43.17%)]\t\tLoss: 0.75706\n",
      "Training Progress: \tEpoch 62 [4160/8883 (46.76%)]\t\tLoss: 0.48749\n",
      "Training Progress: \tEpoch 62 [4480/8883 (50.36%)]\t\tLoss: 0.52437\n",
      "Training Progress: \tEpoch 62 [4800/8883 (53.96%)]\t\tLoss: 0.66052\n",
      "Training Progress: \tEpoch 62 [5120/8883 (57.55%)]\t\tLoss: 0.66179\n",
      "Training Progress: \tEpoch 62 [5440/8883 (61.15%)]\t\tLoss: 0.56947\n",
      "Training Progress: \tEpoch 62 [5760/8883 (64.75%)]\t\tLoss: 0.68537\n",
      "Training Progress: \tEpoch 62 [6080/8883 (68.35%)]\t\tLoss: 0.41953\n",
      "Training Progress: \tEpoch 62 [6400/8883 (71.94%)]\t\tLoss: 0.64712\n",
      "Training Progress: \tEpoch 62 [6720/8883 (75.54%)]\t\tLoss: 0.71202\n",
      "Training Progress: \tEpoch 62 [7040/8883 (79.14%)]\t\tLoss: 0.77855\n",
      "Training Progress: \tEpoch 62 [7360/8883 (82.73%)]\t\tLoss: 0.81578\n",
      "Training Progress: \tEpoch 62 [7680/8883 (86.33%)]\t\tLoss: 0.59349\n",
      "Training Progress: \tEpoch 62 [8000/8883 (89.93%)]\t\tLoss: 0.64776\n",
      "Training Progress: \tEpoch 62 [8320/8883 (93.53%)]\t\tLoss: 0.68710\n",
      "Training Progress: \tEpoch 62 [8640/8883 (97.12%)]\t\tLoss: 0.87158\n",
      "\tTrain loss: 0.01186, Accuracy: 7289/8883 (82.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1652/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1149/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/8883 (0.00%)]\t\tLoss: 0.54339\n",
      "Training Progress: \tEpoch 63 [320/8883 (3.60%)]\t\tLoss: 0.68030\n",
      "Training Progress: \tEpoch 63 [640/8883 (7.19%)]\t\tLoss: 0.42215\n",
      "Training Progress: \tEpoch 63 [960/8883 (10.79%)]\t\tLoss: 0.64461\n",
      "Training Progress: \tEpoch 63 [1280/8883 (14.39%)]\t\tLoss: 0.62281\n",
      "Training Progress: \tEpoch 63 [1600/8883 (17.99%)]\t\tLoss: 0.65686\n",
      "Training Progress: \tEpoch 63 [1920/8883 (21.58%)]\t\tLoss: 0.38608\n",
      "Training Progress: \tEpoch 63 [2240/8883 (25.18%)]\t\tLoss: 0.50723\n",
      "Training Progress: \tEpoch 63 [2560/8883 (28.78%)]\t\tLoss: 0.65457\n",
      "Training Progress: \tEpoch 63 [2880/8883 (32.37%)]\t\tLoss: 0.88335\n",
      "Training Progress: \tEpoch 63 [3200/8883 (35.97%)]\t\tLoss: 0.60271\n",
      "Training Progress: \tEpoch 63 [3520/8883 (39.57%)]\t\tLoss: 0.93699\n",
      "Training Progress: \tEpoch 63 [3840/8883 (43.17%)]\t\tLoss: 0.81376\n",
      "Training Progress: \tEpoch 63 [4160/8883 (46.76%)]\t\tLoss: 0.69977\n",
      "Training Progress: \tEpoch 63 [4480/8883 (50.36%)]\t\tLoss: 0.57516\n",
      "Training Progress: \tEpoch 63 [4800/8883 (53.96%)]\t\tLoss: 0.55788\n",
      "Training Progress: \tEpoch 63 [5120/8883 (57.55%)]\t\tLoss: 0.63292\n",
      "Training Progress: \tEpoch 63 [5440/8883 (61.15%)]\t\tLoss: 0.57514\n",
      "Training Progress: \tEpoch 63 [5760/8883 (64.75%)]\t\tLoss: 0.67197\n",
      "Training Progress: \tEpoch 63 [6080/8883 (68.35%)]\t\tLoss: 0.44085\n",
      "Training Progress: \tEpoch 63 [6400/8883 (71.94%)]\t\tLoss: 0.51862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [6720/8883 (75.54%)]\t\tLoss: 0.75001\n",
      "Training Progress: \tEpoch 63 [7040/8883 (79.14%)]\t\tLoss: 0.69611\n",
      "Training Progress: \tEpoch 63 [7360/8883 (82.73%)]\t\tLoss: 0.70430\n",
      "Training Progress: \tEpoch 63 [7680/8883 (86.33%)]\t\tLoss: 0.53767\n",
      "Training Progress: \tEpoch 63 [8000/8883 (89.93%)]\t\tLoss: 0.51219\n",
      "Training Progress: \tEpoch 63 [8320/8883 (93.53%)]\t\tLoss: 0.65983\n",
      "Training Progress: \tEpoch 63 [8640/8883 (97.12%)]\t\tLoss: 0.66103\n",
      "\tTrain loss: 0.01202, Accuracy: 7268/8883 (81.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1170/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/8883 (0.00%)]\t\tLoss: 0.53248\n",
      "Training Progress: \tEpoch 64 [320/8883 (3.60%)]\t\tLoss: 0.84507\n",
      "Training Progress: \tEpoch 64 [640/8883 (7.19%)]\t\tLoss: 0.70537\n",
      "Training Progress: \tEpoch 64 [960/8883 (10.79%)]\t\tLoss: 0.53437\n",
      "Training Progress: \tEpoch 64 [1280/8883 (14.39%)]\t\tLoss: 0.63735\n",
      "Training Progress: \tEpoch 64 [1600/8883 (17.99%)]\t\tLoss: 0.34014\n",
      "Training Progress: \tEpoch 64 [1920/8883 (21.58%)]\t\tLoss: 0.49058\n",
      "Training Progress: \tEpoch 64 [2240/8883 (25.18%)]\t\tLoss: 0.53778\n",
      "Training Progress: \tEpoch 64 [2560/8883 (28.78%)]\t\tLoss: 0.80039\n",
      "Training Progress: \tEpoch 64 [2880/8883 (32.37%)]\t\tLoss: 0.65660\n",
      "Training Progress: \tEpoch 64 [3200/8883 (35.97%)]\t\tLoss: 0.70930\n",
      "Training Progress: \tEpoch 64 [3520/8883 (39.57%)]\t\tLoss: 0.63891\n",
      "Training Progress: \tEpoch 64 [3840/8883 (43.17%)]\t\tLoss: 0.99141\n",
      "Training Progress: \tEpoch 64 [4160/8883 (46.76%)]\t\tLoss: 0.40343\n",
      "Training Progress: \tEpoch 64 [4480/8883 (50.36%)]\t\tLoss: 0.49733\n",
      "Training Progress: \tEpoch 64 [4800/8883 (53.96%)]\t\tLoss: 0.79874\n",
      "Training Progress: \tEpoch 64 [5120/8883 (57.55%)]\t\tLoss: 0.64541\n",
      "Training Progress: \tEpoch 64 [5440/8883 (61.15%)]\t\tLoss: 0.66736\n",
      "Training Progress: \tEpoch 64 [5760/8883 (64.75%)]\t\tLoss: 0.59562\n",
      "Training Progress: \tEpoch 64 [6080/8883 (68.35%)]\t\tLoss: 0.39584\n",
      "Training Progress: \tEpoch 64 [6400/8883 (71.94%)]\t\tLoss: 0.67895\n",
      "Training Progress: \tEpoch 64 [6720/8883 (75.54%)]\t\tLoss: 0.83617\n",
      "Training Progress: \tEpoch 64 [7040/8883 (79.14%)]\t\tLoss: 0.84561\n",
      "Training Progress: \tEpoch 64 [7360/8883 (82.73%)]\t\tLoss: 0.83560\n",
      "Training Progress: \tEpoch 64 [7680/8883 (86.33%)]\t\tLoss: 0.47101\n",
      "Training Progress: \tEpoch 64 [8000/8883 (89.93%)]\t\tLoss: 0.70682\n",
      "Training Progress: \tEpoch 64 [8320/8883 (93.53%)]\t\tLoss: 0.67360\n",
      "Training Progress: \tEpoch 64 [8640/8883 (97.12%)]\t\tLoss: 0.82671\n",
      "\tTrain loss: 0.01186, Accuracy: 7286/8883 (82.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1657/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1133/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/8883 (0.00%)]\t\tLoss: 0.77358\n",
      "Training Progress: \tEpoch 65 [320/8883 (3.60%)]\t\tLoss: 0.57562\n",
      "Training Progress: \tEpoch 65 [640/8883 (7.19%)]\t\tLoss: 0.51477\n",
      "Training Progress: \tEpoch 65 [960/8883 (10.79%)]\t\tLoss: 0.67176\n",
      "Training Progress: \tEpoch 65 [1280/8883 (14.39%)]\t\tLoss: 1.00753\n",
      "Training Progress: \tEpoch 65 [1600/8883 (17.99%)]\t\tLoss: 0.52134\n",
      "Training Progress: \tEpoch 65 [1920/8883 (21.58%)]\t\tLoss: 0.48616\n",
      "Training Progress: \tEpoch 65 [2240/8883 (25.18%)]\t\tLoss: 0.55752\n",
      "Training Progress: \tEpoch 65 [2560/8883 (28.78%)]\t\tLoss: 0.50152\n",
      "Training Progress: \tEpoch 65 [2880/8883 (32.37%)]\t\tLoss: 0.64491\n",
      "Training Progress: \tEpoch 65 [3200/8883 (35.97%)]\t\tLoss: 0.62073\n",
      "Training Progress: \tEpoch 65 [3520/8883 (39.57%)]\t\tLoss: 1.04541\n",
      "Training Progress: \tEpoch 65 [3840/8883 (43.17%)]\t\tLoss: 0.91093\n",
      "Training Progress: \tEpoch 65 [4160/8883 (46.76%)]\t\tLoss: 0.51130\n",
      "Training Progress: \tEpoch 65 [4480/8883 (50.36%)]\t\tLoss: 0.39894\n",
      "Training Progress: \tEpoch 65 [4800/8883 (53.96%)]\t\tLoss: 0.63022\n",
      "Training Progress: \tEpoch 65 [5120/8883 (57.55%)]\t\tLoss: 0.74181\n",
      "Training Progress: \tEpoch 65 [5440/8883 (61.15%)]\t\tLoss: 0.57188\n",
      "Training Progress: \tEpoch 65 [5760/8883 (64.75%)]\t\tLoss: 0.65857\n",
      "Training Progress: \tEpoch 65 [6080/8883 (68.35%)]\t\tLoss: 0.51106\n",
      "Training Progress: \tEpoch 65 [6400/8883 (71.94%)]\t\tLoss: 0.62723\n",
      "Training Progress: \tEpoch 65 [6720/8883 (75.54%)]\t\tLoss: 0.89117\n",
      "Training Progress: \tEpoch 65 [7040/8883 (79.14%)]\t\tLoss: 0.85157\n",
      "Training Progress: \tEpoch 65 [7360/8883 (82.73%)]\t\tLoss: 0.66328\n",
      "Training Progress: \tEpoch 65 [7680/8883 (86.33%)]\t\tLoss: 0.55240\n",
      "Training Progress: \tEpoch 65 [8000/8883 (89.93%)]\t\tLoss: 0.63330\n",
      "Training Progress: \tEpoch 65 [8320/8883 (93.53%)]\t\tLoss: 0.65897\n",
      "Training Progress: \tEpoch 65 [8640/8883 (97.12%)]\t\tLoss: 0.82227\n",
      "\tTrain loss: 0.01194, Accuracy: 7241/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1643/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1169/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/8883 (0.00%)]\t\tLoss: 0.61618\n",
      "Training Progress: \tEpoch 66 [320/8883 (3.60%)]\t\tLoss: 0.60863\n",
      "Training Progress: \tEpoch 66 [640/8883 (7.19%)]\t\tLoss: 0.68900\n",
      "Training Progress: \tEpoch 66 [960/8883 (10.79%)]\t\tLoss: 0.66530\n",
      "Training Progress: \tEpoch 66 [1280/8883 (14.39%)]\t\tLoss: 0.67414\n",
      "Training Progress: \tEpoch 66 [1600/8883 (17.99%)]\t\tLoss: 0.41261\n",
      "Training Progress: \tEpoch 66 [1920/8883 (21.58%)]\t\tLoss: 0.61507\n",
      "Training Progress: \tEpoch 66 [2240/8883 (25.18%)]\t\tLoss: 0.58706\n",
      "Training Progress: \tEpoch 66 [2560/8883 (28.78%)]\t\tLoss: 0.63763\n",
      "Training Progress: \tEpoch 66 [2880/8883 (32.37%)]\t\tLoss: 0.72065\n",
      "Training Progress: \tEpoch 66 [3200/8883 (35.97%)]\t\tLoss: 0.75194\n",
      "Training Progress: \tEpoch 66 [3520/8883 (39.57%)]\t\tLoss: 0.79957\n",
      "Training Progress: \tEpoch 66 [3840/8883 (43.17%)]\t\tLoss: 0.78900\n",
      "Training Progress: \tEpoch 66 [4160/8883 (46.76%)]\t\tLoss: 0.62327\n",
      "Training Progress: \tEpoch 66 [4480/8883 (50.36%)]\t\tLoss: 0.51671\n",
      "Training Progress: \tEpoch 66 [4800/8883 (53.96%)]\t\tLoss: 0.53123\n",
      "Training Progress: \tEpoch 66 [5120/8883 (57.55%)]\t\tLoss: 0.90913\n",
      "Training Progress: \tEpoch 66 [5440/8883 (61.15%)]\t\tLoss: 0.58699\n",
      "Training Progress: \tEpoch 66 [5760/8883 (64.75%)]\t\tLoss: 0.66656\n",
      "Training Progress: \tEpoch 66 [6080/8883 (68.35%)]\t\tLoss: 0.45605\n",
      "Training Progress: \tEpoch 66 [6400/8883 (71.94%)]\t\tLoss: 0.54614\n",
      "Training Progress: \tEpoch 66 [6720/8883 (75.54%)]\t\tLoss: 0.81737\n",
      "Training Progress: \tEpoch 66 [7040/8883 (79.14%)]\t\tLoss: 0.86132\n",
      "Training Progress: \tEpoch 66 [7360/8883 (82.73%)]\t\tLoss: 0.61890\n",
      "Training Progress: \tEpoch 66 [7680/8883 (86.33%)]\t\tLoss: 0.46109\n",
      "Training Progress: \tEpoch 66 [8000/8883 (89.93%)]\t\tLoss: 0.68322\n",
      "Training Progress: \tEpoch 66 [8320/8883 (93.53%)]\t\tLoss: 0.64702\n",
      "Training Progress: \tEpoch 66 [8640/8883 (97.12%)]\t\tLoss: 0.67259\n",
      "\tTrain loss: 0.01192, Accuracy: 7273/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1659/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1144/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/8883 (0.00%)]\t\tLoss: 0.45361\n",
      "Training Progress: \tEpoch 67 [320/8883 (3.60%)]\t\tLoss: 0.64241\n",
      "Training Progress: \tEpoch 67 [640/8883 (7.19%)]\t\tLoss: 0.36358\n",
      "Training Progress: \tEpoch 67 [960/8883 (10.79%)]\t\tLoss: 0.64523\n",
      "Training Progress: \tEpoch 67 [1280/8883 (14.39%)]\t\tLoss: 0.49419\n",
      "Training Progress: \tEpoch 67 [1600/8883 (17.99%)]\t\tLoss: 0.55029\n",
      "Training Progress: \tEpoch 67 [1920/8883 (21.58%)]\t\tLoss: 0.60869\n",
      "Training Progress: \tEpoch 67 [2240/8883 (25.18%)]\t\tLoss: 0.79297\n",
      "Training Progress: \tEpoch 67 [2560/8883 (28.78%)]\t\tLoss: 0.63331\n",
      "Training Progress: \tEpoch 67 [2880/8883 (32.37%)]\t\tLoss: 0.61416\n",
      "Training Progress: \tEpoch 67 [3200/8883 (35.97%)]\t\tLoss: 0.59353\n",
      "Training Progress: \tEpoch 67 [3520/8883 (39.57%)]\t\tLoss: 1.01625\n",
      "Training Progress: \tEpoch 67 [3840/8883 (43.17%)]\t\tLoss: 0.87007\n",
      "Training Progress: \tEpoch 67 [4160/8883 (46.76%)]\t\tLoss: 0.48516\n",
      "Training Progress: \tEpoch 67 [4480/8883 (50.36%)]\t\tLoss: 0.52359\n",
      "Training Progress: \tEpoch 67 [4800/8883 (53.96%)]\t\tLoss: 0.52857\n",
      "Training Progress: \tEpoch 67 [5120/8883 (57.55%)]\t\tLoss: 0.63767\n",
      "Training Progress: \tEpoch 67 [5440/8883 (61.15%)]\t\tLoss: 0.77283\n",
      "Training Progress: \tEpoch 67 [5760/8883 (64.75%)]\t\tLoss: 0.56742\n",
      "Training Progress: \tEpoch 67 [6080/8883 (68.35%)]\t\tLoss: 0.50089\n",
      "Training Progress: \tEpoch 67 [6400/8883 (71.94%)]\t\tLoss: 0.51053\n",
      "Training Progress: \tEpoch 67 [6720/8883 (75.54%)]\t\tLoss: 0.80359\n",
      "Training Progress: \tEpoch 67 [7040/8883 (79.14%)]\t\tLoss: 0.74608\n",
      "Training Progress: \tEpoch 67 [7360/8883 (82.73%)]\t\tLoss: 0.62190\n",
      "Training Progress: \tEpoch 67 [7680/8883 (86.33%)]\t\tLoss: 0.56795\n",
      "Training Progress: \tEpoch 67 [8000/8883 (89.93%)]\t\tLoss: 0.53313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 67 [8320/8883 (93.53%)]\t\tLoss: 0.63670\n",
      "Training Progress: \tEpoch 67 [8640/8883 (97.12%)]\t\tLoss: 0.98077\n",
      "\tTrain loss: 0.01178, Accuracy: 7265/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1656/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1168/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/8883 (0.00%)]\t\tLoss: 0.52193\n",
      "Training Progress: \tEpoch 68 [320/8883 (3.60%)]\t\tLoss: 0.88618\n",
      "Training Progress: \tEpoch 68 [640/8883 (7.19%)]\t\tLoss: 0.36359\n",
      "Training Progress: \tEpoch 68 [960/8883 (10.79%)]\t\tLoss: 0.63818\n",
      "Training Progress: \tEpoch 68 [1280/8883 (14.39%)]\t\tLoss: 0.60709\n",
      "Training Progress: \tEpoch 68 [1600/8883 (17.99%)]\t\tLoss: 0.42005\n",
      "Training Progress: \tEpoch 68 [1920/8883 (21.58%)]\t\tLoss: 0.56463\n",
      "Training Progress: \tEpoch 68 [2240/8883 (25.18%)]\t\tLoss: 0.50189\n",
      "Training Progress: \tEpoch 68 [2560/8883 (28.78%)]\t\tLoss: 0.46632\n",
      "Training Progress: \tEpoch 68 [2880/8883 (32.37%)]\t\tLoss: 0.60259\n",
      "Training Progress: \tEpoch 68 [3200/8883 (35.97%)]\t\tLoss: 0.67964\n",
      "Training Progress: \tEpoch 68 [3520/8883 (39.57%)]\t\tLoss: 0.67076\n",
      "Training Progress: \tEpoch 68 [3840/8883 (43.17%)]\t\tLoss: 0.70672\n",
      "Training Progress: \tEpoch 68 [4160/8883 (46.76%)]\t\tLoss: 0.53762\n",
      "Training Progress: \tEpoch 68 [4480/8883 (50.36%)]\t\tLoss: 0.41340\n",
      "Training Progress: \tEpoch 68 [4800/8883 (53.96%)]\t\tLoss: 0.67225\n",
      "Training Progress: \tEpoch 68 [5120/8883 (57.55%)]\t\tLoss: 0.54642\n",
      "Training Progress: \tEpoch 68 [5440/8883 (61.15%)]\t\tLoss: 0.67236\n",
      "Training Progress: \tEpoch 68 [5760/8883 (64.75%)]\t\tLoss: 0.63510\n",
      "Training Progress: \tEpoch 68 [6080/8883 (68.35%)]\t\tLoss: 0.38170\n",
      "Training Progress: \tEpoch 68 [6400/8883 (71.94%)]\t\tLoss: 0.62546\n",
      "Training Progress: \tEpoch 68 [6720/8883 (75.54%)]\t\tLoss: 0.70627\n",
      "Training Progress: \tEpoch 68 [7040/8883 (79.14%)]\t\tLoss: 0.66095\n",
      "Training Progress: \tEpoch 68 [7360/8883 (82.73%)]\t\tLoss: 0.89466\n",
      "Training Progress: \tEpoch 68 [7680/8883 (86.33%)]\t\tLoss: 0.61596\n",
      "Training Progress: \tEpoch 68 [8000/8883 (89.93%)]\t\tLoss: 0.72590\n",
      "Training Progress: \tEpoch 68 [8320/8883 (93.53%)]\t\tLoss: 0.58153\n",
      "Training Progress: \tEpoch 68 [8640/8883 (97.12%)]\t\tLoss: 0.67988\n",
      "\tTrain loss: 0.01175, Accuracy: 7270/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1654/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1119/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/8883 (0.00%)]\t\tLoss: 0.57747\n",
      "Training Progress: \tEpoch 69 [320/8883 (3.60%)]\t\tLoss: 0.74538\n",
      "Training Progress: \tEpoch 69 [640/8883 (7.19%)]\t\tLoss: 0.53223\n",
      "Training Progress: \tEpoch 69 [960/8883 (10.79%)]\t\tLoss: 0.78477\n",
      "Training Progress: \tEpoch 69 [1280/8883 (14.39%)]\t\tLoss: 0.65640\n",
      "Training Progress: \tEpoch 69 [1600/8883 (17.99%)]\t\tLoss: 0.42051\n",
      "Training Progress: \tEpoch 69 [1920/8883 (21.58%)]\t\tLoss: 0.40378\n",
      "Training Progress: \tEpoch 69 [2240/8883 (25.18%)]\t\tLoss: 0.58815\n",
      "Training Progress: \tEpoch 69 [2560/8883 (28.78%)]\t\tLoss: 0.42869\n",
      "Training Progress: \tEpoch 69 [2880/8883 (32.37%)]\t\tLoss: 0.59817\n",
      "Training Progress: \tEpoch 69 [3200/8883 (35.97%)]\t\tLoss: 0.86471\n",
      "Training Progress: \tEpoch 69 [3520/8883 (39.57%)]\t\tLoss: 0.67550\n",
      "Training Progress: \tEpoch 69 [3840/8883 (43.17%)]\t\tLoss: 0.73050\n",
      "Training Progress: \tEpoch 69 [4160/8883 (46.76%)]\t\tLoss: 0.52911\n",
      "Training Progress: \tEpoch 69 [4480/8883 (50.36%)]\t\tLoss: 0.48272\n",
      "Training Progress: \tEpoch 69 [4800/8883 (53.96%)]\t\tLoss: 0.75569\n",
      "Training Progress: \tEpoch 69 [5120/8883 (57.55%)]\t\tLoss: 0.72178\n",
      "Training Progress: \tEpoch 69 [5440/8883 (61.15%)]\t\tLoss: 0.73452\n",
      "Training Progress: \tEpoch 69 [5760/8883 (64.75%)]\t\tLoss: 0.60932\n",
      "Training Progress: \tEpoch 69 [6080/8883 (68.35%)]\t\tLoss: 0.40921\n",
      "Training Progress: \tEpoch 69 [6400/8883 (71.94%)]\t\tLoss: 0.52076\n",
      "Training Progress: \tEpoch 69 [6720/8883 (75.54%)]\t\tLoss: 0.74202\n",
      "Training Progress: \tEpoch 69 [7040/8883 (79.14%)]\t\tLoss: 0.72167\n",
      "Training Progress: \tEpoch 69 [7360/8883 (82.73%)]\t\tLoss: 0.97296\n",
      "Training Progress: \tEpoch 69 [7680/8883 (86.33%)]\t\tLoss: 0.49245\n",
      "Training Progress: \tEpoch 69 [8000/8883 (89.93%)]\t\tLoss: 0.58135\n",
      "Training Progress: \tEpoch 69 [8320/8883 (93.53%)]\t\tLoss: 0.58946\n",
      "Training Progress: \tEpoch 69 [8640/8883 (97.12%)]\t\tLoss: 0.77961\n",
      "\tTrain loss: 0.01176, Accuracy: 7251/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1653/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1133/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/8883 (0.00%)]\t\tLoss: 0.56305\n",
      "Training Progress: \tEpoch 70 [320/8883 (3.60%)]\t\tLoss: 0.68008\n",
      "Training Progress: \tEpoch 70 [640/8883 (7.19%)]\t\tLoss: 0.42596\n",
      "Training Progress: \tEpoch 70 [960/8883 (10.79%)]\t\tLoss: 0.62793\n",
      "Training Progress: \tEpoch 70 [1280/8883 (14.39%)]\t\tLoss: 0.66887\n",
      "Training Progress: \tEpoch 70 [1600/8883 (17.99%)]\t\tLoss: 0.54508\n",
      "Training Progress: \tEpoch 70 [1920/8883 (21.58%)]\t\tLoss: 0.49791\n",
      "Training Progress: \tEpoch 70 [2240/8883 (25.18%)]\t\tLoss: 0.44498\n",
      "Training Progress: \tEpoch 70 [2560/8883 (28.78%)]\t\tLoss: 0.56729\n",
      "Training Progress: \tEpoch 70 [2880/8883 (32.37%)]\t\tLoss: 0.54783\n",
      "Training Progress: \tEpoch 70 [3200/8883 (35.97%)]\t\tLoss: 0.83015\n",
      "Training Progress: \tEpoch 70 [3520/8883 (39.57%)]\t\tLoss: 0.69461\n",
      "Training Progress: \tEpoch 70 [3840/8883 (43.17%)]\t\tLoss: 0.67340\n",
      "Training Progress: \tEpoch 70 [4160/8883 (46.76%)]\t\tLoss: 0.72685\n",
      "Training Progress: \tEpoch 70 [4480/8883 (50.36%)]\t\tLoss: 0.51212\n",
      "Training Progress: \tEpoch 70 [4800/8883 (53.96%)]\t\tLoss: 0.54228\n",
      "Training Progress: \tEpoch 70 [5120/8883 (57.55%)]\t\tLoss: 0.77089\n",
      "Training Progress: \tEpoch 70 [5440/8883 (61.15%)]\t\tLoss: 0.71654\n",
      "Training Progress: \tEpoch 70 [5760/8883 (64.75%)]\t\tLoss: 0.58133\n",
      "Training Progress: \tEpoch 70 [6080/8883 (68.35%)]\t\tLoss: 0.51822\n",
      "Training Progress: \tEpoch 70 [6400/8883 (71.94%)]\t\tLoss: 0.46271\n",
      "Training Progress: \tEpoch 70 [6720/8883 (75.54%)]\t\tLoss: 0.70986\n",
      "Training Progress: \tEpoch 70 [7040/8883 (79.14%)]\t\tLoss: 0.93209\n",
      "Training Progress: \tEpoch 70 [7360/8883 (82.73%)]\t\tLoss: 1.03058\n",
      "Training Progress: \tEpoch 70 [7680/8883 (86.33%)]\t\tLoss: 0.54704\n",
      "Training Progress: \tEpoch 70 [8000/8883 (89.93%)]\t\tLoss: 0.55693\n",
      "Training Progress: \tEpoch 70 [8320/8883 (93.53%)]\t\tLoss: 0.60618\n",
      "Training Progress: \tEpoch 70 [8640/8883 (97.12%)]\t\tLoss: 0.74973\n",
      "\tTrain loss: 0.01164, Accuracy: 7266/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1159/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/8883 (0.00%)]\t\tLoss: 0.78968\n",
      "Training Progress: \tEpoch 71 [320/8883 (3.60%)]\t\tLoss: 0.65336\n",
      "Training Progress: \tEpoch 71 [640/8883 (7.19%)]\t\tLoss: 0.55037\n",
      "Training Progress: \tEpoch 71 [960/8883 (10.79%)]\t\tLoss: 0.60679\n",
      "Training Progress: \tEpoch 71 [1280/8883 (14.39%)]\t\tLoss: 0.64697\n",
      "Training Progress: \tEpoch 71 [1600/8883 (17.99%)]\t\tLoss: 0.46673\n",
      "Training Progress: \tEpoch 71 [1920/8883 (21.58%)]\t\tLoss: 0.41454\n",
      "Training Progress: \tEpoch 71 [2240/8883 (25.18%)]\t\tLoss: 0.54686\n",
      "Training Progress: \tEpoch 71 [2560/8883 (28.78%)]\t\tLoss: 0.56453\n",
      "Training Progress: \tEpoch 71 [2880/8883 (32.37%)]\t\tLoss: 0.54946\n",
      "Training Progress: \tEpoch 71 [3200/8883 (35.97%)]\t\tLoss: 0.68933\n",
      "Training Progress: \tEpoch 71 [3520/8883 (39.57%)]\t\tLoss: 0.89067\n",
      "Training Progress: \tEpoch 71 [3840/8883 (43.17%)]\t\tLoss: 0.77967\n",
      "Training Progress: \tEpoch 71 [4160/8883 (46.76%)]\t\tLoss: 0.58306\n",
      "Training Progress: \tEpoch 71 [4480/8883 (50.36%)]\t\tLoss: 0.59117\n",
      "Training Progress: \tEpoch 71 [4800/8883 (53.96%)]\t\tLoss: 0.64786\n",
      "Training Progress: \tEpoch 71 [5120/8883 (57.55%)]\t\tLoss: 0.84861\n",
      "Training Progress: \tEpoch 71 [5440/8883 (61.15%)]\t\tLoss: 0.69079\n",
      "Training Progress: \tEpoch 71 [5760/8883 (64.75%)]\t\tLoss: 0.58400\n",
      "Training Progress: \tEpoch 71 [6080/8883 (68.35%)]\t\tLoss: 0.41344\n",
      "Training Progress: \tEpoch 71 [6400/8883 (71.94%)]\t\tLoss: 0.54664\n",
      "Training Progress: \tEpoch 71 [6720/8883 (75.54%)]\t\tLoss: 0.77809\n",
      "Training Progress: \tEpoch 71 [7040/8883 (79.14%)]\t\tLoss: 1.01665\n",
      "Training Progress: \tEpoch 71 [7360/8883 (82.73%)]\t\tLoss: 0.53983\n",
      "Training Progress: \tEpoch 71 [7680/8883 (86.33%)]\t\tLoss: 0.52661\n",
      "Training Progress: \tEpoch 71 [8000/8883 (89.93%)]\t\tLoss: 0.43444\n",
      "Training Progress: \tEpoch 71 [8320/8883 (93.53%)]\t\tLoss: 0.76246\n",
      "Training Progress: \tEpoch 71 [8640/8883 (97.12%)]\t\tLoss: 0.55044\n",
      "\tTrain loss: 0.01166, Accuracy: 7271/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1657/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1149/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/8883 (0.00%)]\t\tLoss: 0.67362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 72 [320/8883 (3.60%)]\t\tLoss: 0.56912\n",
      "Training Progress: \tEpoch 72 [640/8883 (7.19%)]\t\tLoss: 0.82992\n",
      "Training Progress: \tEpoch 72 [960/8883 (10.79%)]\t\tLoss: 0.55554\n",
      "Training Progress: \tEpoch 72 [1280/8883 (14.39%)]\t\tLoss: 0.53660\n",
      "Training Progress: \tEpoch 72 [1600/8883 (17.99%)]\t\tLoss: 0.55439\n",
      "Training Progress: \tEpoch 72 [1920/8883 (21.58%)]\t\tLoss: 0.56840\n",
      "Training Progress: \tEpoch 72 [2240/8883 (25.18%)]\t\tLoss: 0.57247\n",
      "Training Progress: \tEpoch 72 [2560/8883 (28.78%)]\t\tLoss: 0.77616\n",
      "Training Progress: \tEpoch 72 [2880/8883 (32.37%)]\t\tLoss: 0.50658\n",
      "Training Progress: \tEpoch 72 [3200/8883 (35.97%)]\t\tLoss: 0.77200\n",
      "Training Progress: \tEpoch 72 [3520/8883 (39.57%)]\t\tLoss: 0.82821\n",
      "Training Progress: \tEpoch 72 [3840/8883 (43.17%)]\t\tLoss: 0.59607\n",
      "Training Progress: \tEpoch 72 [4160/8883 (46.76%)]\t\tLoss: 0.45035\n",
      "Training Progress: \tEpoch 72 [4480/8883 (50.36%)]\t\tLoss: 0.42769\n",
      "Training Progress: \tEpoch 72 [4800/8883 (53.96%)]\t\tLoss: 0.45239\n",
      "Training Progress: \tEpoch 72 [5120/8883 (57.55%)]\t\tLoss: 0.95474\n",
      "Training Progress: \tEpoch 72 [5440/8883 (61.15%)]\t\tLoss: 0.59870\n",
      "Training Progress: \tEpoch 72 [5760/8883 (64.75%)]\t\tLoss: 0.71249\n",
      "Training Progress: \tEpoch 72 [6080/8883 (68.35%)]\t\tLoss: 0.50459\n",
      "Training Progress: \tEpoch 72 [6400/8883 (71.94%)]\t\tLoss: 0.66021\n",
      "Training Progress: \tEpoch 72 [6720/8883 (75.54%)]\t\tLoss: 0.96670\n",
      "Training Progress: \tEpoch 72 [7040/8883 (79.14%)]\t\tLoss: 0.74026\n",
      "Training Progress: \tEpoch 72 [7360/8883 (82.73%)]\t\tLoss: 0.69606\n",
      "Training Progress: \tEpoch 72 [7680/8883 (86.33%)]\t\tLoss: 0.43835\n",
      "Training Progress: \tEpoch 72 [8000/8883 (89.93%)]\t\tLoss: 0.61338\n",
      "Training Progress: \tEpoch 72 [8320/8883 (93.53%)]\t\tLoss: 0.44667\n",
      "Training Progress: \tEpoch 72 [8640/8883 (97.12%)]\t\tLoss: 0.57519\n",
      "\tTrain loss: 0.01161, Accuracy: 7284/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1659/1692 (98.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1134/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/8883 (0.00%)]\t\tLoss: 0.56596\n",
      "Training Progress: \tEpoch 73 [320/8883 (3.60%)]\t\tLoss: 0.52136\n",
      "Training Progress: \tEpoch 73 [640/8883 (7.19%)]\t\tLoss: 0.44008\n",
      "Training Progress: \tEpoch 73 [960/8883 (10.79%)]\t\tLoss: 0.61034\n",
      "Training Progress: \tEpoch 73 [1280/8883 (14.39%)]\t\tLoss: 0.62924\n",
      "Training Progress: \tEpoch 73 [1600/8883 (17.99%)]\t\tLoss: 0.63189\n",
      "Training Progress: \tEpoch 73 [1920/8883 (21.58%)]\t\tLoss: 0.66262\n",
      "Training Progress: \tEpoch 73 [2240/8883 (25.18%)]\t\tLoss: 0.61498\n",
      "Training Progress: \tEpoch 73 [2560/8883 (28.78%)]\t\tLoss: 0.53031\n",
      "Training Progress: \tEpoch 73 [2880/8883 (32.37%)]\t\tLoss: 0.59957\n",
      "Training Progress: \tEpoch 73 [3200/8883 (35.97%)]\t\tLoss: 0.57797\n",
      "Training Progress: \tEpoch 73 [3520/8883 (39.57%)]\t\tLoss: 0.77798\n",
      "Training Progress: \tEpoch 73 [3840/8883 (43.17%)]\t\tLoss: 0.73269\n",
      "Training Progress: \tEpoch 73 [4160/8883 (46.76%)]\t\tLoss: 0.46743\n",
      "Training Progress: \tEpoch 73 [4480/8883 (50.36%)]\t\tLoss: 0.54149\n",
      "Training Progress: \tEpoch 73 [4800/8883 (53.96%)]\t\tLoss: 0.47809\n",
      "Training Progress: \tEpoch 73 [5120/8883 (57.55%)]\t\tLoss: 0.57806\n",
      "Training Progress: \tEpoch 73 [5440/8883 (61.15%)]\t\tLoss: 0.99530\n",
      "Training Progress: \tEpoch 73 [5760/8883 (64.75%)]\t\tLoss: 0.75667\n",
      "Training Progress: \tEpoch 73 [6080/8883 (68.35%)]\t\tLoss: 0.49271\n",
      "Training Progress: \tEpoch 73 [6400/8883 (71.94%)]\t\tLoss: 0.65700\n",
      "Training Progress: \tEpoch 73 [6720/8883 (75.54%)]\t\tLoss: 0.81143\n",
      "Training Progress: \tEpoch 73 [7040/8883 (79.14%)]\t\tLoss: 0.67308\n",
      "Training Progress: \tEpoch 73 [7360/8883 (82.73%)]\t\tLoss: 0.56813\n",
      "Training Progress: \tEpoch 73 [7680/8883 (86.33%)]\t\tLoss: 0.44937\n",
      "Training Progress: \tEpoch 73 [8000/8883 (89.93%)]\t\tLoss: 0.74972\n",
      "Training Progress: \tEpoch 73 [8320/8883 (93.53%)]\t\tLoss: 0.60745\n",
      "Training Progress: \tEpoch 73 [8640/8883 (97.12%)]\t\tLoss: 0.72356\n",
      "\tTrain loss: 0.01156, Accuracy: 7278/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1667/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1115/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/8883 (0.00%)]\t\tLoss: 0.53359\n",
      "Training Progress: \tEpoch 74 [320/8883 (3.60%)]\t\tLoss: 0.58412\n",
      "Training Progress: \tEpoch 74 [640/8883 (7.19%)]\t\tLoss: 0.33829\n",
      "Training Progress: \tEpoch 74 [960/8883 (10.79%)]\t\tLoss: 0.56456\n",
      "Training Progress: \tEpoch 74 [1280/8883 (14.39%)]\t\tLoss: 0.62407\n",
      "Training Progress: \tEpoch 74 [1600/8883 (17.99%)]\t\tLoss: 0.34441\n",
      "Training Progress: \tEpoch 74 [1920/8883 (21.58%)]\t\tLoss: 0.46705\n",
      "Training Progress: \tEpoch 74 [2240/8883 (25.18%)]\t\tLoss: 0.50067\n",
      "Training Progress: \tEpoch 74 [2560/8883 (28.78%)]\t\tLoss: 0.69838\n",
      "Training Progress: \tEpoch 74 [2880/8883 (32.37%)]\t\tLoss: 0.54808\n",
      "Training Progress: \tEpoch 74 [3200/8883 (35.97%)]\t\tLoss: 0.68494\n",
      "Training Progress: \tEpoch 74 [3520/8883 (39.57%)]\t\tLoss: 0.66061\n",
      "Training Progress: \tEpoch 74 [3840/8883 (43.17%)]\t\tLoss: 0.73778\n",
      "Training Progress: \tEpoch 74 [4160/8883 (46.76%)]\t\tLoss: 0.70393\n",
      "Training Progress: \tEpoch 74 [4480/8883 (50.36%)]\t\tLoss: 0.42826\n",
      "Training Progress: \tEpoch 74 [4800/8883 (53.96%)]\t\tLoss: 0.68159\n",
      "Training Progress: \tEpoch 74 [5120/8883 (57.55%)]\t\tLoss: 0.52965\n",
      "Training Progress: \tEpoch 74 [5440/8883 (61.15%)]\t\tLoss: 0.67907\n",
      "Training Progress: \tEpoch 74 [5760/8883 (64.75%)]\t\tLoss: 0.58228\n",
      "Training Progress: \tEpoch 74 [6080/8883 (68.35%)]\t\tLoss: 0.42071\n",
      "Training Progress: \tEpoch 74 [6400/8883 (71.94%)]\t\tLoss: 0.73816\n",
      "Training Progress: \tEpoch 74 [6720/8883 (75.54%)]\t\tLoss: 1.18689\n",
      "Training Progress: \tEpoch 74 [7040/8883 (79.14%)]\t\tLoss: 0.72589\n",
      "Training Progress: \tEpoch 74 [7360/8883 (82.73%)]\t\tLoss: 0.55414\n",
      "Training Progress: \tEpoch 74 [7680/8883 (86.33%)]\t\tLoss: 0.44000\n",
      "Training Progress: \tEpoch 74 [8000/8883 (89.93%)]\t\tLoss: 0.79187\n",
      "Training Progress: \tEpoch 74 [8320/8883 (93.53%)]\t\tLoss: 0.68573\n",
      "Training Progress: \tEpoch 74 [8640/8883 (97.12%)]\t\tLoss: 0.54163\n",
      "\tTrain loss: 0.01143, Accuracy: 7256/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1660/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1130/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/8883 (0.00%)]\t\tLoss: 0.40456\n",
      "Training Progress: \tEpoch 75 [320/8883 (3.60%)]\t\tLoss: 0.50968\n",
      "Training Progress: \tEpoch 75 [640/8883 (7.19%)]\t\tLoss: 0.47339\n",
      "Training Progress: \tEpoch 75 [960/8883 (10.79%)]\t\tLoss: 0.87171\n",
      "Training Progress: \tEpoch 75 [1280/8883 (14.39%)]\t\tLoss: 0.58563\n",
      "Training Progress: \tEpoch 75 [1600/8883 (17.99%)]\t\tLoss: 0.45067\n",
      "Training Progress: \tEpoch 75 [1920/8883 (21.58%)]\t\tLoss: 0.60147\n",
      "Training Progress: \tEpoch 75 [2240/8883 (25.18%)]\t\tLoss: 0.57053\n",
      "Training Progress: \tEpoch 75 [2560/8883 (28.78%)]\t\tLoss: 0.60213\n",
      "Training Progress: \tEpoch 75 [2880/8883 (32.37%)]\t\tLoss: 0.71985\n",
      "Training Progress: \tEpoch 75 [3200/8883 (35.97%)]\t\tLoss: 0.65979\n",
      "Training Progress: \tEpoch 75 [3520/8883 (39.57%)]\t\tLoss: 0.61796\n",
      "Training Progress: \tEpoch 75 [3840/8883 (43.17%)]\t\tLoss: 1.03179\n",
      "Training Progress: \tEpoch 75 [4160/8883 (46.76%)]\t\tLoss: 0.59555\n",
      "Training Progress: \tEpoch 75 [4480/8883 (50.36%)]\t\tLoss: 0.43365\n",
      "Training Progress: \tEpoch 75 [4800/8883 (53.96%)]\t\tLoss: 0.61137\n",
      "Training Progress: \tEpoch 75 [5120/8883 (57.55%)]\t\tLoss: 0.62271\n",
      "Training Progress: \tEpoch 75 [5440/8883 (61.15%)]\t\tLoss: 0.64349\n",
      "Training Progress: \tEpoch 75 [5760/8883 (64.75%)]\t\tLoss: 0.54822\n",
      "Training Progress: \tEpoch 75 [6080/8883 (68.35%)]\t\tLoss: 0.36496\n",
      "Training Progress: \tEpoch 75 [6400/8883 (71.94%)]\t\tLoss: 0.58351\n",
      "Training Progress: \tEpoch 75 [6720/8883 (75.54%)]\t\tLoss: 0.59365\n",
      "Training Progress: \tEpoch 75 [7040/8883 (79.14%)]\t\tLoss: 0.90184\n",
      "Training Progress: \tEpoch 75 [7360/8883 (82.73%)]\t\tLoss: 0.65567\n",
      "Training Progress: \tEpoch 75 [7680/8883 (86.33%)]\t\tLoss: 0.47382\n",
      "Training Progress: \tEpoch 75 [8000/8883 (89.93%)]\t\tLoss: 0.48097\n",
      "Training Progress: \tEpoch 75 [8320/8883 (93.53%)]\t\tLoss: 0.67030\n",
      "Training Progress: \tEpoch 75 [8640/8883 (97.12%)]\t\tLoss: 0.73371\n",
      "\tTrain loss: 0.01171, Accuracy: 7241/8883 (81.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1652/1692 (97.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/8883 (0.00%)]\t\tLoss: 0.69674\n",
      "Training Progress: \tEpoch 76 [320/8883 (3.60%)]\t\tLoss: 0.60491\n",
      "Training Progress: \tEpoch 76 [640/8883 (7.19%)]\t\tLoss: 0.55325\n",
      "Training Progress: \tEpoch 76 [960/8883 (10.79%)]\t\tLoss: 0.70982\n",
      "Training Progress: \tEpoch 76 [1280/8883 (14.39%)]\t\tLoss: 0.46424\n",
      "Training Progress: \tEpoch 76 [1600/8883 (17.99%)]\t\tLoss: 0.44205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 76 [1920/8883 (21.58%)]\t\tLoss: 0.38641\n",
      "Training Progress: \tEpoch 76 [2240/8883 (25.18%)]\t\tLoss: 0.49873\n",
      "Training Progress: \tEpoch 76 [2560/8883 (28.78%)]\t\tLoss: 0.50374\n",
      "Training Progress: \tEpoch 76 [2880/8883 (32.37%)]\t\tLoss: 0.47585\n",
      "Training Progress: \tEpoch 76 [3200/8883 (35.97%)]\t\tLoss: 0.65173\n",
      "Training Progress: \tEpoch 76 [3520/8883 (39.57%)]\t\tLoss: 0.82526\n",
      "Training Progress: \tEpoch 76 [3840/8883 (43.17%)]\t\tLoss: 0.89145\n",
      "Training Progress: \tEpoch 76 [4160/8883 (46.76%)]\t\tLoss: 0.47398\n",
      "Training Progress: \tEpoch 76 [4480/8883 (50.36%)]\t\tLoss: 0.52580\n",
      "Training Progress: \tEpoch 76 [4800/8883 (53.96%)]\t\tLoss: 0.46294\n",
      "Training Progress: \tEpoch 76 [5120/8883 (57.55%)]\t\tLoss: 0.57929\n",
      "Training Progress: \tEpoch 76 [5440/8883 (61.15%)]\t\tLoss: 0.85971\n",
      "Training Progress: \tEpoch 76 [5760/8883 (64.75%)]\t\tLoss: 0.52329\n",
      "Training Progress: \tEpoch 76 [6080/8883 (68.35%)]\t\tLoss: 0.37201\n",
      "Training Progress: \tEpoch 76 [6400/8883 (71.94%)]\t\tLoss: 0.34759\n",
      "Training Progress: \tEpoch 76 [6720/8883 (75.54%)]\t\tLoss: 0.85355\n",
      "Training Progress: \tEpoch 76 [7040/8883 (79.14%)]\t\tLoss: 0.73581\n",
      "Training Progress: \tEpoch 76 [7360/8883 (82.73%)]\t\tLoss: 0.75229\n",
      "Training Progress: \tEpoch 76 [7680/8883 (86.33%)]\t\tLoss: 0.44736\n",
      "Training Progress: \tEpoch 76 [8000/8883 (89.93%)]\t\tLoss: 0.56745\n",
      "Training Progress: \tEpoch 76 [8320/8883 (93.53%)]\t\tLoss: 0.68525\n",
      "Training Progress: \tEpoch 76 [8640/8883 (97.12%)]\t\tLoss: 0.74591\n",
      "\tTrain loss: 0.01150, Accuracy: 7279/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1666/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/8883 (0.00%)]\t\tLoss: 0.59417\n",
      "Training Progress: \tEpoch 77 [320/8883 (3.60%)]\t\tLoss: 0.58276\n",
      "Training Progress: \tEpoch 77 [640/8883 (7.19%)]\t\tLoss: 0.44560\n",
      "Training Progress: \tEpoch 77 [960/8883 (10.79%)]\t\tLoss: 0.83657\n",
      "Training Progress: \tEpoch 77 [1280/8883 (14.39%)]\t\tLoss: 0.64393\n",
      "Training Progress: \tEpoch 77 [1600/8883 (17.99%)]\t\tLoss: 0.51616\n",
      "Training Progress: \tEpoch 77 [1920/8883 (21.58%)]\t\tLoss: 0.74677\n",
      "Training Progress: \tEpoch 77 [2240/8883 (25.18%)]\t\tLoss: 0.58226\n",
      "Training Progress: \tEpoch 77 [2560/8883 (28.78%)]\t\tLoss: 0.50578\n",
      "Training Progress: \tEpoch 77 [2880/8883 (32.37%)]\t\tLoss: 0.37273\n",
      "Training Progress: \tEpoch 77 [3200/8883 (35.97%)]\t\tLoss: 0.54423\n",
      "Training Progress: \tEpoch 77 [3520/8883 (39.57%)]\t\tLoss: 0.69063\n",
      "Training Progress: \tEpoch 77 [3840/8883 (43.17%)]\t\tLoss: 0.70170\n",
      "Training Progress: \tEpoch 77 [4160/8883 (46.76%)]\t\tLoss: 0.49099\n",
      "Training Progress: \tEpoch 77 [4480/8883 (50.36%)]\t\tLoss: 0.50757\n",
      "Training Progress: \tEpoch 77 [4800/8883 (53.96%)]\t\tLoss: 0.51652\n",
      "Training Progress: \tEpoch 77 [5120/8883 (57.55%)]\t\tLoss: 0.77791\n",
      "Training Progress: \tEpoch 77 [5440/8883 (61.15%)]\t\tLoss: 0.66739\n",
      "Training Progress: \tEpoch 77 [5760/8883 (64.75%)]\t\tLoss: 0.51802\n",
      "Training Progress: \tEpoch 77 [6080/8883 (68.35%)]\t\tLoss: 0.44189\n",
      "Training Progress: \tEpoch 77 [6400/8883 (71.94%)]\t\tLoss: 0.43238\n",
      "Training Progress: \tEpoch 77 [6720/8883 (75.54%)]\t\tLoss: 0.62837\n",
      "Training Progress: \tEpoch 77 [7040/8883 (79.14%)]\t\tLoss: 0.77288\n",
      "Training Progress: \tEpoch 77 [7360/8883 (82.73%)]\t\tLoss: 0.66894\n",
      "Training Progress: \tEpoch 77 [7680/8883 (86.33%)]\t\tLoss: 0.47643\n",
      "Training Progress: \tEpoch 77 [8000/8883 (89.93%)]\t\tLoss: 0.47424\n",
      "Training Progress: \tEpoch 77 [8320/8883 (93.53%)]\t\tLoss: 0.59056\n",
      "Training Progress: \tEpoch 77 [8640/8883 (97.12%)]\t\tLoss: 0.53172\n",
      "\tTrain loss: 0.01149, Accuracy: 7319/8883 (82.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1667/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/8883 (0.00%)]\t\tLoss: 0.68161\n",
      "Training Progress: \tEpoch 78 [320/8883 (3.60%)]\t\tLoss: 0.65713\n",
      "Training Progress: \tEpoch 78 [640/8883 (7.19%)]\t\tLoss: 0.42987\n",
      "Training Progress: \tEpoch 78 [960/8883 (10.79%)]\t\tLoss: 0.84441\n",
      "Training Progress: \tEpoch 78 [1280/8883 (14.39%)]\t\tLoss: 0.64465\n",
      "Training Progress: \tEpoch 78 [1600/8883 (17.99%)]\t\tLoss: 0.36567\n",
      "Training Progress: \tEpoch 78 [1920/8883 (21.58%)]\t\tLoss: 0.56061\n",
      "Training Progress: \tEpoch 78 [2240/8883 (25.18%)]\t\tLoss: 0.56392\n",
      "Training Progress: \tEpoch 78 [2560/8883 (28.78%)]\t\tLoss: 0.45694\n",
      "Training Progress: \tEpoch 78 [2880/8883 (32.37%)]\t\tLoss: 0.55441\n",
      "Training Progress: \tEpoch 78 [3200/8883 (35.97%)]\t\tLoss: 0.69514\n",
      "Training Progress: \tEpoch 78 [3520/8883 (39.57%)]\t\tLoss: 0.74106\n",
      "Training Progress: \tEpoch 78 [3840/8883 (43.17%)]\t\tLoss: 0.83549\n",
      "Training Progress: \tEpoch 78 [4160/8883 (46.76%)]\t\tLoss: 0.43852\n",
      "Training Progress: \tEpoch 78 [4480/8883 (50.36%)]\t\tLoss: 0.40301\n",
      "Training Progress: \tEpoch 78 [4800/8883 (53.96%)]\t\tLoss: 0.64154\n",
      "Training Progress: \tEpoch 78 [5120/8883 (57.55%)]\t\tLoss: 0.61520\n",
      "Training Progress: \tEpoch 78 [5440/8883 (61.15%)]\t\tLoss: 0.58434\n",
      "Training Progress: \tEpoch 78 [5760/8883 (64.75%)]\t\tLoss: 0.59807\n",
      "Training Progress: \tEpoch 78 [6080/8883 (68.35%)]\t\tLoss: 0.42455\n",
      "Training Progress: \tEpoch 78 [6400/8883 (71.94%)]\t\tLoss: 0.53048\n",
      "Training Progress: \tEpoch 78 [6720/8883 (75.54%)]\t\tLoss: 0.78460\n",
      "Training Progress: \tEpoch 78 [7040/8883 (79.14%)]\t\tLoss: 0.70254\n",
      "Training Progress: \tEpoch 78 [7360/8883 (82.73%)]\t\tLoss: 0.63933\n",
      "Training Progress: \tEpoch 78 [7680/8883 (86.33%)]\t\tLoss: 0.60912\n",
      "Training Progress: \tEpoch 78 [8000/8883 (89.93%)]\t\tLoss: 0.70568\n",
      "Training Progress: \tEpoch 78 [8320/8883 (93.53%)]\t\tLoss: 0.73707\n",
      "Training Progress: \tEpoch 78 [8640/8883 (97.12%)]\t\tLoss: 0.63034\n",
      "\tTrain loss: 0.01155, Accuracy: 7276/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1123/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/8883 (0.00%)]\t\tLoss: 0.65025\n",
      "Training Progress: \tEpoch 79 [320/8883 (3.60%)]\t\tLoss: 0.67289\n",
      "Training Progress: \tEpoch 79 [640/8883 (7.19%)]\t\tLoss: 0.34696\n",
      "Training Progress: \tEpoch 79 [960/8883 (10.79%)]\t\tLoss: 0.43053\n",
      "Training Progress: \tEpoch 79 [1280/8883 (14.39%)]\t\tLoss: 0.52548\n",
      "Training Progress: \tEpoch 79 [1600/8883 (17.99%)]\t\tLoss: 0.40252\n",
      "Training Progress: \tEpoch 79 [1920/8883 (21.58%)]\t\tLoss: 0.58963\n",
      "Training Progress: \tEpoch 79 [2240/8883 (25.18%)]\t\tLoss: 0.45480\n",
      "Training Progress: \tEpoch 79 [2560/8883 (28.78%)]\t\tLoss: 0.57965\n",
      "Training Progress: \tEpoch 79 [2880/8883 (32.37%)]\t\tLoss: 0.68696\n",
      "Training Progress: \tEpoch 79 [3200/8883 (35.97%)]\t\tLoss: 0.65702\n",
      "Training Progress: \tEpoch 79 [3520/8883 (39.57%)]\t\tLoss: 0.75186\n",
      "Training Progress: \tEpoch 79 [3840/8883 (43.17%)]\t\tLoss: 0.81176\n",
      "Training Progress: \tEpoch 79 [4160/8883 (46.76%)]\t\tLoss: 0.55964\n",
      "Training Progress: \tEpoch 79 [4480/8883 (50.36%)]\t\tLoss: 0.55829\n",
      "Training Progress: \tEpoch 79 [4800/8883 (53.96%)]\t\tLoss: 0.59359\n",
      "Training Progress: \tEpoch 79 [5120/8883 (57.55%)]\t\tLoss: 0.51612\n",
      "Training Progress: \tEpoch 79 [5440/8883 (61.15%)]\t\tLoss: 0.72183\n",
      "Training Progress: \tEpoch 79 [5760/8883 (64.75%)]\t\tLoss: 0.47442\n",
      "Training Progress: \tEpoch 79 [6080/8883 (68.35%)]\t\tLoss: 0.33619\n",
      "Training Progress: \tEpoch 79 [6400/8883 (71.94%)]\t\tLoss: 0.46121\n",
      "Training Progress: \tEpoch 79 [6720/8883 (75.54%)]\t\tLoss: 0.65501\n",
      "Training Progress: \tEpoch 79 [7040/8883 (79.14%)]\t\tLoss: 0.66267\n",
      "Training Progress: \tEpoch 79 [7360/8883 (82.73%)]\t\tLoss: 0.76896\n",
      "Training Progress: \tEpoch 79 [7680/8883 (86.33%)]\t\tLoss: 0.57935\n",
      "Training Progress: \tEpoch 79 [8000/8883 (89.93%)]\t\tLoss: 0.56381\n",
      "Training Progress: \tEpoch 79 [8320/8883 (93.53%)]\t\tLoss: 0.60949\n",
      "Training Progress: \tEpoch 79 [8640/8883 (97.12%)]\t\tLoss: 0.69721\n",
      "\tTrain loss: 0.01144, Accuracy: 7269/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1135/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/8883 (0.00%)]\t\tLoss: 0.54883\n",
      "Training Progress: \tEpoch 80 [320/8883 (3.60%)]\t\tLoss: 0.76560\n",
      "Training Progress: \tEpoch 80 [640/8883 (7.19%)]\t\tLoss: 0.50418\n",
      "Training Progress: \tEpoch 80 [960/8883 (10.79%)]\t\tLoss: 0.58600\n",
      "Training Progress: \tEpoch 80 [1280/8883 (14.39%)]\t\tLoss: 0.77459\n",
      "Training Progress: \tEpoch 80 [1600/8883 (17.99%)]\t\tLoss: 0.44114\n",
      "Training Progress: \tEpoch 80 [1920/8883 (21.58%)]\t\tLoss: 0.45117\n",
      "Training Progress: \tEpoch 80 [2240/8883 (25.18%)]\t\tLoss: 0.60929\n",
      "Training Progress: \tEpoch 80 [2560/8883 (28.78%)]\t\tLoss: 0.63663\n",
      "Training Progress: \tEpoch 80 [2880/8883 (32.37%)]\t\tLoss: 0.67882\n",
      "Training Progress: \tEpoch 80 [3200/8883 (35.97%)]\t\tLoss: 0.64130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 80 [3520/8883 (39.57%)]\t\tLoss: 0.60326\n",
      "Training Progress: \tEpoch 80 [3840/8883 (43.17%)]\t\tLoss: 0.87973\n",
      "Training Progress: \tEpoch 80 [4160/8883 (46.76%)]\t\tLoss: 0.83205\n",
      "Training Progress: \tEpoch 80 [4480/8883 (50.36%)]\t\tLoss: 0.54868\n",
      "Training Progress: \tEpoch 80 [4800/8883 (53.96%)]\t\tLoss: 0.38789\n",
      "Training Progress: \tEpoch 80 [5120/8883 (57.55%)]\t\tLoss: 0.65156\n",
      "Training Progress: \tEpoch 80 [5440/8883 (61.15%)]\t\tLoss: 0.64786\n",
      "Training Progress: \tEpoch 80 [5760/8883 (64.75%)]\t\tLoss: 0.60675\n",
      "Training Progress: \tEpoch 80 [6080/8883 (68.35%)]\t\tLoss: 0.45681\n",
      "Training Progress: \tEpoch 80 [6400/8883 (71.94%)]\t\tLoss: 0.73405\n",
      "Training Progress: \tEpoch 80 [6720/8883 (75.54%)]\t\tLoss: 0.66070\n",
      "Training Progress: \tEpoch 80 [7040/8883 (79.14%)]\t\tLoss: 0.73366\n",
      "Training Progress: \tEpoch 80 [7360/8883 (82.73%)]\t\tLoss: 0.56794\n",
      "Training Progress: \tEpoch 80 [7680/8883 (86.33%)]\t\tLoss: 0.59189\n",
      "Training Progress: \tEpoch 80 [8000/8883 (89.93%)]\t\tLoss: 0.52573\n",
      "Training Progress: \tEpoch 80 [8320/8883 (93.53%)]\t\tLoss: 0.54068\n",
      "Training Progress: \tEpoch 80 [8640/8883 (97.12%)]\t\tLoss: 0.69414\n",
      "\tTrain loss: 0.01130, Accuracy: 7297/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1675/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1152/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/8883 (0.00%)]\t\tLoss: 0.64910\n",
      "Training Progress: \tEpoch 81 [320/8883 (3.60%)]\t\tLoss: 0.78164\n",
      "Training Progress: \tEpoch 81 [640/8883 (7.19%)]\t\tLoss: 0.68691\n",
      "Training Progress: \tEpoch 81 [960/8883 (10.79%)]\t\tLoss: 0.55284\n",
      "Training Progress: \tEpoch 81 [1280/8883 (14.39%)]\t\tLoss: 0.66751\n",
      "Training Progress: \tEpoch 81 [1600/8883 (17.99%)]\t\tLoss: 0.43722\n",
      "Training Progress: \tEpoch 81 [1920/8883 (21.58%)]\t\tLoss: 0.42686\n",
      "Training Progress: \tEpoch 81 [2240/8883 (25.18%)]\t\tLoss: 0.59189\n",
      "Training Progress: \tEpoch 81 [2560/8883 (28.78%)]\t\tLoss: 0.57612\n",
      "Training Progress: \tEpoch 81 [2880/8883 (32.37%)]\t\tLoss: 0.56192\n",
      "Training Progress: \tEpoch 81 [3200/8883 (35.97%)]\t\tLoss: 0.67189\n",
      "Training Progress: \tEpoch 81 [3520/8883 (39.57%)]\t\tLoss: 0.77522\n",
      "Training Progress: \tEpoch 81 [3840/8883 (43.17%)]\t\tLoss: 0.81742\n",
      "Training Progress: \tEpoch 81 [4160/8883 (46.76%)]\t\tLoss: 0.60821\n",
      "Training Progress: \tEpoch 81 [4480/8883 (50.36%)]\t\tLoss: 0.46717\n",
      "Training Progress: \tEpoch 81 [4800/8883 (53.96%)]\t\tLoss: 0.43025\n",
      "Training Progress: \tEpoch 81 [5120/8883 (57.55%)]\t\tLoss: 0.90709\n",
      "Training Progress: \tEpoch 81 [5440/8883 (61.15%)]\t\tLoss: 0.56843\n",
      "Training Progress: \tEpoch 81 [5760/8883 (64.75%)]\t\tLoss: 0.52194\n",
      "Training Progress: \tEpoch 81 [6080/8883 (68.35%)]\t\tLoss: 0.50512\n",
      "Training Progress: \tEpoch 81 [6400/8883 (71.94%)]\t\tLoss: 0.68653\n",
      "Training Progress: \tEpoch 81 [6720/8883 (75.54%)]\t\tLoss: 0.71298\n",
      "Training Progress: \tEpoch 81 [7040/8883 (79.14%)]\t\tLoss: 0.76181\n",
      "Training Progress: \tEpoch 81 [7360/8883 (82.73%)]\t\tLoss: 0.66384\n",
      "Training Progress: \tEpoch 81 [7680/8883 (86.33%)]\t\tLoss: 0.73927\n",
      "Training Progress: \tEpoch 81 [8000/8883 (89.93%)]\t\tLoss: 0.60409\n",
      "Training Progress: \tEpoch 81 [8320/8883 (93.53%)]\t\tLoss: 0.58314\n",
      "Training Progress: \tEpoch 81 [8640/8883 (97.12%)]\t\tLoss: 0.90847\n",
      "\tTrain loss: 0.01138, Accuracy: 7295/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1672/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/8883 (0.00%)]\t\tLoss: 0.47858\n",
      "Training Progress: \tEpoch 82 [320/8883 (3.60%)]\t\tLoss: 0.43284\n",
      "Training Progress: \tEpoch 82 [640/8883 (7.19%)]\t\tLoss: 0.53317\n",
      "Training Progress: \tEpoch 82 [960/8883 (10.79%)]\t\tLoss: 0.70076\n",
      "Training Progress: \tEpoch 82 [1280/8883 (14.39%)]\t\tLoss: 0.56218\n",
      "Training Progress: \tEpoch 82 [1600/8883 (17.99%)]\t\tLoss: 0.34136\n",
      "Training Progress: \tEpoch 82 [1920/8883 (21.58%)]\t\tLoss: 0.79347\n",
      "Training Progress: \tEpoch 82 [2240/8883 (25.18%)]\t\tLoss: 0.56516\n",
      "Training Progress: \tEpoch 82 [2560/8883 (28.78%)]\t\tLoss: 0.64787\n",
      "Training Progress: \tEpoch 82 [2880/8883 (32.37%)]\t\tLoss: 0.68977\n",
      "Training Progress: \tEpoch 82 [3200/8883 (35.97%)]\t\tLoss: 0.50541\n",
      "Training Progress: \tEpoch 82 [3520/8883 (39.57%)]\t\tLoss: 0.80838\n",
      "Training Progress: \tEpoch 82 [3840/8883 (43.17%)]\t\tLoss: 0.74919\n",
      "Training Progress: \tEpoch 82 [4160/8883 (46.76%)]\t\tLoss: 0.46310\n",
      "Training Progress: \tEpoch 82 [4480/8883 (50.36%)]\t\tLoss: 0.53582\n",
      "Training Progress: \tEpoch 82 [4800/8883 (53.96%)]\t\tLoss: 0.50369\n",
      "Training Progress: \tEpoch 82 [5120/8883 (57.55%)]\t\tLoss: 0.69401\n",
      "Training Progress: \tEpoch 82 [5440/8883 (61.15%)]\t\tLoss: 0.64912\n",
      "Training Progress: \tEpoch 82 [5760/8883 (64.75%)]\t\tLoss: 0.49984\n",
      "Training Progress: \tEpoch 82 [6080/8883 (68.35%)]\t\tLoss: 0.43505\n",
      "Training Progress: \tEpoch 82 [6400/8883 (71.94%)]\t\tLoss: 0.49414\n",
      "Training Progress: \tEpoch 82 [6720/8883 (75.54%)]\t\tLoss: 0.85935\n",
      "Training Progress: \tEpoch 82 [7040/8883 (79.14%)]\t\tLoss: 0.65485\n",
      "Training Progress: \tEpoch 82 [7360/8883 (82.73%)]\t\tLoss: 0.60423\n",
      "Training Progress: \tEpoch 82 [7680/8883 (86.33%)]\t\tLoss: 0.46982\n",
      "Training Progress: \tEpoch 82 [8000/8883 (89.93%)]\t\tLoss: 0.66161\n",
      "Training Progress: \tEpoch 82 [8320/8883 (93.53%)]\t\tLoss: 0.46036\n",
      "Training Progress: \tEpoch 82 [8640/8883 (97.12%)]\t\tLoss: 0.53108\n",
      "\tTrain loss: 0.01134, Accuracy: 7280/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1662/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1135/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/8883 (0.00%)]\t\tLoss: 0.66313\n",
      "Training Progress: \tEpoch 83 [320/8883 (3.60%)]\t\tLoss: 0.55662\n",
      "Training Progress: \tEpoch 83 [640/8883 (7.19%)]\t\tLoss: 0.38368\n",
      "Training Progress: \tEpoch 83 [960/8883 (10.79%)]\t\tLoss: 0.73724\n",
      "Training Progress: \tEpoch 83 [1280/8883 (14.39%)]\t\tLoss: 0.60859\n",
      "Training Progress: \tEpoch 83 [1600/8883 (17.99%)]\t\tLoss: 0.22668\n",
      "Training Progress: \tEpoch 83 [1920/8883 (21.58%)]\t\tLoss: 0.40192\n",
      "Training Progress: \tEpoch 83 [2240/8883 (25.18%)]\t\tLoss: 0.84640\n",
      "Training Progress: \tEpoch 83 [2560/8883 (28.78%)]\t\tLoss: 0.76533\n",
      "Training Progress: \tEpoch 83 [2880/8883 (32.37%)]\t\tLoss: 0.60574\n",
      "Training Progress: \tEpoch 83 [3200/8883 (35.97%)]\t\tLoss: 0.86501\n",
      "Training Progress: \tEpoch 83 [3520/8883 (39.57%)]\t\tLoss: 0.78269\n",
      "Training Progress: \tEpoch 83 [3840/8883 (43.17%)]\t\tLoss: 0.69100\n",
      "Training Progress: \tEpoch 83 [4160/8883 (46.76%)]\t\tLoss: 0.74001\n",
      "Training Progress: \tEpoch 83 [4480/8883 (50.36%)]\t\tLoss: 0.50299\n",
      "Training Progress: \tEpoch 83 [4800/8883 (53.96%)]\t\tLoss: 0.70645\n",
      "Training Progress: \tEpoch 83 [5120/8883 (57.55%)]\t\tLoss: 0.72272\n",
      "Training Progress: \tEpoch 83 [5440/8883 (61.15%)]\t\tLoss: 0.68004\n",
      "Training Progress: \tEpoch 83 [5760/8883 (64.75%)]\t\tLoss: 0.73077\n",
      "Training Progress: \tEpoch 83 [6080/8883 (68.35%)]\t\tLoss: 0.43069\n",
      "Training Progress: \tEpoch 83 [6400/8883 (71.94%)]\t\tLoss: 0.42714\n",
      "Training Progress: \tEpoch 83 [6720/8883 (75.54%)]\t\tLoss: 0.61939\n",
      "Training Progress: \tEpoch 83 [7040/8883 (79.14%)]\t\tLoss: 0.93788\n",
      "Training Progress: \tEpoch 83 [7360/8883 (82.73%)]\t\tLoss: 0.67748\n",
      "Training Progress: \tEpoch 83 [7680/8883 (86.33%)]\t\tLoss: 0.50980\n",
      "Training Progress: \tEpoch 83 [8000/8883 (89.93%)]\t\tLoss: 0.56526\n",
      "Training Progress: \tEpoch 83 [8320/8883 (93.53%)]\t\tLoss: 0.72286\n",
      "Training Progress: \tEpoch 83 [8640/8883 (97.12%)]\t\tLoss: 0.50349\n",
      "\tTrain loss: 0.01135, Accuracy: 7276/8883 (81.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1668/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1134/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/8883 (0.00%)]\t\tLoss: 0.47157\n",
      "Training Progress: \tEpoch 84 [320/8883 (3.60%)]\t\tLoss: 0.57884\n",
      "Training Progress: \tEpoch 84 [640/8883 (7.19%)]\t\tLoss: 0.54780\n",
      "Training Progress: \tEpoch 84 [960/8883 (10.79%)]\t\tLoss: 0.56461\n",
      "Training Progress: \tEpoch 84 [1280/8883 (14.39%)]\t\tLoss: 0.79615\n",
      "Training Progress: \tEpoch 84 [1600/8883 (17.99%)]\t\tLoss: 0.52001\n",
      "Training Progress: \tEpoch 84 [1920/8883 (21.58%)]\t\tLoss: 0.61534\n",
      "Training Progress: \tEpoch 84 [2240/8883 (25.18%)]\t\tLoss: 0.41468\n",
      "Training Progress: \tEpoch 84 [2560/8883 (28.78%)]\t\tLoss: 0.54335\n",
      "Training Progress: \tEpoch 84 [2880/8883 (32.37%)]\t\tLoss: 0.60133\n",
      "Training Progress: \tEpoch 84 [3200/8883 (35.97%)]\t\tLoss: 0.51646\n",
      "Training Progress: \tEpoch 84 [3520/8883 (39.57%)]\t\tLoss: 0.78461\n",
      "Training Progress: \tEpoch 84 [3840/8883 (43.17%)]\t\tLoss: 0.75620\n",
      "Training Progress: \tEpoch 84 [4160/8883 (46.76%)]\t\tLoss: 0.44533\n",
      "Training Progress: \tEpoch 84 [4480/8883 (50.36%)]\t\tLoss: 0.40255\n",
      "Training Progress: \tEpoch 84 [4800/8883 (53.96%)]\t\tLoss: 0.91411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [5120/8883 (57.55%)]\t\tLoss: 0.64169\n",
      "Training Progress: \tEpoch 84 [5440/8883 (61.15%)]\t\tLoss: 0.66595\n",
      "Training Progress: \tEpoch 84 [5760/8883 (64.75%)]\t\tLoss: 0.48739\n",
      "Training Progress: \tEpoch 84 [6080/8883 (68.35%)]\t\tLoss: 0.46943\n",
      "Training Progress: \tEpoch 84 [6400/8883 (71.94%)]\t\tLoss: 0.52662\n",
      "Training Progress: \tEpoch 84 [6720/8883 (75.54%)]\t\tLoss: 0.77233\n",
      "Training Progress: \tEpoch 84 [7040/8883 (79.14%)]\t\tLoss: 0.68823\n",
      "Training Progress: \tEpoch 84 [7360/8883 (82.73%)]\t\tLoss: 0.62721\n",
      "Training Progress: \tEpoch 84 [7680/8883 (86.33%)]\t\tLoss: 0.46912\n",
      "Training Progress: \tEpoch 84 [8000/8883 (89.93%)]\t\tLoss: 0.45710\n",
      "Training Progress: \tEpoch 84 [8320/8883 (93.53%)]\t\tLoss: 0.59943\n",
      "Training Progress: \tEpoch 84 [8640/8883 (97.12%)]\t\tLoss: 0.74946\n",
      "\tTrain loss: 0.01143, Accuracy: 7261/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1670/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1130/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/8883 (0.00%)]\t\tLoss: 0.47736\n",
      "Training Progress: \tEpoch 85 [320/8883 (3.60%)]\t\tLoss: 0.57538\n",
      "Training Progress: \tEpoch 85 [640/8883 (7.19%)]\t\tLoss: 0.43875\n",
      "Training Progress: \tEpoch 85 [960/8883 (10.79%)]\t\tLoss: 0.87646\n",
      "Training Progress: \tEpoch 85 [1280/8883 (14.39%)]\t\tLoss: 0.54863\n",
      "Training Progress: \tEpoch 85 [1600/8883 (17.99%)]\t\tLoss: 0.43454\n",
      "Training Progress: \tEpoch 85 [1920/8883 (21.58%)]\t\tLoss: 0.57138\n",
      "Training Progress: \tEpoch 85 [2240/8883 (25.18%)]\t\tLoss: 0.44162\n",
      "Training Progress: \tEpoch 85 [2560/8883 (28.78%)]\t\tLoss: 0.57753\n",
      "Training Progress: \tEpoch 85 [2880/8883 (32.37%)]\t\tLoss: 0.52245\n",
      "Training Progress: \tEpoch 85 [3200/8883 (35.97%)]\t\tLoss: 0.72900\n",
      "Training Progress: \tEpoch 85 [3520/8883 (39.57%)]\t\tLoss: 0.77173\n",
      "Training Progress: \tEpoch 85 [3840/8883 (43.17%)]\t\tLoss: 0.74253\n",
      "Training Progress: \tEpoch 85 [4160/8883 (46.76%)]\t\tLoss: 0.51526\n",
      "Training Progress: \tEpoch 85 [4480/8883 (50.36%)]\t\tLoss: 0.49221\n",
      "Training Progress: \tEpoch 85 [4800/8883 (53.96%)]\t\tLoss: 0.36322\n",
      "Training Progress: \tEpoch 85 [5120/8883 (57.55%)]\t\tLoss: 0.77229\n",
      "Training Progress: \tEpoch 85 [5440/8883 (61.15%)]\t\tLoss: 0.67101\n",
      "Training Progress: \tEpoch 85 [5760/8883 (64.75%)]\t\tLoss: 0.65191\n",
      "Training Progress: \tEpoch 85 [6080/8883 (68.35%)]\t\tLoss: 0.53098\n",
      "Training Progress: \tEpoch 85 [6400/8883 (71.94%)]\t\tLoss: 0.64667\n",
      "Training Progress: \tEpoch 85 [6720/8883 (75.54%)]\t\tLoss: 0.71073\n",
      "Training Progress: \tEpoch 85 [7040/8883 (79.14%)]\t\tLoss: 0.80395\n",
      "Training Progress: \tEpoch 85 [7360/8883 (82.73%)]\t\tLoss: 0.61215\n",
      "Training Progress: \tEpoch 85 [7680/8883 (86.33%)]\t\tLoss: 0.41313\n",
      "Training Progress: \tEpoch 85 [8000/8883 (89.93%)]\t\tLoss: 0.55951\n",
      "Training Progress: \tEpoch 85 [8320/8883 (93.53%)]\t\tLoss: 0.66242\n",
      "Training Progress: \tEpoch 85 [8640/8883 (97.12%)]\t\tLoss: 0.56196\n",
      "\tTrain loss: 0.01132, Accuracy: 7276/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1672/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1160/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/8883 (0.00%)]\t\tLoss: 0.53911\n",
      "Training Progress: \tEpoch 86 [320/8883 (3.60%)]\t\tLoss: 0.68214\n",
      "Training Progress: \tEpoch 86 [640/8883 (7.19%)]\t\tLoss: 0.53464\n",
      "Training Progress: \tEpoch 86 [960/8883 (10.79%)]\t\tLoss: 0.53828\n",
      "Training Progress: \tEpoch 86 [1280/8883 (14.39%)]\t\tLoss: 0.56874\n",
      "Training Progress: \tEpoch 86 [1600/8883 (17.99%)]\t\tLoss: 0.37089\n",
      "Training Progress: \tEpoch 86 [1920/8883 (21.58%)]\t\tLoss: 0.41338\n",
      "Training Progress: \tEpoch 86 [2240/8883 (25.18%)]\t\tLoss: 0.58379\n",
      "Training Progress: \tEpoch 86 [2560/8883 (28.78%)]\t\tLoss: 0.50812\n",
      "Training Progress: \tEpoch 86 [2880/8883 (32.37%)]\t\tLoss: 0.55586\n",
      "Training Progress: \tEpoch 86 [3200/8883 (35.97%)]\t\tLoss: 0.62243\n",
      "Training Progress: \tEpoch 86 [3520/8883 (39.57%)]\t\tLoss: 0.70481\n",
      "Training Progress: \tEpoch 86 [3840/8883 (43.17%)]\t\tLoss: 0.70093\n",
      "Training Progress: \tEpoch 86 [4160/8883 (46.76%)]\t\tLoss: 0.43719\n",
      "Training Progress: \tEpoch 86 [4480/8883 (50.36%)]\t\tLoss: 0.56386\n",
      "Training Progress: \tEpoch 86 [4800/8883 (53.96%)]\t\tLoss: 0.69475\n",
      "Training Progress: \tEpoch 86 [5120/8883 (57.55%)]\t\tLoss: 0.68432\n",
      "Training Progress: \tEpoch 86 [5440/8883 (61.15%)]\t\tLoss: 0.57506\n",
      "Training Progress: \tEpoch 86 [5760/8883 (64.75%)]\t\tLoss: 0.74104\n",
      "Training Progress: \tEpoch 86 [6080/8883 (68.35%)]\t\tLoss: 0.45444\n",
      "Training Progress: \tEpoch 86 [6400/8883 (71.94%)]\t\tLoss: 0.61317\n",
      "Training Progress: \tEpoch 86 [6720/8883 (75.54%)]\t\tLoss: 0.76509\n",
      "Training Progress: \tEpoch 86 [7040/8883 (79.14%)]\t\tLoss: 0.70666\n",
      "Training Progress: \tEpoch 86 [7360/8883 (82.73%)]\t\tLoss: 0.67418\n",
      "Training Progress: \tEpoch 86 [7680/8883 (86.33%)]\t\tLoss: 0.50241\n",
      "Training Progress: \tEpoch 86 [8000/8883 (89.93%)]\t\tLoss: 0.57433\n",
      "Training Progress: \tEpoch 86 [8320/8883 (93.53%)]\t\tLoss: 0.52184\n",
      "Training Progress: \tEpoch 86 [8640/8883 (97.12%)]\t\tLoss: 0.69729\n",
      "\tTrain loss: 0.01127, Accuracy: 7289/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1674/1692 (98.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1151/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/8883 (0.00%)]\t\tLoss: 0.51693\n",
      "Training Progress: \tEpoch 87 [320/8883 (3.60%)]\t\tLoss: 0.63528\n",
      "Training Progress: \tEpoch 87 [640/8883 (7.19%)]\t\tLoss: 0.62582\n",
      "Training Progress: \tEpoch 87 [960/8883 (10.79%)]\t\tLoss: 0.63267\n",
      "Training Progress: \tEpoch 87 [1280/8883 (14.39%)]\t\tLoss: 0.68490\n",
      "Training Progress: \tEpoch 87 [1600/8883 (17.99%)]\t\tLoss: 0.24205\n",
      "Training Progress: \tEpoch 87 [1920/8883 (21.58%)]\t\tLoss: 0.59015\n",
      "Training Progress: \tEpoch 87 [2240/8883 (25.18%)]\t\tLoss: 0.46564\n",
      "Training Progress: \tEpoch 87 [2560/8883 (28.78%)]\t\tLoss: 0.55028\n",
      "Training Progress: \tEpoch 87 [2880/8883 (32.37%)]\t\tLoss: 0.74371\n",
      "Training Progress: \tEpoch 87 [3200/8883 (35.97%)]\t\tLoss: 0.76928\n",
      "Training Progress: \tEpoch 87 [3520/8883 (39.57%)]\t\tLoss: 0.75023\n",
      "Training Progress: \tEpoch 87 [3840/8883 (43.17%)]\t\tLoss: 0.72743\n",
      "Training Progress: \tEpoch 87 [4160/8883 (46.76%)]\t\tLoss: 0.48688\n",
      "Training Progress: \tEpoch 87 [4480/8883 (50.36%)]\t\tLoss: 0.61424\n",
      "Training Progress: \tEpoch 87 [4800/8883 (53.96%)]\t\tLoss: 0.67491\n",
      "Training Progress: \tEpoch 87 [5120/8883 (57.55%)]\t\tLoss: 0.55453\n",
      "Training Progress: \tEpoch 87 [5440/8883 (61.15%)]\t\tLoss: 0.60922\n",
      "Training Progress: \tEpoch 87 [5760/8883 (64.75%)]\t\tLoss: 0.48336\n",
      "Training Progress: \tEpoch 87 [6080/8883 (68.35%)]\t\tLoss: 0.44493\n",
      "Training Progress: \tEpoch 87 [6400/8883 (71.94%)]\t\tLoss: 0.65993\n",
      "Training Progress: \tEpoch 87 [6720/8883 (75.54%)]\t\tLoss: 0.80936\n",
      "Training Progress: \tEpoch 87 [7040/8883 (79.14%)]\t\tLoss: 0.71320\n",
      "Training Progress: \tEpoch 87 [7360/8883 (82.73%)]\t\tLoss: 0.61081\n",
      "Training Progress: \tEpoch 87 [7680/8883 (86.33%)]\t\tLoss: 0.55438\n",
      "Training Progress: \tEpoch 87 [8000/8883 (89.93%)]\t\tLoss: 0.63807\n",
      "Training Progress: \tEpoch 87 [8320/8883 (93.53%)]\t\tLoss: 0.51804\n",
      "Training Progress: \tEpoch 87 [8640/8883 (97.12%)]\t\tLoss: 0.70380\n",
      "\tTrain loss: 0.01133, Accuracy: 7278/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1676/1692 (99.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1148/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/8883 (0.00%)]\t\tLoss: 0.62185\n",
      "Training Progress: \tEpoch 88 [320/8883 (3.60%)]\t\tLoss: 0.62551\n",
      "Training Progress: \tEpoch 88 [640/8883 (7.19%)]\t\tLoss: 0.44778\n",
      "Training Progress: \tEpoch 88 [960/8883 (10.79%)]\t\tLoss: 0.58427\n",
      "Training Progress: \tEpoch 88 [1280/8883 (14.39%)]\t\tLoss: 0.62995\n",
      "Training Progress: \tEpoch 88 [1600/8883 (17.99%)]\t\tLoss: 0.44816\n",
      "Training Progress: \tEpoch 88 [1920/8883 (21.58%)]\t\tLoss: 0.48262\n",
      "Training Progress: \tEpoch 88 [2240/8883 (25.18%)]\t\tLoss: 0.51385\n",
      "Training Progress: \tEpoch 88 [2560/8883 (28.78%)]\t\tLoss: 0.61520\n",
      "Training Progress: \tEpoch 88 [2880/8883 (32.37%)]\t\tLoss: 0.57229\n",
      "Training Progress: \tEpoch 88 [3200/8883 (35.97%)]\t\tLoss: 0.82147\n",
      "Training Progress: \tEpoch 88 [3520/8883 (39.57%)]\t\tLoss: 0.78650\n",
      "Training Progress: \tEpoch 88 [3840/8883 (43.17%)]\t\tLoss: 0.57117\n",
      "Training Progress: \tEpoch 88 [4160/8883 (46.76%)]\t\tLoss: 0.54226\n",
      "Training Progress: \tEpoch 88 [4480/8883 (50.36%)]\t\tLoss: 0.91183\n",
      "Training Progress: \tEpoch 88 [4800/8883 (53.96%)]\t\tLoss: 0.64274\n",
      "Training Progress: \tEpoch 88 [5120/8883 (57.55%)]\t\tLoss: 0.64832\n",
      "Training Progress: \tEpoch 88 [5440/8883 (61.15%)]\t\tLoss: 0.74100\n",
      "Training Progress: \tEpoch 88 [5760/8883 (64.75%)]\t\tLoss: 0.54801\n",
      "Training Progress: \tEpoch 88 [6080/8883 (68.35%)]\t\tLoss: 0.62302\n",
      "Training Progress: \tEpoch 88 [6400/8883 (71.94%)]\t\tLoss: 0.56098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 88 [6720/8883 (75.54%)]\t\tLoss: 0.88077\n",
      "Training Progress: \tEpoch 88 [7040/8883 (79.14%)]\t\tLoss: 0.80978\n",
      "Training Progress: \tEpoch 88 [7360/8883 (82.73%)]\t\tLoss: 0.63058\n",
      "Training Progress: \tEpoch 88 [7680/8883 (86.33%)]\t\tLoss: 0.45778\n",
      "Training Progress: \tEpoch 88 [8000/8883 (89.93%)]\t\tLoss: 0.77266\n",
      "Training Progress: \tEpoch 88 [8320/8883 (93.53%)]\t\tLoss: 0.75109\n",
      "Training Progress: \tEpoch 88 [8640/8883 (97.12%)]\t\tLoss: 0.57951\n",
      "\tTrain loss: 0.01130, Accuracy: 7293/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1666/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1156/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/8883 (0.00%)]\t\tLoss: 0.61985\n",
      "Training Progress: \tEpoch 89 [320/8883 (3.60%)]\t\tLoss: 0.56199\n",
      "Training Progress: \tEpoch 89 [640/8883 (7.19%)]\t\tLoss: 0.62332\n",
      "Training Progress: \tEpoch 89 [960/8883 (10.79%)]\t\tLoss: 0.80305\n",
      "Training Progress: \tEpoch 89 [1280/8883 (14.39%)]\t\tLoss: 0.55522\n",
      "Training Progress: \tEpoch 89 [1600/8883 (17.99%)]\t\tLoss: 0.38596\n",
      "Training Progress: \tEpoch 89 [1920/8883 (21.58%)]\t\tLoss: 0.56237\n",
      "Training Progress: \tEpoch 89 [2240/8883 (25.18%)]\t\tLoss: 0.47127\n",
      "Training Progress: \tEpoch 89 [2560/8883 (28.78%)]\t\tLoss: 0.66533\n",
      "Training Progress: \tEpoch 89 [2880/8883 (32.37%)]\t\tLoss: 0.53270\n",
      "Training Progress: \tEpoch 89 [3200/8883 (35.97%)]\t\tLoss: 0.92091\n",
      "Training Progress: \tEpoch 89 [3520/8883 (39.57%)]\t\tLoss: 0.77968\n",
      "Training Progress: \tEpoch 89 [3840/8883 (43.17%)]\t\tLoss: 0.65191\n",
      "Training Progress: \tEpoch 89 [4160/8883 (46.76%)]\t\tLoss: 0.59514\n",
      "Training Progress: \tEpoch 89 [4480/8883 (50.36%)]\t\tLoss: 0.68869\n",
      "Training Progress: \tEpoch 89 [4800/8883 (53.96%)]\t\tLoss: 0.36107\n",
      "Training Progress: \tEpoch 89 [5120/8883 (57.55%)]\t\tLoss: 0.82009\n",
      "Training Progress: \tEpoch 89 [5440/8883 (61.15%)]\t\tLoss: 0.75857\n",
      "Training Progress: \tEpoch 89 [5760/8883 (64.75%)]\t\tLoss: 0.61383\n",
      "Training Progress: \tEpoch 89 [6080/8883 (68.35%)]\t\tLoss: 0.36772\n",
      "Training Progress: \tEpoch 89 [6400/8883 (71.94%)]\t\tLoss: 0.60805\n",
      "Training Progress: \tEpoch 89 [6720/8883 (75.54%)]\t\tLoss: 0.71838\n",
      "Training Progress: \tEpoch 89 [7040/8883 (79.14%)]\t\tLoss: 0.72297\n",
      "Training Progress: \tEpoch 89 [7360/8883 (82.73%)]\t\tLoss: 0.74930\n",
      "Training Progress: \tEpoch 89 [7680/8883 (86.33%)]\t\tLoss: 0.49944\n",
      "Training Progress: \tEpoch 89 [8000/8883 (89.93%)]\t\tLoss: 0.55922\n",
      "Training Progress: \tEpoch 89 [8320/8883 (93.53%)]\t\tLoss: 0.84940\n",
      "Training Progress: \tEpoch 89 [8640/8883 (97.12%)]\t\tLoss: 0.63956\n",
      "\tTrain loss: 0.01121, Accuracy: 7262/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1669/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1141/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/8883 (0.00%)]\t\tLoss: 0.48593\n",
      "Training Progress: \tEpoch 90 [320/8883 (3.60%)]\t\tLoss: 0.64961\n",
      "Training Progress: \tEpoch 90 [640/8883 (7.19%)]\t\tLoss: 0.43836\n",
      "Training Progress: \tEpoch 90 [960/8883 (10.79%)]\t\tLoss: 0.63778\n",
      "Training Progress: \tEpoch 90 [1280/8883 (14.39%)]\t\tLoss: 0.63849\n",
      "Training Progress: \tEpoch 90 [1600/8883 (17.99%)]\t\tLoss: 0.55241\n",
      "Training Progress: \tEpoch 90 [1920/8883 (21.58%)]\t\tLoss: 0.49147\n",
      "Training Progress: \tEpoch 90 [2240/8883 (25.18%)]\t\tLoss: 0.53080\n",
      "Training Progress: \tEpoch 90 [2560/8883 (28.78%)]\t\tLoss: 0.64959\n",
      "Training Progress: \tEpoch 90 [2880/8883 (32.37%)]\t\tLoss: 0.47000\n",
      "Training Progress: \tEpoch 90 [3200/8883 (35.97%)]\t\tLoss: 0.57929\n",
      "Training Progress: \tEpoch 90 [3520/8883 (39.57%)]\t\tLoss: 0.89827\n",
      "Training Progress: \tEpoch 90 [3840/8883 (43.17%)]\t\tLoss: 0.74037\n",
      "Training Progress: \tEpoch 90 [4160/8883 (46.76%)]\t\tLoss: 0.60593\n",
      "Training Progress: \tEpoch 90 [4480/8883 (50.36%)]\t\tLoss: 0.53922\n",
      "Training Progress: \tEpoch 90 [4800/8883 (53.96%)]\t\tLoss: 0.54107\n",
      "Training Progress: \tEpoch 90 [5120/8883 (57.55%)]\t\tLoss: 0.66212\n",
      "Training Progress: \tEpoch 90 [5440/8883 (61.15%)]\t\tLoss: 0.73722\n",
      "Training Progress: \tEpoch 90 [5760/8883 (64.75%)]\t\tLoss: 0.62213\n",
      "Training Progress: \tEpoch 90 [6080/8883 (68.35%)]\t\tLoss: 0.36886\n",
      "Training Progress: \tEpoch 90 [6400/8883 (71.94%)]\t\tLoss: 0.39447\n",
      "Training Progress: \tEpoch 90 [6720/8883 (75.54%)]\t\tLoss: 0.70512\n",
      "Training Progress: \tEpoch 90 [7040/8883 (79.14%)]\t\tLoss: 0.81520\n",
      "Training Progress: \tEpoch 90 [7360/8883 (82.73%)]\t\tLoss: 0.53817\n",
      "Training Progress: \tEpoch 90 [7680/8883 (86.33%)]\t\tLoss: 0.40953\n",
      "Training Progress: \tEpoch 90 [8000/8883 (89.93%)]\t\tLoss: 0.56828\n",
      "Training Progress: \tEpoch 90 [8320/8883 (93.53%)]\t\tLoss: 0.48088\n",
      "Training Progress: \tEpoch 90 [8640/8883 (97.12%)]\t\tLoss: 0.67509\n",
      "\tTrain loss: 0.01126, Accuracy: 7281/8883 (81.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1148/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/8883 (0.00%)]\t\tLoss: 0.50629\n",
      "Training Progress: \tEpoch 91 [320/8883 (3.60%)]\t\tLoss: 0.74217\n",
      "Training Progress: \tEpoch 91 [640/8883 (7.19%)]\t\tLoss: 0.44357\n",
      "Training Progress: \tEpoch 91 [960/8883 (10.79%)]\t\tLoss: 0.80725\n",
      "Training Progress: \tEpoch 91 [1280/8883 (14.39%)]\t\tLoss: 0.54284\n",
      "Training Progress: \tEpoch 91 [1600/8883 (17.99%)]\t\tLoss: 0.35100\n",
      "Training Progress: \tEpoch 91 [1920/8883 (21.58%)]\t\tLoss: 0.57167\n",
      "Training Progress: \tEpoch 91 [2240/8883 (25.18%)]\t\tLoss: 0.46455\n",
      "Training Progress: \tEpoch 91 [2560/8883 (28.78%)]\t\tLoss: 0.45829\n",
      "Training Progress: \tEpoch 91 [2880/8883 (32.37%)]\t\tLoss: 0.51189\n",
      "Training Progress: \tEpoch 91 [3200/8883 (35.97%)]\t\tLoss: 0.55401\n",
      "Training Progress: \tEpoch 91 [3520/8883 (39.57%)]\t\tLoss: 0.81805\n",
      "Training Progress: \tEpoch 91 [3840/8883 (43.17%)]\t\tLoss: 1.03655\n",
      "Training Progress: \tEpoch 91 [4160/8883 (46.76%)]\t\tLoss: 0.68671\n",
      "Training Progress: \tEpoch 91 [4480/8883 (50.36%)]\t\tLoss: 0.31755\n",
      "Training Progress: \tEpoch 91 [4800/8883 (53.96%)]\t\tLoss: 0.47692\n",
      "Training Progress: \tEpoch 91 [5120/8883 (57.55%)]\t\tLoss: 0.65929\n",
      "Training Progress: \tEpoch 91 [5440/8883 (61.15%)]\t\tLoss: 0.67268\n",
      "Training Progress: \tEpoch 91 [5760/8883 (64.75%)]\t\tLoss: 0.72938\n",
      "Training Progress: \tEpoch 91 [6080/8883 (68.35%)]\t\tLoss: 0.30819\n",
      "Training Progress: \tEpoch 91 [6400/8883 (71.94%)]\t\tLoss: 0.74784\n",
      "Training Progress: \tEpoch 91 [6720/8883 (75.54%)]\t\tLoss: 0.74378\n",
      "Training Progress: \tEpoch 91 [7040/8883 (79.14%)]\t\tLoss: 0.62571\n",
      "Training Progress: \tEpoch 91 [7360/8883 (82.73%)]\t\tLoss: 0.88825\n",
      "Training Progress: \tEpoch 91 [7680/8883 (86.33%)]\t\tLoss: 0.48339\n",
      "Training Progress: \tEpoch 91 [8000/8883 (89.93%)]\t\tLoss: 0.56828\n",
      "Training Progress: \tEpoch 91 [8320/8883 (93.53%)]\t\tLoss: 0.74716\n",
      "Training Progress: \tEpoch 91 [8640/8883 (97.12%)]\t\tLoss: 0.58764\n",
      "\tTrain loss: 0.01119, Accuracy: 7291/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1668/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1130/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/8883 (0.00%)]\t\tLoss: 0.45663\n",
      "Training Progress: \tEpoch 92 [320/8883 (3.60%)]\t\tLoss: 0.47253\n",
      "Training Progress: \tEpoch 92 [640/8883 (7.19%)]\t\tLoss: 0.45616\n",
      "Training Progress: \tEpoch 92 [960/8883 (10.79%)]\t\tLoss: 0.55031\n",
      "Training Progress: \tEpoch 92 [1280/8883 (14.39%)]\t\tLoss: 0.65328\n",
      "Training Progress: \tEpoch 92 [1600/8883 (17.99%)]\t\tLoss: 0.40018\n",
      "Training Progress: \tEpoch 92 [1920/8883 (21.58%)]\t\tLoss: 0.52358\n",
      "Training Progress: \tEpoch 92 [2240/8883 (25.18%)]\t\tLoss: 0.75098\n",
      "Training Progress: \tEpoch 92 [2560/8883 (28.78%)]\t\tLoss: 0.53674\n",
      "Training Progress: \tEpoch 92 [2880/8883 (32.37%)]\t\tLoss: 0.57149\n",
      "Training Progress: \tEpoch 92 [3200/8883 (35.97%)]\t\tLoss: 0.55631\n",
      "Training Progress: \tEpoch 92 [3520/8883 (39.57%)]\t\tLoss: 0.59276\n",
      "Training Progress: \tEpoch 92 [3840/8883 (43.17%)]\t\tLoss: 0.65606\n",
      "Training Progress: \tEpoch 92 [4160/8883 (46.76%)]\t\tLoss: 0.56286\n",
      "Training Progress: \tEpoch 92 [4480/8883 (50.36%)]\t\tLoss: 0.39832\n",
      "Training Progress: \tEpoch 92 [4800/8883 (53.96%)]\t\tLoss: 0.31904\n",
      "Training Progress: \tEpoch 92 [5120/8883 (57.55%)]\t\tLoss: 0.58715\n",
      "Training Progress: \tEpoch 92 [5440/8883 (61.15%)]\t\tLoss: 0.76476\n",
      "Training Progress: \tEpoch 92 [5760/8883 (64.75%)]\t\tLoss: 0.55603\n",
      "Training Progress: \tEpoch 92 [6080/8883 (68.35%)]\t\tLoss: 0.43776\n",
      "Training Progress: \tEpoch 92 [6400/8883 (71.94%)]\t\tLoss: 0.60903\n",
      "Training Progress: \tEpoch 92 [6720/8883 (75.54%)]\t\tLoss: 0.76776\n",
      "Training Progress: \tEpoch 92 [7040/8883 (79.14%)]\t\tLoss: 0.62578\n",
      "Training Progress: \tEpoch 92 [7360/8883 (82.73%)]\t\tLoss: 0.84090\n",
      "Training Progress: \tEpoch 92 [7680/8883 (86.33%)]\t\tLoss: 0.57219\n",
      "Training Progress: \tEpoch 92 [8000/8883 (89.93%)]\t\tLoss: 0.56524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 92 [8320/8883 (93.53%)]\t\tLoss: 0.62687\n",
      "Training Progress: \tEpoch 92 [8640/8883 (97.12%)]\t\tLoss: 0.55858\n",
      "\tTrain loss: 0.01115, Accuracy: 7309/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1666/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1127/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/8883 (0.00%)]\t\tLoss: 0.58773\n",
      "Training Progress: \tEpoch 93 [320/8883 (3.60%)]\t\tLoss: 0.57297\n",
      "Training Progress: \tEpoch 93 [640/8883 (7.19%)]\t\tLoss: 0.59659\n",
      "Training Progress: \tEpoch 93 [960/8883 (10.79%)]\t\tLoss: 0.73050\n",
      "Training Progress: \tEpoch 93 [1280/8883 (14.39%)]\t\tLoss: 0.50783\n",
      "Training Progress: \tEpoch 93 [1600/8883 (17.99%)]\t\tLoss: 0.40217\n",
      "Training Progress: \tEpoch 93 [1920/8883 (21.58%)]\t\tLoss: 0.53374\n",
      "Training Progress: \tEpoch 93 [2240/8883 (25.18%)]\t\tLoss: 0.71957\n",
      "Training Progress: \tEpoch 93 [2560/8883 (28.78%)]\t\tLoss: 0.63777\n",
      "Training Progress: \tEpoch 93 [2880/8883 (32.37%)]\t\tLoss: 0.52805\n",
      "Training Progress: \tEpoch 93 [3200/8883 (35.97%)]\t\tLoss: 0.85447\n",
      "Training Progress: \tEpoch 93 [3520/8883 (39.57%)]\t\tLoss: 0.58678\n",
      "Training Progress: \tEpoch 93 [3840/8883 (43.17%)]\t\tLoss: 0.99569\n",
      "Training Progress: \tEpoch 93 [4160/8883 (46.76%)]\t\tLoss: 0.48322\n",
      "Training Progress: \tEpoch 93 [4480/8883 (50.36%)]\t\tLoss: 0.42890\n",
      "Training Progress: \tEpoch 93 [4800/8883 (53.96%)]\t\tLoss: 0.30704\n",
      "Training Progress: \tEpoch 93 [5120/8883 (57.55%)]\t\tLoss: 0.54598\n",
      "Training Progress: \tEpoch 93 [5440/8883 (61.15%)]\t\tLoss: 0.55394\n",
      "Training Progress: \tEpoch 93 [5760/8883 (64.75%)]\t\tLoss: 0.62814\n",
      "Training Progress: \tEpoch 93 [6080/8883 (68.35%)]\t\tLoss: 0.56746\n",
      "Training Progress: \tEpoch 93 [6400/8883 (71.94%)]\t\tLoss: 0.66544\n",
      "Training Progress: \tEpoch 93 [6720/8883 (75.54%)]\t\tLoss: 0.72121\n",
      "Training Progress: \tEpoch 93 [7040/8883 (79.14%)]\t\tLoss: 0.72182\n",
      "Training Progress: \tEpoch 93 [7360/8883 (82.73%)]\t\tLoss: 0.63559\n",
      "Training Progress: \tEpoch 93 [7680/8883 (86.33%)]\t\tLoss: 0.52998\n",
      "Training Progress: \tEpoch 93 [8000/8883 (89.93%)]\t\tLoss: 0.70880\n",
      "Training Progress: \tEpoch 93 [8320/8883 (93.53%)]\t\tLoss: 0.58224\n",
      "Training Progress: \tEpoch 93 [8640/8883 (97.12%)]\t\tLoss: 1.14385\n",
      "\tTrain loss: 0.01121, Accuracy: 7289/8883 (82.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1670/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1145/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/8883 (0.00%)]\t\tLoss: 0.58803\n",
      "Training Progress: \tEpoch 94 [320/8883 (3.60%)]\t\tLoss: 0.50123\n",
      "Training Progress: \tEpoch 94 [640/8883 (7.19%)]\t\tLoss: 0.60244\n",
      "Training Progress: \tEpoch 94 [960/8883 (10.79%)]\t\tLoss: 0.53857\n",
      "Training Progress: \tEpoch 94 [1280/8883 (14.39%)]\t\tLoss: 0.72708\n",
      "Training Progress: \tEpoch 94 [1600/8883 (17.99%)]\t\tLoss: 0.48041\n",
      "Training Progress: \tEpoch 94 [1920/8883 (21.58%)]\t\tLoss: 0.63742\n",
      "Training Progress: \tEpoch 94 [2240/8883 (25.18%)]\t\tLoss: 0.55730\n",
      "Training Progress: \tEpoch 94 [2560/8883 (28.78%)]\t\tLoss: 0.62561\n",
      "Training Progress: \tEpoch 94 [2880/8883 (32.37%)]\t\tLoss: 0.61020\n",
      "Training Progress: \tEpoch 94 [3200/8883 (35.97%)]\t\tLoss: 0.59949\n",
      "Training Progress: \tEpoch 94 [3520/8883 (39.57%)]\t\tLoss: 0.79431\n",
      "Training Progress: \tEpoch 94 [3840/8883 (43.17%)]\t\tLoss: 0.59053\n",
      "Training Progress: \tEpoch 94 [4160/8883 (46.76%)]\t\tLoss: 0.59712\n",
      "Training Progress: \tEpoch 94 [4480/8883 (50.36%)]\t\tLoss: 0.64340\n",
      "Training Progress: \tEpoch 94 [4800/8883 (53.96%)]\t\tLoss: 0.48763\n",
      "Training Progress: \tEpoch 94 [5120/8883 (57.55%)]\t\tLoss: 0.57119\n",
      "Training Progress: \tEpoch 94 [5440/8883 (61.15%)]\t\tLoss: 0.70831\n",
      "Training Progress: \tEpoch 94 [5760/8883 (64.75%)]\t\tLoss: 0.47279\n",
      "Training Progress: \tEpoch 94 [6080/8883 (68.35%)]\t\tLoss: 0.31793\n",
      "Training Progress: \tEpoch 94 [6400/8883 (71.94%)]\t\tLoss: 0.40598\n",
      "Training Progress: \tEpoch 94 [6720/8883 (75.54%)]\t\tLoss: 0.55113\n",
      "Training Progress: \tEpoch 94 [7040/8883 (79.14%)]\t\tLoss: 0.60764\n",
      "Training Progress: \tEpoch 94 [7360/8883 (82.73%)]\t\tLoss: 0.67699\n",
      "Training Progress: \tEpoch 94 [7680/8883 (86.33%)]\t\tLoss: 0.55886\n",
      "Training Progress: \tEpoch 94 [8000/8883 (89.93%)]\t\tLoss: 0.48249\n",
      "Training Progress: \tEpoch 94 [8320/8883 (93.53%)]\t\tLoss: 0.54283\n",
      "Training Progress: \tEpoch 94 [8640/8883 (97.12%)]\t\tLoss: 0.84834\n",
      "\tTrain loss: 0.01120, Accuracy: 7281/8883 (81.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/8883 (0.00%)]\t\tLoss: 0.44419\n",
      "Training Progress: \tEpoch 95 [320/8883 (3.60%)]\t\tLoss: 0.59021\n",
      "Training Progress: \tEpoch 95 [640/8883 (7.19%)]\t\tLoss: 0.35280\n",
      "Training Progress: \tEpoch 95 [960/8883 (10.79%)]\t\tLoss: 0.46725\n",
      "Training Progress: \tEpoch 95 [1280/8883 (14.39%)]\t\tLoss: 0.64400\n",
      "Training Progress: \tEpoch 95 [1600/8883 (17.99%)]\t\tLoss: 0.37174\n",
      "Training Progress: \tEpoch 95 [1920/8883 (21.58%)]\t\tLoss: 0.53516\n",
      "Training Progress: \tEpoch 95 [2240/8883 (25.18%)]\t\tLoss: 0.55225\n",
      "Training Progress: \tEpoch 95 [2560/8883 (28.78%)]\t\tLoss: 0.47073\n",
      "Training Progress: \tEpoch 95 [2880/8883 (32.37%)]\t\tLoss: 0.49565\n",
      "Training Progress: \tEpoch 95 [3200/8883 (35.97%)]\t\tLoss: 0.79643\n",
      "Training Progress: \tEpoch 95 [3520/8883 (39.57%)]\t\tLoss: 0.62249\n",
      "Training Progress: \tEpoch 95 [3840/8883 (43.17%)]\t\tLoss: 0.63919\n",
      "Training Progress: \tEpoch 95 [4160/8883 (46.76%)]\t\tLoss: 0.44284\n",
      "Training Progress: \tEpoch 95 [4480/8883 (50.36%)]\t\tLoss: 0.65330\n",
      "Training Progress: \tEpoch 95 [4800/8883 (53.96%)]\t\tLoss: 0.74508\n",
      "Training Progress: \tEpoch 95 [5120/8883 (57.55%)]\t\tLoss: 0.60772\n",
      "Training Progress: \tEpoch 95 [5440/8883 (61.15%)]\t\tLoss: 0.54146\n",
      "Training Progress: \tEpoch 95 [5760/8883 (64.75%)]\t\tLoss: 0.57362\n",
      "Training Progress: \tEpoch 95 [6080/8883 (68.35%)]\t\tLoss: 0.42588\n",
      "Training Progress: \tEpoch 95 [6400/8883 (71.94%)]\t\tLoss: 0.57678\n",
      "Training Progress: \tEpoch 95 [6720/8883 (75.54%)]\t\tLoss: 0.72101\n",
      "Training Progress: \tEpoch 95 [7040/8883 (79.14%)]\t\tLoss: 0.57001\n",
      "Training Progress: \tEpoch 95 [7360/8883 (82.73%)]\t\tLoss: 0.70277\n",
      "Training Progress: \tEpoch 95 [7680/8883 (86.33%)]\t\tLoss: 0.59863\n",
      "Training Progress: \tEpoch 95 [8000/8883 (89.93%)]\t\tLoss: 0.61459\n",
      "Training Progress: \tEpoch 95 [8320/8883 (93.53%)]\t\tLoss: 0.77555\n",
      "Training Progress: \tEpoch 95 [8640/8883 (97.12%)]\t\tLoss: 0.70552\n",
      "\tTrain loss: 0.01122, Accuracy: 7306/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1673/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/8883 (0.00%)]\t\tLoss: 0.39276\n",
      "Training Progress: \tEpoch 96 [320/8883 (3.60%)]\t\tLoss: 0.78165\n",
      "Training Progress: \tEpoch 96 [640/8883 (7.19%)]\t\tLoss: 0.57476\n",
      "Training Progress: \tEpoch 96 [960/8883 (10.79%)]\t\tLoss: 0.75769\n",
      "Training Progress: \tEpoch 96 [1280/8883 (14.39%)]\t\tLoss: 0.55513\n",
      "Training Progress: \tEpoch 96 [1600/8883 (17.99%)]\t\tLoss: 0.44000\n",
      "Training Progress: \tEpoch 96 [1920/8883 (21.58%)]\t\tLoss: 0.64344\n",
      "Training Progress: \tEpoch 96 [2240/8883 (25.18%)]\t\tLoss: 0.57524\n",
      "Training Progress: \tEpoch 96 [2560/8883 (28.78%)]\t\tLoss: 0.54225\n",
      "Training Progress: \tEpoch 96 [2880/8883 (32.37%)]\t\tLoss: 0.52475\n",
      "Training Progress: \tEpoch 96 [3200/8883 (35.97%)]\t\tLoss: 0.55386\n",
      "Training Progress: \tEpoch 96 [3520/8883 (39.57%)]\t\tLoss: 0.73026\n",
      "Training Progress: \tEpoch 96 [3840/8883 (43.17%)]\t\tLoss: 0.57724\n",
      "Training Progress: \tEpoch 96 [4160/8883 (46.76%)]\t\tLoss: 0.58678\n",
      "Training Progress: \tEpoch 96 [4480/8883 (50.36%)]\t\tLoss: 0.41836\n",
      "Training Progress: \tEpoch 96 [4800/8883 (53.96%)]\t\tLoss: 0.60016\n",
      "Training Progress: \tEpoch 96 [5120/8883 (57.55%)]\t\tLoss: 0.65702\n",
      "Training Progress: \tEpoch 96 [5440/8883 (61.15%)]\t\tLoss: 0.62346\n",
      "Training Progress: \tEpoch 96 [5760/8883 (64.75%)]\t\tLoss: 0.47971\n",
      "Training Progress: \tEpoch 96 [6080/8883 (68.35%)]\t\tLoss: 0.36551\n",
      "Training Progress: \tEpoch 96 [6400/8883 (71.94%)]\t\tLoss: 0.51240\n",
      "Training Progress: \tEpoch 96 [6720/8883 (75.54%)]\t\tLoss: 0.95569\n",
      "Training Progress: \tEpoch 96 [7040/8883 (79.14%)]\t\tLoss: 0.66167\n",
      "Training Progress: \tEpoch 96 [7360/8883 (82.73%)]\t\tLoss: 0.66129\n",
      "Training Progress: \tEpoch 96 [7680/8883 (86.33%)]\t\tLoss: 0.54388\n",
      "Training Progress: \tEpoch 96 [8000/8883 (89.93%)]\t\tLoss: 0.73252\n",
      "Training Progress: \tEpoch 96 [8320/8883 (93.53%)]\t\tLoss: 0.52849\n",
      "Training Progress: \tEpoch 96 [8640/8883 (97.12%)]\t\tLoss: 0.73313\n",
      "\tTrain loss: 0.01116, Accuracy: 7299/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1673/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1173/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/8883 (0.00%)]\t\tLoss: 0.48282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 97 [320/8883 (3.60%)]\t\tLoss: 0.73396\n",
      "Training Progress: \tEpoch 97 [640/8883 (7.19%)]\t\tLoss: 0.42530\n",
      "Training Progress: \tEpoch 97 [960/8883 (10.79%)]\t\tLoss: 0.73681\n",
      "Training Progress: \tEpoch 97 [1280/8883 (14.39%)]\t\tLoss: 0.60750\n",
      "Training Progress: \tEpoch 97 [1600/8883 (17.99%)]\t\tLoss: 0.44265\n",
      "Training Progress: \tEpoch 97 [1920/8883 (21.58%)]\t\tLoss: 0.46889\n",
      "Training Progress: \tEpoch 97 [2240/8883 (25.18%)]\t\tLoss: 0.55233\n",
      "Training Progress: \tEpoch 97 [2560/8883 (28.78%)]\t\tLoss: 0.73420\n",
      "Training Progress: \tEpoch 97 [2880/8883 (32.37%)]\t\tLoss: 0.66311\n",
      "Training Progress: \tEpoch 97 [3200/8883 (35.97%)]\t\tLoss: 0.71963\n",
      "Training Progress: \tEpoch 97 [3520/8883 (39.57%)]\t\tLoss: 0.69527\n",
      "Training Progress: \tEpoch 97 [3840/8883 (43.17%)]\t\tLoss: 0.92508\n",
      "Training Progress: \tEpoch 97 [4160/8883 (46.76%)]\t\tLoss: 0.47175\n",
      "Training Progress: \tEpoch 97 [4480/8883 (50.36%)]\t\tLoss: 0.49848\n",
      "Training Progress: \tEpoch 97 [4800/8883 (53.96%)]\t\tLoss: 0.43397\n",
      "Training Progress: \tEpoch 97 [5120/8883 (57.55%)]\t\tLoss: 0.68671\n",
      "Training Progress: \tEpoch 97 [5440/8883 (61.15%)]\t\tLoss: 0.76029\n",
      "Training Progress: \tEpoch 97 [5760/8883 (64.75%)]\t\tLoss: 0.58893\n",
      "Training Progress: \tEpoch 97 [6080/8883 (68.35%)]\t\tLoss: 0.49219\n",
      "Training Progress: \tEpoch 97 [6400/8883 (71.94%)]\t\tLoss: 0.39965\n",
      "Training Progress: \tEpoch 97 [6720/8883 (75.54%)]\t\tLoss: 0.65629\n",
      "Training Progress: \tEpoch 97 [7040/8883 (79.14%)]\t\tLoss: 0.59693\n",
      "Training Progress: \tEpoch 97 [7360/8883 (82.73%)]\t\tLoss: 0.69821\n",
      "Training Progress: \tEpoch 97 [7680/8883 (86.33%)]\t\tLoss: 0.53512\n",
      "Training Progress: \tEpoch 97 [8000/8883 (89.93%)]\t\tLoss: 0.48762\n",
      "Training Progress: \tEpoch 97 [8320/8883 (93.53%)]\t\tLoss: 0.60319\n",
      "Training Progress: \tEpoch 97 [8640/8883 (97.12%)]\t\tLoss: 0.67394\n",
      "\tTrain loss: 0.01119, Accuracy: 7297/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1678/1692 (99.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1138/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/8883 (0.00%)]\t\tLoss: 0.41852\n",
      "Training Progress: \tEpoch 98 [320/8883 (3.60%)]\t\tLoss: 0.81086\n",
      "Training Progress: \tEpoch 98 [640/8883 (7.19%)]\t\tLoss: 0.44262\n",
      "Training Progress: \tEpoch 98 [960/8883 (10.79%)]\t\tLoss: 0.63400\n",
      "Training Progress: \tEpoch 98 [1280/8883 (14.39%)]\t\tLoss: 0.71666\n",
      "Training Progress: \tEpoch 98 [1600/8883 (17.99%)]\t\tLoss: 0.44830\n",
      "Training Progress: \tEpoch 98 [1920/8883 (21.58%)]\t\tLoss: 0.56189\n",
      "Training Progress: \tEpoch 98 [2240/8883 (25.18%)]\t\tLoss: 0.75548\n",
      "Training Progress: \tEpoch 98 [2560/8883 (28.78%)]\t\tLoss: 0.60129\n",
      "Training Progress: \tEpoch 98 [2880/8883 (32.37%)]\t\tLoss: 0.58536\n",
      "Training Progress: \tEpoch 98 [3200/8883 (35.97%)]\t\tLoss: 0.51744\n",
      "Training Progress: \tEpoch 98 [3520/8883 (39.57%)]\t\tLoss: 0.62776\n",
      "Training Progress: \tEpoch 98 [3840/8883 (43.17%)]\t\tLoss: 0.73069\n",
      "Training Progress: \tEpoch 98 [4160/8883 (46.76%)]\t\tLoss: 0.47289\n",
      "Training Progress: \tEpoch 98 [4480/8883 (50.36%)]\t\tLoss: 0.46950\n",
      "Training Progress: \tEpoch 98 [4800/8883 (53.96%)]\t\tLoss: 0.58237\n",
      "Training Progress: \tEpoch 98 [5120/8883 (57.55%)]\t\tLoss: 0.60745\n",
      "Training Progress: \tEpoch 98 [5440/8883 (61.15%)]\t\tLoss: 0.52968\n",
      "Training Progress: \tEpoch 98 [5760/8883 (64.75%)]\t\tLoss: 0.37942\n",
      "Training Progress: \tEpoch 98 [6080/8883 (68.35%)]\t\tLoss: 0.38904\n",
      "Training Progress: \tEpoch 98 [6400/8883 (71.94%)]\t\tLoss: 0.42922\n",
      "Training Progress: \tEpoch 98 [6720/8883 (75.54%)]\t\tLoss: 0.70019\n",
      "Training Progress: \tEpoch 98 [7040/8883 (79.14%)]\t\tLoss: 0.66811\n",
      "Training Progress: \tEpoch 98 [7360/8883 (82.73%)]\t\tLoss: 0.74402\n",
      "Training Progress: \tEpoch 98 [7680/8883 (86.33%)]\t\tLoss: 0.55824\n",
      "Training Progress: \tEpoch 98 [8000/8883 (89.93%)]\t\tLoss: 0.63589\n",
      "Training Progress: \tEpoch 98 [8320/8883 (93.53%)]\t\tLoss: 0.70149\n",
      "Training Progress: \tEpoch 98 [8640/8883 (97.12%)]\t\tLoss: 0.55120\n",
      "\tTrain loss: 0.01108, Accuracy: 7292/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1672/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/8883 (0.00%)]\t\tLoss: 0.63288\n",
      "Training Progress: \tEpoch 99 [320/8883 (3.60%)]\t\tLoss: 0.88343\n",
      "Training Progress: \tEpoch 99 [640/8883 (7.19%)]\t\tLoss: 0.50110\n",
      "Training Progress: \tEpoch 99 [960/8883 (10.79%)]\t\tLoss: 0.63831\n",
      "Training Progress: \tEpoch 99 [1280/8883 (14.39%)]\t\tLoss: 0.51405\n",
      "Training Progress: \tEpoch 99 [1600/8883 (17.99%)]\t\tLoss: 0.52912\n",
      "Training Progress: \tEpoch 99 [1920/8883 (21.58%)]\t\tLoss: 0.57320\n",
      "Training Progress: \tEpoch 99 [2240/8883 (25.18%)]\t\tLoss: 0.46803\n",
      "Training Progress: \tEpoch 99 [2560/8883 (28.78%)]\t\tLoss: 0.48238\n",
      "Training Progress: \tEpoch 99 [2880/8883 (32.37%)]\t\tLoss: 0.61874\n",
      "Training Progress: \tEpoch 99 [3200/8883 (35.97%)]\t\tLoss: 0.62106\n",
      "Training Progress: \tEpoch 99 [3520/8883 (39.57%)]\t\tLoss: 0.88392\n",
      "Training Progress: \tEpoch 99 [3840/8883 (43.17%)]\t\tLoss: 1.01636\n",
      "Training Progress: \tEpoch 99 [4160/8883 (46.76%)]\t\tLoss: 0.45007\n",
      "Training Progress: \tEpoch 99 [4480/8883 (50.36%)]\t\tLoss: 0.34237\n",
      "Training Progress: \tEpoch 99 [4800/8883 (53.96%)]\t\tLoss: 0.47582\n",
      "Training Progress: \tEpoch 99 [5120/8883 (57.55%)]\t\tLoss: 0.75730\n",
      "Training Progress: \tEpoch 99 [5440/8883 (61.15%)]\t\tLoss: 0.77985\n",
      "Training Progress: \tEpoch 99 [5760/8883 (64.75%)]\t\tLoss: 0.62566\n",
      "Training Progress: \tEpoch 99 [6080/8883 (68.35%)]\t\tLoss: 0.29339\n",
      "Training Progress: \tEpoch 99 [6400/8883 (71.94%)]\t\tLoss: 0.40770\n",
      "Training Progress: \tEpoch 99 [6720/8883 (75.54%)]\t\tLoss: 0.64280\n",
      "Training Progress: \tEpoch 99 [7040/8883 (79.14%)]\t\tLoss: 0.67437\n",
      "Training Progress: \tEpoch 99 [7360/8883 (82.73%)]\t\tLoss: 0.79786\n",
      "Training Progress: \tEpoch 99 [7680/8883 (86.33%)]\t\tLoss: 0.49923\n",
      "Training Progress: \tEpoch 99 [8000/8883 (89.93%)]\t\tLoss: 0.51354\n",
      "Training Progress: \tEpoch 99 [8320/8883 (93.53%)]\t\tLoss: 0.58252\n",
      "Training Progress: \tEpoch 99 [8640/8883 (97.12%)]\t\tLoss: 0.76471\n",
      "\tTrain loss: 0.01115, Accuracy: 7289/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1679/1692 (99.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1059/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/8883 (0.00%)]\t\tLoss: 0.50256\n",
      "Training Progress: \tEpoch 100 [320/8883 (3.60%)]\t\tLoss: 0.66530\n",
      "Training Progress: \tEpoch 100 [640/8883 (7.19%)]\t\tLoss: 0.48869\n",
      "Training Progress: \tEpoch 100 [960/8883 (10.79%)]\t\tLoss: 0.61696\n",
      "Training Progress: \tEpoch 100 [1280/8883 (14.39%)]\t\tLoss: 0.67418\n",
      "Training Progress: \tEpoch 100 [1600/8883 (17.99%)]\t\tLoss: 0.36388\n",
      "Training Progress: \tEpoch 100 [1920/8883 (21.58%)]\t\tLoss: 0.41512\n",
      "Training Progress: \tEpoch 100 [2240/8883 (25.18%)]\t\tLoss: 0.53342\n",
      "Training Progress: \tEpoch 100 [2560/8883 (28.78%)]\t\tLoss: 0.50235\n",
      "Training Progress: \tEpoch 100 [2880/8883 (32.37%)]\t\tLoss: 0.52412\n",
      "Training Progress: \tEpoch 100 [3200/8883 (35.97%)]\t\tLoss: 0.66638\n",
      "Training Progress: \tEpoch 100 [3520/8883 (39.57%)]\t\tLoss: 0.60836\n",
      "Training Progress: \tEpoch 100 [3840/8883 (43.17%)]\t\tLoss: 0.64215\n",
      "Training Progress: \tEpoch 100 [4160/8883 (46.76%)]\t\tLoss: 0.51559\n",
      "Training Progress: \tEpoch 100 [4480/8883 (50.36%)]\t\tLoss: 0.39492\n",
      "Training Progress: \tEpoch 100 [4800/8883 (53.96%)]\t\tLoss: 0.41644\n",
      "Training Progress: \tEpoch 100 [5120/8883 (57.55%)]\t\tLoss: 0.74901\n",
      "Training Progress: \tEpoch 100 [5440/8883 (61.15%)]\t\tLoss: 0.57354\n",
      "Training Progress: \tEpoch 100 [5760/8883 (64.75%)]\t\tLoss: 0.52880\n",
      "Training Progress: \tEpoch 100 [6080/8883 (68.35%)]\t\tLoss: 0.44109\n",
      "Training Progress: \tEpoch 100 [6400/8883 (71.94%)]\t\tLoss: 0.53593\n",
      "Training Progress: \tEpoch 100 [6720/8883 (75.54%)]\t\tLoss: 0.83167\n",
      "Training Progress: \tEpoch 100 [7040/8883 (79.14%)]\t\tLoss: 0.63638\n",
      "Training Progress: \tEpoch 100 [7360/8883 (82.73%)]\t\tLoss: 0.62204\n",
      "Training Progress: \tEpoch 100 [7680/8883 (86.33%)]\t\tLoss: 0.41276\n",
      "Training Progress: \tEpoch 100 [8000/8883 (89.93%)]\t\tLoss: 0.62374\n",
      "Training Progress: \tEpoch 100 [8320/8883 (93.53%)]\t\tLoss: 0.47568\n",
      "Training Progress: \tEpoch 100 [8640/8883 (97.12%)]\t\tLoss: 0.57153\n",
      "\tTrain loss: 0.01107, Accuracy: 7296/8883 (82.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1113/1772 (62.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9923167848699763\n",
      "Best test accuracy:\n",
      "0.6642212189616253\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn, cnn_optimizer, data_loaders_wgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6695d6",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4fd96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/8883 (0.00%)]\t\tLoss: 1.52343\n",
      "Training Progress: \tEpoch 1 [320/8883 (3.60%)]\t\tLoss: 1.40141\n",
      "Training Progress: \tEpoch 1 [640/8883 (7.19%)]\t\tLoss: 1.42502\n",
      "Training Progress: \tEpoch 1 [960/8883 (10.79%)]\t\tLoss: 1.58219\n",
      "Training Progress: \tEpoch 1 [1280/8883 (14.39%)]\t\tLoss: 1.35191\n",
      "Training Progress: \tEpoch 1 [1600/8883 (17.99%)]\t\tLoss: 1.33632\n",
      "Training Progress: \tEpoch 1 [1920/8883 (21.58%)]\t\tLoss: 1.40616\n",
      "Training Progress: \tEpoch 1 [2240/8883 (25.18%)]\t\tLoss: 1.49652\n",
      "Training Progress: \tEpoch 1 [2560/8883 (28.78%)]\t\tLoss: 1.36854\n",
      "Training Progress: \tEpoch 1 [2880/8883 (32.37%)]\t\tLoss: 1.38680\n",
      "Training Progress: \tEpoch 1 [3200/8883 (35.97%)]\t\tLoss: 1.41699\n",
      "Training Progress: \tEpoch 1 [3520/8883 (39.57%)]\t\tLoss: 1.47125\n",
      "Training Progress: \tEpoch 1 [3840/8883 (43.17%)]\t\tLoss: 1.41455\n",
      "Training Progress: \tEpoch 1 [4160/8883 (46.76%)]\t\tLoss: 1.40149\n",
      "Training Progress: \tEpoch 1 [4480/8883 (50.36%)]\t\tLoss: 1.32715\n",
      "Training Progress: \tEpoch 1 [4800/8883 (53.96%)]\t\tLoss: 1.47531\n",
      "Training Progress: \tEpoch 1 [5120/8883 (57.55%)]\t\tLoss: 1.40755\n",
      "Training Progress: \tEpoch 1 [5440/8883 (61.15%)]\t\tLoss: 1.36215\n",
      "Training Progress: \tEpoch 1 [5760/8883 (64.75%)]\t\tLoss: 1.36721\n",
      "Training Progress: \tEpoch 1 [6080/8883 (68.35%)]\t\tLoss: 1.39216\n",
      "Training Progress: \tEpoch 1 [6400/8883 (71.94%)]\t\tLoss: 1.40627\n",
      "Training Progress: \tEpoch 1 [6720/8883 (75.54%)]\t\tLoss: 1.47023\n",
      "Training Progress: \tEpoch 1 [7040/8883 (79.14%)]\t\tLoss: 1.43285\n",
      "Training Progress: \tEpoch 1 [7360/8883 (82.73%)]\t\tLoss: 1.38912\n",
      "Training Progress: \tEpoch 1 [7680/8883 (86.33%)]\t\tLoss: 1.36059\n",
      "Training Progress: \tEpoch 1 [8000/8883 (89.93%)]\t\tLoss: 1.40091\n",
      "Training Progress: \tEpoch 1 [8320/8883 (93.53%)]\t\tLoss: 1.45147\n",
      "Training Progress: \tEpoch 1 [8640/8883 (97.12%)]\t\tLoss: 1.38734\n",
      "\tTrain loss: 0.04308, Accuracy: 2568/8883 (28.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 525/1692 (31.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 467/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/8883 (0.00%)]\t\tLoss: 1.37139\n",
      "Training Progress: \tEpoch 2 [320/8883 (3.60%)]\t\tLoss: 1.42228\n",
      "Training Progress: \tEpoch 2 [640/8883 (7.19%)]\t\tLoss: 1.43303\n",
      "Training Progress: \tEpoch 2 [960/8883 (10.79%)]\t\tLoss: 1.41725\n",
      "Training Progress: \tEpoch 2 [1280/8883 (14.39%)]\t\tLoss: 1.41509\n",
      "Training Progress: \tEpoch 2 [1600/8883 (17.99%)]\t\tLoss: 1.43640\n",
      "Training Progress: \tEpoch 2 [1920/8883 (21.58%)]\t\tLoss: 1.39877\n",
      "Training Progress: \tEpoch 2 [2240/8883 (25.18%)]\t\tLoss: 1.40304\n",
      "Training Progress: \tEpoch 2 [2560/8883 (28.78%)]\t\tLoss: 1.41998\n",
      "Training Progress: \tEpoch 2 [2880/8883 (32.37%)]\t\tLoss: 1.34581\n",
      "Training Progress: \tEpoch 2 [3200/8883 (35.97%)]\t\tLoss: 1.39166\n",
      "Training Progress: \tEpoch 2 [3520/8883 (39.57%)]\t\tLoss: 1.41250\n",
      "Training Progress: \tEpoch 2 [3840/8883 (43.17%)]\t\tLoss: 1.34712\n",
      "Training Progress: \tEpoch 2 [4160/8883 (46.76%)]\t\tLoss: 1.44067\n",
      "Training Progress: \tEpoch 2 [4480/8883 (50.36%)]\t\tLoss: 1.38236\n",
      "Training Progress: \tEpoch 2 [4800/8883 (53.96%)]\t\tLoss: 1.39005\n",
      "Training Progress: \tEpoch 2 [5120/8883 (57.55%)]\t\tLoss: 1.40364\n",
      "Training Progress: \tEpoch 2 [5440/8883 (61.15%)]\t\tLoss: 1.34806\n",
      "Training Progress: \tEpoch 2 [5760/8883 (64.75%)]\t\tLoss: 1.33407\n",
      "Training Progress: \tEpoch 2 [6080/8883 (68.35%)]\t\tLoss: 1.36078\n",
      "Training Progress: \tEpoch 2 [6400/8883 (71.94%)]\t\tLoss: 1.36243\n",
      "Training Progress: \tEpoch 2 [6720/8883 (75.54%)]\t\tLoss: 1.36500\n",
      "Training Progress: \tEpoch 2 [7040/8883 (79.14%)]\t\tLoss: 1.40093\n",
      "Training Progress: \tEpoch 2 [7360/8883 (82.73%)]\t\tLoss: 1.40299\n",
      "Training Progress: \tEpoch 2 [7680/8883 (86.33%)]\t\tLoss: 1.39417\n",
      "Training Progress: \tEpoch 2 [8000/8883 (89.93%)]\t\tLoss: 1.34518\n",
      "Training Progress: \tEpoch 2 [8320/8883 (93.53%)]\t\tLoss: 1.43422\n",
      "Training Progress: \tEpoch 2 [8640/8883 (97.12%)]\t\tLoss: 1.37941\n",
      "\tTrain loss: 0.04280, Accuracy: 2822/8883 (31.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 552/1692 (32.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 492/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/8883 (0.00%)]\t\tLoss: 1.35085\n",
      "Training Progress: \tEpoch 3 [320/8883 (3.60%)]\t\tLoss: 1.40692\n",
      "Training Progress: \tEpoch 3 [640/8883 (7.19%)]\t\tLoss: 1.35525\n",
      "Training Progress: \tEpoch 3 [960/8883 (10.79%)]\t\tLoss: 1.35641\n",
      "Training Progress: \tEpoch 3 [1280/8883 (14.39%)]\t\tLoss: 1.43250\n",
      "Training Progress: \tEpoch 3 [1600/8883 (17.99%)]\t\tLoss: 1.35382\n",
      "Training Progress: \tEpoch 3 [1920/8883 (21.58%)]\t\tLoss: 1.29284\n",
      "Training Progress: \tEpoch 3 [2240/8883 (25.18%)]\t\tLoss: 1.35903\n",
      "Training Progress: \tEpoch 3 [2560/8883 (28.78%)]\t\tLoss: 1.36969\n",
      "Training Progress: \tEpoch 3 [2880/8883 (32.37%)]\t\tLoss: 1.38542\n",
      "Training Progress: \tEpoch 3 [3200/8883 (35.97%)]\t\tLoss: 1.35441\n",
      "Training Progress: \tEpoch 3 [3520/8883 (39.57%)]\t\tLoss: 1.32452\n",
      "Training Progress: \tEpoch 3 [3840/8883 (43.17%)]\t\tLoss: 1.31992\n",
      "Training Progress: \tEpoch 3 [4160/8883 (46.76%)]\t\tLoss: 1.40560\n",
      "Training Progress: \tEpoch 3 [4480/8883 (50.36%)]\t\tLoss: 1.31503\n",
      "Training Progress: \tEpoch 3 [4800/8883 (53.96%)]\t\tLoss: 1.37576\n",
      "Training Progress: \tEpoch 3 [5120/8883 (57.55%)]\t\tLoss: 1.41026\n",
      "Training Progress: \tEpoch 3 [5440/8883 (61.15%)]\t\tLoss: 1.36485\n",
      "Training Progress: \tEpoch 3 [5760/8883 (64.75%)]\t\tLoss: 1.43429\n",
      "Training Progress: \tEpoch 3 [6080/8883 (68.35%)]\t\tLoss: 1.39135\n",
      "Training Progress: \tEpoch 3 [6400/8883 (71.94%)]\t\tLoss: 1.37680\n",
      "Training Progress: \tEpoch 3 [6720/8883 (75.54%)]\t\tLoss: 1.40033\n",
      "Training Progress: \tEpoch 3 [7040/8883 (79.14%)]\t\tLoss: 1.44207\n",
      "Training Progress: \tEpoch 3 [7360/8883 (82.73%)]\t\tLoss: 1.33462\n",
      "Training Progress: \tEpoch 3 [7680/8883 (86.33%)]\t\tLoss: 1.38211\n",
      "Training Progress: \tEpoch 3 [8000/8883 (89.93%)]\t\tLoss: 1.36024\n",
      "Training Progress: \tEpoch 3 [8320/8883 (93.53%)]\t\tLoss: 1.37219\n",
      "Training Progress: \tEpoch 3 [8640/8883 (97.12%)]\t\tLoss: 1.34492\n",
      "\tTrain loss: 0.04258, Accuracy: 2908/8883 (32.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 550/1692 (32.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 468/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/8883 (0.00%)]\t\tLoss: 1.41556\n",
      "Training Progress: \tEpoch 4 [320/8883 (3.60%)]\t\tLoss: 1.36923\n",
      "Training Progress: \tEpoch 4 [640/8883 (7.19%)]\t\tLoss: 1.38497\n",
      "Training Progress: \tEpoch 4 [960/8883 (10.79%)]\t\tLoss: 1.39931\n",
      "Training Progress: \tEpoch 4 [1280/8883 (14.39%)]\t\tLoss: 1.31848\n",
      "Training Progress: \tEpoch 4 [1600/8883 (17.99%)]\t\tLoss: 1.41054\n",
      "Training Progress: \tEpoch 4 [1920/8883 (21.58%)]\t\tLoss: 1.33414\n",
      "Training Progress: \tEpoch 4 [2240/8883 (25.18%)]\t\tLoss: 1.35727\n",
      "Training Progress: \tEpoch 4 [2560/8883 (28.78%)]\t\tLoss: 1.37795\n",
      "Training Progress: \tEpoch 4 [2880/8883 (32.37%)]\t\tLoss: 1.38210\n",
      "Training Progress: \tEpoch 4 [3200/8883 (35.97%)]\t\tLoss: 1.32451\n",
      "Training Progress: \tEpoch 4 [3520/8883 (39.57%)]\t\tLoss: 1.36654\n",
      "Training Progress: \tEpoch 4 [3840/8883 (43.17%)]\t\tLoss: 1.39423\n",
      "Training Progress: \tEpoch 4 [4160/8883 (46.76%)]\t\tLoss: 1.34458\n",
      "Training Progress: \tEpoch 4 [4480/8883 (50.36%)]\t\tLoss: 1.34283\n",
      "Training Progress: \tEpoch 4 [4800/8883 (53.96%)]\t\tLoss: 1.37503\n",
      "Training Progress: \tEpoch 4 [5120/8883 (57.55%)]\t\tLoss: 1.39095\n",
      "Training Progress: \tEpoch 4 [5440/8883 (61.15%)]\t\tLoss: 1.29394\n",
      "Training Progress: \tEpoch 4 [5760/8883 (64.75%)]\t\tLoss: 1.42182\n",
      "Training Progress: \tEpoch 4 [6080/8883 (68.35%)]\t\tLoss: 1.36112\n",
      "Training Progress: \tEpoch 4 [6400/8883 (71.94%)]\t\tLoss: 1.37846\n",
      "Training Progress: \tEpoch 4 [6720/8883 (75.54%)]\t\tLoss: 1.35123\n",
      "Training Progress: \tEpoch 4 [7040/8883 (79.14%)]\t\tLoss: 1.37562\n",
      "Training Progress: \tEpoch 4 [7360/8883 (82.73%)]\t\tLoss: 1.30522\n",
      "Training Progress: \tEpoch 4 [7680/8883 (86.33%)]\t\tLoss: 1.41430\n",
      "Training Progress: \tEpoch 4 [8000/8883 (89.93%)]\t\tLoss: 1.34443\n",
      "Training Progress: \tEpoch 4 [8320/8883 (93.53%)]\t\tLoss: 1.39985\n",
      "Training Progress: \tEpoch 4 [8640/8883 (97.12%)]\t\tLoss: 1.37910\n",
      "\tTrain loss: 0.04205, Accuracy: 3012/8883 (33.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 561/1692 (33.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 488/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/8883 (0.00%)]\t\tLoss: 1.35925\n",
      "Training Progress: \tEpoch 5 [320/8883 (3.60%)]\t\tLoss: 1.40775\n",
      "Training Progress: \tEpoch 5 [640/8883 (7.19%)]\t\tLoss: 1.38435\n",
      "Training Progress: \tEpoch 5 [960/8883 (10.79%)]\t\tLoss: 1.31819\n",
      "Training Progress: \tEpoch 5 [1280/8883 (14.39%)]\t\tLoss: 1.32937\n",
      "Training Progress: \tEpoch 5 [1600/8883 (17.99%)]\t\tLoss: 1.39491\n",
      "Training Progress: \tEpoch 5 [1920/8883 (21.58%)]\t\tLoss: 1.33494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 5 [2240/8883 (25.18%)]\t\tLoss: 1.30953\n",
      "Training Progress: \tEpoch 5 [2560/8883 (28.78%)]\t\tLoss: 1.32546\n",
      "Training Progress: \tEpoch 5 [2880/8883 (32.37%)]\t\tLoss: 1.41747\n",
      "Training Progress: \tEpoch 5 [3200/8883 (35.97%)]\t\tLoss: 1.34271\n",
      "Training Progress: \tEpoch 5 [3520/8883 (39.57%)]\t\tLoss: 1.34613\n",
      "Training Progress: \tEpoch 5 [3840/8883 (43.17%)]\t\tLoss: 1.35431\n",
      "Training Progress: \tEpoch 5 [4160/8883 (46.76%)]\t\tLoss: 1.28556\n",
      "Training Progress: \tEpoch 5 [4480/8883 (50.36%)]\t\tLoss: 1.29052\n",
      "Training Progress: \tEpoch 5 [4800/8883 (53.96%)]\t\tLoss: 1.35214\n",
      "Training Progress: \tEpoch 5 [5120/8883 (57.55%)]\t\tLoss: 1.37125\n",
      "Training Progress: \tEpoch 5 [5440/8883 (61.15%)]\t\tLoss: 1.31455\n",
      "Training Progress: \tEpoch 5 [5760/8883 (64.75%)]\t\tLoss: 1.38390\n",
      "Training Progress: \tEpoch 5 [6080/8883 (68.35%)]\t\tLoss: 1.32511\n",
      "Training Progress: \tEpoch 5 [6400/8883 (71.94%)]\t\tLoss: 1.31881\n",
      "Training Progress: \tEpoch 5 [6720/8883 (75.54%)]\t\tLoss: 1.36203\n",
      "Training Progress: \tEpoch 5 [7040/8883 (79.14%)]\t\tLoss: 1.34792\n",
      "Training Progress: \tEpoch 5 [7360/8883 (82.73%)]\t\tLoss: 1.30526\n",
      "Training Progress: \tEpoch 5 [7680/8883 (86.33%)]\t\tLoss: 1.34064\n",
      "Training Progress: \tEpoch 5 [8000/8883 (89.93%)]\t\tLoss: 1.30707\n",
      "Training Progress: \tEpoch 5 [8320/8883 (93.53%)]\t\tLoss: 1.35719\n",
      "Training Progress: \tEpoch 5 [8640/8883 (97.12%)]\t\tLoss: 1.35600\n",
      "\tTrain loss: 0.04142, Accuracy: 3145/8883 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 596/1692 (35.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 509/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/8883 (0.00%)]\t\tLoss: 1.36610\n",
      "Training Progress: \tEpoch 6 [320/8883 (3.60%)]\t\tLoss: 1.36998\n",
      "Training Progress: \tEpoch 6 [640/8883 (7.19%)]\t\tLoss: 1.34165\n",
      "Training Progress: \tEpoch 6 [960/8883 (10.79%)]\t\tLoss: 1.33615\n",
      "Training Progress: \tEpoch 6 [1280/8883 (14.39%)]\t\tLoss: 1.31730\n",
      "Training Progress: \tEpoch 6 [1600/8883 (17.99%)]\t\tLoss: 1.44518\n",
      "Training Progress: \tEpoch 6 [1920/8883 (21.58%)]\t\tLoss: 1.29485\n",
      "Training Progress: \tEpoch 6 [2240/8883 (25.18%)]\t\tLoss: 1.28902\n",
      "Training Progress: \tEpoch 6 [2560/8883 (28.78%)]\t\tLoss: 1.27817\n",
      "Training Progress: \tEpoch 6 [2880/8883 (32.37%)]\t\tLoss: 1.32369\n",
      "Training Progress: \tEpoch 6 [3200/8883 (35.97%)]\t\tLoss: 1.30774\n",
      "Training Progress: \tEpoch 6 [3520/8883 (39.57%)]\t\tLoss: 1.29625\n",
      "Training Progress: \tEpoch 6 [3840/8883 (43.17%)]\t\tLoss: 1.34964\n",
      "Training Progress: \tEpoch 6 [4160/8883 (46.76%)]\t\tLoss: 1.29193\n",
      "Training Progress: \tEpoch 6 [4480/8883 (50.36%)]\t\tLoss: 1.25501\n",
      "Training Progress: \tEpoch 6 [4800/8883 (53.96%)]\t\tLoss: 1.39628\n",
      "Training Progress: \tEpoch 6 [5120/8883 (57.55%)]\t\tLoss: 1.29289\n",
      "Training Progress: \tEpoch 6 [5440/8883 (61.15%)]\t\tLoss: 1.29949\n",
      "Training Progress: \tEpoch 6 [5760/8883 (64.75%)]\t\tLoss: 1.36615\n",
      "Training Progress: \tEpoch 6 [6080/8883 (68.35%)]\t\tLoss: 1.30870\n",
      "Training Progress: \tEpoch 6 [6400/8883 (71.94%)]\t\tLoss: 1.36877\n",
      "Training Progress: \tEpoch 6 [6720/8883 (75.54%)]\t\tLoss: 1.38890\n",
      "Training Progress: \tEpoch 6 [7040/8883 (79.14%)]\t\tLoss: 1.32016\n",
      "Training Progress: \tEpoch 6 [7360/8883 (82.73%)]\t\tLoss: 1.30356\n",
      "Training Progress: \tEpoch 6 [7680/8883 (86.33%)]\t\tLoss: 1.29767\n",
      "Training Progress: \tEpoch 6 [8000/8883 (89.93%)]\t\tLoss: 1.30743\n",
      "Training Progress: \tEpoch 6 [8320/8883 (93.53%)]\t\tLoss: 1.33620\n",
      "Training Progress: \tEpoch 6 [8640/8883 (97.12%)]\t\tLoss: 1.30934\n",
      "\tTrain loss: 0.04132, Accuracy: 3162/8883 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 606/1692 (35.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 490/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/8883 (0.00%)]\t\tLoss: 1.33441\n",
      "Training Progress: \tEpoch 7 [320/8883 (3.60%)]\t\tLoss: 1.31261\n",
      "Training Progress: \tEpoch 7 [640/8883 (7.19%)]\t\tLoss: 1.39324\n",
      "Training Progress: \tEpoch 7 [960/8883 (10.79%)]\t\tLoss: 1.28597\n",
      "Training Progress: \tEpoch 7 [1280/8883 (14.39%)]\t\tLoss: 1.36461\n",
      "Training Progress: \tEpoch 7 [1600/8883 (17.99%)]\t\tLoss: 1.33318\n",
      "Training Progress: \tEpoch 7 [1920/8883 (21.58%)]\t\tLoss: 1.32959\n",
      "Training Progress: \tEpoch 7 [2240/8883 (25.18%)]\t\tLoss: 1.26430\n",
      "Training Progress: \tEpoch 7 [2560/8883 (28.78%)]\t\tLoss: 1.32935\n",
      "Training Progress: \tEpoch 7 [2880/8883 (32.37%)]\t\tLoss: 1.45616\n",
      "Training Progress: \tEpoch 7 [3200/8883 (35.97%)]\t\tLoss: 1.31181\n",
      "Training Progress: \tEpoch 7 [3520/8883 (39.57%)]\t\tLoss: 1.30713\n",
      "Training Progress: \tEpoch 7 [3840/8883 (43.17%)]\t\tLoss: 1.31922\n",
      "Training Progress: \tEpoch 7 [4160/8883 (46.76%)]\t\tLoss: 1.29314\n",
      "Training Progress: \tEpoch 7 [4480/8883 (50.36%)]\t\tLoss: 1.26777\n",
      "Training Progress: \tEpoch 7 [4800/8883 (53.96%)]\t\tLoss: 1.35532\n",
      "Training Progress: \tEpoch 7 [5120/8883 (57.55%)]\t\tLoss: 1.29213\n",
      "Training Progress: \tEpoch 7 [5440/8883 (61.15%)]\t\tLoss: 1.32186\n",
      "Training Progress: \tEpoch 7 [5760/8883 (64.75%)]\t\tLoss: 1.39006\n",
      "Training Progress: \tEpoch 7 [6080/8883 (68.35%)]\t\tLoss: 1.30602\n",
      "Training Progress: \tEpoch 7 [6400/8883 (71.94%)]\t\tLoss: 1.33247\n",
      "Training Progress: \tEpoch 7 [6720/8883 (75.54%)]\t\tLoss: 1.35275\n",
      "Training Progress: \tEpoch 7 [7040/8883 (79.14%)]\t\tLoss: 1.31585\n",
      "Training Progress: \tEpoch 7 [7360/8883 (82.73%)]\t\tLoss: 1.24912\n",
      "Training Progress: \tEpoch 7 [7680/8883 (86.33%)]\t\tLoss: 1.28646\n",
      "Training Progress: \tEpoch 7 [8000/8883 (89.93%)]\t\tLoss: 1.32480\n",
      "Training Progress: \tEpoch 7 [8320/8883 (93.53%)]\t\tLoss: 1.35333\n",
      "Training Progress: \tEpoch 7 [8640/8883 (97.12%)]\t\tLoss: 1.31653\n",
      "\tTrain loss: 0.04047, Accuracy: 3347/8883 (37.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 624/1692 (36.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/8883 (0.00%)]\t\tLoss: 1.37435\n",
      "Training Progress: \tEpoch 8 [320/8883 (3.60%)]\t\tLoss: 1.34423\n",
      "Training Progress: \tEpoch 8 [640/8883 (7.19%)]\t\tLoss: 1.37573\n",
      "Training Progress: \tEpoch 8 [960/8883 (10.79%)]\t\tLoss: 1.31181\n",
      "Training Progress: \tEpoch 8 [1280/8883 (14.39%)]\t\tLoss: 1.33204\n",
      "Training Progress: \tEpoch 8 [1600/8883 (17.99%)]\t\tLoss: 1.37484\n",
      "Training Progress: \tEpoch 8 [1920/8883 (21.58%)]\t\tLoss: 1.25372\n",
      "Training Progress: \tEpoch 8 [2240/8883 (25.18%)]\t\tLoss: 1.32689\n",
      "Training Progress: \tEpoch 8 [2560/8883 (28.78%)]\t\tLoss: 1.33087\n",
      "Training Progress: \tEpoch 8 [2880/8883 (32.37%)]\t\tLoss: 1.34905\n",
      "Training Progress: \tEpoch 8 [3200/8883 (35.97%)]\t\tLoss: 1.25668\n",
      "Training Progress: \tEpoch 8 [3520/8883 (39.57%)]\t\tLoss: 1.30464\n",
      "Training Progress: \tEpoch 8 [3840/8883 (43.17%)]\t\tLoss: 1.30580\n",
      "Training Progress: \tEpoch 8 [4160/8883 (46.76%)]\t\tLoss: 1.26754\n",
      "Training Progress: \tEpoch 8 [4480/8883 (50.36%)]\t\tLoss: 1.18578\n",
      "Training Progress: \tEpoch 8 [4800/8883 (53.96%)]\t\tLoss: 1.31344\n",
      "Training Progress: \tEpoch 8 [5120/8883 (57.55%)]\t\tLoss: 1.25249\n",
      "Training Progress: \tEpoch 8 [5440/8883 (61.15%)]\t\tLoss: 1.28352\n",
      "Training Progress: \tEpoch 8 [5760/8883 (64.75%)]\t\tLoss: 1.29826\n",
      "Training Progress: \tEpoch 8 [6080/8883 (68.35%)]\t\tLoss: 1.25137\n",
      "Training Progress: \tEpoch 8 [6400/8883 (71.94%)]\t\tLoss: 1.23415\n",
      "Training Progress: \tEpoch 8 [6720/8883 (75.54%)]\t\tLoss: 1.25962\n",
      "Training Progress: \tEpoch 8 [7040/8883 (79.14%)]\t\tLoss: 1.22129\n",
      "Training Progress: \tEpoch 8 [7360/8883 (82.73%)]\t\tLoss: 1.17543\n",
      "Training Progress: \tEpoch 8 [7680/8883 (86.33%)]\t\tLoss: 1.23475\n",
      "Training Progress: \tEpoch 8 [8000/8883 (89.93%)]\t\tLoss: 1.21819\n",
      "Training Progress: \tEpoch 8 [8320/8883 (93.53%)]\t\tLoss: 1.33912\n",
      "Training Progress: \tEpoch 8 [8640/8883 (97.12%)]\t\tLoss: 1.23317\n",
      "\tTrain loss: 0.03949, Accuracy: 3596/8883 (40.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 668/1692 (39.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/8883 (0.00%)]\t\tLoss: 1.32946\n",
      "Training Progress: \tEpoch 9 [320/8883 (3.60%)]\t\tLoss: 1.32797\n",
      "Training Progress: \tEpoch 9 [640/8883 (7.19%)]\t\tLoss: 1.32566\n",
      "Training Progress: \tEpoch 9 [960/8883 (10.79%)]\t\tLoss: 1.30830\n",
      "Training Progress: \tEpoch 9 [1280/8883 (14.39%)]\t\tLoss: 1.31212\n",
      "Training Progress: \tEpoch 9 [1600/8883 (17.99%)]\t\tLoss: 1.36422\n",
      "Training Progress: \tEpoch 9 [1920/8883 (21.58%)]\t\tLoss: 1.35094\n",
      "Training Progress: \tEpoch 9 [2240/8883 (25.18%)]\t\tLoss: 1.26627\n",
      "Training Progress: \tEpoch 9 [2560/8883 (28.78%)]\t\tLoss: 1.30108\n",
      "Training Progress: \tEpoch 9 [2880/8883 (32.37%)]\t\tLoss: 1.34779\n",
      "Training Progress: \tEpoch 9 [3200/8883 (35.97%)]\t\tLoss: 1.28346\n",
      "Training Progress: \tEpoch 9 [3520/8883 (39.57%)]\t\tLoss: 1.26618\n",
      "Training Progress: \tEpoch 9 [3840/8883 (43.17%)]\t\tLoss: 1.31010\n",
      "Training Progress: \tEpoch 9 [4160/8883 (46.76%)]\t\tLoss: 1.20016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 9 [4480/8883 (50.36%)]\t\tLoss: 1.16060\n",
      "Training Progress: \tEpoch 9 [4800/8883 (53.96%)]\t\tLoss: 1.27441\n",
      "Training Progress: \tEpoch 9 [5120/8883 (57.55%)]\t\tLoss: 1.24406\n",
      "Training Progress: \tEpoch 9 [5440/8883 (61.15%)]\t\tLoss: 1.33353\n",
      "Training Progress: \tEpoch 9 [5760/8883 (64.75%)]\t\tLoss: 1.32801\n",
      "Training Progress: \tEpoch 9 [6080/8883 (68.35%)]\t\tLoss: 1.25986\n",
      "Training Progress: \tEpoch 9 [6400/8883 (71.94%)]\t\tLoss: 1.27178\n",
      "Training Progress: \tEpoch 9 [6720/8883 (75.54%)]\t\tLoss: 1.27085\n",
      "Training Progress: \tEpoch 9 [7040/8883 (79.14%)]\t\tLoss: 1.26755\n",
      "Training Progress: \tEpoch 9 [7360/8883 (82.73%)]\t\tLoss: 1.20434\n",
      "Training Progress: \tEpoch 9 [7680/8883 (86.33%)]\t\tLoss: 1.20410\n",
      "Training Progress: \tEpoch 9 [8000/8883 (89.93%)]\t\tLoss: 1.33015\n",
      "Training Progress: \tEpoch 9 [8320/8883 (93.53%)]\t\tLoss: 1.29727\n",
      "Training Progress: \tEpoch 9 [8640/8883 (97.12%)]\t\tLoss: 1.20799\n",
      "\tTrain loss: 0.03845, Accuracy: 3832/8883 (43.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 710/1692 (41.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 540/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/8883 (0.00%)]\t\tLoss: 1.29954\n",
      "Training Progress: \tEpoch 10 [320/8883 (3.60%)]\t\tLoss: 1.30895\n",
      "Training Progress: \tEpoch 10 [640/8883 (7.19%)]\t\tLoss: 1.32440\n",
      "Training Progress: \tEpoch 10 [960/8883 (10.79%)]\t\tLoss: 1.27095\n",
      "Training Progress: \tEpoch 10 [1280/8883 (14.39%)]\t\tLoss: 1.27802\n",
      "Training Progress: \tEpoch 10 [1600/8883 (17.99%)]\t\tLoss: 1.29823\n",
      "Training Progress: \tEpoch 10 [1920/8883 (21.58%)]\t\tLoss: 1.29523\n",
      "Training Progress: \tEpoch 10 [2240/8883 (25.18%)]\t\tLoss: 1.22437\n",
      "Training Progress: \tEpoch 10 [2560/8883 (28.78%)]\t\tLoss: 1.27875\n",
      "Training Progress: \tEpoch 10 [2880/8883 (32.37%)]\t\tLoss: 1.35336\n",
      "Training Progress: \tEpoch 10 [3200/8883 (35.97%)]\t\tLoss: 1.26387\n",
      "Training Progress: \tEpoch 10 [3520/8883 (39.57%)]\t\tLoss: 1.26117\n",
      "Training Progress: \tEpoch 10 [3840/8883 (43.17%)]\t\tLoss: 1.30650\n",
      "Training Progress: \tEpoch 10 [4160/8883 (46.76%)]\t\tLoss: 1.24293\n",
      "Training Progress: \tEpoch 10 [4480/8883 (50.36%)]\t\tLoss: 1.18745\n",
      "Training Progress: \tEpoch 10 [4800/8883 (53.96%)]\t\tLoss: 1.24928\n",
      "Training Progress: \tEpoch 10 [5120/8883 (57.55%)]\t\tLoss: 1.22256\n",
      "Training Progress: \tEpoch 10 [5440/8883 (61.15%)]\t\tLoss: 1.22353\n",
      "Training Progress: \tEpoch 10 [5760/8883 (64.75%)]\t\tLoss: 1.27112\n",
      "Training Progress: \tEpoch 10 [6080/8883 (68.35%)]\t\tLoss: 1.21383\n",
      "Training Progress: \tEpoch 10 [6400/8883 (71.94%)]\t\tLoss: 1.24027\n",
      "Training Progress: \tEpoch 10 [6720/8883 (75.54%)]\t\tLoss: 1.27464\n",
      "Training Progress: \tEpoch 10 [7040/8883 (79.14%)]\t\tLoss: 1.29216\n",
      "Training Progress: \tEpoch 10 [7360/8883 (82.73%)]\t\tLoss: 1.21041\n",
      "Training Progress: \tEpoch 10 [7680/8883 (86.33%)]\t\tLoss: 1.17401\n",
      "Training Progress: \tEpoch 10 [8000/8883 (89.93%)]\t\tLoss: 1.23466\n",
      "Training Progress: \tEpoch 10 [8320/8883 (93.53%)]\t\tLoss: 1.23610\n",
      "Training Progress: \tEpoch 10 [8640/8883 (97.12%)]\t\tLoss: 1.26500\n",
      "\tTrain loss: 0.03767, Accuracy: 3929/8883 (44.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 714/1692 (42.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/8883 (0.00%)]\t\tLoss: 1.31902\n",
      "Training Progress: \tEpoch 11 [320/8883 (3.60%)]\t\tLoss: 1.24006\n",
      "Training Progress: \tEpoch 11 [640/8883 (7.19%)]\t\tLoss: 1.24726\n",
      "Training Progress: \tEpoch 11 [960/8883 (10.79%)]\t\tLoss: 1.27684\n",
      "Training Progress: \tEpoch 11 [1280/8883 (14.39%)]\t\tLoss: 1.24669\n",
      "Training Progress: \tEpoch 11 [1600/8883 (17.99%)]\t\tLoss: 1.20269\n",
      "Training Progress: \tEpoch 11 [1920/8883 (21.58%)]\t\tLoss: 1.19817\n",
      "Training Progress: \tEpoch 11 [2240/8883 (25.18%)]\t\tLoss: 1.29396\n",
      "Training Progress: \tEpoch 11 [2560/8883 (28.78%)]\t\tLoss: 1.34218\n",
      "Training Progress: \tEpoch 11 [2880/8883 (32.37%)]\t\tLoss: 1.25027\n",
      "Training Progress: \tEpoch 11 [3200/8883 (35.97%)]\t\tLoss: 1.23541\n",
      "Training Progress: \tEpoch 11 [3520/8883 (39.57%)]\t\tLoss: 1.32177\n",
      "Training Progress: \tEpoch 11 [3840/8883 (43.17%)]\t\tLoss: 1.22431\n",
      "Training Progress: \tEpoch 11 [4160/8883 (46.76%)]\t\tLoss: 1.16867\n",
      "Training Progress: \tEpoch 11 [4480/8883 (50.36%)]\t\tLoss: 1.20184\n",
      "Training Progress: \tEpoch 11 [4800/8883 (53.96%)]\t\tLoss: 1.17361\n",
      "Training Progress: \tEpoch 11 [5120/8883 (57.55%)]\t\tLoss: 1.16855\n",
      "Training Progress: \tEpoch 11 [5440/8883 (61.15%)]\t\tLoss: 1.10470\n",
      "Training Progress: \tEpoch 11 [5760/8883 (64.75%)]\t\tLoss: 1.27641\n",
      "Training Progress: \tEpoch 11 [6080/8883 (68.35%)]\t\tLoss: 1.15037\n",
      "Training Progress: \tEpoch 11 [6400/8883 (71.94%)]\t\tLoss: 1.22529\n",
      "Training Progress: \tEpoch 11 [6720/8883 (75.54%)]\t\tLoss: 1.15970\n",
      "Training Progress: \tEpoch 11 [7040/8883 (79.14%)]\t\tLoss: 1.17923\n",
      "Training Progress: \tEpoch 11 [7360/8883 (82.73%)]\t\tLoss: 1.18883\n",
      "Training Progress: \tEpoch 11 [7680/8883 (86.33%)]\t\tLoss: 1.15604\n",
      "Training Progress: \tEpoch 11 [8000/8883 (89.93%)]\t\tLoss: 1.19751\n",
      "Training Progress: \tEpoch 11 [8320/8883 (93.53%)]\t\tLoss: 1.21504\n",
      "Training Progress: \tEpoch 11 [8640/8883 (97.12%)]\t\tLoss: 1.23166\n",
      "\tTrain loss: 0.03660, Accuracy: 4107/8883 (46.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 746/1692 (44.00%)\n",
      "\tTest loss: 0.00085, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/8883 (0.00%)]\t\tLoss: 1.32451\n",
      "Training Progress: \tEpoch 12 [320/8883 (3.60%)]\t\tLoss: 1.19472\n",
      "Training Progress: \tEpoch 12 [640/8883 (7.19%)]\t\tLoss: 1.24860\n",
      "Training Progress: \tEpoch 12 [960/8883 (10.79%)]\t\tLoss: 1.24556\n",
      "Training Progress: \tEpoch 12 [1280/8883 (14.39%)]\t\tLoss: 1.21504\n",
      "Training Progress: \tEpoch 12 [1600/8883 (17.99%)]\t\tLoss: 1.28168\n",
      "Training Progress: \tEpoch 12 [1920/8883 (21.58%)]\t\tLoss: 1.29098\n",
      "Training Progress: \tEpoch 12 [2240/8883 (25.18%)]\t\tLoss: 1.18171\n",
      "Training Progress: \tEpoch 12 [2560/8883 (28.78%)]\t\tLoss: 1.23027\n",
      "Training Progress: \tEpoch 12 [2880/8883 (32.37%)]\t\tLoss: 1.33448\n",
      "Training Progress: \tEpoch 12 [3200/8883 (35.97%)]\t\tLoss: 1.38043\n",
      "Training Progress: \tEpoch 12 [3520/8883 (39.57%)]\t\tLoss: 1.20516\n",
      "Training Progress: \tEpoch 12 [3840/8883 (43.17%)]\t\tLoss: 1.28085\n",
      "Training Progress: \tEpoch 12 [4160/8883 (46.76%)]\t\tLoss: 1.14795\n",
      "Training Progress: \tEpoch 12 [4480/8883 (50.36%)]\t\tLoss: 1.24226\n",
      "Training Progress: \tEpoch 12 [4800/8883 (53.96%)]\t\tLoss: 1.23725\n",
      "Training Progress: \tEpoch 12 [5120/8883 (57.55%)]\t\tLoss: 1.18736\n",
      "Training Progress: \tEpoch 12 [5440/8883 (61.15%)]\t\tLoss: 1.27282\n",
      "Training Progress: \tEpoch 12 [5760/8883 (64.75%)]\t\tLoss: 1.29910\n",
      "Training Progress: \tEpoch 12 [6080/8883 (68.35%)]\t\tLoss: 1.21022\n",
      "Training Progress: \tEpoch 12 [6400/8883 (71.94%)]\t\tLoss: 1.17366\n",
      "Training Progress: \tEpoch 12 [6720/8883 (75.54%)]\t\tLoss: 1.31597\n",
      "Training Progress: \tEpoch 12 [7040/8883 (79.14%)]\t\tLoss: 1.27063\n",
      "Training Progress: \tEpoch 12 [7360/8883 (82.73%)]\t\tLoss: 1.11566\n",
      "Training Progress: \tEpoch 12 [7680/8883 (86.33%)]\t\tLoss: 1.01196\n",
      "Training Progress: \tEpoch 12 [8000/8883 (89.93%)]\t\tLoss: 1.18134\n",
      "Training Progress: \tEpoch 12 [8320/8883 (93.53%)]\t\tLoss: 1.20730\n",
      "Training Progress: \tEpoch 12 [8640/8883 (97.12%)]\t\tLoss: 1.23144\n",
      "\tTrain loss: 0.03587, Accuracy: 4219/8883 (47.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 764/1692 (45.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/8883 (0.00%)]\t\tLoss: 1.28823\n",
      "Training Progress: \tEpoch 13 [320/8883 (3.60%)]\t\tLoss: 1.23353\n",
      "Training Progress: \tEpoch 13 [640/8883 (7.19%)]\t\tLoss: 1.20976\n",
      "Training Progress: \tEpoch 13 [960/8883 (10.79%)]\t\tLoss: 1.23127\n",
      "Training Progress: \tEpoch 13 [1280/8883 (14.39%)]\t\tLoss: 1.25298\n",
      "Training Progress: \tEpoch 13 [1600/8883 (17.99%)]\t\tLoss: 1.31713\n",
      "Training Progress: \tEpoch 13 [1920/8883 (21.58%)]\t\tLoss: 1.20333\n",
      "Training Progress: \tEpoch 13 [2240/8883 (25.18%)]\t\tLoss: 1.25631\n",
      "Training Progress: \tEpoch 13 [2560/8883 (28.78%)]\t\tLoss: 1.20676\n",
      "Training Progress: \tEpoch 13 [2880/8883 (32.37%)]\t\tLoss: 1.32932\n",
      "Training Progress: \tEpoch 13 [3200/8883 (35.97%)]\t\tLoss: 1.09567\n",
      "Training Progress: \tEpoch 13 [3520/8883 (39.57%)]\t\tLoss: 1.25631\n",
      "Training Progress: \tEpoch 13 [3840/8883 (43.17%)]\t\tLoss: 1.16614\n",
      "Training Progress: \tEpoch 13 [4160/8883 (46.76%)]\t\tLoss: 1.18538\n",
      "Training Progress: \tEpoch 13 [4480/8883 (50.36%)]\t\tLoss: 1.21166\n",
      "Training Progress: \tEpoch 13 [4800/8883 (53.96%)]\t\tLoss: 1.21893\n",
      "Training Progress: \tEpoch 13 [5120/8883 (57.55%)]\t\tLoss: 1.17429\n",
      "Training Progress: \tEpoch 13 [5440/8883 (61.15%)]\t\tLoss: 1.09621\n",
      "Training Progress: \tEpoch 13 [5760/8883 (64.75%)]\t\tLoss: 1.27891\n",
      "Training Progress: \tEpoch 13 [6080/8883 (68.35%)]\t\tLoss: 1.05665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 13 [6400/8883 (71.94%)]\t\tLoss: 1.21749\n",
      "Training Progress: \tEpoch 13 [6720/8883 (75.54%)]\t\tLoss: 1.15911\n",
      "Training Progress: \tEpoch 13 [7040/8883 (79.14%)]\t\tLoss: 1.20912\n",
      "Training Progress: \tEpoch 13 [7360/8883 (82.73%)]\t\tLoss: 1.10154\n",
      "Training Progress: \tEpoch 13 [7680/8883 (86.33%)]\t\tLoss: 1.04230\n",
      "Training Progress: \tEpoch 13 [8000/8883 (89.93%)]\t\tLoss: 1.09241\n",
      "Training Progress: \tEpoch 13 [8320/8883 (93.53%)]\t\tLoss: 1.12717\n",
      "Training Progress: \tEpoch 13 [8640/8883 (97.12%)]\t\tLoss: 1.20445\n",
      "\tTrain loss: 0.03515, Accuracy: 4281/8883 (48.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 784/1692 (46.00%)\n",
      "\tTest loss: 0.00087, Accuracy: 537/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/8883 (0.00%)]\t\tLoss: 1.30621\n",
      "Training Progress: \tEpoch 14 [320/8883 (3.60%)]\t\tLoss: 1.24538\n",
      "Training Progress: \tEpoch 14 [640/8883 (7.19%)]\t\tLoss: 1.24225\n",
      "Training Progress: \tEpoch 14 [960/8883 (10.79%)]\t\tLoss: 1.33059\n",
      "Training Progress: \tEpoch 14 [1280/8883 (14.39%)]\t\tLoss: 1.12809\n",
      "Training Progress: \tEpoch 14 [1600/8883 (17.99%)]\t\tLoss: 1.37039\n",
      "Training Progress: \tEpoch 14 [1920/8883 (21.58%)]\t\tLoss: 1.21245\n",
      "Training Progress: \tEpoch 14 [2240/8883 (25.18%)]\t\tLoss: 1.13471\n",
      "Training Progress: \tEpoch 14 [2560/8883 (28.78%)]\t\tLoss: 1.15778\n",
      "Training Progress: \tEpoch 14 [2880/8883 (32.37%)]\t\tLoss: 1.30743\n",
      "Training Progress: \tEpoch 14 [3200/8883 (35.97%)]\t\tLoss: 1.17011\n",
      "Training Progress: \tEpoch 14 [3520/8883 (39.57%)]\t\tLoss: 1.22998\n",
      "Training Progress: \tEpoch 14 [3840/8883 (43.17%)]\t\tLoss: 1.13145\n",
      "Training Progress: \tEpoch 14 [4160/8883 (46.76%)]\t\tLoss: 1.09679\n",
      "Training Progress: \tEpoch 14 [4480/8883 (50.36%)]\t\tLoss: 1.12583\n",
      "Training Progress: \tEpoch 14 [4800/8883 (53.96%)]\t\tLoss: 1.16456\n",
      "Training Progress: \tEpoch 14 [5120/8883 (57.55%)]\t\tLoss: 1.09041\n",
      "Training Progress: \tEpoch 14 [5440/8883 (61.15%)]\t\tLoss: 1.12585\n",
      "Training Progress: \tEpoch 14 [5760/8883 (64.75%)]\t\tLoss: 1.18051\n",
      "Training Progress: \tEpoch 14 [6080/8883 (68.35%)]\t\tLoss: 1.04902\n",
      "Training Progress: \tEpoch 14 [6400/8883 (71.94%)]\t\tLoss: 1.05189\n",
      "Training Progress: \tEpoch 14 [6720/8883 (75.54%)]\t\tLoss: 1.13481\n",
      "Training Progress: \tEpoch 14 [7040/8883 (79.14%)]\t\tLoss: 1.25517\n",
      "Training Progress: \tEpoch 14 [7360/8883 (82.73%)]\t\tLoss: 1.05901\n",
      "Training Progress: \tEpoch 14 [7680/8883 (86.33%)]\t\tLoss: 1.19797\n",
      "Training Progress: \tEpoch 14 [8000/8883 (89.93%)]\t\tLoss: 1.19088\n",
      "Training Progress: \tEpoch 14 [8320/8883 (93.53%)]\t\tLoss: 1.14074\n",
      "Training Progress: \tEpoch 14 [8640/8883 (97.12%)]\t\tLoss: 1.18441\n",
      "\tTrain loss: 0.03438, Accuracy: 4411/8883 (49.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 838/1692 (49.00%)\n",
      "\tTest loss: 0.00088, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/8883 (0.00%)]\t\tLoss: 1.23499\n",
      "Training Progress: \tEpoch 15 [320/8883 (3.60%)]\t\tLoss: 1.12201\n",
      "Training Progress: \tEpoch 15 [640/8883 (7.19%)]\t\tLoss: 1.23589\n",
      "Training Progress: \tEpoch 15 [960/8883 (10.79%)]\t\tLoss: 1.07947\n",
      "Training Progress: \tEpoch 15 [1280/8883 (14.39%)]\t\tLoss: 1.25407\n",
      "Training Progress: \tEpoch 15 [1600/8883 (17.99%)]\t\tLoss: 1.28031\n",
      "Training Progress: \tEpoch 15 [1920/8883 (21.58%)]\t\tLoss: 1.08664\n",
      "Training Progress: \tEpoch 15 [2240/8883 (25.18%)]\t\tLoss: 1.27163\n",
      "Training Progress: \tEpoch 15 [2560/8883 (28.78%)]\t\tLoss: 1.38634\n",
      "Training Progress: \tEpoch 15 [2880/8883 (32.37%)]\t\tLoss: 1.27947\n",
      "Training Progress: \tEpoch 15 [3200/8883 (35.97%)]\t\tLoss: 1.11134\n",
      "Training Progress: \tEpoch 15 [3520/8883 (39.57%)]\t\tLoss: 1.16322\n",
      "Training Progress: \tEpoch 15 [3840/8883 (43.17%)]\t\tLoss: 1.03803\n",
      "Training Progress: \tEpoch 15 [4160/8883 (46.76%)]\t\tLoss: 1.15739\n",
      "Training Progress: \tEpoch 15 [4480/8883 (50.36%)]\t\tLoss: 1.01943\n",
      "Training Progress: \tEpoch 15 [4800/8883 (53.96%)]\t\tLoss: 1.11313\n",
      "Training Progress: \tEpoch 15 [5120/8883 (57.55%)]\t\tLoss: 1.07800\n",
      "Training Progress: \tEpoch 15 [5440/8883 (61.15%)]\t\tLoss: 1.13748\n",
      "Training Progress: \tEpoch 15 [5760/8883 (64.75%)]\t\tLoss: 1.12118\n",
      "Training Progress: \tEpoch 15 [6080/8883 (68.35%)]\t\tLoss: 1.10509\n",
      "Training Progress: \tEpoch 15 [6400/8883 (71.94%)]\t\tLoss: 1.05718\n",
      "Training Progress: \tEpoch 15 [6720/8883 (75.54%)]\t\tLoss: 1.12496\n",
      "Training Progress: \tEpoch 15 [7040/8883 (79.14%)]\t\tLoss: 1.16592\n",
      "Training Progress: \tEpoch 15 [7360/8883 (82.73%)]\t\tLoss: 1.06883\n",
      "Training Progress: \tEpoch 15 [7680/8883 (86.33%)]\t\tLoss: 0.94220\n",
      "Training Progress: \tEpoch 15 [8000/8883 (89.93%)]\t\tLoss: 0.97511\n",
      "Training Progress: \tEpoch 15 [8320/8883 (93.53%)]\t\tLoss: 1.28308\n",
      "Training Progress: \tEpoch 15 [8640/8883 (97.12%)]\t\tLoss: 1.13139\n",
      "\tTrain loss: 0.03364, Accuracy: 4621/8883 (52.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 861/1692 (50.00%)\n",
      "\tTest loss: 0.00088, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/8883 (0.00%)]\t\tLoss: 1.25638\n",
      "Training Progress: \tEpoch 16 [320/8883 (3.60%)]\t\tLoss: 1.11905\n",
      "Training Progress: \tEpoch 16 [640/8883 (7.19%)]\t\tLoss: 1.20920\n",
      "Training Progress: \tEpoch 16 [960/8883 (10.79%)]\t\tLoss: 1.28647\n",
      "Training Progress: \tEpoch 16 [1280/8883 (14.39%)]\t\tLoss: 1.21908\n",
      "Training Progress: \tEpoch 16 [1600/8883 (17.99%)]\t\tLoss: 1.37565\n",
      "Training Progress: \tEpoch 16 [1920/8883 (21.58%)]\t\tLoss: 1.05941\n",
      "Training Progress: \tEpoch 16 [2240/8883 (25.18%)]\t\tLoss: 1.04150\n",
      "Training Progress: \tEpoch 16 [2560/8883 (28.78%)]\t\tLoss: 1.08794\n",
      "Training Progress: \tEpoch 16 [2880/8883 (32.37%)]\t\tLoss: 1.16285\n",
      "Training Progress: \tEpoch 16 [3200/8883 (35.97%)]\t\tLoss: 1.03463\n",
      "Training Progress: \tEpoch 16 [3520/8883 (39.57%)]\t\tLoss: 1.09185\n",
      "Training Progress: \tEpoch 16 [3840/8883 (43.17%)]\t\tLoss: 1.08954\n",
      "Training Progress: \tEpoch 16 [4160/8883 (46.76%)]\t\tLoss: 1.12006\n",
      "Training Progress: \tEpoch 16 [4480/8883 (50.36%)]\t\tLoss: 1.13788\n",
      "Training Progress: \tEpoch 16 [4800/8883 (53.96%)]\t\tLoss: 1.00404\n",
      "Training Progress: \tEpoch 16 [5120/8883 (57.55%)]\t\tLoss: 1.00971\n",
      "Training Progress: \tEpoch 16 [5440/8883 (61.15%)]\t\tLoss: 1.06559\n",
      "Training Progress: \tEpoch 16 [5760/8883 (64.75%)]\t\tLoss: 1.21963\n",
      "Training Progress: \tEpoch 16 [6080/8883 (68.35%)]\t\tLoss: 1.03841\n",
      "Training Progress: \tEpoch 16 [6400/8883 (71.94%)]\t\tLoss: 1.09526\n",
      "Training Progress: \tEpoch 16 [6720/8883 (75.54%)]\t\tLoss: 1.09127\n",
      "Training Progress: \tEpoch 16 [7040/8883 (79.14%)]\t\tLoss: 1.18060\n",
      "Training Progress: \tEpoch 16 [7360/8883 (82.73%)]\t\tLoss: 1.06016\n",
      "Training Progress: \tEpoch 16 [7680/8883 (86.33%)]\t\tLoss: 0.95370\n",
      "Training Progress: \tEpoch 16 [8000/8883 (89.93%)]\t\tLoss: 0.99200\n",
      "Training Progress: \tEpoch 16 [8320/8883 (93.53%)]\t\tLoss: 1.17541\n",
      "Training Progress: \tEpoch 16 [8640/8883 (97.12%)]\t\tLoss: 1.35090\n",
      "\tTrain loss: 0.03292, Accuracy: 4643/8883 (52.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 850/1692 (50.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 553/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/8883 (0.00%)]\t\tLoss: 1.20992\n",
      "Training Progress: \tEpoch 17 [320/8883 (3.60%)]\t\tLoss: 1.10441\n",
      "Training Progress: \tEpoch 17 [640/8883 (7.19%)]\t\tLoss: 1.25144\n",
      "Training Progress: \tEpoch 17 [960/8883 (10.79%)]\t\tLoss: 1.06417\n",
      "Training Progress: \tEpoch 17 [1280/8883 (14.39%)]\t\tLoss: 1.09693\n",
      "Training Progress: \tEpoch 17 [1600/8883 (17.99%)]\t\tLoss: 1.18321\n",
      "Training Progress: \tEpoch 17 [1920/8883 (21.58%)]\t\tLoss: 1.04275\n",
      "Training Progress: \tEpoch 17 [2240/8883 (25.18%)]\t\tLoss: 1.03941\n",
      "Training Progress: \tEpoch 17 [2560/8883 (28.78%)]\t\tLoss: 1.34100\n",
      "Training Progress: \tEpoch 17 [2880/8883 (32.37%)]\t\tLoss: 1.24162\n",
      "Training Progress: \tEpoch 17 [3200/8883 (35.97%)]\t\tLoss: 1.06455\n",
      "Training Progress: \tEpoch 17 [3520/8883 (39.57%)]\t\tLoss: 1.09327\n",
      "Training Progress: \tEpoch 17 [3840/8883 (43.17%)]\t\tLoss: 1.08205\n",
      "Training Progress: \tEpoch 17 [4160/8883 (46.76%)]\t\tLoss: 1.07737\n",
      "Training Progress: \tEpoch 17 [4480/8883 (50.36%)]\t\tLoss: 0.92063\n",
      "Training Progress: \tEpoch 17 [4800/8883 (53.96%)]\t\tLoss: 0.99751\n",
      "Training Progress: \tEpoch 17 [5120/8883 (57.55%)]\t\tLoss: 1.00103\n",
      "Training Progress: \tEpoch 17 [5440/8883 (61.15%)]\t\tLoss: 1.07396\n",
      "Training Progress: \tEpoch 17 [5760/8883 (64.75%)]\t\tLoss: 1.12219\n",
      "Training Progress: \tEpoch 17 [6080/8883 (68.35%)]\t\tLoss: 1.04694\n",
      "Training Progress: \tEpoch 17 [6400/8883 (71.94%)]\t\tLoss: 1.06846\n",
      "Training Progress: \tEpoch 17 [6720/8883 (75.54%)]\t\tLoss: 1.10953\n",
      "Training Progress: \tEpoch 17 [7040/8883 (79.14%)]\t\tLoss: 1.09972\n",
      "Training Progress: \tEpoch 17 [7360/8883 (82.73%)]\t\tLoss: 1.16001\n",
      "Training Progress: \tEpoch 17 [7680/8883 (86.33%)]\t\tLoss: 0.95372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 17 [8000/8883 (89.93%)]\t\tLoss: 1.13295\n",
      "Training Progress: \tEpoch 17 [8320/8883 (93.53%)]\t\tLoss: 1.24254\n",
      "Training Progress: \tEpoch 17 [8640/8883 (97.12%)]\t\tLoss: 1.11830\n",
      "\tTrain loss: 0.03104, Accuracy: 4882/8883 (54.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 887/1692 (52.00%)\n",
      "\tTest loss: 0.00095, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/8883 (0.00%)]\t\tLoss: 1.01844\n",
      "Training Progress: \tEpoch 18 [320/8883 (3.60%)]\t\tLoss: 1.04549\n",
      "Training Progress: \tEpoch 18 [640/8883 (7.19%)]\t\tLoss: 1.09613\n",
      "Training Progress: \tEpoch 18 [960/8883 (10.79%)]\t\tLoss: 0.95411\n",
      "Training Progress: \tEpoch 18 [1280/8883 (14.39%)]\t\tLoss: 1.24308\n",
      "Training Progress: \tEpoch 18 [1600/8883 (17.99%)]\t\tLoss: 1.25011\n",
      "Training Progress: \tEpoch 18 [1920/8883 (21.58%)]\t\tLoss: 1.04148\n",
      "Training Progress: \tEpoch 18 [2240/8883 (25.18%)]\t\tLoss: 1.14390\n",
      "Training Progress: \tEpoch 18 [2560/8883 (28.78%)]\t\tLoss: 1.13967\n",
      "Training Progress: \tEpoch 18 [2880/8883 (32.37%)]\t\tLoss: 1.16490\n",
      "Training Progress: \tEpoch 18 [3200/8883 (35.97%)]\t\tLoss: 1.15275\n",
      "Training Progress: \tEpoch 18 [3520/8883 (39.57%)]\t\tLoss: 1.17588\n",
      "Training Progress: \tEpoch 18 [3840/8883 (43.17%)]\t\tLoss: 1.16586\n",
      "Training Progress: \tEpoch 18 [4160/8883 (46.76%)]\t\tLoss: 0.97257\n",
      "Training Progress: \tEpoch 18 [4480/8883 (50.36%)]\t\tLoss: 0.95375\n",
      "Training Progress: \tEpoch 18 [4800/8883 (53.96%)]\t\tLoss: 1.02028\n",
      "Training Progress: \tEpoch 18 [5120/8883 (57.55%)]\t\tLoss: 1.02224\n",
      "Training Progress: \tEpoch 18 [5440/8883 (61.15%)]\t\tLoss: 1.03731\n",
      "Training Progress: \tEpoch 18 [5760/8883 (64.75%)]\t\tLoss: 1.04577\n",
      "Training Progress: \tEpoch 18 [6080/8883 (68.35%)]\t\tLoss: 1.09602\n",
      "Training Progress: \tEpoch 18 [6400/8883 (71.94%)]\t\tLoss: 1.08077\n",
      "Training Progress: \tEpoch 18 [6720/8883 (75.54%)]\t\tLoss: 0.89063\n",
      "Training Progress: \tEpoch 18 [7040/8883 (79.14%)]\t\tLoss: 1.12573\n",
      "Training Progress: \tEpoch 18 [7360/8883 (82.73%)]\t\tLoss: 1.02413\n",
      "Training Progress: \tEpoch 18 [7680/8883 (86.33%)]\t\tLoss: 0.98167\n",
      "Training Progress: \tEpoch 18 [8000/8883 (89.93%)]\t\tLoss: 1.01963\n",
      "Training Progress: \tEpoch 18 [8320/8883 (93.53%)]\t\tLoss: 1.18390\n",
      "Training Progress: \tEpoch 18 [8640/8883 (97.12%)]\t\tLoss: 1.09905\n",
      "\tTrain loss: 0.03128, Accuracy: 4893/8883 (55.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 925/1692 (54.00%)\n",
      "\tTest loss: 0.00096, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/8883 (0.00%)]\t\tLoss: 1.09891\n",
      "Training Progress: \tEpoch 19 [320/8883 (3.60%)]\t\tLoss: 1.08739\n",
      "Training Progress: \tEpoch 19 [640/8883 (7.19%)]\t\tLoss: 1.10947\n",
      "Training Progress: \tEpoch 19 [960/8883 (10.79%)]\t\tLoss: 1.04326\n",
      "Training Progress: \tEpoch 19 [1280/8883 (14.39%)]\t\tLoss: 1.16590\n",
      "Training Progress: \tEpoch 19 [1600/8883 (17.99%)]\t\tLoss: 1.36602\n",
      "Training Progress: \tEpoch 19 [1920/8883 (21.58%)]\t\tLoss: 1.02857\n",
      "Training Progress: \tEpoch 19 [2240/8883 (25.18%)]\t\tLoss: 1.06217\n",
      "Training Progress: \tEpoch 19 [2560/8883 (28.78%)]\t\tLoss: 1.16490\n",
      "Training Progress: \tEpoch 19 [2880/8883 (32.37%)]\t\tLoss: 1.33357\n",
      "Training Progress: \tEpoch 19 [3200/8883 (35.97%)]\t\tLoss: 1.02545\n",
      "Training Progress: \tEpoch 19 [3520/8883 (39.57%)]\t\tLoss: 1.05291\n",
      "Training Progress: \tEpoch 19 [3840/8883 (43.17%)]\t\tLoss: 1.17423\n",
      "Training Progress: \tEpoch 19 [4160/8883 (46.76%)]\t\tLoss: 0.96597\n",
      "Training Progress: \tEpoch 19 [4480/8883 (50.36%)]\t\tLoss: 1.04034\n",
      "Training Progress: \tEpoch 19 [4800/8883 (53.96%)]\t\tLoss: 0.89967\n",
      "Training Progress: \tEpoch 19 [5120/8883 (57.55%)]\t\tLoss: 0.98249\n",
      "Training Progress: \tEpoch 19 [5440/8883 (61.15%)]\t\tLoss: 1.06504\n",
      "Training Progress: \tEpoch 19 [5760/8883 (64.75%)]\t\tLoss: 1.08417\n",
      "Training Progress: \tEpoch 19 [6080/8883 (68.35%)]\t\tLoss: 0.99298\n",
      "Training Progress: \tEpoch 19 [6400/8883 (71.94%)]\t\tLoss: 1.11109\n",
      "Training Progress: \tEpoch 19 [6720/8883 (75.54%)]\t\tLoss: 0.98334\n",
      "Training Progress: \tEpoch 19 [7040/8883 (79.14%)]\t\tLoss: 1.11703\n",
      "Training Progress: \tEpoch 19 [7360/8883 (82.73%)]\t\tLoss: 0.90868\n",
      "Training Progress: \tEpoch 19 [7680/8883 (86.33%)]\t\tLoss: 0.93223\n",
      "Training Progress: \tEpoch 19 [8000/8883 (89.93%)]\t\tLoss: 1.03953\n",
      "Training Progress: \tEpoch 19 [8320/8883 (93.53%)]\t\tLoss: 1.12251\n",
      "Training Progress: \tEpoch 19 [8640/8883 (97.12%)]\t\tLoss: 0.98428\n",
      "\tTrain loss: 0.02983, Accuracy: 5076/8883 (57.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 978/1692 (57.00%)\n",
      "\tTest loss: 0.00099, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/8883 (0.00%)]\t\tLoss: 1.23640\n",
      "Training Progress: \tEpoch 20 [320/8883 (3.60%)]\t\tLoss: 1.04881\n",
      "Training Progress: \tEpoch 20 [640/8883 (7.19%)]\t\tLoss: 0.98078\n",
      "Training Progress: \tEpoch 20 [960/8883 (10.79%)]\t\tLoss: 1.17472\n",
      "Training Progress: \tEpoch 20 [1280/8883 (14.39%)]\t\tLoss: 1.09944\n",
      "Training Progress: \tEpoch 20 [1600/8883 (17.99%)]\t\tLoss: 1.32479\n",
      "Training Progress: \tEpoch 20 [1920/8883 (21.58%)]\t\tLoss: 0.88003\n",
      "Training Progress: \tEpoch 20 [2240/8883 (25.18%)]\t\tLoss: 1.01800\n",
      "Training Progress: \tEpoch 20 [2560/8883 (28.78%)]\t\tLoss: 1.13481\n",
      "Training Progress: \tEpoch 20 [2880/8883 (32.37%)]\t\tLoss: 1.05941\n",
      "Training Progress: \tEpoch 20 [3200/8883 (35.97%)]\t\tLoss: 1.04780\n",
      "Training Progress: \tEpoch 20 [3520/8883 (39.57%)]\t\tLoss: 1.08396\n",
      "Training Progress: \tEpoch 20 [3840/8883 (43.17%)]\t\tLoss: 1.09255\n",
      "Training Progress: \tEpoch 20 [4160/8883 (46.76%)]\t\tLoss: 0.87012\n",
      "Training Progress: \tEpoch 20 [4480/8883 (50.36%)]\t\tLoss: 1.13163\n",
      "Training Progress: \tEpoch 20 [4800/8883 (53.96%)]\t\tLoss: 1.06235\n",
      "Training Progress: \tEpoch 20 [5120/8883 (57.55%)]\t\tLoss: 0.96141\n",
      "Training Progress: \tEpoch 20 [5440/8883 (61.15%)]\t\tLoss: 0.96400\n",
      "Training Progress: \tEpoch 20 [5760/8883 (64.75%)]\t\tLoss: 1.02672\n",
      "Training Progress: \tEpoch 20 [6080/8883 (68.35%)]\t\tLoss: 0.94333\n",
      "Training Progress: \tEpoch 20 [6400/8883 (71.94%)]\t\tLoss: 1.11188\n",
      "Training Progress: \tEpoch 20 [6720/8883 (75.54%)]\t\tLoss: 0.88181\n",
      "Training Progress: \tEpoch 20 [7040/8883 (79.14%)]\t\tLoss: 1.07543\n",
      "Training Progress: \tEpoch 20 [7360/8883 (82.73%)]\t\tLoss: 0.90583\n",
      "Training Progress: \tEpoch 20 [7680/8883 (86.33%)]\t\tLoss: 0.83742\n",
      "Training Progress: \tEpoch 20 [8000/8883 (89.93%)]\t\tLoss: 1.05793\n",
      "Training Progress: \tEpoch 20 [8320/8883 (93.53%)]\t\tLoss: 1.08311\n",
      "Training Progress: \tEpoch 20 [8640/8883 (97.12%)]\t\tLoss: 1.05853\n",
      "\tTrain loss: 0.03019, Accuracy: 5006/8883 (56.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 952/1692 (56.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 504/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/8883 (0.00%)]\t\tLoss: 0.94283\n",
      "Training Progress: \tEpoch 21 [320/8883 (3.60%)]\t\tLoss: 0.97918\n",
      "Training Progress: \tEpoch 21 [640/8883 (7.19%)]\t\tLoss: 1.10446\n",
      "Training Progress: \tEpoch 21 [960/8883 (10.79%)]\t\tLoss: 1.00942\n",
      "Training Progress: \tEpoch 21 [1280/8883 (14.39%)]\t\tLoss: 1.08930\n",
      "Training Progress: \tEpoch 21 [1600/8883 (17.99%)]\t\tLoss: 1.22588\n",
      "Training Progress: \tEpoch 21 [1920/8883 (21.58%)]\t\tLoss: 1.05720\n",
      "Training Progress: \tEpoch 21 [2240/8883 (25.18%)]\t\tLoss: 1.15286\n",
      "Training Progress: \tEpoch 21 [2560/8883 (28.78%)]\t\tLoss: 1.18065\n",
      "Training Progress: \tEpoch 21 [2880/8883 (32.37%)]\t\tLoss: 1.00608\n",
      "Training Progress: \tEpoch 21 [3200/8883 (35.97%)]\t\tLoss: 0.97620\n",
      "Training Progress: \tEpoch 21 [3520/8883 (39.57%)]\t\tLoss: 1.09403\n",
      "Training Progress: \tEpoch 21 [3840/8883 (43.17%)]\t\tLoss: 1.30035\n",
      "Training Progress: \tEpoch 21 [4160/8883 (46.76%)]\t\tLoss: 0.99976\n",
      "Training Progress: \tEpoch 21 [4480/8883 (50.36%)]\t\tLoss: 1.04447\n",
      "Training Progress: \tEpoch 21 [4800/8883 (53.96%)]\t\tLoss: 0.96999\n",
      "Training Progress: \tEpoch 21 [5120/8883 (57.55%)]\t\tLoss: 0.96008\n",
      "Training Progress: \tEpoch 21 [5440/8883 (61.15%)]\t\tLoss: 1.19477\n",
      "Training Progress: \tEpoch 21 [5760/8883 (64.75%)]\t\tLoss: 1.06120\n",
      "Training Progress: \tEpoch 21 [6080/8883 (68.35%)]\t\tLoss: 0.90263\n",
      "Training Progress: \tEpoch 21 [6400/8883 (71.94%)]\t\tLoss: 0.93375\n",
      "Training Progress: \tEpoch 21 [6720/8883 (75.54%)]\t\tLoss: 1.02730\n",
      "Training Progress: \tEpoch 21 [7040/8883 (79.14%)]\t\tLoss: 1.08239\n",
      "Training Progress: \tEpoch 21 [7360/8883 (82.73%)]\t\tLoss: 1.02698\n",
      "Training Progress: \tEpoch 21 [7680/8883 (86.33%)]\t\tLoss: 0.87248\n",
      "Training Progress: \tEpoch 21 [8000/8883 (89.93%)]\t\tLoss: 0.98388\n",
      "Training Progress: \tEpoch 21 [8320/8883 (93.53%)]\t\tLoss: 1.15837\n",
      "Training Progress: \tEpoch 21 [8640/8883 (97.12%)]\t\tLoss: 1.10668\n",
      "\tTrain loss: 0.02851, Accuracy: 5245/8883 (59.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 991/1692 (58.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 497/1772 (28.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 22 [0/8883 (0.00%)]\t\tLoss: 0.95159\n",
      "Training Progress: \tEpoch 22 [320/8883 (3.60%)]\t\tLoss: 1.03424\n",
      "Training Progress: \tEpoch 22 [640/8883 (7.19%)]\t\tLoss: 1.11961\n",
      "Training Progress: \tEpoch 22 [960/8883 (10.79%)]\t\tLoss: 1.00437\n",
      "Training Progress: \tEpoch 22 [1280/8883 (14.39%)]\t\tLoss: 1.04860\n",
      "Training Progress: \tEpoch 22 [1600/8883 (17.99%)]\t\tLoss: 1.11183\n",
      "Training Progress: \tEpoch 22 [1920/8883 (21.58%)]\t\tLoss: 1.04519\n",
      "Training Progress: \tEpoch 22 [2240/8883 (25.18%)]\t\tLoss: 1.02326\n",
      "Training Progress: \tEpoch 22 [2560/8883 (28.78%)]\t\tLoss: 1.00057\n",
      "Training Progress: \tEpoch 22 [2880/8883 (32.37%)]\t\tLoss: 1.14804\n",
      "Training Progress: \tEpoch 22 [3200/8883 (35.97%)]\t\tLoss: 1.06565\n",
      "Training Progress: \tEpoch 22 [3520/8883 (39.57%)]\t\tLoss: 1.12618\n",
      "Training Progress: \tEpoch 22 [3840/8883 (43.17%)]\t\tLoss: 1.10261\n",
      "Training Progress: \tEpoch 22 [4160/8883 (46.76%)]\t\tLoss: 0.84525\n",
      "Training Progress: \tEpoch 22 [4480/8883 (50.36%)]\t\tLoss: 0.83228\n",
      "Training Progress: \tEpoch 22 [4800/8883 (53.96%)]\t\tLoss: 0.77269\n",
      "Training Progress: \tEpoch 22 [5120/8883 (57.55%)]\t\tLoss: 0.95622\n",
      "Training Progress: \tEpoch 22 [5440/8883 (61.15%)]\t\tLoss: 1.10576\n",
      "Training Progress: \tEpoch 22 [5760/8883 (64.75%)]\t\tLoss: 1.02724\n",
      "Training Progress: \tEpoch 22 [6080/8883 (68.35%)]\t\tLoss: 0.99145\n",
      "Training Progress: \tEpoch 22 [6400/8883 (71.94%)]\t\tLoss: 1.04737\n",
      "Training Progress: \tEpoch 22 [6720/8883 (75.54%)]\t\tLoss: 0.93683\n",
      "Training Progress: \tEpoch 22 [7040/8883 (79.14%)]\t\tLoss: 0.98211\n",
      "Training Progress: \tEpoch 22 [7360/8883 (82.73%)]\t\tLoss: 0.90584\n",
      "Training Progress: \tEpoch 22 [7680/8883 (86.33%)]\t\tLoss: 0.89048\n",
      "Training Progress: \tEpoch 22 [8000/8883 (89.93%)]\t\tLoss: 0.85185\n",
      "Training Progress: \tEpoch 22 [8320/8883 (93.53%)]\t\tLoss: 1.13161\n",
      "Training Progress: \tEpoch 22 [8640/8883 (97.12%)]\t\tLoss: 1.05964\n",
      "\tTrain loss: 0.02786, Accuracy: 5301/8883 (59.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1006/1692 (59.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/8883 (0.00%)]\t\tLoss: 1.00275\n",
      "Training Progress: \tEpoch 23 [320/8883 (3.60%)]\t\tLoss: 1.12703\n",
      "Training Progress: \tEpoch 23 [640/8883 (7.19%)]\t\tLoss: 1.10898\n",
      "Training Progress: \tEpoch 23 [960/8883 (10.79%)]\t\tLoss: 0.93372\n",
      "Training Progress: \tEpoch 23 [1280/8883 (14.39%)]\t\tLoss: 1.10980\n",
      "Training Progress: \tEpoch 23 [1600/8883 (17.99%)]\t\tLoss: 1.18458\n",
      "Training Progress: \tEpoch 23 [1920/8883 (21.58%)]\t\tLoss: 0.91204\n",
      "Training Progress: \tEpoch 23 [2240/8883 (25.18%)]\t\tLoss: 0.92928\n",
      "Training Progress: \tEpoch 23 [2560/8883 (28.78%)]\t\tLoss: 1.05406\n",
      "Training Progress: \tEpoch 23 [2880/8883 (32.37%)]\t\tLoss: 1.05094\n",
      "Training Progress: \tEpoch 23 [3200/8883 (35.97%)]\t\tLoss: 0.89831\n",
      "Training Progress: \tEpoch 23 [3520/8883 (39.57%)]\t\tLoss: 1.22702\n",
      "Training Progress: \tEpoch 23 [3840/8883 (43.17%)]\t\tLoss: 1.04158\n",
      "Training Progress: \tEpoch 23 [4160/8883 (46.76%)]\t\tLoss: 0.82829\n",
      "Training Progress: \tEpoch 23 [4480/8883 (50.36%)]\t\tLoss: 0.92310\n",
      "Training Progress: \tEpoch 23 [4800/8883 (53.96%)]\t\tLoss: 0.85924\n",
      "Training Progress: \tEpoch 23 [5120/8883 (57.55%)]\t\tLoss: 0.89953\n",
      "Training Progress: \tEpoch 23 [5440/8883 (61.15%)]\t\tLoss: 1.05338\n",
      "Training Progress: \tEpoch 23 [5760/8883 (64.75%)]\t\tLoss: 0.90362\n",
      "Training Progress: \tEpoch 23 [6080/8883 (68.35%)]\t\tLoss: 1.17094\n",
      "Training Progress: \tEpoch 23 [6400/8883 (71.94%)]\t\tLoss: 1.04912\n",
      "Training Progress: \tEpoch 23 [6720/8883 (75.54%)]\t\tLoss: 1.01301\n",
      "Training Progress: \tEpoch 23 [7040/8883 (79.14%)]\t\tLoss: 1.04011\n",
      "Training Progress: \tEpoch 23 [7360/8883 (82.73%)]\t\tLoss: 0.88332\n",
      "Training Progress: \tEpoch 23 [7680/8883 (86.33%)]\t\tLoss: 0.98921\n",
      "Training Progress: \tEpoch 23 [8000/8883 (89.93%)]\t\tLoss: 0.79717\n",
      "Training Progress: \tEpoch 23 [8320/8883 (93.53%)]\t\tLoss: 0.98505\n",
      "Training Progress: \tEpoch 23 [8640/8883 (97.12%)]\t\tLoss: 1.03152\n",
      "\tTrain loss: 0.02692, Accuracy: 5480/8883 (61.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1032/1692 (60.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 559/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/8883 (0.00%)]\t\tLoss: 0.90905\n",
      "Training Progress: \tEpoch 24 [320/8883 (3.60%)]\t\tLoss: 1.05719\n",
      "Training Progress: \tEpoch 24 [640/8883 (7.19%)]\t\tLoss: 0.88427\n",
      "Training Progress: \tEpoch 24 [960/8883 (10.79%)]\t\tLoss: 0.85699\n",
      "Training Progress: \tEpoch 24 [1280/8883 (14.39%)]\t\tLoss: 0.93714\n",
      "Training Progress: \tEpoch 24 [1600/8883 (17.99%)]\t\tLoss: 1.10999\n",
      "Training Progress: \tEpoch 24 [1920/8883 (21.58%)]\t\tLoss: 1.00226\n",
      "Training Progress: \tEpoch 24 [2240/8883 (25.18%)]\t\tLoss: 0.92457\n",
      "Training Progress: \tEpoch 24 [2560/8883 (28.78%)]\t\tLoss: 1.01870\n",
      "Training Progress: \tEpoch 24 [2880/8883 (32.37%)]\t\tLoss: 1.09875\n",
      "Training Progress: \tEpoch 24 [3200/8883 (35.97%)]\t\tLoss: 1.03046\n",
      "Training Progress: \tEpoch 24 [3520/8883 (39.57%)]\t\tLoss: 1.07810\n",
      "Training Progress: \tEpoch 24 [3840/8883 (43.17%)]\t\tLoss: 1.03897\n",
      "Training Progress: \tEpoch 24 [4160/8883 (46.76%)]\t\tLoss: 0.82312\n",
      "Training Progress: \tEpoch 24 [4480/8883 (50.36%)]\t\tLoss: 0.89583\n",
      "Training Progress: \tEpoch 24 [4800/8883 (53.96%)]\t\tLoss: 0.80179\n",
      "Training Progress: \tEpoch 24 [5120/8883 (57.55%)]\t\tLoss: 1.04072\n",
      "Training Progress: \tEpoch 24 [5440/8883 (61.15%)]\t\tLoss: 1.01280\n",
      "Training Progress: \tEpoch 24 [5760/8883 (64.75%)]\t\tLoss: 0.92067\n",
      "Training Progress: \tEpoch 24 [6080/8883 (68.35%)]\t\tLoss: 0.99374\n",
      "Training Progress: \tEpoch 24 [6400/8883 (71.94%)]\t\tLoss: 0.96141\n",
      "Training Progress: \tEpoch 24 [6720/8883 (75.54%)]\t\tLoss: 0.91176\n",
      "Training Progress: \tEpoch 24 [7040/8883 (79.14%)]\t\tLoss: 1.00307\n",
      "Training Progress: \tEpoch 24 [7360/8883 (82.73%)]\t\tLoss: 0.87073\n",
      "Training Progress: \tEpoch 24 [7680/8883 (86.33%)]\t\tLoss: 0.84002\n",
      "Training Progress: \tEpoch 24 [8000/8883 (89.93%)]\t\tLoss: 0.87518\n",
      "Training Progress: \tEpoch 24 [8320/8883 (93.53%)]\t\tLoss: 1.06604\n",
      "Training Progress: \tEpoch 24 [8640/8883 (97.12%)]\t\tLoss: 0.97690\n",
      "\tTrain loss: 0.02700, Accuracy: 5446/8883 (61.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1036/1692 (61.00%)\n",
      "\tTest loss: 0.00105, Accuracy: 508/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/8883 (0.00%)]\t\tLoss: 1.02595\n",
      "Training Progress: \tEpoch 25 [320/8883 (3.60%)]\t\tLoss: 1.04790\n",
      "Training Progress: \tEpoch 25 [640/8883 (7.19%)]\t\tLoss: 0.97026\n",
      "Training Progress: \tEpoch 25 [960/8883 (10.79%)]\t\tLoss: 0.91741\n",
      "Training Progress: \tEpoch 25 [1280/8883 (14.39%)]\t\tLoss: 0.97925\n",
      "Training Progress: \tEpoch 25 [1600/8883 (17.99%)]\t\tLoss: 1.22283\n",
      "Training Progress: \tEpoch 25 [1920/8883 (21.58%)]\t\tLoss: 0.88898\n",
      "Training Progress: \tEpoch 25 [2240/8883 (25.18%)]\t\tLoss: 0.98058\n",
      "Training Progress: \tEpoch 25 [2560/8883 (28.78%)]\t\tLoss: 1.01381\n",
      "Training Progress: \tEpoch 25 [2880/8883 (32.37%)]\t\tLoss: 1.05285\n",
      "Training Progress: \tEpoch 25 [3200/8883 (35.97%)]\t\tLoss: 1.19557\n",
      "Training Progress: \tEpoch 25 [3520/8883 (39.57%)]\t\tLoss: 0.86573\n",
      "Training Progress: \tEpoch 25 [3840/8883 (43.17%)]\t\tLoss: 1.08437\n",
      "Training Progress: \tEpoch 25 [4160/8883 (46.76%)]\t\tLoss: 0.90619\n",
      "Training Progress: \tEpoch 25 [4480/8883 (50.36%)]\t\tLoss: 0.78059\n",
      "Training Progress: \tEpoch 25 [4800/8883 (53.96%)]\t\tLoss: 0.83059\n",
      "Training Progress: \tEpoch 25 [5120/8883 (57.55%)]\t\tLoss: 0.82720\n",
      "Training Progress: \tEpoch 25 [5440/8883 (61.15%)]\t\tLoss: 1.03046\n",
      "Training Progress: \tEpoch 25 [5760/8883 (64.75%)]\t\tLoss: 0.78515\n",
      "Training Progress: \tEpoch 25 [6080/8883 (68.35%)]\t\tLoss: 0.87110\n",
      "Training Progress: \tEpoch 25 [6400/8883 (71.94%)]\t\tLoss: 0.95583\n",
      "Training Progress: \tEpoch 25 [6720/8883 (75.54%)]\t\tLoss: 1.14913\n",
      "Training Progress: \tEpoch 25 [7040/8883 (79.14%)]\t\tLoss: 1.12888\n",
      "Training Progress: \tEpoch 25 [7360/8883 (82.73%)]\t\tLoss: 0.85433\n",
      "Training Progress: \tEpoch 25 [7680/8883 (86.33%)]\t\tLoss: 0.79847\n",
      "Training Progress: \tEpoch 25 [8000/8883 (89.93%)]\t\tLoss: 0.81405\n",
      "Training Progress: \tEpoch 25 [8320/8883 (93.53%)]\t\tLoss: 1.03669\n",
      "Training Progress: \tEpoch 25 [8640/8883 (97.12%)]\t\tLoss: 0.96153\n",
      "\tTrain loss: 0.02588, Accuracy: 5532/8883 (62.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1039/1692 (61.00%)\n",
      "\tTest loss: 0.00111, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/8883 (0.00%)]\t\tLoss: 0.83372\n",
      "Training Progress: \tEpoch 26 [320/8883 (3.60%)]\t\tLoss: 0.89586\n",
      "Training Progress: \tEpoch 26 [640/8883 (7.19%)]\t\tLoss: 0.92696\n",
      "Training Progress: \tEpoch 26 [960/8883 (10.79%)]\t\tLoss: 0.84894\n",
      "Training Progress: \tEpoch 26 [1280/8883 (14.39%)]\t\tLoss: 0.98391\n",
      "Training Progress: \tEpoch 26 [1600/8883 (17.99%)]\t\tLoss: 0.97997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 26 [1920/8883 (21.58%)]\t\tLoss: 0.94924\n",
      "Training Progress: \tEpoch 26 [2240/8883 (25.18%)]\t\tLoss: 0.97496\n",
      "Training Progress: \tEpoch 26 [2560/8883 (28.78%)]\t\tLoss: 1.04541\n",
      "Training Progress: \tEpoch 26 [2880/8883 (32.37%)]\t\tLoss: 0.98283\n",
      "Training Progress: \tEpoch 26 [3200/8883 (35.97%)]\t\tLoss: 0.92109\n",
      "Training Progress: \tEpoch 26 [3520/8883 (39.57%)]\t\tLoss: 0.90255\n",
      "Training Progress: \tEpoch 26 [3840/8883 (43.17%)]\t\tLoss: 0.97279\n",
      "Training Progress: \tEpoch 26 [4160/8883 (46.76%)]\t\tLoss: 0.80793\n",
      "Training Progress: \tEpoch 26 [4480/8883 (50.36%)]\t\tLoss: 0.86932\n",
      "Training Progress: \tEpoch 26 [4800/8883 (53.96%)]\t\tLoss: 0.64804\n",
      "Training Progress: \tEpoch 26 [5120/8883 (57.55%)]\t\tLoss: 0.83627\n",
      "Training Progress: \tEpoch 26 [5440/8883 (61.15%)]\t\tLoss: 0.81666\n",
      "Training Progress: \tEpoch 26 [5760/8883 (64.75%)]\t\tLoss: 0.79482\n",
      "Training Progress: \tEpoch 26 [6080/8883 (68.35%)]\t\tLoss: 0.85091\n",
      "Training Progress: \tEpoch 26 [6400/8883 (71.94%)]\t\tLoss: 1.03798\n",
      "Training Progress: \tEpoch 26 [6720/8883 (75.54%)]\t\tLoss: 0.86554\n",
      "Training Progress: \tEpoch 26 [7040/8883 (79.14%)]\t\tLoss: 1.07604\n",
      "Training Progress: \tEpoch 26 [7360/8883 (82.73%)]\t\tLoss: 0.97531\n",
      "Training Progress: \tEpoch 26 [7680/8883 (86.33%)]\t\tLoss: 0.91601\n",
      "Training Progress: \tEpoch 26 [8000/8883 (89.93%)]\t\tLoss: 0.83723\n",
      "Training Progress: \tEpoch 26 [8320/8883 (93.53%)]\t\tLoss: 1.19425\n",
      "Training Progress: \tEpoch 26 [8640/8883 (97.12%)]\t\tLoss: 0.90931\n",
      "\tTrain loss: 0.02603, Accuracy: 5577/8883 (62.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1049/1692 (61.00%)\n",
      "\tTest loss: 0.00113, Accuracy: 506/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/8883 (0.00%)]\t\tLoss: 0.97568\n",
      "Training Progress: \tEpoch 27 [320/8883 (3.60%)]\t\tLoss: 1.06857\n",
      "Training Progress: \tEpoch 27 [640/8883 (7.19%)]\t\tLoss: 1.09123\n",
      "Training Progress: \tEpoch 27 [960/8883 (10.79%)]\t\tLoss: 0.78719\n",
      "Training Progress: \tEpoch 27 [1280/8883 (14.39%)]\t\tLoss: 0.96327\n",
      "Training Progress: \tEpoch 27 [1600/8883 (17.99%)]\t\tLoss: 1.18029\n",
      "Training Progress: \tEpoch 27 [1920/8883 (21.58%)]\t\tLoss: 0.82069\n",
      "Training Progress: \tEpoch 27 [2240/8883 (25.18%)]\t\tLoss: 0.92013\n",
      "Training Progress: \tEpoch 27 [2560/8883 (28.78%)]\t\tLoss: 1.15986\n",
      "Training Progress: \tEpoch 27 [2880/8883 (32.37%)]\t\tLoss: 0.93170\n",
      "Training Progress: \tEpoch 27 [3200/8883 (35.97%)]\t\tLoss: 0.86341\n",
      "Training Progress: \tEpoch 27 [3520/8883 (39.57%)]\t\tLoss: 1.05004\n",
      "Training Progress: \tEpoch 27 [3840/8883 (43.17%)]\t\tLoss: 1.11811\n",
      "Training Progress: \tEpoch 27 [4160/8883 (46.76%)]\t\tLoss: 0.79524\n",
      "Training Progress: \tEpoch 27 [4480/8883 (50.36%)]\t\tLoss: 0.68220\n",
      "Training Progress: \tEpoch 27 [4800/8883 (53.96%)]\t\tLoss: 0.86537\n",
      "Training Progress: \tEpoch 27 [5120/8883 (57.55%)]\t\tLoss: 0.75040\n",
      "Training Progress: \tEpoch 27 [5440/8883 (61.15%)]\t\tLoss: 1.02185\n",
      "Training Progress: \tEpoch 27 [5760/8883 (64.75%)]\t\tLoss: 0.82206\n",
      "Training Progress: \tEpoch 27 [6080/8883 (68.35%)]\t\tLoss: 1.07948\n",
      "Training Progress: \tEpoch 27 [6400/8883 (71.94%)]\t\tLoss: 0.87623\n",
      "Training Progress: \tEpoch 27 [6720/8883 (75.54%)]\t\tLoss: 0.84815\n",
      "Training Progress: \tEpoch 27 [7040/8883 (79.14%)]\t\tLoss: 1.15147\n",
      "Training Progress: \tEpoch 27 [7360/8883 (82.73%)]\t\tLoss: 0.85078\n",
      "Training Progress: \tEpoch 27 [7680/8883 (86.33%)]\t\tLoss: 0.69356\n",
      "Training Progress: \tEpoch 27 [8000/8883 (89.93%)]\t\tLoss: 0.82837\n",
      "Training Progress: \tEpoch 27 [8320/8883 (93.53%)]\t\tLoss: 1.11589\n",
      "Training Progress: \tEpoch 27 [8640/8883 (97.12%)]\t\tLoss: 0.91391\n",
      "\tTrain loss: 0.02436, Accuracy: 5805/8883 (65.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1090/1692 (64.00%)\n",
      "\tTest loss: 0.00116, Accuracy: 487/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/8883 (0.00%)]\t\tLoss: 0.87691\n",
      "Training Progress: \tEpoch 28 [320/8883 (3.60%)]\t\tLoss: 1.12089\n",
      "Training Progress: \tEpoch 28 [640/8883 (7.19%)]\t\tLoss: 1.04879\n",
      "Training Progress: \tEpoch 28 [960/8883 (10.79%)]\t\tLoss: 1.25857\n",
      "Training Progress: \tEpoch 28 [1280/8883 (14.39%)]\t\tLoss: 1.00553\n",
      "Training Progress: \tEpoch 28 [1600/8883 (17.99%)]\t\tLoss: 1.24128\n",
      "Training Progress: \tEpoch 28 [1920/8883 (21.58%)]\t\tLoss: 0.76379\n",
      "Training Progress: \tEpoch 28 [2240/8883 (25.18%)]\t\tLoss: 1.13219\n",
      "Training Progress: \tEpoch 28 [2560/8883 (28.78%)]\t\tLoss: 1.02863\n",
      "Training Progress: \tEpoch 28 [2880/8883 (32.37%)]\t\tLoss: 1.00343\n",
      "Training Progress: \tEpoch 28 [3200/8883 (35.97%)]\t\tLoss: 1.05725\n",
      "Training Progress: \tEpoch 28 [3520/8883 (39.57%)]\t\tLoss: 0.84421\n",
      "Training Progress: \tEpoch 28 [3840/8883 (43.17%)]\t\tLoss: 1.00616\n",
      "Training Progress: \tEpoch 28 [4160/8883 (46.76%)]\t\tLoss: 0.77528\n",
      "Training Progress: \tEpoch 28 [4480/8883 (50.36%)]\t\tLoss: 0.74625\n",
      "Training Progress: \tEpoch 28 [4800/8883 (53.96%)]\t\tLoss: 0.72075\n",
      "Training Progress: \tEpoch 28 [5120/8883 (57.55%)]\t\tLoss: 0.86790\n",
      "Training Progress: \tEpoch 28 [5440/8883 (61.15%)]\t\tLoss: 0.88147\n",
      "Training Progress: \tEpoch 28 [5760/8883 (64.75%)]\t\tLoss: 0.71164\n",
      "Training Progress: \tEpoch 28 [6080/8883 (68.35%)]\t\tLoss: 0.96892\n",
      "Training Progress: \tEpoch 28 [6400/8883 (71.94%)]\t\tLoss: 0.95296\n",
      "Training Progress: \tEpoch 28 [6720/8883 (75.54%)]\t\tLoss: 1.09878\n",
      "Training Progress: \tEpoch 28 [7040/8883 (79.14%)]\t\tLoss: 1.10728\n",
      "Training Progress: \tEpoch 28 [7360/8883 (82.73%)]\t\tLoss: 0.99227\n",
      "Training Progress: \tEpoch 28 [7680/8883 (86.33%)]\t\tLoss: 0.84981\n",
      "Training Progress: \tEpoch 28 [8000/8883 (89.93%)]\t\tLoss: 0.85274\n",
      "Training Progress: \tEpoch 28 [8320/8883 (93.53%)]\t\tLoss: 1.07539\n",
      "Training Progress: \tEpoch 28 [8640/8883 (97.12%)]\t\tLoss: 0.95630\n",
      "\tTrain loss: 0.02450, Accuracy: 5724/8883 (64.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1084/1692 (64.00%)\n",
      "\tTest loss: 0.00111, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/8883 (0.00%)]\t\tLoss: 0.81630\n",
      "Training Progress: \tEpoch 29 [320/8883 (3.60%)]\t\tLoss: 0.88709\n",
      "Training Progress: \tEpoch 29 [640/8883 (7.19%)]\t\tLoss: 0.87420\n",
      "Training Progress: \tEpoch 29 [960/8883 (10.79%)]\t\tLoss: 0.96205\n",
      "Training Progress: \tEpoch 29 [1280/8883 (14.39%)]\t\tLoss: 1.16488\n",
      "Training Progress: \tEpoch 29 [1600/8883 (17.99%)]\t\tLoss: 0.93186\n",
      "Training Progress: \tEpoch 29 [1920/8883 (21.58%)]\t\tLoss: 0.75991\n",
      "Training Progress: \tEpoch 29 [2240/8883 (25.18%)]\t\tLoss: 0.87158\n",
      "Training Progress: \tEpoch 29 [2560/8883 (28.78%)]\t\tLoss: 0.95696\n",
      "Training Progress: \tEpoch 29 [2880/8883 (32.37%)]\t\tLoss: 0.92411\n",
      "Training Progress: \tEpoch 29 [3200/8883 (35.97%)]\t\tLoss: 0.78018\n",
      "Training Progress: \tEpoch 29 [3520/8883 (39.57%)]\t\tLoss: 0.94437\n",
      "Training Progress: \tEpoch 29 [3840/8883 (43.17%)]\t\tLoss: 0.95373\n",
      "Training Progress: \tEpoch 29 [4160/8883 (46.76%)]\t\tLoss: 0.78221\n",
      "Training Progress: \tEpoch 29 [4480/8883 (50.36%)]\t\tLoss: 0.71399\n",
      "Training Progress: \tEpoch 29 [4800/8883 (53.96%)]\t\tLoss: 0.73407\n",
      "Training Progress: \tEpoch 29 [5120/8883 (57.55%)]\t\tLoss: 0.87485\n",
      "Training Progress: \tEpoch 29 [5440/8883 (61.15%)]\t\tLoss: 0.97038\n",
      "Training Progress: \tEpoch 29 [5760/8883 (64.75%)]\t\tLoss: 0.75842\n",
      "Training Progress: \tEpoch 29 [6080/8883 (68.35%)]\t\tLoss: 0.88670\n",
      "Training Progress: \tEpoch 29 [6400/8883 (71.94%)]\t\tLoss: 0.92554\n",
      "Training Progress: \tEpoch 29 [6720/8883 (75.54%)]\t\tLoss: 0.87053\n",
      "Training Progress: \tEpoch 29 [7040/8883 (79.14%)]\t\tLoss: 0.90021\n",
      "Training Progress: \tEpoch 29 [7360/8883 (82.73%)]\t\tLoss: 0.87951\n",
      "Training Progress: \tEpoch 29 [7680/8883 (86.33%)]\t\tLoss: 0.93119\n",
      "Training Progress: \tEpoch 29 [8000/8883 (89.93%)]\t\tLoss: 0.77419\n",
      "Training Progress: \tEpoch 29 [8320/8883 (93.53%)]\t\tLoss: 0.90679\n",
      "Training Progress: \tEpoch 29 [8640/8883 (97.12%)]\t\tLoss: 0.94495\n",
      "\tTrain loss: 0.02357, Accuracy: 5893/8883 (66.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1122/1692 (66.00%)\n",
      "\tTest loss: 0.00111, Accuracy: 478/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/8883 (0.00%)]\t\tLoss: 0.86931\n",
      "Training Progress: \tEpoch 30 [320/8883 (3.60%)]\t\tLoss: 1.05915\n",
      "Training Progress: \tEpoch 30 [640/8883 (7.19%)]\t\tLoss: 0.83533\n",
      "Training Progress: \tEpoch 30 [960/8883 (10.79%)]\t\tLoss: 0.91731\n",
      "Training Progress: \tEpoch 30 [1280/8883 (14.39%)]\t\tLoss: 0.97783\n",
      "Training Progress: \tEpoch 30 [1600/8883 (17.99%)]\t\tLoss: 1.00277\n",
      "Training Progress: \tEpoch 30 [1920/8883 (21.58%)]\t\tLoss: 0.80991\n",
      "Training Progress: \tEpoch 30 [2240/8883 (25.18%)]\t\tLoss: 0.78937\n",
      "Training Progress: \tEpoch 30 [2560/8883 (28.78%)]\t\tLoss: 0.91740\n",
      "Training Progress: \tEpoch 30 [2880/8883 (32.37%)]\t\tLoss: 0.95272\n",
      "Training Progress: \tEpoch 30 [3200/8883 (35.97%)]\t\tLoss: 1.03527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 30 [3520/8883 (39.57%)]\t\tLoss: 0.92590\n",
      "Training Progress: \tEpoch 30 [3840/8883 (43.17%)]\t\tLoss: 0.94144\n",
      "Training Progress: \tEpoch 30 [4160/8883 (46.76%)]\t\tLoss: 0.85647\n",
      "Training Progress: \tEpoch 30 [4480/8883 (50.36%)]\t\tLoss: 0.74070\n",
      "Training Progress: \tEpoch 30 [4800/8883 (53.96%)]\t\tLoss: 0.83689\n",
      "Training Progress: \tEpoch 30 [5120/8883 (57.55%)]\t\tLoss: 0.98608\n",
      "Training Progress: \tEpoch 30 [5440/8883 (61.15%)]\t\tLoss: 0.94487\n",
      "Training Progress: \tEpoch 30 [5760/8883 (64.75%)]\t\tLoss: 0.71853\n",
      "Training Progress: \tEpoch 30 [6080/8883 (68.35%)]\t\tLoss: 0.70451\n",
      "Training Progress: \tEpoch 30 [6400/8883 (71.94%)]\t\tLoss: 0.90704\n",
      "Training Progress: \tEpoch 30 [6720/8883 (75.54%)]\t\tLoss: 0.94657\n",
      "Training Progress: \tEpoch 30 [7040/8883 (79.14%)]\t\tLoss: 0.92263\n",
      "Training Progress: \tEpoch 30 [7360/8883 (82.73%)]\t\tLoss: 0.85090\n",
      "Training Progress: \tEpoch 30 [7680/8883 (86.33%)]\t\tLoss: 0.68377\n",
      "Training Progress: \tEpoch 30 [8000/8883 (89.93%)]\t\tLoss: 0.73459\n",
      "Training Progress: \tEpoch 30 [8320/8883 (93.53%)]\t\tLoss: 1.19831\n",
      "Training Progress: \tEpoch 30 [8640/8883 (97.12%)]\t\tLoss: 0.83525\n",
      "\tTrain loss: 0.02341, Accuracy: 5891/8883 (66.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1156/1692 (68.00%)\n",
      "\tTest loss: 0.00121, Accuracy: 485/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/8883 (0.00%)]\t\tLoss: 0.87104\n",
      "Training Progress: \tEpoch 31 [320/8883 (3.60%)]\t\tLoss: 1.00672\n",
      "Training Progress: \tEpoch 31 [640/8883 (7.19%)]\t\tLoss: 0.82107\n",
      "Training Progress: \tEpoch 31 [960/8883 (10.79%)]\t\tLoss: 1.05854\n",
      "Training Progress: \tEpoch 31 [1280/8883 (14.39%)]\t\tLoss: 0.92924\n",
      "Training Progress: \tEpoch 31 [1600/8883 (17.99%)]\t\tLoss: 1.20447\n",
      "Training Progress: \tEpoch 31 [1920/8883 (21.58%)]\t\tLoss: 0.63884\n",
      "Training Progress: \tEpoch 31 [2240/8883 (25.18%)]\t\tLoss: 0.93042\n",
      "Training Progress: \tEpoch 31 [2560/8883 (28.78%)]\t\tLoss: 0.96572\n",
      "Training Progress: \tEpoch 31 [2880/8883 (32.37%)]\t\tLoss: 0.94661\n",
      "Training Progress: \tEpoch 31 [3200/8883 (35.97%)]\t\tLoss: 0.80083\n",
      "Training Progress: \tEpoch 31 [3520/8883 (39.57%)]\t\tLoss: 1.12348\n",
      "Training Progress: \tEpoch 31 [3840/8883 (43.17%)]\t\tLoss: 0.83939\n",
      "Training Progress: \tEpoch 31 [4160/8883 (46.76%)]\t\tLoss: 0.77598\n",
      "Training Progress: \tEpoch 31 [4480/8883 (50.36%)]\t\tLoss: 0.75086\n",
      "Training Progress: \tEpoch 31 [4800/8883 (53.96%)]\t\tLoss: 0.71648\n",
      "Training Progress: \tEpoch 31 [5120/8883 (57.55%)]\t\tLoss: 0.84255\n",
      "Training Progress: \tEpoch 31 [5440/8883 (61.15%)]\t\tLoss: 0.87377\n",
      "Training Progress: \tEpoch 31 [5760/8883 (64.75%)]\t\tLoss: 0.83757\n",
      "Training Progress: \tEpoch 31 [6080/8883 (68.35%)]\t\tLoss: 1.02067\n",
      "Training Progress: \tEpoch 31 [6400/8883 (71.94%)]\t\tLoss: 0.94755\n",
      "Training Progress: \tEpoch 31 [6720/8883 (75.54%)]\t\tLoss: 0.88258\n",
      "Training Progress: \tEpoch 31 [7040/8883 (79.14%)]\t\tLoss: 0.86708\n",
      "Training Progress: \tEpoch 31 [7360/8883 (82.73%)]\t\tLoss: 0.76559\n",
      "Training Progress: \tEpoch 31 [7680/8883 (86.33%)]\t\tLoss: 0.75738\n",
      "Training Progress: \tEpoch 31 [8000/8883 (89.93%)]\t\tLoss: 0.65948\n",
      "Training Progress: \tEpoch 31 [8320/8883 (93.53%)]\t\tLoss: 1.02982\n",
      "Training Progress: \tEpoch 31 [8640/8883 (97.12%)]\t\tLoss: 0.98937\n",
      "\tTrain loss: 0.02191, Accuracy: 6115/8883 (68.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1187/1692 (70.00%)\n",
      "\tTest loss: 0.00112, Accuracy: 539/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/8883 (0.00%)]\t\tLoss: 0.70272\n",
      "Training Progress: \tEpoch 32 [320/8883 (3.60%)]\t\tLoss: 0.88453\n",
      "Training Progress: \tEpoch 32 [640/8883 (7.19%)]\t\tLoss: 0.87027\n",
      "Training Progress: \tEpoch 32 [960/8883 (10.79%)]\t\tLoss: 0.81328\n",
      "Training Progress: \tEpoch 32 [1280/8883 (14.39%)]\t\tLoss: 0.91915\n",
      "Training Progress: \tEpoch 32 [1600/8883 (17.99%)]\t\tLoss: 0.92151\n",
      "Training Progress: \tEpoch 32 [1920/8883 (21.58%)]\t\tLoss: 0.86155\n",
      "Training Progress: \tEpoch 32 [2240/8883 (25.18%)]\t\tLoss: 0.79666\n",
      "Training Progress: \tEpoch 32 [2560/8883 (28.78%)]\t\tLoss: 1.09304\n",
      "Training Progress: \tEpoch 32 [2880/8883 (32.37%)]\t\tLoss: 1.05796\n",
      "Training Progress: \tEpoch 32 [3200/8883 (35.97%)]\t\tLoss: 0.77822\n",
      "Training Progress: \tEpoch 32 [3520/8883 (39.57%)]\t\tLoss: 0.85908\n",
      "Training Progress: \tEpoch 32 [3840/8883 (43.17%)]\t\tLoss: 0.90575\n",
      "Training Progress: \tEpoch 32 [4160/8883 (46.76%)]\t\tLoss: 0.73497\n",
      "Training Progress: \tEpoch 32 [4480/8883 (50.36%)]\t\tLoss: 0.61699\n",
      "Training Progress: \tEpoch 32 [4800/8883 (53.96%)]\t\tLoss: 0.52291\n",
      "Training Progress: \tEpoch 32 [5120/8883 (57.55%)]\t\tLoss: 0.92664\n",
      "Training Progress: \tEpoch 32 [5440/8883 (61.15%)]\t\tLoss: 0.83974\n",
      "Training Progress: \tEpoch 32 [5760/8883 (64.75%)]\t\tLoss: 0.73410\n",
      "Training Progress: \tEpoch 32 [6080/8883 (68.35%)]\t\tLoss: 1.06766\n",
      "Training Progress: \tEpoch 32 [6400/8883 (71.94%)]\t\tLoss: 0.84973\n",
      "Training Progress: \tEpoch 32 [6720/8883 (75.54%)]\t\tLoss: 0.86461\n",
      "Training Progress: \tEpoch 32 [7040/8883 (79.14%)]\t\tLoss: 0.89252\n",
      "Training Progress: \tEpoch 32 [7360/8883 (82.73%)]\t\tLoss: 0.87987\n",
      "Training Progress: \tEpoch 32 [7680/8883 (86.33%)]\t\tLoss: 0.72921\n",
      "Training Progress: \tEpoch 32 [8000/8883 (89.93%)]\t\tLoss: 0.88979\n",
      "Training Progress: \tEpoch 32 [8320/8883 (93.53%)]\t\tLoss: 0.98678\n",
      "Training Progress: \tEpoch 32 [8640/8883 (97.12%)]\t\tLoss: 0.97964\n",
      "\tTrain loss: 0.02163, Accuracy: 6074/8883 (68.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1178/1692 (69.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/8883 (0.00%)]\t\tLoss: 0.79072\n",
      "Training Progress: \tEpoch 33 [320/8883 (3.60%)]\t\tLoss: 0.85045\n",
      "Training Progress: \tEpoch 33 [640/8883 (7.19%)]\t\tLoss: 0.80137\n",
      "Training Progress: \tEpoch 33 [960/8883 (10.79%)]\t\tLoss: 0.88844\n",
      "Training Progress: \tEpoch 33 [1280/8883 (14.39%)]\t\tLoss: 0.80798\n",
      "Training Progress: \tEpoch 33 [1600/8883 (17.99%)]\t\tLoss: 1.05369\n",
      "Training Progress: \tEpoch 33 [1920/8883 (21.58%)]\t\tLoss: 0.66606\n",
      "Training Progress: \tEpoch 33 [2240/8883 (25.18%)]\t\tLoss: 0.76890\n",
      "Training Progress: \tEpoch 33 [2560/8883 (28.78%)]\t\tLoss: 1.06043\n",
      "Training Progress: \tEpoch 33 [2880/8883 (32.37%)]\t\tLoss: 0.96373\n",
      "Training Progress: \tEpoch 33 [3200/8883 (35.97%)]\t\tLoss: 0.95817\n",
      "Training Progress: \tEpoch 33 [3520/8883 (39.57%)]\t\tLoss: 0.94884\n",
      "Training Progress: \tEpoch 33 [3840/8883 (43.17%)]\t\tLoss: 0.90443\n",
      "Training Progress: \tEpoch 33 [4160/8883 (46.76%)]\t\tLoss: 0.65610\n",
      "Training Progress: \tEpoch 33 [4480/8883 (50.36%)]\t\tLoss: 0.70898\n",
      "Training Progress: \tEpoch 33 [4800/8883 (53.96%)]\t\tLoss: 0.60515\n",
      "Training Progress: \tEpoch 33 [5120/8883 (57.55%)]\t\tLoss: 0.81759\n",
      "Training Progress: \tEpoch 33 [5440/8883 (61.15%)]\t\tLoss: 0.63808\n",
      "Training Progress: \tEpoch 33 [5760/8883 (64.75%)]\t\tLoss: 0.69845\n",
      "Training Progress: \tEpoch 33 [6080/8883 (68.35%)]\t\tLoss: 0.71599\n",
      "Training Progress: \tEpoch 33 [6400/8883 (71.94%)]\t\tLoss: 0.82915\n",
      "Training Progress: \tEpoch 33 [6720/8883 (75.54%)]\t\tLoss: 0.80423\n",
      "Training Progress: \tEpoch 33 [7040/8883 (79.14%)]\t\tLoss: 0.85106\n",
      "Training Progress: \tEpoch 33 [7360/8883 (82.73%)]\t\tLoss: 0.76595\n",
      "Training Progress: \tEpoch 33 [7680/8883 (86.33%)]\t\tLoss: 0.79355\n",
      "Training Progress: \tEpoch 33 [8000/8883 (89.93%)]\t\tLoss: 0.93422\n",
      "Training Progress: \tEpoch 33 [8320/8883 (93.53%)]\t\tLoss: 0.93842\n",
      "Training Progress: \tEpoch 33 [8640/8883 (97.12%)]\t\tLoss: 0.79055\n",
      "\tTrain loss: 0.02145, Accuracy: 6142/8883 (69.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1192/1692 (70.00%)\n",
      "\tTest loss: 0.00116, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/8883 (0.00%)]\t\tLoss: 0.66106\n",
      "Training Progress: \tEpoch 34 [320/8883 (3.60%)]\t\tLoss: 0.78748\n",
      "Training Progress: \tEpoch 34 [640/8883 (7.19%)]\t\tLoss: 0.73648\n",
      "Training Progress: \tEpoch 34 [960/8883 (10.79%)]\t\tLoss: 0.72141\n",
      "Training Progress: \tEpoch 34 [1280/8883 (14.39%)]\t\tLoss: 0.98323\n",
      "Training Progress: \tEpoch 34 [1600/8883 (17.99%)]\t\tLoss: 1.12724\n",
      "Training Progress: \tEpoch 34 [1920/8883 (21.58%)]\t\tLoss: 0.63906\n",
      "Training Progress: \tEpoch 34 [2240/8883 (25.18%)]\t\tLoss: 0.83609\n",
      "Training Progress: \tEpoch 34 [2560/8883 (28.78%)]\t\tLoss: 1.01789\n",
      "Training Progress: \tEpoch 34 [2880/8883 (32.37%)]\t\tLoss: 1.07821\n",
      "Training Progress: \tEpoch 34 [3200/8883 (35.97%)]\t\tLoss: 0.74811\n",
      "Training Progress: \tEpoch 34 [3520/8883 (39.57%)]\t\tLoss: 0.91159\n",
      "Training Progress: \tEpoch 34 [3840/8883 (43.17%)]\t\tLoss: 1.16495\n",
      "Training Progress: \tEpoch 34 [4160/8883 (46.76%)]\t\tLoss: 0.80499\n",
      "Training Progress: \tEpoch 34 [4480/8883 (50.36%)]\t\tLoss: 0.52356\n",
      "Training Progress: \tEpoch 34 [4800/8883 (53.96%)]\t\tLoss: 0.76039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 34 [5120/8883 (57.55%)]\t\tLoss: 0.75100\n",
      "Training Progress: \tEpoch 34 [5440/8883 (61.15%)]\t\tLoss: 0.72243\n",
      "Training Progress: \tEpoch 34 [5760/8883 (64.75%)]\t\tLoss: 0.58396\n",
      "Training Progress: \tEpoch 34 [6080/8883 (68.35%)]\t\tLoss: 0.75127\n",
      "Training Progress: \tEpoch 34 [6400/8883 (71.94%)]\t\tLoss: 0.78804\n",
      "Training Progress: \tEpoch 34 [6720/8883 (75.54%)]\t\tLoss: 1.00033\n",
      "Training Progress: \tEpoch 34 [7040/8883 (79.14%)]\t\tLoss: 0.81268\n",
      "Training Progress: \tEpoch 34 [7360/8883 (82.73%)]\t\tLoss: 0.85481\n",
      "Training Progress: \tEpoch 34 [7680/8883 (86.33%)]\t\tLoss: 0.75942\n",
      "Training Progress: \tEpoch 34 [8000/8883 (89.93%)]\t\tLoss: 0.66129\n",
      "Training Progress: \tEpoch 34 [8320/8883 (93.53%)]\t\tLoss: 0.92035\n",
      "Training Progress: \tEpoch 34 [8640/8883 (97.12%)]\t\tLoss: 0.92081\n",
      "\tTrain loss: 0.02107, Accuracy: 6153/8883 (69.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1191/1692 (70.00%)\n",
      "\tTest loss: 0.00127, Accuracy: 510/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/8883 (0.00%)]\t\tLoss: 0.85510\n",
      "Training Progress: \tEpoch 35 [320/8883 (3.60%)]\t\tLoss: 0.84116\n",
      "Training Progress: \tEpoch 35 [640/8883 (7.19%)]\t\tLoss: 0.70203\n",
      "Training Progress: \tEpoch 35 [960/8883 (10.79%)]\t\tLoss: 0.61363\n",
      "Training Progress: \tEpoch 35 [1280/8883 (14.39%)]\t\tLoss: 1.14918\n",
      "Training Progress: \tEpoch 35 [1600/8883 (17.99%)]\t\tLoss: 0.97238\n",
      "Training Progress: \tEpoch 35 [1920/8883 (21.58%)]\t\tLoss: 0.82480\n",
      "Training Progress: \tEpoch 35 [2240/8883 (25.18%)]\t\tLoss: 0.82407\n",
      "Training Progress: \tEpoch 35 [2560/8883 (28.78%)]\t\tLoss: 0.81322\n",
      "Training Progress: \tEpoch 35 [2880/8883 (32.37%)]\t\tLoss: 0.71355\n",
      "Training Progress: \tEpoch 35 [3200/8883 (35.97%)]\t\tLoss: 0.70830\n",
      "Training Progress: \tEpoch 35 [3520/8883 (39.57%)]\t\tLoss: 0.79220\n",
      "Training Progress: \tEpoch 35 [3840/8883 (43.17%)]\t\tLoss: 0.88587\n",
      "Training Progress: \tEpoch 35 [4160/8883 (46.76%)]\t\tLoss: 0.69900\n",
      "Training Progress: \tEpoch 35 [4480/8883 (50.36%)]\t\tLoss: 0.70457\n",
      "Training Progress: \tEpoch 35 [4800/8883 (53.96%)]\t\tLoss: 0.64284\n",
      "Training Progress: \tEpoch 35 [5120/8883 (57.55%)]\t\tLoss: 0.86431\n",
      "Training Progress: \tEpoch 35 [5440/8883 (61.15%)]\t\tLoss: 0.80357\n",
      "Training Progress: \tEpoch 35 [5760/8883 (64.75%)]\t\tLoss: 0.70214\n",
      "Training Progress: \tEpoch 35 [6080/8883 (68.35%)]\t\tLoss: 0.76921\n",
      "Training Progress: \tEpoch 35 [6400/8883 (71.94%)]\t\tLoss: 0.65066\n",
      "Training Progress: \tEpoch 35 [6720/8883 (75.54%)]\t\tLoss: 0.76348\n",
      "Training Progress: \tEpoch 35 [7040/8883 (79.14%)]\t\tLoss: 0.95366\n",
      "Training Progress: \tEpoch 35 [7360/8883 (82.73%)]\t\tLoss: 0.75242\n",
      "Training Progress: \tEpoch 35 [7680/8883 (86.33%)]\t\tLoss: 0.68287\n",
      "Training Progress: \tEpoch 35 [8000/8883 (89.93%)]\t\tLoss: 0.65807\n",
      "Training Progress: \tEpoch 35 [8320/8883 (93.53%)]\t\tLoss: 0.87381\n",
      "Training Progress: \tEpoch 35 [8640/8883 (97.12%)]\t\tLoss: 0.86255\n",
      "\tTrain loss: 0.02122, Accuracy: 6098/8883 (68.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00129, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/8883 (0.00%)]\t\tLoss: 0.73194\n",
      "Training Progress: \tEpoch 36 [320/8883 (3.60%)]\t\tLoss: 0.68483\n",
      "Training Progress: \tEpoch 36 [640/8883 (7.19%)]\t\tLoss: 0.67241\n",
      "Training Progress: \tEpoch 36 [960/8883 (10.79%)]\t\tLoss: 0.69541\n",
      "Training Progress: \tEpoch 36 [1280/8883 (14.39%)]\t\tLoss: 0.84390\n",
      "Training Progress: \tEpoch 36 [1600/8883 (17.99%)]\t\tLoss: 0.86122\n",
      "Training Progress: \tEpoch 36 [1920/8883 (21.58%)]\t\tLoss: 0.76850\n",
      "Training Progress: \tEpoch 36 [2240/8883 (25.18%)]\t\tLoss: 0.81659\n",
      "Training Progress: \tEpoch 36 [2560/8883 (28.78%)]\t\tLoss: 0.98581\n",
      "Training Progress: \tEpoch 36 [2880/8883 (32.37%)]\t\tLoss: 1.04782\n",
      "Training Progress: \tEpoch 36 [3200/8883 (35.97%)]\t\tLoss: 0.79337\n",
      "Training Progress: \tEpoch 36 [3520/8883 (39.57%)]\t\tLoss: 0.85495\n",
      "Training Progress: \tEpoch 36 [3840/8883 (43.17%)]\t\tLoss: 0.92272\n",
      "Training Progress: \tEpoch 36 [4160/8883 (46.76%)]\t\tLoss: 0.85872\n",
      "Training Progress: \tEpoch 36 [4480/8883 (50.36%)]\t\tLoss: 0.67357\n",
      "Training Progress: \tEpoch 36 [4800/8883 (53.96%)]\t\tLoss: 0.60641\n",
      "Training Progress: \tEpoch 36 [5120/8883 (57.55%)]\t\tLoss: 0.73490\n",
      "Training Progress: \tEpoch 36 [5440/8883 (61.15%)]\t\tLoss: 0.82050\n",
      "Training Progress: \tEpoch 36 [5760/8883 (64.75%)]\t\tLoss: 0.71780\n",
      "Training Progress: \tEpoch 36 [6080/8883 (68.35%)]\t\tLoss: 0.58540\n",
      "Training Progress: \tEpoch 36 [6400/8883 (71.94%)]\t\tLoss: 0.73590\n",
      "Training Progress: \tEpoch 36 [6720/8883 (75.54%)]\t\tLoss: 0.81787\n",
      "Training Progress: \tEpoch 36 [7040/8883 (79.14%)]\t\tLoss: 0.88225\n",
      "Training Progress: \tEpoch 36 [7360/8883 (82.73%)]\t\tLoss: 0.69614\n",
      "Training Progress: \tEpoch 36 [7680/8883 (86.33%)]\t\tLoss: 0.59406\n",
      "Training Progress: \tEpoch 36 [8000/8883 (89.93%)]\t\tLoss: 0.59157\n",
      "Training Progress: \tEpoch 36 [8320/8883 (93.53%)]\t\tLoss: 1.11537\n",
      "Training Progress: \tEpoch 36 [8640/8883 (97.12%)]\t\tLoss: 0.81279\n",
      "\tTrain loss: 0.02023, Accuracy: 6308/8883 (71.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00126, Accuracy: 497/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/8883 (0.00%)]\t\tLoss: 0.71467\n",
      "Training Progress: \tEpoch 37 [320/8883 (3.60%)]\t\tLoss: 0.74964\n",
      "Training Progress: \tEpoch 37 [640/8883 (7.19%)]\t\tLoss: 0.72884\n",
      "Training Progress: \tEpoch 37 [960/8883 (10.79%)]\t\tLoss: 0.60432\n",
      "Training Progress: \tEpoch 37 [1280/8883 (14.39%)]\t\tLoss: 0.82418\n",
      "Training Progress: \tEpoch 37 [1600/8883 (17.99%)]\t\tLoss: 0.81254\n",
      "Training Progress: \tEpoch 37 [1920/8883 (21.58%)]\t\tLoss: 0.63303\n",
      "Training Progress: \tEpoch 37 [2240/8883 (25.18%)]\t\tLoss: 0.74347\n",
      "Training Progress: \tEpoch 37 [2560/8883 (28.78%)]\t\tLoss: 0.96732\n",
      "Training Progress: \tEpoch 37 [2880/8883 (32.37%)]\t\tLoss: 0.86573\n",
      "Training Progress: \tEpoch 37 [3200/8883 (35.97%)]\t\tLoss: 0.77342\n",
      "Training Progress: \tEpoch 37 [3520/8883 (39.57%)]\t\tLoss: 0.81363\n",
      "Training Progress: \tEpoch 37 [3840/8883 (43.17%)]\t\tLoss: 0.95940\n",
      "Training Progress: \tEpoch 37 [4160/8883 (46.76%)]\t\tLoss: 0.76536\n",
      "Training Progress: \tEpoch 37 [4480/8883 (50.36%)]\t\tLoss: 0.64671\n",
      "Training Progress: \tEpoch 37 [4800/8883 (53.96%)]\t\tLoss: 0.61087\n",
      "Training Progress: \tEpoch 37 [5120/8883 (57.55%)]\t\tLoss: 0.67255\n",
      "Training Progress: \tEpoch 37 [5440/8883 (61.15%)]\t\tLoss: 0.68751\n",
      "Training Progress: \tEpoch 37 [5760/8883 (64.75%)]\t\tLoss: 0.84526\n",
      "Training Progress: \tEpoch 37 [6080/8883 (68.35%)]\t\tLoss: 0.73895\n",
      "Training Progress: \tEpoch 37 [6400/8883 (71.94%)]\t\tLoss: 0.74683\n",
      "Training Progress: \tEpoch 37 [6720/8883 (75.54%)]\t\tLoss: 0.75412\n",
      "Training Progress: \tEpoch 37 [7040/8883 (79.14%)]\t\tLoss: 0.81476\n",
      "Training Progress: \tEpoch 37 [7360/8883 (82.73%)]\t\tLoss: 0.99320\n",
      "Training Progress: \tEpoch 37 [7680/8883 (86.33%)]\t\tLoss: 0.62632\n",
      "Training Progress: \tEpoch 37 [8000/8883 (89.93%)]\t\tLoss: 0.76100\n",
      "Training Progress: \tEpoch 37 [8320/8883 (93.53%)]\t\tLoss: 0.92850\n",
      "Training Progress: \tEpoch 37 [8640/8883 (97.12%)]\t\tLoss: 0.78863\n",
      "\tTrain loss: 0.01985, Accuracy: 6321/8883 (71.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1220/1692 (72.00%)\n",
      "\tTest loss: 0.00124, Accuracy: 520/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/8883 (0.00%)]\t\tLoss: 0.70965\n",
      "Training Progress: \tEpoch 38 [320/8883 (3.60%)]\t\tLoss: 0.84779\n",
      "Training Progress: \tEpoch 38 [640/8883 (7.19%)]\t\tLoss: 0.87280\n",
      "Training Progress: \tEpoch 38 [960/8883 (10.79%)]\t\tLoss: 0.60477\n",
      "Training Progress: \tEpoch 38 [1280/8883 (14.39%)]\t\tLoss: 0.85285\n",
      "Training Progress: \tEpoch 38 [1600/8883 (17.99%)]\t\tLoss: 0.69034\n",
      "Training Progress: \tEpoch 38 [1920/8883 (21.58%)]\t\tLoss: 0.76455\n",
      "Training Progress: \tEpoch 38 [2240/8883 (25.18%)]\t\tLoss: 0.73281\n",
      "Training Progress: \tEpoch 38 [2560/8883 (28.78%)]\t\tLoss: 0.97724\n",
      "Training Progress: \tEpoch 38 [2880/8883 (32.37%)]\t\tLoss: 0.75551\n",
      "Training Progress: \tEpoch 38 [3200/8883 (35.97%)]\t\tLoss: 0.79182\n",
      "Training Progress: \tEpoch 38 [3520/8883 (39.57%)]\t\tLoss: 0.94623\n",
      "Training Progress: \tEpoch 38 [3840/8883 (43.17%)]\t\tLoss: 0.97676\n",
      "Training Progress: \tEpoch 38 [4160/8883 (46.76%)]\t\tLoss: 0.55539\n",
      "Training Progress: \tEpoch 38 [4480/8883 (50.36%)]\t\tLoss: 0.72728\n",
      "Training Progress: \tEpoch 38 [4800/8883 (53.96%)]\t\tLoss: 0.68704\n",
      "Training Progress: \tEpoch 38 [5120/8883 (57.55%)]\t\tLoss: 0.65799\n",
      "Training Progress: \tEpoch 38 [5440/8883 (61.15%)]\t\tLoss: 0.84851\n",
      "Training Progress: \tEpoch 38 [5760/8883 (64.75%)]\t\tLoss: 0.56556\n",
      "Training Progress: \tEpoch 38 [6080/8883 (68.35%)]\t\tLoss: 0.56089\n",
      "Training Progress: \tEpoch 38 [6400/8883 (71.94%)]\t\tLoss: 0.84173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 38 [6720/8883 (75.54%)]\t\tLoss: 0.81697\n",
      "Training Progress: \tEpoch 38 [7040/8883 (79.14%)]\t\tLoss: 0.99258\n",
      "Training Progress: \tEpoch 38 [7360/8883 (82.73%)]\t\tLoss: 0.86498\n",
      "Training Progress: \tEpoch 38 [7680/8883 (86.33%)]\t\tLoss: 0.70790\n",
      "Training Progress: \tEpoch 38 [8000/8883 (89.93%)]\t\tLoss: 0.69426\n",
      "Training Progress: \tEpoch 38 [8320/8883 (93.53%)]\t\tLoss: 1.07307\n",
      "Training Progress: \tEpoch 38 [8640/8883 (97.12%)]\t\tLoss: 0.91162\n",
      "\tTrain loss: 0.01929, Accuracy: 6409/8883 (72.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1271/1692 (75.00%)\n",
      "\tTest loss: 0.00127, Accuracy: 529/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/8883 (0.00%)]\t\tLoss: 0.78442\n",
      "Training Progress: \tEpoch 39 [320/8883 (3.60%)]\t\tLoss: 0.64980\n",
      "Training Progress: \tEpoch 39 [640/8883 (7.19%)]\t\tLoss: 0.73845\n",
      "Training Progress: \tEpoch 39 [960/8883 (10.79%)]\t\tLoss: 0.57182\n",
      "Training Progress: \tEpoch 39 [1280/8883 (14.39%)]\t\tLoss: 0.95065\n",
      "Training Progress: \tEpoch 39 [1600/8883 (17.99%)]\t\tLoss: 1.02742\n",
      "Training Progress: \tEpoch 39 [1920/8883 (21.58%)]\t\tLoss: 0.53708\n",
      "Training Progress: \tEpoch 39 [2240/8883 (25.18%)]\t\tLoss: 0.93570\n",
      "Training Progress: \tEpoch 39 [2560/8883 (28.78%)]\t\tLoss: 0.94638\n",
      "Training Progress: \tEpoch 39 [2880/8883 (32.37%)]\t\tLoss: 1.00606\n",
      "Training Progress: \tEpoch 39 [3200/8883 (35.97%)]\t\tLoss: 0.62423\n",
      "Training Progress: \tEpoch 39 [3520/8883 (39.57%)]\t\tLoss: 0.80812\n",
      "Training Progress: \tEpoch 39 [3840/8883 (43.17%)]\t\tLoss: 1.06795\n",
      "Training Progress: \tEpoch 39 [4160/8883 (46.76%)]\t\tLoss: 0.59143\n",
      "Training Progress: \tEpoch 39 [4480/8883 (50.36%)]\t\tLoss: 0.58420\n",
      "Training Progress: \tEpoch 39 [4800/8883 (53.96%)]\t\tLoss: 0.57998\n",
      "Training Progress: \tEpoch 39 [5120/8883 (57.55%)]\t\tLoss: 0.89679\n",
      "Training Progress: \tEpoch 39 [5440/8883 (61.15%)]\t\tLoss: 0.60571\n",
      "Training Progress: \tEpoch 39 [5760/8883 (64.75%)]\t\tLoss: 0.57046\n",
      "Training Progress: \tEpoch 39 [6080/8883 (68.35%)]\t\tLoss: 0.74816\n",
      "Training Progress: \tEpoch 39 [6400/8883 (71.94%)]\t\tLoss: 0.79111\n",
      "Training Progress: \tEpoch 39 [6720/8883 (75.54%)]\t\tLoss: 0.62503\n",
      "Training Progress: \tEpoch 39 [7040/8883 (79.14%)]\t\tLoss: 0.79501\n",
      "Training Progress: \tEpoch 39 [7360/8883 (82.73%)]\t\tLoss: 0.72197\n",
      "Training Progress: \tEpoch 39 [7680/8883 (86.33%)]\t\tLoss: 0.59390\n",
      "Training Progress: \tEpoch 39 [8000/8883 (89.93%)]\t\tLoss: 0.62334\n",
      "Training Progress: \tEpoch 39 [8320/8883 (93.53%)]\t\tLoss: 0.73562\n",
      "Training Progress: \tEpoch 39 [8640/8883 (97.12%)]\t\tLoss: 0.74587\n",
      "\tTrain loss: 0.01881, Accuracy: 6419/8883 (72.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1260/1692 (74.00%)\n",
      "\tTest loss: 0.00133, Accuracy: 492/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/8883 (0.00%)]\t\tLoss: 0.66970\n",
      "Training Progress: \tEpoch 40 [320/8883 (3.60%)]\t\tLoss: 0.84125\n",
      "Training Progress: \tEpoch 40 [640/8883 (7.19%)]\t\tLoss: 0.66230\n",
      "Training Progress: \tEpoch 40 [960/8883 (10.79%)]\t\tLoss: 0.71447\n",
      "Training Progress: \tEpoch 40 [1280/8883 (14.39%)]\t\tLoss: 0.95487\n",
      "Training Progress: \tEpoch 40 [1600/8883 (17.99%)]\t\tLoss: 0.79881\n",
      "Training Progress: \tEpoch 40 [1920/8883 (21.58%)]\t\tLoss: 0.49841\n",
      "Training Progress: \tEpoch 40 [2240/8883 (25.18%)]\t\tLoss: 0.79227\n",
      "Training Progress: \tEpoch 40 [2560/8883 (28.78%)]\t\tLoss: 0.78649\n",
      "Training Progress: \tEpoch 40 [2880/8883 (32.37%)]\t\tLoss: 0.77065\n",
      "Training Progress: \tEpoch 40 [3200/8883 (35.97%)]\t\tLoss: 0.94152\n",
      "Training Progress: \tEpoch 40 [3520/8883 (39.57%)]\t\tLoss: 0.71246\n",
      "Training Progress: \tEpoch 40 [3840/8883 (43.17%)]\t\tLoss: 0.98966\n",
      "Training Progress: \tEpoch 40 [4160/8883 (46.76%)]\t\tLoss: 0.53265\n",
      "Training Progress: \tEpoch 40 [4480/8883 (50.36%)]\t\tLoss: 0.65062\n",
      "Training Progress: \tEpoch 40 [4800/8883 (53.96%)]\t\tLoss: 0.42361\n",
      "Training Progress: \tEpoch 40 [5120/8883 (57.55%)]\t\tLoss: 0.68082\n",
      "Training Progress: \tEpoch 40 [5440/8883 (61.15%)]\t\tLoss: 0.69580\n",
      "Training Progress: \tEpoch 40 [5760/8883 (64.75%)]\t\tLoss: 0.58140\n",
      "Training Progress: \tEpoch 40 [6080/8883 (68.35%)]\t\tLoss: 0.82260\n",
      "Training Progress: \tEpoch 40 [6400/8883 (71.94%)]\t\tLoss: 0.50605\n",
      "Training Progress: \tEpoch 40 [6720/8883 (75.54%)]\t\tLoss: 0.76232\n",
      "Training Progress: \tEpoch 40 [7040/8883 (79.14%)]\t\tLoss: 0.78942\n",
      "Training Progress: \tEpoch 40 [7360/8883 (82.73%)]\t\tLoss: 0.64939\n",
      "Training Progress: \tEpoch 40 [7680/8883 (86.33%)]\t\tLoss: 0.76245\n",
      "Training Progress: \tEpoch 40 [8000/8883 (89.93%)]\t\tLoss: 0.60384\n",
      "Training Progress: \tEpoch 40 [8320/8883 (93.53%)]\t\tLoss: 0.88222\n",
      "Training Progress: \tEpoch 40 [8640/8883 (97.12%)]\t\tLoss: 0.82116\n",
      "\tTrain loss: 0.01851, Accuracy: 6470/8883 (72.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1272/1692 (75.00%)\n",
      "\tTest loss: 0.00133, Accuracy: 530/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/8883 (0.00%)]\t\tLoss: 0.69145\n",
      "Training Progress: \tEpoch 41 [320/8883 (3.60%)]\t\tLoss: 0.81290\n",
      "Training Progress: \tEpoch 41 [640/8883 (7.19%)]\t\tLoss: 0.71119\n",
      "Training Progress: \tEpoch 41 [960/8883 (10.79%)]\t\tLoss: 0.78818\n",
      "Training Progress: \tEpoch 41 [1280/8883 (14.39%)]\t\tLoss: 0.88391\n",
      "Training Progress: \tEpoch 41 [1600/8883 (17.99%)]\t\tLoss: 0.67554\n",
      "Training Progress: \tEpoch 41 [1920/8883 (21.58%)]\t\tLoss: 0.70711\n",
      "Training Progress: \tEpoch 41 [2240/8883 (25.18%)]\t\tLoss: 0.61631\n",
      "Training Progress: \tEpoch 41 [2560/8883 (28.78%)]\t\tLoss: 0.91342\n",
      "Training Progress: \tEpoch 41 [2880/8883 (32.37%)]\t\tLoss: 0.68822\n",
      "Training Progress: \tEpoch 41 [3200/8883 (35.97%)]\t\tLoss: 0.76518\n",
      "Training Progress: \tEpoch 41 [3520/8883 (39.57%)]\t\tLoss: 0.90067\n",
      "Training Progress: \tEpoch 41 [3840/8883 (43.17%)]\t\tLoss: 0.79965\n",
      "Training Progress: \tEpoch 41 [4160/8883 (46.76%)]\t\tLoss: 0.56291\n",
      "Training Progress: \tEpoch 41 [4480/8883 (50.36%)]\t\tLoss: 0.66392\n",
      "Training Progress: \tEpoch 41 [4800/8883 (53.96%)]\t\tLoss: 0.45525\n",
      "Training Progress: \tEpoch 41 [5120/8883 (57.55%)]\t\tLoss: 0.70633\n",
      "Training Progress: \tEpoch 41 [5440/8883 (61.15%)]\t\tLoss: 0.60287\n",
      "Training Progress: \tEpoch 41 [5760/8883 (64.75%)]\t\tLoss: 0.62104\n",
      "Training Progress: \tEpoch 41 [6080/8883 (68.35%)]\t\tLoss: 0.68783\n",
      "Training Progress: \tEpoch 41 [6400/8883 (71.94%)]\t\tLoss: 0.52849\n",
      "Training Progress: \tEpoch 41 [6720/8883 (75.54%)]\t\tLoss: 0.64371\n",
      "Training Progress: \tEpoch 41 [7040/8883 (79.14%)]\t\tLoss: 0.79388\n",
      "Training Progress: \tEpoch 41 [7360/8883 (82.73%)]\t\tLoss: 0.70771\n",
      "Training Progress: \tEpoch 41 [7680/8883 (86.33%)]\t\tLoss: 0.65683\n",
      "Training Progress: \tEpoch 41 [8000/8883 (89.93%)]\t\tLoss: 0.52328\n",
      "Training Progress: \tEpoch 41 [8320/8883 (93.53%)]\t\tLoss: 0.98283\n",
      "Training Progress: \tEpoch 41 [8640/8883 (97.12%)]\t\tLoss: 0.71015\n",
      "\tTrain loss: 0.01760, Accuracy: 6588/8883 (74.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1335/1692 (78.00%)\n",
      "\tTest loss: 0.00134, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/8883 (0.00%)]\t\tLoss: 0.65105\n",
      "Training Progress: \tEpoch 42 [320/8883 (3.60%)]\t\tLoss: 0.57415\n",
      "Training Progress: \tEpoch 42 [640/8883 (7.19%)]\t\tLoss: 0.54648\n",
      "Training Progress: \tEpoch 42 [960/8883 (10.79%)]\t\tLoss: 0.67530\n",
      "Training Progress: \tEpoch 42 [1280/8883 (14.39%)]\t\tLoss: 0.80411\n",
      "Training Progress: \tEpoch 42 [1600/8883 (17.99%)]\t\tLoss: 0.78040\n",
      "Training Progress: \tEpoch 42 [1920/8883 (21.58%)]\t\tLoss: 0.55332\n",
      "Training Progress: \tEpoch 42 [2240/8883 (25.18%)]\t\tLoss: 0.88712\n",
      "Training Progress: \tEpoch 42 [2560/8883 (28.78%)]\t\tLoss: 0.95558\n",
      "Training Progress: \tEpoch 42 [2880/8883 (32.37%)]\t\tLoss: 0.77638\n",
      "Training Progress: \tEpoch 42 [3200/8883 (35.97%)]\t\tLoss: 0.63490\n",
      "Training Progress: \tEpoch 42 [3520/8883 (39.57%)]\t\tLoss: 0.78417\n",
      "Training Progress: \tEpoch 42 [3840/8883 (43.17%)]\t\tLoss: 0.76615\n",
      "Training Progress: \tEpoch 42 [4160/8883 (46.76%)]\t\tLoss: 0.56213\n",
      "Training Progress: \tEpoch 42 [4480/8883 (50.36%)]\t\tLoss: 0.66654\n",
      "Training Progress: \tEpoch 42 [4800/8883 (53.96%)]\t\tLoss: 0.54957\n",
      "Training Progress: \tEpoch 42 [5120/8883 (57.55%)]\t\tLoss: 0.63908\n",
      "Training Progress: \tEpoch 42 [5440/8883 (61.15%)]\t\tLoss: 0.64084\n",
      "Training Progress: \tEpoch 42 [5760/8883 (64.75%)]\t\tLoss: 0.61356\n",
      "Training Progress: \tEpoch 42 [6080/8883 (68.35%)]\t\tLoss: 0.56699\n",
      "Training Progress: \tEpoch 42 [6400/8883 (71.94%)]\t\tLoss: 0.63260\n",
      "Training Progress: \tEpoch 42 [6720/8883 (75.54%)]\t\tLoss: 0.82137\n",
      "Training Progress: \tEpoch 42 [7040/8883 (79.14%)]\t\tLoss: 0.87892\n",
      "Training Progress: \tEpoch 42 [7360/8883 (82.73%)]\t\tLoss: 0.66915\n",
      "Training Progress: \tEpoch 42 [7680/8883 (86.33%)]\t\tLoss: 0.64834\n",
      "Training Progress: \tEpoch 42 [8000/8883 (89.93%)]\t\tLoss: 0.60839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [8320/8883 (93.53%)]\t\tLoss: 0.81019\n",
      "Training Progress: \tEpoch 42 [8640/8883 (97.12%)]\t\tLoss: 0.71966\n",
      "\tTrain loss: 0.01842, Accuracy: 6503/8883 (73.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1289/1692 (76.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/8883 (0.00%)]\t\tLoss: 0.59585\n",
      "Training Progress: \tEpoch 43 [320/8883 (3.60%)]\t\tLoss: 0.79473\n",
      "Training Progress: \tEpoch 43 [640/8883 (7.19%)]\t\tLoss: 0.52393\n",
      "Training Progress: \tEpoch 43 [960/8883 (10.79%)]\t\tLoss: 0.83093\n",
      "Training Progress: \tEpoch 43 [1280/8883 (14.39%)]\t\tLoss: 0.78995\n",
      "Training Progress: \tEpoch 43 [1600/8883 (17.99%)]\t\tLoss: 0.49403\n",
      "Training Progress: \tEpoch 43 [1920/8883 (21.58%)]\t\tLoss: 0.63807\n",
      "Training Progress: \tEpoch 43 [2240/8883 (25.18%)]\t\tLoss: 0.71934\n",
      "Training Progress: \tEpoch 43 [2560/8883 (28.78%)]\t\tLoss: 1.07503\n",
      "Training Progress: \tEpoch 43 [2880/8883 (32.37%)]\t\tLoss: 0.72481\n",
      "Training Progress: \tEpoch 43 [3200/8883 (35.97%)]\t\tLoss: 0.90617\n",
      "Training Progress: \tEpoch 43 [3520/8883 (39.57%)]\t\tLoss: 0.81947\n",
      "Training Progress: \tEpoch 43 [3840/8883 (43.17%)]\t\tLoss: 0.85677\n",
      "Training Progress: \tEpoch 43 [4160/8883 (46.76%)]\t\tLoss: 0.57554\n",
      "Training Progress: \tEpoch 43 [4480/8883 (50.36%)]\t\tLoss: 0.67865\n",
      "Training Progress: \tEpoch 43 [4800/8883 (53.96%)]\t\tLoss: 0.52330\n",
      "Training Progress: \tEpoch 43 [5120/8883 (57.55%)]\t\tLoss: 0.90129\n",
      "Training Progress: \tEpoch 43 [5440/8883 (61.15%)]\t\tLoss: 0.56072\n",
      "Training Progress: \tEpoch 43 [5760/8883 (64.75%)]\t\tLoss: 0.65723\n",
      "Training Progress: \tEpoch 43 [6080/8883 (68.35%)]\t\tLoss: 0.59274\n",
      "Training Progress: \tEpoch 43 [6400/8883 (71.94%)]\t\tLoss: 0.66575\n",
      "Training Progress: \tEpoch 43 [6720/8883 (75.54%)]\t\tLoss: 0.75885\n",
      "Training Progress: \tEpoch 43 [7040/8883 (79.14%)]\t\tLoss: 1.14871\n",
      "Training Progress: \tEpoch 43 [7360/8883 (82.73%)]\t\tLoss: 0.82382\n",
      "Training Progress: \tEpoch 43 [7680/8883 (86.33%)]\t\tLoss: 0.63173\n",
      "Training Progress: \tEpoch 43 [8000/8883 (89.93%)]\t\tLoss: 0.74093\n",
      "Training Progress: \tEpoch 43 [8320/8883 (93.53%)]\t\tLoss: 1.03659\n",
      "Training Progress: \tEpoch 43 [8640/8883 (97.12%)]\t\tLoss: 0.57774\n",
      "\tTrain loss: 0.01749, Accuracy: 6614/8883 (74.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00135, Accuracy: 503/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/8883 (0.00%)]\t\tLoss: 0.61135\n",
      "Training Progress: \tEpoch 44 [320/8883 (3.60%)]\t\tLoss: 0.91730\n",
      "Training Progress: \tEpoch 44 [640/8883 (7.19%)]\t\tLoss: 0.59104\n",
      "Training Progress: \tEpoch 44 [960/8883 (10.79%)]\t\tLoss: 0.68294\n",
      "Training Progress: \tEpoch 44 [1280/8883 (14.39%)]\t\tLoss: 0.58409\n",
      "Training Progress: \tEpoch 44 [1600/8883 (17.99%)]\t\tLoss: 0.62667\n",
      "Training Progress: \tEpoch 44 [1920/8883 (21.58%)]\t\tLoss: 0.62571\n",
      "Training Progress: \tEpoch 44 [2240/8883 (25.18%)]\t\tLoss: 0.66269\n",
      "Training Progress: \tEpoch 44 [2560/8883 (28.78%)]\t\tLoss: 0.84550\n",
      "Training Progress: \tEpoch 44 [2880/8883 (32.37%)]\t\tLoss: 0.83807\n",
      "Training Progress: \tEpoch 44 [3200/8883 (35.97%)]\t\tLoss: 0.91587\n",
      "Training Progress: \tEpoch 44 [3520/8883 (39.57%)]\t\tLoss: 0.91275\n",
      "Training Progress: \tEpoch 44 [3840/8883 (43.17%)]\t\tLoss: 0.82671\n",
      "Training Progress: \tEpoch 44 [4160/8883 (46.76%)]\t\tLoss: 0.68423\n",
      "Training Progress: \tEpoch 44 [4480/8883 (50.36%)]\t\tLoss: 0.55654\n",
      "Training Progress: \tEpoch 44 [4800/8883 (53.96%)]\t\tLoss: 0.65588\n",
      "Training Progress: \tEpoch 44 [5120/8883 (57.55%)]\t\tLoss: 0.60556\n",
      "Training Progress: \tEpoch 44 [5440/8883 (61.15%)]\t\tLoss: 0.60851\n",
      "Training Progress: \tEpoch 44 [5760/8883 (64.75%)]\t\tLoss: 0.71653\n",
      "Training Progress: \tEpoch 44 [6080/8883 (68.35%)]\t\tLoss: 0.62301\n",
      "Training Progress: \tEpoch 44 [6400/8883 (71.94%)]\t\tLoss: 0.60651\n",
      "Training Progress: \tEpoch 44 [6720/8883 (75.54%)]\t\tLoss: 0.72399\n",
      "Training Progress: \tEpoch 44 [7040/8883 (79.14%)]\t\tLoss: 0.82418\n",
      "Training Progress: \tEpoch 44 [7360/8883 (82.73%)]\t\tLoss: 0.62233\n",
      "Training Progress: \tEpoch 44 [7680/8883 (86.33%)]\t\tLoss: 0.91303\n",
      "Training Progress: \tEpoch 44 [8000/8883 (89.93%)]\t\tLoss: 0.67875\n",
      "Training Progress: \tEpoch 44 [8320/8883 (93.53%)]\t\tLoss: 0.91387\n",
      "Training Progress: \tEpoch 44 [8640/8883 (97.12%)]\t\tLoss: 0.81113\n",
      "\tTrain loss: 0.01833, Accuracy: 6487/8883 (73.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1306/1692 (77.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/8883 (0.00%)]\t\tLoss: 0.64103\n",
      "Training Progress: \tEpoch 45 [320/8883 (3.60%)]\t\tLoss: 0.80904\n",
      "Training Progress: \tEpoch 45 [640/8883 (7.19%)]\t\tLoss: 0.59945\n",
      "Training Progress: \tEpoch 45 [960/8883 (10.79%)]\t\tLoss: 0.64755\n",
      "Training Progress: \tEpoch 45 [1280/8883 (14.39%)]\t\tLoss: 0.64869\n",
      "Training Progress: \tEpoch 45 [1600/8883 (17.99%)]\t\tLoss: 0.67492\n",
      "Training Progress: \tEpoch 45 [1920/8883 (21.58%)]\t\tLoss: 0.58386\n",
      "Training Progress: \tEpoch 45 [2240/8883 (25.18%)]\t\tLoss: 0.55372\n",
      "Training Progress: \tEpoch 45 [2560/8883 (28.78%)]\t\tLoss: 1.05770\n",
      "Training Progress: \tEpoch 45 [2880/8883 (32.37%)]\t\tLoss: 0.72046\n",
      "Training Progress: \tEpoch 45 [3200/8883 (35.97%)]\t\tLoss: 0.59347\n",
      "Training Progress: \tEpoch 45 [3520/8883 (39.57%)]\t\tLoss: 0.67998\n",
      "Training Progress: \tEpoch 45 [3840/8883 (43.17%)]\t\tLoss: 0.83554\n",
      "Training Progress: \tEpoch 45 [4160/8883 (46.76%)]\t\tLoss: 0.51333\n",
      "Training Progress: \tEpoch 45 [4480/8883 (50.36%)]\t\tLoss: 0.50345\n",
      "Training Progress: \tEpoch 45 [4800/8883 (53.96%)]\t\tLoss: 0.36359\n",
      "Training Progress: \tEpoch 45 [5120/8883 (57.55%)]\t\tLoss: 0.68301\n",
      "Training Progress: \tEpoch 45 [5440/8883 (61.15%)]\t\tLoss: 1.08194\n",
      "Training Progress: \tEpoch 45 [5760/8883 (64.75%)]\t\tLoss: 0.59396\n",
      "Training Progress: \tEpoch 45 [6080/8883 (68.35%)]\t\tLoss: 0.60322\n",
      "Training Progress: \tEpoch 45 [6400/8883 (71.94%)]\t\tLoss: 0.68760\n",
      "Training Progress: \tEpoch 45 [6720/8883 (75.54%)]\t\tLoss: 0.62679\n",
      "Training Progress: \tEpoch 45 [7040/8883 (79.14%)]\t\tLoss: 0.88827\n",
      "Training Progress: \tEpoch 45 [7360/8883 (82.73%)]\t\tLoss: 0.64268\n",
      "Training Progress: \tEpoch 45 [7680/8883 (86.33%)]\t\tLoss: 0.65082\n",
      "Training Progress: \tEpoch 45 [8000/8883 (89.93%)]\t\tLoss: 0.84238\n",
      "Training Progress: \tEpoch 45 [8320/8883 (93.53%)]\t\tLoss: 0.78455\n",
      "Training Progress: \tEpoch 45 [8640/8883 (97.12%)]\t\tLoss: 0.78388\n",
      "\tTrain loss: 0.01687, Accuracy: 6650/8883 (74.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1331/1692 (78.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/8883 (0.00%)]\t\tLoss: 0.81581\n",
      "Training Progress: \tEpoch 46 [320/8883 (3.60%)]\t\tLoss: 0.91983\n",
      "Training Progress: \tEpoch 46 [640/8883 (7.19%)]\t\tLoss: 0.47805\n",
      "Training Progress: \tEpoch 46 [960/8883 (10.79%)]\t\tLoss: 0.58924\n",
      "Training Progress: \tEpoch 46 [1280/8883 (14.39%)]\t\tLoss: 0.81629\n",
      "Training Progress: \tEpoch 46 [1600/8883 (17.99%)]\t\tLoss: 0.90601\n",
      "Training Progress: \tEpoch 46 [1920/8883 (21.58%)]\t\tLoss: 0.56435\n",
      "Training Progress: \tEpoch 46 [2240/8883 (25.18%)]\t\tLoss: 0.69696\n",
      "Training Progress: \tEpoch 46 [2560/8883 (28.78%)]\t\tLoss: 0.96204\n",
      "Training Progress: \tEpoch 46 [2880/8883 (32.37%)]\t\tLoss: 0.80272\n",
      "Training Progress: \tEpoch 46 [3200/8883 (35.97%)]\t\tLoss: 0.69147\n",
      "Training Progress: \tEpoch 46 [3520/8883 (39.57%)]\t\tLoss: 0.83044\n",
      "Training Progress: \tEpoch 46 [3840/8883 (43.17%)]\t\tLoss: 0.82587\n",
      "Training Progress: \tEpoch 46 [4160/8883 (46.76%)]\t\tLoss: 0.43483\n",
      "Training Progress: \tEpoch 46 [4480/8883 (50.36%)]\t\tLoss: 0.72330\n",
      "Training Progress: \tEpoch 46 [4800/8883 (53.96%)]\t\tLoss: 0.58017\n",
      "Training Progress: \tEpoch 46 [5120/8883 (57.55%)]\t\tLoss: 0.56644\n",
      "Training Progress: \tEpoch 46 [5440/8883 (61.15%)]\t\tLoss: 0.62122\n",
      "Training Progress: \tEpoch 46 [5760/8883 (64.75%)]\t\tLoss: 0.72932\n",
      "Training Progress: \tEpoch 46 [6080/8883 (68.35%)]\t\tLoss: 0.69885\n",
      "Training Progress: \tEpoch 46 [6400/8883 (71.94%)]\t\tLoss: 0.76972\n",
      "Training Progress: \tEpoch 46 [6720/8883 (75.54%)]\t\tLoss: 0.70125\n",
      "Training Progress: \tEpoch 46 [7040/8883 (79.14%)]\t\tLoss: 0.74121\n",
      "Training Progress: \tEpoch 46 [7360/8883 (82.73%)]\t\tLoss: 0.74931\n",
      "Training Progress: \tEpoch 46 [7680/8883 (86.33%)]\t\tLoss: 0.72818\n",
      "Training Progress: \tEpoch 46 [8000/8883 (89.93%)]\t\tLoss: 0.55327\n",
      "Training Progress: \tEpoch 46 [8320/8883 (93.53%)]\t\tLoss: 0.89024\n",
      "Training Progress: \tEpoch 46 [8640/8883 (97.12%)]\t\tLoss: 0.74168\n",
      "\tTrain loss: 0.01673, Accuracy: 6687/8883 (75.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1343/1692 (79.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/8883 (0.00%)]\t\tLoss: 0.79129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 47 [320/8883 (3.60%)]\t\tLoss: 0.75602\n",
      "Training Progress: \tEpoch 47 [640/8883 (7.19%)]\t\tLoss: 0.64217\n",
      "Training Progress: \tEpoch 47 [960/8883 (10.79%)]\t\tLoss: 0.71545\n",
      "Training Progress: \tEpoch 47 [1280/8883 (14.39%)]\t\tLoss: 0.63425\n",
      "Training Progress: \tEpoch 47 [1600/8883 (17.99%)]\t\tLoss: 0.50295\n",
      "Training Progress: \tEpoch 47 [1920/8883 (21.58%)]\t\tLoss: 0.51061\n",
      "Training Progress: \tEpoch 47 [2240/8883 (25.18%)]\t\tLoss: 0.77862\n",
      "Training Progress: \tEpoch 47 [2560/8883 (28.78%)]\t\tLoss: 0.80617\n",
      "Training Progress: \tEpoch 47 [2880/8883 (32.37%)]\t\tLoss: 0.55249\n",
      "Training Progress: \tEpoch 47 [3200/8883 (35.97%)]\t\tLoss: 0.81854\n",
      "Training Progress: \tEpoch 47 [3520/8883 (39.57%)]\t\tLoss: 0.71119\n",
      "Training Progress: \tEpoch 47 [3840/8883 (43.17%)]\t\tLoss: 0.62694\n",
      "Training Progress: \tEpoch 47 [4160/8883 (46.76%)]\t\tLoss: 0.49651\n",
      "Training Progress: \tEpoch 47 [4480/8883 (50.36%)]\t\tLoss: 0.54077\n",
      "Training Progress: \tEpoch 47 [4800/8883 (53.96%)]\t\tLoss: 0.39911\n",
      "Training Progress: \tEpoch 47 [5120/8883 (57.55%)]\t\tLoss: 0.60891\n",
      "Training Progress: \tEpoch 47 [5440/8883 (61.15%)]\t\tLoss: 0.77228\n",
      "Training Progress: \tEpoch 47 [5760/8883 (64.75%)]\t\tLoss: 0.84040\n",
      "Training Progress: \tEpoch 47 [6080/8883 (68.35%)]\t\tLoss: 0.58943\n",
      "Training Progress: \tEpoch 47 [6400/8883 (71.94%)]\t\tLoss: 0.55694\n",
      "Training Progress: \tEpoch 47 [6720/8883 (75.54%)]\t\tLoss: 0.56921\n",
      "Training Progress: \tEpoch 47 [7040/8883 (79.14%)]\t\tLoss: 0.63420\n",
      "Training Progress: \tEpoch 47 [7360/8883 (82.73%)]\t\tLoss: 0.73956\n",
      "Training Progress: \tEpoch 47 [7680/8883 (86.33%)]\t\tLoss: 0.57874\n",
      "Training Progress: \tEpoch 47 [8000/8883 (89.93%)]\t\tLoss: 0.84904\n",
      "Training Progress: \tEpoch 47 [8320/8883 (93.53%)]\t\tLoss: 0.78308\n",
      "Training Progress: \tEpoch 47 [8640/8883 (97.12%)]\t\tLoss: 0.75272\n",
      "\tTrain loss: 0.01654, Accuracy: 6674/8883 (75.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1350/1692 (79.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/8883 (0.00%)]\t\tLoss: 0.57060\n",
      "Training Progress: \tEpoch 48 [320/8883 (3.60%)]\t\tLoss: 0.81993\n",
      "Training Progress: \tEpoch 48 [640/8883 (7.19%)]\t\tLoss: 0.43736\n",
      "Training Progress: \tEpoch 48 [960/8883 (10.79%)]\t\tLoss: 0.52274\n",
      "Training Progress: \tEpoch 48 [1280/8883 (14.39%)]\t\tLoss: 0.69386\n",
      "Training Progress: \tEpoch 48 [1600/8883 (17.99%)]\t\tLoss: 0.47845\n",
      "Training Progress: \tEpoch 48 [1920/8883 (21.58%)]\t\tLoss: 0.65940\n",
      "Training Progress: \tEpoch 48 [2240/8883 (25.18%)]\t\tLoss: 0.56571\n",
      "Training Progress: \tEpoch 48 [2560/8883 (28.78%)]\t\tLoss: 0.85797\n",
      "Training Progress: \tEpoch 48 [2880/8883 (32.37%)]\t\tLoss: 0.68173\n",
      "Training Progress: \tEpoch 48 [3200/8883 (35.97%)]\t\tLoss: 0.61130\n",
      "Training Progress: \tEpoch 48 [3520/8883 (39.57%)]\t\tLoss: 0.88692\n",
      "Training Progress: \tEpoch 48 [3840/8883 (43.17%)]\t\tLoss: 0.68995\n",
      "Training Progress: \tEpoch 48 [4160/8883 (46.76%)]\t\tLoss: 0.56701\n",
      "Training Progress: \tEpoch 48 [4480/8883 (50.36%)]\t\tLoss: 0.40781\n",
      "Training Progress: \tEpoch 48 [4800/8883 (53.96%)]\t\tLoss: 0.61434\n",
      "Training Progress: \tEpoch 48 [5120/8883 (57.55%)]\t\tLoss: 0.73903\n",
      "Training Progress: \tEpoch 48 [5440/8883 (61.15%)]\t\tLoss: 0.60151\n",
      "Training Progress: \tEpoch 48 [5760/8883 (64.75%)]\t\tLoss: 0.51105\n",
      "Training Progress: \tEpoch 48 [6080/8883 (68.35%)]\t\tLoss: 0.52496\n",
      "Training Progress: \tEpoch 48 [6400/8883 (71.94%)]\t\tLoss: 0.76200\n",
      "Training Progress: \tEpoch 48 [6720/8883 (75.54%)]\t\tLoss: 0.67625\n",
      "Training Progress: \tEpoch 48 [7040/8883 (79.14%)]\t\tLoss: 0.71859\n",
      "Training Progress: \tEpoch 48 [7360/8883 (82.73%)]\t\tLoss: 0.63371\n",
      "Training Progress: \tEpoch 48 [7680/8883 (86.33%)]\t\tLoss: 0.80530\n",
      "Training Progress: \tEpoch 48 [8000/8883 (89.93%)]\t\tLoss: 0.61613\n",
      "Training Progress: \tEpoch 48 [8320/8883 (93.53%)]\t\tLoss: 0.72896\n",
      "Training Progress: \tEpoch 48 [8640/8883 (97.12%)]\t\tLoss: 0.62553\n",
      "\tTrain loss: 0.01611, Accuracy: 6729/8883 (75.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1373/1692 (81.00%)\n",
      "\tTest loss: 0.00142, Accuracy: 502/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/8883 (0.00%)]\t\tLoss: 0.70264\n",
      "Training Progress: \tEpoch 49 [320/8883 (3.60%)]\t\tLoss: 0.72008\n",
      "Training Progress: \tEpoch 49 [640/8883 (7.19%)]\t\tLoss: 0.45387\n",
      "Training Progress: \tEpoch 49 [960/8883 (10.79%)]\t\tLoss: 0.61663\n",
      "Training Progress: \tEpoch 49 [1280/8883 (14.39%)]\t\tLoss: 0.74748\n",
      "Training Progress: \tEpoch 49 [1600/8883 (17.99%)]\t\tLoss: 0.83803\n",
      "Training Progress: \tEpoch 49 [1920/8883 (21.58%)]\t\tLoss: 0.63291\n",
      "Training Progress: \tEpoch 49 [2240/8883 (25.18%)]\t\tLoss: 0.84764\n",
      "Training Progress: \tEpoch 49 [2560/8883 (28.78%)]\t\tLoss: 0.62461\n",
      "Training Progress: \tEpoch 49 [2880/8883 (32.37%)]\t\tLoss: 0.82506\n",
      "Training Progress: \tEpoch 49 [3200/8883 (35.97%)]\t\tLoss: 0.52699\n",
      "Training Progress: \tEpoch 49 [3520/8883 (39.57%)]\t\tLoss: 0.84930\n",
      "Training Progress: \tEpoch 49 [3840/8883 (43.17%)]\t\tLoss: 0.83165\n",
      "Training Progress: \tEpoch 49 [4160/8883 (46.76%)]\t\tLoss: 0.41486\n",
      "Training Progress: \tEpoch 49 [4480/8883 (50.36%)]\t\tLoss: 0.32976\n",
      "Training Progress: \tEpoch 49 [4800/8883 (53.96%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 49 [5120/8883 (57.55%)]\t\tLoss: 0.74853\n",
      "Training Progress: \tEpoch 49 [5440/8883 (61.15%)]\t\tLoss: 0.57786\n",
      "Training Progress: \tEpoch 49 [5760/8883 (64.75%)]\t\tLoss: 0.79770\n",
      "Training Progress: \tEpoch 49 [6080/8883 (68.35%)]\t\tLoss: 0.56591\n",
      "Training Progress: \tEpoch 49 [6400/8883 (71.94%)]\t\tLoss: 0.60655\n",
      "Training Progress: \tEpoch 49 [6720/8883 (75.54%)]\t\tLoss: 0.59642\n",
      "Training Progress: \tEpoch 49 [7040/8883 (79.14%)]\t\tLoss: 0.73570\n",
      "Training Progress: \tEpoch 49 [7360/8883 (82.73%)]\t\tLoss: 0.61856\n",
      "Training Progress: \tEpoch 49 [7680/8883 (86.33%)]\t\tLoss: 0.64583\n",
      "Training Progress: \tEpoch 49 [8000/8883 (89.93%)]\t\tLoss: 0.45577\n",
      "Training Progress: \tEpoch 49 [8320/8883 (93.53%)]\t\tLoss: 0.79573\n",
      "Training Progress: \tEpoch 49 [8640/8883 (97.12%)]\t\tLoss: 0.78095\n",
      "\tTrain loss: 0.01586, Accuracy: 6768/8883 (76.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1367/1692 (80.00%)\n",
      "\tTest loss: 0.00142, Accuracy: 539/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/8883 (0.00%)]\t\tLoss: 0.49152\n",
      "Training Progress: \tEpoch 50 [320/8883 (3.60%)]\t\tLoss: 0.68791\n",
      "Training Progress: \tEpoch 50 [640/8883 (7.19%)]\t\tLoss: 0.95568\n",
      "Training Progress: \tEpoch 50 [960/8883 (10.79%)]\t\tLoss: 0.58131\n",
      "Training Progress: \tEpoch 50 [1280/8883 (14.39%)]\t\tLoss: 0.59391\n",
      "Training Progress: \tEpoch 50 [1600/8883 (17.99%)]\t\tLoss: 0.59474\n",
      "Training Progress: \tEpoch 50 [1920/8883 (21.58%)]\t\tLoss: 0.52324\n",
      "Training Progress: \tEpoch 50 [2240/8883 (25.18%)]\t\tLoss: 0.61681\n",
      "Training Progress: \tEpoch 50 [2560/8883 (28.78%)]\t\tLoss: 0.68693\n",
      "Training Progress: \tEpoch 50 [2880/8883 (32.37%)]\t\tLoss: 0.69904\n",
      "Training Progress: \tEpoch 50 [3200/8883 (35.97%)]\t\tLoss: 0.50302\n",
      "Training Progress: \tEpoch 50 [3520/8883 (39.57%)]\t\tLoss: 0.99362\n",
      "Training Progress: \tEpoch 50 [3840/8883 (43.17%)]\t\tLoss: 0.74347\n",
      "Training Progress: \tEpoch 50 [4160/8883 (46.76%)]\t\tLoss: 0.62791\n",
      "Training Progress: \tEpoch 50 [4480/8883 (50.36%)]\t\tLoss: 0.43455\n",
      "Training Progress: \tEpoch 50 [4800/8883 (53.96%)]\t\tLoss: 0.51378\n",
      "Training Progress: \tEpoch 50 [5120/8883 (57.55%)]\t\tLoss: 0.60295\n",
      "Training Progress: \tEpoch 50 [5440/8883 (61.15%)]\t\tLoss: 0.63547\n",
      "Training Progress: \tEpoch 50 [5760/8883 (64.75%)]\t\tLoss: 0.51929\n",
      "Training Progress: \tEpoch 50 [6080/8883 (68.35%)]\t\tLoss: 0.49503\n",
      "Training Progress: \tEpoch 50 [6400/8883 (71.94%)]\t\tLoss: 0.46179\n",
      "Training Progress: \tEpoch 50 [6720/8883 (75.54%)]\t\tLoss: 0.86717\n",
      "Training Progress: \tEpoch 50 [7040/8883 (79.14%)]\t\tLoss: 0.72839\n",
      "Training Progress: \tEpoch 50 [7360/8883 (82.73%)]\t\tLoss: 0.90452\n",
      "Training Progress: \tEpoch 50 [7680/8883 (86.33%)]\t\tLoss: 0.56231\n",
      "Training Progress: \tEpoch 50 [8000/8883 (89.93%)]\t\tLoss: 0.62422\n",
      "Training Progress: \tEpoch 50 [8320/8883 (93.53%)]\t\tLoss: 0.77668\n",
      "Training Progress: \tEpoch 50 [8640/8883 (97.12%)]\t\tLoss: 0.78914\n",
      "\tTrain loss: 0.01584, Accuracy: 6756/8883 (76.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1362/1692 (80.00%)\n",
      "\tTest loss: 0.00148, Accuracy: 520/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/8883 (0.00%)]\t\tLoss: 0.53507\n",
      "Training Progress: \tEpoch 51 [320/8883 (3.60%)]\t\tLoss: 0.57857\n",
      "Training Progress: \tEpoch 51 [640/8883 (7.19%)]\t\tLoss: 0.66945\n",
      "Training Progress: \tEpoch 51 [960/8883 (10.79%)]\t\tLoss: 0.71373\n",
      "Training Progress: \tEpoch 51 [1280/8883 (14.39%)]\t\tLoss: 0.80204\n",
      "Training Progress: \tEpoch 51 [1600/8883 (17.99%)]\t\tLoss: 0.46259\n",
      "Training Progress: \tEpoch 51 [1920/8883 (21.58%)]\t\tLoss: 0.59116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 51 [2240/8883 (25.18%)]\t\tLoss: 0.66343\n",
      "Training Progress: \tEpoch 51 [2560/8883 (28.78%)]\t\tLoss: 0.67385\n",
      "Training Progress: \tEpoch 51 [2880/8883 (32.37%)]\t\tLoss: 0.77433\n",
      "Training Progress: \tEpoch 51 [3200/8883 (35.97%)]\t\tLoss: 0.74914\n",
      "Training Progress: \tEpoch 51 [3520/8883 (39.57%)]\t\tLoss: 0.64551\n",
      "Training Progress: \tEpoch 51 [3840/8883 (43.17%)]\t\tLoss: 0.70375\n",
      "Training Progress: \tEpoch 51 [4160/8883 (46.76%)]\t\tLoss: 0.66185\n",
      "Training Progress: \tEpoch 51 [4480/8883 (50.36%)]\t\tLoss: 0.56566\n",
      "Training Progress: \tEpoch 51 [4800/8883 (53.96%)]\t\tLoss: 0.41941\n",
      "Training Progress: \tEpoch 51 [5120/8883 (57.55%)]\t\tLoss: 0.67330\n",
      "Training Progress: \tEpoch 51 [5440/8883 (61.15%)]\t\tLoss: 0.58773\n",
      "Training Progress: \tEpoch 51 [5760/8883 (64.75%)]\t\tLoss: 0.43492\n",
      "Training Progress: \tEpoch 51 [6080/8883 (68.35%)]\t\tLoss: 0.49125\n",
      "Training Progress: \tEpoch 51 [6400/8883 (71.94%)]\t\tLoss: 0.60017\n",
      "Training Progress: \tEpoch 51 [6720/8883 (75.54%)]\t\tLoss: 0.74888\n",
      "Training Progress: \tEpoch 51 [7040/8883 (79.14%)]\t\tLoss: 0.80982\n",
      "Training Progress: \tEpoch 51 [7360/8883 (82.73%)]\t\tLoss: 0.69810\n",
      "Training Progress: \tEpoch 51 [7680/8883 (86.33%)]\t\tLoss: 0.59142\n",
      "Training Progress: \tEpoch 51 [8000/8883 (89.93%)]\t\tLoss: 0.61010\n",
      "Training Progress: \tEpoch 51 [8320/8883 (93.53%)]\t\tLoss: 0.67524\n",
      "Training Progress: \tEpoch 51 [8640/8883 (97.12%)]\t\tLoss: 0.74286\n",
      "\tTrain loss: 0.01583, Accuracy: 6774/8883 (76.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1377/1692 (81.00%)\n",
      "\tTest loss: 0.00148, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/8883 (0.00%)]\t\tLoss: 0.83738\n",
      "Training Progress: \tEpoch 52 [320/8883 (3.60%)]\t\tLoss: 0.58264\n",
      "Training Progress: \tEpoch 52 [640/8883 (7.19%)]\t\tLoss: 0.51679\n",
      "Training Progress: \tEpoch 52 [960/8883 (10.79%)]\t\tLoss: 0.55407\n",
      "Training Progress: \tEpoch 52 [1280/8883 (14.39%)]\t\tLoss: 0.53902\n",
      "Training Progress: \tEpoch 52 [1600/8883 (17.99%)]\t\tLoss: 0.57651\n",
      "Training Progress: \tEpoch 52 [1920/8883 (21.58%)]\t\tLoss: 0.41681\n",
      "Training Progress: \tEpoch 52 [2240/8883 (25.18%)]\t\tLoss: 0.57644\n",
      "Training Progress: \tEpoch 52 [2560/8883 (28.78%)]\t\tLoss: 0.67407\n",
      "Training Progress: \tEpoch 52 [2880/8883 (32.37%)]\t\tLoss: 0.68926\n",
      "Training Progress: \tEpoch 52 [3200/8883 (35.97%)]\t\tLoss: 0.79908\n",
      "Training Progress: \tEpoch 52 [3520/8883 (39.57%)]\t\tLoss: 0.67362\n",
      "Training Progress: \tEpoch 52 [3840/8883 (43.17%)]\t\tLoss: 0.78768\n",
      "Training Progress: \tEpoch 52 [4160/8883 (46.76%)]\t\tLoss: 0.48015\n",
      "Training Progress: \tEpoch 52 [4480/8883 (50.36%)]\t\tLoss: 0.36201\n",
      "Training Progress: \tEpoch 52 [4800/8883 (53.96%)]\t\tLoss: 0.46124\n",
      "Training Progress: \tEpoch 52 [5120/8883 (57.55%)]\t\tLoss: 0.84804\n",
      "Training Progress: \tEpoch 52 [5440/8883 (61.15%)]\t\tLoss: 0.67846\n",
      "Training Progress: \tEpoch 52 [5760/8883 (64.75%)]\t\tLoss: 0.71290\n",
      "Training Progress: \tEpoch 52 [6080/8883 (68.35%)]\t\tLoss: 0.51403\n",
      "Training Progress: \tEpoch 52 [6400/8883 (71.94%)]\t\tLoss: 0.56369\n",
      "Training Progress: \tEpoch 52 [6720/8883 (75.54%)]\t\tLoss: 0.72030\n",
      "Training Progress: \tEpoch 52 [7040/8883 (79.14%)]\t\tLoss: 0.80082\n",
      "Training Progress: \tEpoch 52 [7360/8883 (82.73%)]\t\tLoss: 0.61153\n",
      "Training Progress: \tEpoch 52 [7680/8883 (86.33%)]\t\tLoss: 0.51691\n",
      "Training Progress: \tEpoch 52 [8000/8883 (89.93%)]\t\tLoss: 0.46083\n",
      "Training Progress: \tEpoch 52 [8320/8883 (93.53%)]\t\tLoss: 0.73924\n",
      "Training Progress: \tEpoch 52 [8640/8883 (97.12%)]\t\tLoss: 0.61484\n",
      "\tTrain loss: 0.01616, Accuracy: 6687/8883 (75.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1361/1692 (80.00%)\n",
      "\tTest loss: 0.00152, Accuracy: 513/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/8883 (0.00%)]\t\tLoss: 0.48420\n",
      "Training Progress: \tEpoch 53 [320/8883 (3.60%)]\t\tLoss: 0.62459\n",
      "Training Progress: \tEpoch 53 [640/8883 (7.19%)]\t\tLoss: 0.70641\n",
      "Training Progress: \tEpoch 53 [960/8883 (10.79%)]\t\tLoss: 0.57276\n",
      "Training Progress: \tEpoch 53 [1280/8883 (14.39%)]\t\tLoss: 0.74107\n",
      "Training Progress: \tEpoch 53 [1600/8883 (17.99%)]\t\tLoss: 0.49605\n",
      "Training Progress: \tEpoch 53 [1920/8883 (21.58%)]\t\tLoss: 0.46553\n",
      "Training Progress: \tEpoch 53 [2240/8883 (25.18%)]\t\tLoss: 0.79267\n",
      "Training Progress: \tEpoch 53 [2560/8883 (28.78%)]\t\tLoss: 0.93690\n",
      "Training Progress: \tEpoch 53 [2880/8883 (32.37%)]\t\tLoss: 0.79138\n",
      "Training Progress: \tEpoch 53 [3200/8883 (35.97%)]\t\tLoss: 0.57388\n",
      "Training Progress: \tEpoch 53 [3520/8883 (39.57%)]\t\tLoss: 0.73013\n",
      "Training Progress: \tEpoch 53 [3840/8883 (43.17%)]\t\tLoss: 0.96703\n",
      "Training Progress: \tEpoch 53 [4160/8883 (46.76%)]\t\tLoss: 0.45985\n",
      "Training Progress: \tEpoch 53 [4480/8883 (50.36%)]\t\tLoss: 0.36038\n",
      "Training Progress: \tEpoch 53 [4800/8883 (53.96%)]\t\tLoss: 0.49359\n",
      "Training Progress: \tEpoch 53 [5120/8883 (57.55%)]\t\tLoss: 0.71112\n",
      "Training Progress: \tEpoch 53 [5440/8883 (61.15%)]\t\tLoss: 0.67395\n",
      "Training Progress: \tEpoch 53 [5760/8883 (64.75%)]\t\tLoss: 0.58408\n",
      "Training Progress: \tEpoch 53 [6080/8883 (68.35%)]\t\tLoss: 0.38427\n",
      "Training Progress: \tEpoch 53 [6400/8883 (71.94%)]\t\tLoss: 0.55464\n",
      "Training Progress: \tEpoch 53 [6720/8883 (75.54%)]\t\tLoss: 0.80052\n",
      "Training Progress: \tEpoch 53 [7040/8883 (79.14%)]\t\tLoss: 0.69117\n",
      "Training Progress: \tEpoch 53 [7360/8883 (82.73%)]\t\tLoss: 0.54374\n",
      "Training Progress: \tEpoch 53 [7680/8883 (86.33%)]\t\tLoss: 0.50127\n",
      "Training Progress: \tEpoch 53 [8000/8883 (89.93%)]\t\tLoss: 0.44851\n",
      "Training Progress: \tEpoch 53 [8320/8883 (93.53%)]\t\tLoss: 0.83463\n",
      "Training Progress: \tEpoch 53 [8640/8883 (97.12%)]\t\tLoss: 0.63317\n",
      "\tTrain loss: 0.01477, Accuracy: 6882/8883 (77.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1414/1692 (83.00%)\n",
      "\tTest loss: 0.00153, Accuracy: 516/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/8883 (0.00%)]\t\tLoss: 0.56191\n",
      "Training Progress: \tEpoch 54 [320/8883 (3.60%)]\t\tLoss: 0.60081\n",
      "Training Progress: \tEpoch 54 [640/8883 (7.19%)]\t\tLoss: 0.62269\n",
      "Training Progress: \tEpoch 54 [960/8883 (10.79%)]\t\tLoss: 0.54607\n",
      "Training Progress: \tEpoch 54 [1280/8883 (14.39%)]\t\tLoss: 0.62510\n",
      "Training Progress: \tEpoch 54 [1600/8883 (17.99%)]\t\tLoss: 0.57723\n",
      "Training Progress: \tEpoch 54 [1920/8883 (21.58%)]\t\tLoss: 0.56949\n",
      "Training Progress: \tEpoch 54 [2240/8883 (25.18%)]\t\tLoss: 0.71434\n",
      "Training Progress: \tEpoch 54 [2560/8883 (28.78%)]\t\tLoss: 0.75758\n",
      "Training Progress: \tEpoch 54 [2880/8883 (32.37%)]\t\tLoss: 0.67037\n",
      "Training Progress: \tEpoch 54 [3200/8883 (35.97%)]\t\tLoss: 0.56252\n",
      "Training Progress: \tEpoch 54 [3520/8883 (39.57%)]\t\tLoss: 0.65069\n",
      "Training Progress: \tEpoch 54 [3840/8883 (43.17%)]\t\tLoss: 0.88185\n",
      "Training Progress: \tEpoch 54 [4160/8883 (46.76%)]\t\tLoss: 0.41566\n",
      "Training Progress: \tEpoch 54 [4480/8883 (50.36%)]\t\tLoss: 0.62118\n",
      "Training Progress: \tEpoch 54 [4800/8883 (53.96%)]\t\tLoss: 0.44800\n",
      "Training Progress: \tEpoch 54 [5120/8883 (57.55%)]\t\tLoss: 0.73772\n",
      "Training Progress: \tEpoch 54 [5440/8883 (61.15%)]\t\tLoss: 0.75036\n",
      "Training Progress: \tEpoch 54 [5760/8883 (64.75%)]\t\tLoss: 0.64394\n",
      "Training Progress: \tEpoch 54 [6080/8883 (68.35%)]\t\tLoss: 0.31356\n",
      "Training Progress: \tEpoch 54 [6400/8883 (71.94%)]\t\tLoss: 0.69852\n",
      "Training Progress: \tEpoch 54 [6720/8883 (75.54%)]\t\tLoss: 0.78287\n",
      "Training Progress: \tEpoch 54 [7040/8883 (79.14%)]\t\tLoss: 0.69916\n",
      "Training Progress: \tEpoch 54 [7360/8883 (82.73%)]\t\tLoss: 0.68336\n",
      "Training Progress: \tEpoch 54 [7680/8883 (86.33%)]\t\tLoss: 0.59784\n",
      "Training Progress: \tEpoch 54 [8000/8883 (89.93%)]\t\tLoss: 0.42620\n",
      "Training Progress: \tEpoch 54 [8320/8883 (93.53%)]\t\tLoss: 0.65948\n",
      "Training Progress: \tEpoch 54 [8640/8883 (97.12%)]\t\tLoss: 0.65172\n",
      "\tTrain loss: 0.01507, Accuracy: 6832/8883 (76.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1394/1692 (82.00%)\n",
      "\tTest loss: 0.00151, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/8883 (0.00%)]\t\tLoss: 0.67839\n",
      "Training Progress: \tEpoch 55 [320/8883 (3.60%)]\t\tLoss: 0.70701\n",
      "Training Progress: \tEpoch 55 [640/8883 (7.19%)]\t\tLoss: 0.61267\n",
      "Training Progress: \tEpoch 55 [960/8883 (10.79%)]\t\tLoss: 0.78174\n",
      "Training Progress: \tEpoch 55 [1280/8883 (14.39%)]\t\tLoss: 0.54942\n",
      "Training Progress: \tEpoch 55 [1600/8883 (17.99%)]\t\tLoss: 0.43896\n",
      "Training Progress: \tEpoch 55 [1920/8883 (21.58%)]\t\tLoss: 0.53842\n",
      "Training Progress: \tEpoch 55 [2240/8883 (25.18%)]\t\tLoss: 0.60801\n",
      "Training Progress: \tEpoch 55 [2560/8883 (28.78%)]\t\tLoss: 0.76035\n",
      "Training Progress: \tEpoch 55 [2880/8883 (32.37%)]\t\tLoss: 0.88394\n",
      "Training Progress: \tEpoch 55 [3200/8883 (35.97%)]\t\tLoss: 0.48184\n",
      "Training Progress: \tEpoch 55 [3520/8883 (39.57%)]\t\tLoss: 0.84213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 55 [3840/8883 (43.17%)]\t\tLoss: 0.72502\n",
      "Training Progress: \tEpoch 55 [4160/8883 (46.76%)]\t\tLoss: 0.50042\n",
      "Training Progress: \tEpoch 55 [4480/8883 (50.36%)]\t\tLoss: 0.57833\n",
      "Training Progress: \tEpoch 55 [4800/8883 (53.96%)]\t\tLoss: 0.45745\n",
      "Training Progress: \tEpoch 55 [5120/8883 (57.55%)]\t\tLoss: 0.69192\n",
      "Training Progress: \tEpoch 55 [5440/8883 (61.15%)]\t\tLoss: 0.57472\n",
      "Training Progress: \tEpoch 55 [5760/8883 (64.75%)]\t\tLoss: 0.48914\n",
      "Training Progress: \tEpoch 55 [6080/8883 (68.35%)]\t\tLoss: 0.45776\n",
      "Training Progress: \tEpoch 55 [6400/8883 (71.94%)]\t\tLoss: 0.75358\n",
      "Training Progress: \tEpoch 55 [6720/8883 (75.54%)]\t\tLoss: 0.73701\n",
      "Training Progress: \tEpoch 55 [7040/8883 (79.14%)]\t\tLoss: 0.76071\n",
      "Training Progress: \tEpoch 55 [7360/8883 (82.73%)]\t\tLoss: 0.62727\n",
      "Training Progress: \tEpoch 55 [7680/8883 (86.33%)]\t\tLoss: 0.56881\n",
      "Training Progress: \tEpoch 55 [8000/8883 (89.93%)]\t\tLoss: 0.54937\n",
      "Training Progress: \tEpoch 55 [8320/8883 (93.53%)]\t\tLoss: 0.86741\n",
      "Training Progress: \tEpoch 55 [8640/8883 (97.12%)]\t\tLoss: 0.60810\n",
      "\tTrain loss: 0.01510, Accuracy: 6845/8883 (77.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1373/1692 (81.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/8883 (0.00%)]\t\tLoss: 0.45029\n",
      "Training Progress: \tEpoch 56 [320/8883 (3.60%)]\t\tLoss: 0.60562\n",
      "Training Progress: \tEpoch 56 [640/8883 (7.19%)]\t\tLoss: 0.60350\n",
      "Training Progress: \tEpoch 56 [960/8883 (10.79%)]\t\tLoss: 0.57532\n",
      "Training Progress: \tEpoch 56 [1280/8883 (14.39%)]\t\tLoss: 0.48903\n",
      "Training Progress: \tEpoch 56 [1600/8883 (17.99%)]\t\tLoss: 0.58947\n",
      "Training Progress: \tEpoch 56 [1920/8883 (21.58%)]\t\tLoss: 0.48879\n",
      "Training Progress: \tEpoch 56 [2240/8883 (25.18%)]\t\tLoss: 0.73735\n",
      "Training Progress: \tEpoch 56 [2560/8883 (28.78%)]\t\tLoss: 0.90109\n",
      "Training Progress: \tEpoch 56 [2880/8883 (32.37%)]\t\tLoss: 0.51286\n",
      "Training Progress: \tEpoch 56 [3200/8883 (35.97%)]\t\tLoss: 0.70629\n",
      "Training Progress: \tEpoch 56 [3520/8883 (39.57%)]\t\tLoss: 0.66594\n",
      "Training Progress: \tEpoch 56 [3840/8883 (43.17%)]\t\tLoss: 0.57627\n",
      "Training Progress: \tEpoch 56 [4160/8883 (46.76%)]\t\tLoss: 0.46376\n",
      "Training Progress: \tEpoch 56 [4480/8883 (50.36%)]\t\tLoss: 0.36236\n",
      "Training Progress: \tEpoch 56 [4800/8883 (53.96%)]\t\tLoss: 0.50159\n",
      "Training Progress: \tEpoch 56 [5120/8883 (57.55%)]\t\tLoss: 0.56536\n",
      "Training Progress: \tEpoch 56 [5440/8883 (61.15%)]\t\tLoss: 0.48940\n",
      "Training Progress: \tEpoch 56 [5760/8883 (64.75%)]\t\tLoss: 0.61983\n",
      "Training Progress: \tEpoch 56 [6080/8883 (68.35%)]\t\tLoss: 0.40866\n",
      "Training Progress: \tEpoch 56 [6400/8883 (71.94%)]\t\tLoss: 0.52325\n",
      "Training Progress: \tEpoch 56 [6720/8883 (75.54%)]\t\tLoss: 0.64376\n",
      "Training Progress: \tEpoch 56 [7040/8883 (79.14%)]\t\tLoss: 0.70761\n",
      "Training Progress: \tEpoch 56 [7360/8883 (82.73%)]\t\tLoss: 0.56598\n",
      "Training Progress: \tEpoch 56 [7680/8883 (86.33%)]\t\tLoss: 0.53343\n",
      "Training Progress: \tEpoch 56 [8000/8883 (89.93%)]\t\tLoss: 0.58648\n",
      "Training Progress: \tEpoch 56 [8320/8883 (93.53%)]\t\tLoss: 0.57720\n",
      "Training Progress: \tEpoch 56 [8640/8883 (97.12%)]\t\tLoss: 0.74241\n",
      "\tTrain loss: 0.01525, Accuracy: 6830/8883 (76.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1367/1692 (80.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 498/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/8883 (0.00%)]\t\tLoss: 0.52461\n",
      "Training Progress: \tEpoch 57 [320/8883 (3.60%)]\t\tLoss: 0.67009\n",
      "Training Progress: \tEpoch 57 [640/8883 (7.19%)]\t\tLoss: 0.47937\n",
      "Training Progress: \tEpoch 57 [960/8883 (10.79%)]\t\tLoss: 0.70620\n",
      "Training Progress: \tEpoch 57 [1280/8883 (14.39%)]\t\tLoss: 0.67151\n",
      "Training Progress: \tEpoch 57 [1600/8883 (17.99%)]\t\tLoss: 0.67424\n",
      "Training Progress: \tEpoch 57 [1920/8883 (21.58%)]\t\tLoss: 0.53609\n",
      "Training Progress: \tEpoch 57 [2240/8883 (25.18%)]\t\tLoss: 0.79559\n",
      "Training Progress: \tEpoch 57 [2560/8883 (28.78%)]\t\tLoss: 0.74264\n",
      "Training Progress: \tEpoch 57 [2880/8883 (32.37%)]\t\tLoss: 0.59353\n",
      "Training Progress: \tEpoch 57 [3200/8883 (35.97%)]\t\tLoss: 0.63070\n",
      "Training Progress: \tEpoch 57 [3520/8883 (39.57%)]\t\tLoss: 0.77112\n",
      "Training Progress: \tEpoch 57 [3840/8883 (43.17%)]\t\tLoss: 0.71963\n",
      "Training Progress: \tEpoch 57 [4160/8883 (46.76%)]\t\tLoss: 0.55833\n",
      "Training Progress: \tEpoch 57 [4480/8883 (50.36%)]\t\tLoss: 0.42930\n",
      "Training Progress: \tEpoch 57 [4800/8883 (53.96%)]\t\tLoss: 0.55404\n",
      "Training Progress: \tEpoch 57 [5120/8883 (57.55%)]\t\tLoss: 0.72117\n",
      "Training Progress: \tEpoch 57 [5440/8883 (61.15%)]\t\tLoss: 0.81170\n",
      "Training Progress: \tEpoch 57 [5760/8883 (64.75%)]\t\tLoss: 0.60653\n",
      "Training Progress: \tEpoch 57 [6080/8883 (68.35%)]\t\tLoss: 0.39584\n",
      "Training Progress: \tEpoch 57 [6400/8883 (71.94%)]\t\tLoss: 0.66994\n",
      "Training Progress: \tEpoch 57 [6720/8883 (75.54%)]\t\tLoss: 0.59628\n",
      "Training Progress: \tEpoch 57 [7040/8883 (79.14%)]\t\tLoss: 0.71860\n",
      "Training Progress: \tEpoch 57 [7360/8883 (82.73%)]\t\tLoss: 0.65860\n",
      "Training Progress: \tEpoch 57 [7680/8883 (86.33%)]\t\tLoss: 0.49364\n",
      "Training Progress: \tEpoch 57 [8000/8883 (89.93%)]\t\tLoss: 0.50264\n",
      "Training Progress: \tEpoch 57 [8320/8883 (93.53%)]\t\tLoss: 0.56895\n",
      "Training Progress: \tEpoch 57 [8640/8883 (97.12%)]\t\tLoss: 0.56334\n",
      "\tTrain loss: 0.01449, Accuracy: 6898/8883 (77.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1438/1692 (84.00%)\n",
      "\tTest loss: 0.00149, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/8883 (0.00%)]\t\tLoss: 0.72614\n",
      "Training Progress: \tEpoch 58 [320/8883 (3.60%)]\t\tLoss: 0.72467\n",
      "Training Progress: \tEpoch 58 [640/8883 (7.19%)]\t\tLoss: 0.58034\n",
      "Training Progress: \tEpoch 58 [960/8883 (10.79%)]\t\tLoss: 0.64548\n",
      "Training Progress: \tEpoch 58 [1280/8883 (14.39%)]\t\tLoss: 0.65423\n",
      "Training Progress: \tEpoch 58 [1600/8883 (17.99%)]\t\tLoss: 0.47573\n",
      "Training Progress: \tEpoch 58 [1920/8883 (21.58%)]\t\tLoss: 0.58270\n",
      "Training Progress: \tEpoch 58 [2240/8883 (25.18%)]\t\tLoss: 0.65027\n",
      "Training Progress: \tEpoch 58 [2560/8883 (28.78%)]\t\tLoss: 0.71734\n",
      "Training Progress: \tEpoch 58 [2880/8883 (32.37%)]\t\tLoss: 0.57859\n",
      "Training Progress: \tEpoch 58 [3200/8883 (35.97%)]\t\tLoss: 0.50321\n",
      "Training Progress: \tEpoch 58 [3520/8883 (39.57%)]\t\tLoss: 0.64391\n",
      "Training Progress: \tEpoch 58 [3840/8883 (43.17%)]\t\tLoss: 0.66846\n",
      "Training Progress: \tEpoch 58 [4160/8883 (46.76%)]\t\tLoss: 0.45312\n",
      "Training Progress: \tEpoch 58 [4480/8883 (50.36%)]\t\tLoss: 0.45369\n",
      "Training Progress: \tEpoch 58 [4800/8883 (53.96%)]\t\tLoss: 0.45684\n",
      "Training Progress: \tEpoch 58 [5120/8883 (57.55%)]\t\tLoss: 0.80597\n",
      "Training Progress: \tEpoch 58 [5440/8883 (61.15%)]\t\tLoss: 0.56676\n",
      "Training Progress: \tEpoch 58 [5760/8883 (64.75%)]\t\tLoss: 0.57218\n",
      "Training Progress: \tEpoch 58 [6080/8883 (68.35%)]\t\tLoss: 0.59048\n",
      "Training Progress: \tEpoch 58 [6400/8883 (71.94%)]\t\tLoss: 0.47990\n",
      "Training Progress: \tEpoch 58 [6720/8883 (75.54%)]\t\tLoss: 0.73010\n",
      "Training Progress: \tEpoch 58 [7040/8883 (79.14%)]\t\tLoss: 0.70299\n",
      "Training Progress: \tEpoch 58 [7360/8883 (82.73%)]\t\tLoss: 0.78305\n",
      "Training Progress: \tEpoch 58 [7680/8883 (86.33%)]\t\tLoss: 0.46619\n",
      "Training Progress: \tEpoch 58 [8000/8883 (89.93%)]\t\tLoss: 0.42102\n",
      "Training Progress: \tEpoch 58 [8320/8883 (93.53%)]\t\tLoss: 0.67748\n",
      "Training Progress: \tEpoch 58 [8640/8883 (97.12%)]\t\tLoss: 0.48941\n",
      "\tTrain loss: 0.01468, Accuracy: 6891/8883 (77.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1402/1692 (82.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 524/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/8883 (0.00%)]\t\tLoss: 0.46343\n",
      "Training Progress: \tEpoch 59 [320/8883 (3.60%)]\t\tLoss: 0.61535\n",
      "Training Progress: \tEpoch 59 [640/8883 (7.19%)]\t\tLoss: 0.64990\n",
      "Training Progress: \tEpoch 59 [960/8883 (10.79%)]\t\tLoss: 0.64738\n",
      "Training Progress: \tEpoch 59 [1280/8883 (14.39%)]\t\tLoss: 0.67243\n",
      "Training Progress: \tEpoch 59 [1600/8883 (17.99%)]\t\tLoss: 0.71706\n",
      "Training Progress: \tEpoch 59 [1920/8883 (21.58%)]\t\tLoss: 0.48116\n",
      "Training Progress: \tEpoch 59 [2240/8883 (25.18%)]\t\tLoss: 0.64861\n",
      "Training Progress: \tEpoch 59 [2560/8883 (28.78%)]\t\tLoss: 1.07511\n",
      "Training Progress: \tEpoch 59 [2880/8883 (32.37%)]\t\tLoss: 0.69126\n",
      "Training Progress: \tEpoch 59 [3200/8883 (35.97%)]\t\tLoss: 0.51625\n",
      "Training Progress: \tEpoch 59 [3520/8883 (39.57%)]\t\tLoss: 1.04035\n",
      "Training Progress: \tEpoch 59 [3840/8883 (43.17%)]\t\tLoss: 0.57153\n",
      "Training Progress: \tEpoch 59 [4160/8883 (46.76%)]\t\tLoss: 0.40866\n",
      "Training Progress: \tEpoch 59 [4480/8883 (50.36%)]\t\tLoss: 0.43017\n",
      "Training Progress: \tEpoch 59 [4800/8883 (53.96%)]\t\tLoss: 0.46126\n",
      "Training Progress: \tEpoch 59 [5120/8883 (57.55%)]\t\tLoss: 0.57454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 59 [5440/8883 (61.15%)]\t\tLoss: 0.45205\n",
      "Training Progress: \tEpoch 59 [5760/8883 (64.75%)]\t\tLoss: 0.43808\n",
      "Training Progress: \tEpoch 59 [6080/8883 (68.35%)]\t\tLoss: 0.62644\n",
      "Training Progress: \tEpoch 59 [6400/8883 (71.94%)]\t\tLoss: 0.49116\n",
      "Training Progress: \tEpoch 59 [6720/8883 (75.54%)]\t\tLoss: 0.76387\n",
      "Training Progress: \tEpoch 59 [7040/8883 (79.14%)]\t\tLoss: 0.73331\n",
      "Training Progress: \tEpoch 59 [7360/8883 (82.73%)]\t\tLoss: 0.51892\n",
      "Training Progress: \tEpoch 59 [7680/8883 (86.33%)]\t\tLoss: 0.54894\n",
      "Training Progress: \tEpoch 59 [8000/8883 (89.93%)]\t\tLoss: 0.58265\n",
      "Training Progress: \tEpoch 59 [8320/8883 (93.53%)]\t\tLoss: 0.72718\n",
      "Training Progress: \tEpoch 59 [8640/8883 (97.12%)]\t\tLoss: 0.72641\n",
      "\tTrain loss: 0.01438, Accuracy: 6855/8883 (77.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1434/1692 (84.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 568/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/8883 (0.00%)]\t\tLoss: 0.51863\n",
      "Training Progress: \tEpoch 60 [320/8883 (3.60%)]\t\tLoss: 0.50162\n",
      "Training Progress: \tEpoch 60 [640/8883 (7.19%)]\t\tLoss: 0.37141\n",
      "Training Progress: \tEpoch 60 [960/8883 (10.79%)]\t\tLoss: 0.56529\n",
      "Training Progress: \tEpoch 60 [1280/8883 (14.39%)]\t\tLoss: 0.58521\n",
      "Training Progress: \tEpoch 60 [1600/8883 (17.99%)]\t\tLoss: 0.67068\n",
      "Training Progress: \tEpoch 60 [1920/8883 (21.58%)]\t\tLoss: 0.44777\n",
      "Training Progress: \tEpoch 60 [2240/8883 (25.18%)]\t\tLoss: 0.45233\n",
      "Training Progress: \tEpoch 60 [2560/8883 (28.78%)]\t\tLoss: 0.54677\n",
      "Training Progress: \tEpoch 60 [2880/8883 (32.37%)]\t\tLoss: 0.68241\n",
      "Training Progress: \tEpoch 60 [3200/8883 (35.97%)]\t\tLoss: 0.46149\n",
      "Training Progress: \tEpoch 60 [3520/8883 (39.57%)]\t\tLoss: 0.70926\n",
      "Training Progress: \tEpoch 60 [3840/8883 (43.17%)]\t\tLoss: 0.86573\n",
      "Training Progress: \tEpoch 60 [4160/8883 (46.76%)]\t\tLoss: 0.40297\n",
      "Training Progress: \tEpoch 60 [4480/8883 (50.36%)]\t\tLoss: 0.33800\n",
      "Training Progress: \tEpoch 60 [4800/8883 (53.96%)]\t\tLoss: 0.41752\n",
      "Training Progress: \tEpoch 60 [5120/8883 (57.55%)]\t\tLoss: 0.50494\n",
      "Training Progress: \tEpoch 60 [5440/8883 (61.15%)]\t\tLoss: 0.74090\n",
      "Training Progress: \tEpoch 60 [5760/8883 (64.75%)]\t\tLoss: 0.55929\n",
      "Training Progress: \tEpoch 60 [6080/8883 (68.35%)]\t\tLoss: 0.34767\n",
      "Training Progress: \tEpoch 60 [6400/8883 (71.94%)]\t\tLoss: 0.55363\n",
      "Training Progress: \tEpoch 60 [6720/8883 (75.54%)]\t\tLoss: 0.59558\n",
      "Training Progress: \tEpoch 60 [7040/8883 (79.14%)]\t\tLoss: 0.71629\n",
      "Training Progress: \tEpoch 60 [7360/8883 (82.73%)]\t\tLoss: 0.49043\n",
      "Training Progress: \tEpoch 60 [7680/8883 (86.33%)]\t\tLoss: 0.51335\n",
      "Training Progress: \tEpoch 60 [8000/8883 (89.93%)]\t\tLoss: 0.69429\n",
      "Training Progress: \tEpoch 60 [8320/8883 (93.53%)]\t\tLoss: 0.69039\n",
      "Training Progress: \tEpoch 60 [8640/8883 (97.12%)]\t\tLoss: 0.62550\n",
      "\tTrain loss: 0.01409, Accuracy: 6954/8883 (78.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1428/1692 (84.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/8883 (0.00%)]\t\tLoss: 0.54213\n",
      "Training Progress: \tEpoch 61 [320/8883 (3.60%)]\t\tLoss: 0.54999\n",
      "Training Progress: \tEpoch 61 [640/8883 (7.19%)]\t\tLoss: 0.30047\n",
      "Training Progress: \tEpoch 61 [960/8883 (10.79%)]\t\tLoss: 0.75338\n",
      "Training Progress: \tEpoch 61 [1280/8883 (14.39%)]\t\tLoss: 0.47513\n",
      "Training Progress: \tEpoch 61 [1600/8883 (17.99%)]\t\tLoss: 0.43663\n",
      "Training Progress: \tEpoch 61 [1920/8883 (21.58%)]\t\tLoss: 0.37199\n",
      "Training Progress: \tEpoch 61 [2240/8883 (25.18%)]\t\tLoss: 0.74401\n",
      "Training Progress: \tEpoch 61 [2560/8883 (28.78%)]\t\tLoss: 0.75339\n",
      "Training Progress: \tEpoch 61 [2880/8883 (32.37%)]\t\tLoss: 0.53388\n",
      "Training Progress: \tEpoch 61 [3200/8883 (35.97%)]\t\tLoss: 0.49065\n",
      "Training Progress: \tEpoch 61 [3520/8883 (39.57%)]\t\tLoss: 0.73241\n",
      "Training Progress: \tEpoch 61 [3840/8883 (43.17%)]\t\tLoss: 0.73463\n",
      "Training Progress: \tEpoch 61 [4160/8883 (46.76%)]\t\tLoss: 0.55331\n",
      "Training Progress: \tEpoch 61 [4480/8883 (50.36%)]\t\tLoss: 0.37111\n",
      "Training Progress: \tEpoch 61 [4800/8883 (53.96%)]\t\tLoss: 0.53032\n",
      "Training Progress: \tEpoch 61 [5120/8883 (57.55%)]\t\tLoss: 0.76634\n",
      "Training Progress: \tEpoch 61 [5440/8883 (61.15%)]\t\tLoss: 0.59532\n",
      "Training Progress: \tEpoch 61 [5760/8883 (64.75%)]\t\tLoss: 0.47720\n",
      "Training Progress: \tEpoch 61 [6080/8883 (68.35%)]\t\tLoss: 0.42769\n",
      "Training Progress: \tEpoch 61 [6400/8883 (71.94%)]\t\tLoss: 0.48211\n",
      "Training Progress: \tEpoch 61 [6720/8883 (75.54%)]\t\tLoss: 0.90632\n",
      "Training Progress: \tEpoch 61 [7040/8883 (79.14%)]\t\tLoss: 0.74285\n",
      "Training Progress: \tEpoch 61 [7360/8883 (82.73%)]\t\tLoss: 0.59527\n",
      "Training Progress: \tEpoch 61 [7680/8883 (86.33%)]\t\tLoss: 0.65011\n",
      "Training Progress: \tEpoch 61 [8000/8883 (89.93%)]\t\tLoss: 0.45622\n",
      "Training Progress: \tEpoch 61 [8320/8883 (93.53%)]\t\tLoss: 0.66961\n",
      "Training Progress: \tEpoch 61 [8640/8883 (97.12%)]\t\tLoss: 0.48811\n",
      "\tTrain loss: 0.01383, Accuracy: 6959/8883 (78.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1442/1692 (85.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/8883 (0.00%)]\t\tLoss: 0.49907\n",
      "Training Progress: \tEpoch 62 [320/8883 (3.60%)]\t\tLoss: 0.53248\n",
      "Training Progress: \tEpoch 62 [640/8883 (7.19%)]\t\tLoss: 0.43610\n",
      "Training Progress: \tEpoch 62 [960/8883 (10.79%)]\t\tLoss: 0.59438\n",
      "Training Progress: \tEpoch 62 [1280/8883 (14.39%)]\t\tLoss: 0.75884\n",
      "Training Progress: \tEpoch 62 [1600/8883 (17.99%)]\t\tLoss: 0.67267\n",
      "Training Progress: \tEpoch 62 [1920/8883 (21.58%)]\t\tLoss: 0.56437\n",
      "Training Progress: \tEpoch 62 [2240/8883 (25.18%)]\t\tLoss: 0.66091\n",
      "Training Progress: \tEpoch 62 [2560/8883 (28.78%)]\t\tLoss: 0.87462\n",
      "Training Progress: \tEpoch 62 [2880/8883 (32.37%)]\t\tLoss: 0.51635\n",
      "Training Progress: \tEpoch 62 [3200/8883 (35.97%)]\t\tLoss: 0.46029\n",
      "Training Progress: \tEpoch 62 [3520/8883 (39.57%)]\t\tLoss: 1.00954\n",
      "Training Progress: \tEpoch 62 [3840/8883 (43.17%)]\t\tLoss: 0.80312\n",
      "Training Progress: \tEpoch 62 [4160/8883 (46.76%)]\t\tLoss: 0.38381\n",
      "Training Progress: \tEpoch 62 [4480/8883 (50.36%)]\t\tLoss: 0.39943\n",
      "Training Progress: \tEpoch 62 [4800/8883 (53.96%)]\t\tLoss: 0.39275\n",
      "Training Progress: \tEpoch 62 [5120/8883 (57.55%)]\t\tLoss: 0.58827\n",
      "Training Progress: \tEpoch 62 [5440/8883 (61.15%)]\t\tLoss: 0.48885\n",
      "Training Progress: \tEpoch 62 [5760/8883 (64.75%)]\t\tLoss: 0.48003\n",
      "Training Progress: \tEpoch 62 [6080/8883 (68.35%)]\t\tLoss: 0.56079\n",
      "Training Progress: \tEpoch 62 [6400/8883 (71.94%)]\t\tLoss: 0.58256\n",
      "Training Progress: \tEpoch 62 [6720/8883 (75.54%)]\t\tLoss: 0.65643\n",
      "Training Progress: \tEpoch 62 [7040/8883 (79.14%)]\t\tLoss: 0.71168\n",
      "Training Progress: \tEpoch 62 [7360/8883 (82.73%)]\t\tLoss: 0.62082\n",
      "Training Progress: \tEpoch 62 [7680/8883 (86.33%)]\t\tLoss: 0.44357\n",
      "Training Progress: \tEpoch 62 [8000/8883 (89.93%)]\t\tLoss: 0.44637\n",
      "Training Progress: \tEpoch 62 [8320/8883 (93.53%)]\t\tLoss: 0.80600\n",
      "Training Progress: \tEpoch 62 [8640/8883 (97.12%)]\t\tLoss: 0.63009\n",
      "\tTrain loss: 0.01425, Accuracy: 6930/8883 (78.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1433/1692 (84.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/8883 (0.00%)]\t\tLoss: 0.52672\n",
      "Training Progress: \tEpoch 63 [320/8883 (3.60%)]\t\tLoss: 0.57113\n",
      "Training Progress: \tEpoch 63 [640/8883 (7.19%)]\t\tLoss: 0.60512\n",
      "Training Progress: \tEpoch 63 [960/8883 (10.79%)]\t\tLoss: 0.54048\n",
      "Training Progress: \tEpoch 63 [1280/8883 (14.39%)]\t\tLoss: 0.60121\n",
      "Training Progress: \tEpoch 63 [1600/8883 (17.99%)]\t\tLoss: 0.49942\n",
      "Training Progress: \tEpoch 63 [1920/8883 (21.58%)]\t\tLoss: 0.57680\n",
      "Training Progress: \tEpoch 63 [2240/8883 (25.18%)]\t\tLoss: 0.52347\n",
      "Training Progress: \tEpoch 63 [2560/8883 (28.78%)]\t\tLoss: 0.65347\n",
      "Training Progress: \tEpoch 63 [2880/8883 (32.37%)]\t\tLoss: 0.58507\n",
      "Training Progress: \tEpoch 63 [3200/8883 (35.97%)]\t\tLoss: 0.71572\n",
      "Training Progress: \tEpoch 63 [3520/8883 (39.57%)]\t\tLoss: 0.63470\n",
      "Training Progress: \tEpoch 63 [3840/8883 (43.17%)]\t\tLoss: 1.01461\n",
      "Training Progress: \tEpoch 63 [4160/8883 (46.76%)]\t\tLoss: 0.46155\n",
      "Training Progress: \tEpoch 63 [4480/8883 (50.36%)]\t\tLoss: 0.55191\n",
      "Training Progress: \tEpoch 63 [4800/8883 (53.96%)]\t\tLoss: 0.61886\n",
      "Training Progress: \tEpoch 63 [5120/8883 (57.55%)]\t\tLoss: 0.58337\n",
      "Training Progress: \tEpoch 63 [5440/8883 (61.15%)]\t\tLoss: 0.55925\n",
      "Training Progress: \tEpoch 63 [5760/8883 (64.75%)]\t\tLoss: 0.51432\n",
      "Training Progress: \tEpoch 63 [6080/8883 (68.35%)]\t\tLoss: 0.43146\n",
      "Training Progress: \tEpoch 63 [6400/8883 (71.94%)]\t\tLoss: 0.52867\n",
      "Training Progress: \tEpoch 63 [6720/8883 (75.54%)]\t\tLoss: 0.54313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [7040/8883 (79.14%)]\t\tLoss: 0.56140\n",
      "Training Progress: \tEpoch 63 [7360/8883 (82.73%)]\t\tLoss: 0.53043\n",
      "Training Progress: \tEpoch 63 [7680/8883 (86.33%)]\t\tLoss: 0.62378\n",
      "Training Progress: \tEpoch 63 [8000/8883 (89.93%)]\t\tLoss: 0.44503\n",
      "Training Progress: \tEpoch 63 [8320/8883 (93.53%)]\t\tLoss: 0.49997\n",
      "Training Progress: \tEpoch 63 [8640/8883 (97.12%)]\t\tLoss: 0.50864\n",
      "\tTrain loss: 0.01392, Accuracy: 6935/8883 (78.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1441/1692 (85.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 511/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/8883 (0.00%)]\t\tLoss: 0.49699\n",
      "Training Progress: \tEpoch 64 [320/8883 (3.60%)]\t\tLoss: 0.55777\n",
      "Training Progress: \tEpoch 64 [640/8883 (7.19%)]\t\tLoss: 0.47463\n",
      "Training Progress: \tEpoch 64 [960/8883 (10.79%)]\t\tLoss: 0.51475\n",
      "Training Progress: \tEpoch 64 [1280/8883 (14.39%)]\t\tLoss: 0.47063\n",
      "Training Progress: \tEpoch 64 [1600/8883 (17.99%)]\t\tLoss: 0.65807\n",
      "Training Progress: \tEpoch 64 [1920/8883 (21.58%)]\t\tLoss: 0.32880\n",
      "Training Progress: \tEpoch 64 [2240/8883 (25.18%)]\t\tLoss: 0.66787\n",
      "Training Progress: \tEpoch 64 [2560/8883 (28.78%)]\t\tLoss: 0.63733\n",
      "Training Progress: \tEpoch 64 [2880/8883 (32.37%)]\t\tLoss: 0.45387\n",
      "Training Progress: \tEpoch 64 [3200/8883 (35.97%)]\t\tLoss: 0.50820\n",
      "Training Progress: \tEpoch 64 [3520/8883 (39.57%)]\t\tLoss: 0.58872\n",
      "Training Progress: \tEpoch 64 [3840/8883 (43.17%)]\t\tLoss: 0.58224\n",
      "Training Progress: \tEpoch 64 [4160/8883 (46.76%)]\t\tLoss: 0.38380\n",
      "Training Progress: \tEpoch 64 [4480/8883 (50.36%)]\t\tLoss: 0.30807\n",
      "Training Progress: \tEpoch 64 [4800/8883 (53.96%)]\t\tLoss: 0.41559\n",
      "Training Progress: \tEpoch 64 [5120/8883 (57.55%)]\t\tLoss: 0.80286\n",
      "Training Progress: \tEpoch 64 [5440/8883 (61.15%)]\t\tLoss: 0.62107\n",
      "Training Progress: \tEpoch 64 [5760/8883 (64.75%)]\t\tLoss: 0.44447\n",
      "Training Progress: \tEpoch 64 [6080/8883 (68.35%)]\t\tLoss: 0.37596\n",
      "Training Progress: \tEpoch 64 [6400/8883 (71.94%)]\t\tLoss: 0.46155\n",
      "Training Progress: \tEpoch 64 [6720/8883 (75.54%)]\t\tLoss: 0.59339\n",
      "Training Progress: \tEpoch 64 [7040/8883 (79.14%)]\t\tLoss: 0.71884\n",
      "Training Progress: \tEpoch 64 [7360/8883 (82.73%)]\t\tLoss: 0.56540\n",
      "Training Progress: \tEpoch 64 [7680/8883 (86.33%)]\t\tLoss: 0.54876\n",
      "Training Progress: \tEpoch 64 [8000/8883 (89.93%)]\t\tLoss: 0.47820\n",
      "Training Progress: \tEpoch 64 [8320/8883 (93.53%)]\t\tLoss: 0.71669\n",
      "Training Progress: \tEpoch 64 [8640/8883 (97.12%)]\t\tLoss: 0.71589\n",
      "\tTrain loss: 0.01363, Accuracy: 7021/8883 (79.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1454/1692 (85.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 542/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/8883 (0.00%)]\t\tLoss: 0.55074\n",
      "Training Progress: \tEpoch 65 [320/8883 (3.60%)]\t\tLoss: 0.59507\n",
      "Training Progress: \tEpoch 65 [640/8883 (7.19%)]\t\tLoss: 0.26246\n",
      "Training Progress: \tEpoch 65 [960/8883 (10.79%)]\t\tLoss: 0.44425\n",
      "Training Progress: \tEpoch 65 [1280/8883 (14.39%)]\t\tLoss: 0.55766\n",
      "Training Progress: \tEpoch 65 [1600/8883 (17.99%)]\t\tLoss: 0.54510\n",
      "Training Progress: \tEpoch 65 [1920/8883 (21.58%)]\t\tLoss: 0.38452\n",
      "Training Progress: \tEpoch 65 [2240/8883 (25.18%)]\t\tLoss: 0.61365\n",
      "Training Progress: \tEpoch 65 [2560/8883 (28.78%)]\t\tLoss: 0.59868\n",
      "Training Progress: \tEpoch 65 [2880/8883 (32.37%)]\t\tLoss: 0.71769\n",
      "Training Progress: \tEpoch 65 [3200/8883 (35.97%)]\t\tLoss: 0.39369\n",
      "Training Progress: \tEpoch 65 [3520/8883 (39.57%)]\t\tLoss: 0.72704\n",
      "Training Progress: \tEpoch 65 [3840/8883 (43.17%)]\t\tLoss: 0.57058\n",
      "Training Progress: \tEpoch 65 [4160/8883 (46.76%)]\t\tLoss: 0.40737\n",
      "Training Progress: \tEpoch 65 [4480/8883 (50.36%)]\t\tLoss: 0.46608\n",
      "Training Progress: \tEpoch 65 [4800/8883 (53.96%)]\t\tLoss: 0.36345\n",
      "Training Progress: \tEpoch 65 [5120/8883 (57.55%)]\t\tLoss: 0.61673\n",
      "Training Progress: \tEpoch 65 [5440/8883 (61.15%)]\t\tLoss: 0.79581\n",
      "Training Progress: \tEpoch 65 [5760/8883 (64.75%)]\t\tLoss: 0.66675\n",
      "Training Progress: \tEpoch 65 [6080/8883 (68.35%)]\t\tLoss: 0.45422\n",
      "Training Progress: \tEpoch 65 [6400/8883 (71.94%)]\t\tLoss: 0.39575\n",
      "Training Progress: \tEpoch 65 [6720/8883 (75.54%)]\t\tLoss: 0.73447\n",
      "Training Progress: \tEpoch 65 [7040/8883 (79.14%)]\t\tLoss: 1.01172\n",
      "Training Progress: \tEpoch 65 [7360/8883 (82.73%)]\t\tLoss: 0.58622\n",
      "Training Progress: \tEpoch 65 [7680/8883 (86.33%)]\t\tLoss: 0.59773\n",
      "Training Progress: \tEpoch 65 [8000/8883 (89.93%)]\t\tLoss: 0.49283\n",
      "Training Progress: \tEpoch 65 [8320/8883 (93.53%)]\t\tLoss: 0.67090\n",
      "Training Progress: \tEpoch 65 [8640/8883 (97.12%)]\t\tLoss: 0.44446\n",
      "\tTrain loss: 0.01364, Accuracy: 6982/8883 (78.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1467/1692 (86.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 537/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/8883 (0.00%)]\t\tLoss: 0.57313\n",
      "Training Progress: \tEpoch 66 [320/8883 (3.60%)]\t\tLoss: 0.75966\n",
      "Training Progress: \tEpoch 66 [640/8883 (7.19%)]\t\tLoss: 0.46998\n",
      "Training Progress: \tEpoch 66 [960/8883 (10.79%)]\t\tLoss: 0.65589\n",
      "Training Progress: \tEpoch 66 [1280/8883 (14.39%)]\t\tLoss: 0.50833\n",
      "Training Progress: \tEpoch 66 [1600/8883 (17.99%)]\t\tLoss: 0.48974\n",
      "Training Progress: \tEpoch 66 [1920/8883 (21.58%)]\t\tLoss: 0.48545\n",
      "Training Progress: \tEpoch 66 [2240/8883 (25.18%)]\t\tLoss: 0.63120\n",
      "Training Progress: \tEpoch 66 [2560/8883 (28.78%)]\t\tLoss: 0.65193\n",
      "Training Progress: \tEpoch 66 [2880/8883 (32.37%)]\t\tLoss: 0.56072\n",
      "Training Progress: \tEpoch 66 [3200/8883 (35.97%)]\t\tLoss: 0.54966\n",
      "Training Progress: \tEpoch 66 [3520/8883 (39.57%)]\t\tLoss: 0.57660\n",
      "Training Progress: \tEpoch 66 [3840/8883 (43.17%)]\t\tLoss: 0.59786\n",
      "Training Progress: \tEpoch 66 [4160/8883 (46.76%)]\t\tLoss: 0.61271\n",
      "Training Progress: \tEpoch 66 [4480/8883 (50.36%)]\t\tLoss: 0.49579\n",
      "Training Progress: \tEpoch 66 [4800/8883 (53.96%)]\t\tLoss: 0.39152\n",
      "Training Progress: \tEpoch 66 [5120/8883 (57.55%)]\t\tLoss: 0.81048\n",
      "Training Progress: \tEpoch 66 [5440/8883 (61.15%)]\t\tLoss: 0.56032\n",
      "Training Progress: \tEpoch 66 [5760/8883 (64.75%)]\t\tLoss: 0.43465\n",
      "Training Progress: \tEpoch 66 [6080/8883 (68.35%)]\t\tLoss: 0.56276\n",
      "Training Progress: \tEpoch 66 [6400/8883 (71.94%)]\t\tLoss: 0.43485\n",
      "Training Progress: \tEpoch 66 [6720/8883 (75.54%)]\t\tLoss: 0.63625\n",
      "Training Progress: \tEpoch 66 [7040/8883 (79.14%)]\t\tLoss: 0.80903\n",
      "Training Progress: \tEpoch 66 [7360/8883 (82.73%)]\t\tLoss: 0.52658\n",
      "Training Progress: \tEpoch 66 [7680/8883 (86.33%)]\t\tLoss: 0.53675\n",
      "Training Progress: \tEpoch 66 [8000/8883 (89.93%)]\t\tLoss: 0.58492\n",
      "Training Progress: \tEpoch 66 [8320/8883 (93.53%)]\t\tLoss: 0.63100\n",
      "Training Progress: \tEpoch 66 [8640/8883 (97.12%)]\t\tLoss: 0.57906\n",
      "\tTrain loss: 0.01347, Accuracy: 6999/8883 (78.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1464/1692 (86.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/8883 (0.00%)]\t\tLoss: 0.58873\n",
      "Training Progress: \tEpoch 67 [320/8883 (3.60%)]\t\tLoss: 0.80569\n",
      "Training Progress: \tEpoch 67 [640/8883 (7.19%)]\t\tLoss: 0.56030\n",
      "Training Progress: \tEpoch 67 [960/8883 (10.79%)]\t\tLoss: 0.49297\n",
      "Training Progress: \tEpoch 67 [1280/8883 (14.39%)]\t\tLoss: 0.54843\n",
      "Training Progress: \tEpoch 67 [1600/8883 (17.99%)]\t\tLoss: 0.63819\n",
      "Training Progress: \tEpoch 67 [1920/8883 (21.58%)]\t\tLoss: 0.47812\n",
      "Training Progress: \tEpoch 67 [2240/8883 (25.18%)]\t\tLoss: 0.71077\n",
      "Training Progress: \tEpoch 67 [2560/8883 (28.78%)]\t\tLoss: 0.56242\n",
      "Training Progress: \tEpoch 67 [2880/8883 (32.37%)]\t\tLoss: 0.48376\n",
      "Training Progress: \tEpoch 67 [3200/8883 (35.97%)]\t\tLoss: 0.62219\n",
      "Training Progress: \tEpoch 67 [3520/8883 (39.57%)]\t\tLoss: 0.82002\n",
      "Training Progress: \tEpoch 67 [3840/8883 (43.17%)]\t\tLoss: 0.67621\n",
      "Training Progress: \tEpoch 67 [4160/8883 (46.76%)]\t\tLoss: 0.44078\n",
      "Training Progress: \tEpoch 67 [4480/8883 (50.36%)]\t\tLoss: 0.38579\n",
      "Training Progress: \tEpoch 67 [4800/8883 (53.96%)]\t\tLoss: 0.35114\n",
      "Training Progress: \tEpoch 67 [5120/8883 (57.55%)]\t\tLoss: 0.60033\n",
      "Training Progress: \tEpoch 67 [5440/8883 (61.15%)]\t\tLoss: 0.56311\n",
      "Training Progress: \tEpoch 67 [5760/8883 (64.75%)]\t\tLoss: 0.71368\n",
      "Training Progress: \tEpoch 67 [6080/8883 (68.35%)]\t\tLoss: 0.41380\n",
      "Training Progress: \tEpoch 67 [6400/8883 (71.94%)]\t\tLoss: 0.59619\n",
      "Training Progress: \tEpoch 67 [6720/8883 (75.54%)]\t\tLoss: 0.63314\n",
      "Training Progress: \tEpoch 67 [7040/8883 (79.14%)]\t\tLoss: 0.75971\n",
      "Training Progress: \tEpoch 67 [7360/8883 (82.73%)]\t\tLoss: 0.51367\n",
      "Training Progress: \tEpoch 67 [7680/8883 (86.33%)]\t\tLoss: 0.49366\n",
      "Training Progress: \tEpoch 67 [8000/8883 (89.93%)]\t\tLoss: 0.55581\n",
      "Training Progress: \tEpoch 67 [8320/8883 (93.53%)]\t\tLoss: 0.55300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 67 [8640/8883 (97.12%)]\t\tLoss: 0.57575\n",
      "\tTrain loss: 0.01339, Accuracy: 7014/8883 (78.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1425/1692 (84.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/8883 (0.00%)]\t\tLoss: 0.62325\n",
      "Training Progress: \tEpoch 68 [320/8883 (3.60%)]\t\tLoss: 0.54859\n",
      "Training Progress: \tEpoch 68 [640/8883 (7.19%)]\t\tLoss: 0.49295\n",
      "Training Progress: \tEpoch 68 [960/8883 (10.79%)]\t\tLoss: 0.55038\n",
      "Training Progress: \tEpoch 68 [1280/8883 (14.39%)]\t\tLoss: 0.41493\n",
      "Training Progress: \tEpoch 68 [1600/8883 (17.99%)]\t\tLoss: 0.60489\n",
      "Training Progress: \tEpoch 68 [1920/8883 (21.58%)]\t\tLoss: 0.50215\n",
      "Training Progress: \tEpoch 68 [2240/8883 (25.18%)]\t\tLoss: 0.65383\n",
      "Training Progress: \tEpoch 68 [2560/8883 (28.78%)]\t\tLoss: 0.58427\n",
      "Training Progress: \tEpoch 68 [2880/8883 (32.37%)]\t\tLoss: 0.45591\n",
      "Training Progress: \tEpoch 68 [3200/8883 (35.97%)]\t\tLoss: 0.59625\n",
      "Training Progress: \tEpoch 68 [3520/8883 (39.57%)]\t\tLoss: 0.77174\n",
      "Training Progress: \tEpoch 68 [3840/8883 (43.17%)]\t\tLoss: 0.85634\n",
      "Training Progress: \tEpoch 68 [4160/8883 (46.76%)]\t\tLoss: 0.50203\n",
      "Training Progress: \tEpoch 68 [4480/8883 (50.36%)]\t\tLoss: 0.43419\n",
      "Training Progress: \tEpoch 68 [4800/8883 (53.96%)]\t\tLoss: 0.40507\n",
      "Training Progress: \tEpoch 68 [5120/8883 (57.55%)]\t\tLoss: 0.66552\n",
      "Training Progress: \tEpoch 68 [5440/8883 (61.15%)]\t\tLoss: 0.63478\n",
      "Training Progress: \tEpoch 68 [5760/8883 (64.75%)]\t\tLoss: 0.53799\n",
      "Training Progress: \tEpoch 68 [6080/8883 (68.35%)]\t\tLoss: 0.43801\n",
      "Training Progress: \tEpoch 68 [6400/8883 (71.94%)]\t\tLoss: 0.87190\n",
      "Training Progress: \tEpoch 68 [6720/8883 (75.54%)]\t\tLoss: 0.55977\n",
      "Training Progress: \tEpoch 68 [7040/8883 (79.14%)]\t\tLoss: 0.79436\n",
      "Training Progress: \tEpoch 68 [7360/8883 (82.73%)]\t\tLoss: 0.55221\n",
      "Training Progress: \tEpoch 68 [7680/8883 (86.33%)]\t\tLoss: 0.44033\n",
      "Training Progress: \tEpoch 68 [8000/8883 (89.93%)]\t\tLoss: 0.45608\n",
      "Training Progress: \tEpoch 68 [8320/8883 (93.53%)]\t\tLoss: 0.48080\n",
      "Training Progress: \tEpoch 68 [8640/8883 (97.12%)]\t\tLoss: 0.51601\n",
      "\tTrain loss: 0.01346, Accuracy: 7011/8883 (78.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1458/1692 (86.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/8883 (0.00%)]\t\tLoss: 0.60779\n",
      "Training Progress: \tEpoch 69 [320/8883 (3.60%)]\t\tLoss: 0.41830\n",
      "Training Progress: \tEpoch 69 [640/8883 (7.19%)]\t\tLoss: 0.38176\n",
      "Training Progress: \tEpoch 69 [960/8883 (10.79%)]\t\tLoss: 0.51104\n",
      "Training Progress: \tEpoch 69 [1280/8883 (14.39%)]\t\tLoss: 0.59007\n",
      "Training Progress: \tEpoch 69 [1600/8883 (17.99%)]\t\tLoss: 0.78668\n",
      "Training Progress: \tEpoch 69 [1920/8883 (21.58%)]\t\tLoss: 0.53832\n",
      "Training Progress: \tEpoch 69 [2240/8883 (25.18%)]\t\tLoss: 0.53637\n",
      "Training Progress: \tEpoch 69 [2560/8883 (28.78%)]\t\tLoss: 0.69772\n",
      "Training Progress: \tEpoch 69 [2880/8883 (32.37%)]\t\tLoss: 0.59284\n",
      "Training Progress: \tEpoch 69 [3200/8883 (35.97%)]\t\tLoss: 0.48387\n",
      "Training Progress: \tEpoch 69 [3520/8883 (39.57%)]\t\tLoss: 0.83007\n",
      "Training Progress: \tEpoch 69 [3840/8883 (43.17%)]\t\tLoss: 0.56156\n",
      "Training Progress: \tEpoch 69 [4160/8883 (46.76%)]\t\tLoss: 0.45092\n",
      "Training Progress: \tEpoch 69 [4480/8883 (50.36%)]\t\tLoss: 0.48182\n",
      "Training Progress: \tEpoch 69 [4800/8883 (53.96%)]\t\tLoss: 0.27211\n",
      "Training Progress: \tEpoch 69 [5120/8883 (57.55%)]\t\tLoss: 0.57565\n",
      "Training Progress: \tEpoch 69 [5440/8883 (61.15%)]\t\tLoss: 0.76027\n",
      "Training Progress: \tEpoch 69 [5760/8883 (64.75%)]\t\tLoss: 0.75275\n",
      "Training Progress: \tEpoch 69 [6080/8883 (68.35%)]\t\tLoss: 0.45728\n",
      "Training Progress: \tEpoch 69 [6400/8883 (71.94%)]\t\tLoss: 0.53254\n",
      "Training Progress: \tEpoch 69 [6720/8883 (75.54%)]\t\tLoss: 0.74539\n",
      "Training Progress: \tEpoch 69 [7040/8883 (79.14%)]\t\tLoss: 0.80235\n",
      "Training Progress: \tEpoch 69 [7360/8883 (82.73%)]\t\tLoss: 0.67558\n",
      "Training Progress: \tEpoch 69 [7680/8883 (86.33%)]\t\tLoss: 0.46011\n",
      "Training Progress: \tEpoch 69 [8000/8883 (89.93%)]\t\tLoss: 0.39194\n",
      "Training Progress: \tEpoch 69 [8320/8883 (93.53%)]\t\tLoss: 0.93431\n",
      "Training Progress: \tEpoch 69 [8640/8883 (97.12%)]\t\tLoss: 0.54387\n",
      "\tTrain loss: 0.01368, Accuracy: 6980/8883 (78.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1443/1692 (85.00%)\n",
      "\tTest loss: 0.00170, Accuracy: 510/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/8883 (0.00%)]\t\tLoss: 0.65498\n",
      "Training Progress: \tEpoch 70 [320/8883 (3.60%)]\t\tLoss: 0.61877\n",
      "Training Progress: \tEpoch 70 [640/8883 (7.19%)]\t\tLoss: 0.60652\n",
      "Training Progress: \tEpoch 70 [960/8883 (10.79%)]\t\tLoss: 0.55381\n",
      "Training Progress: \tEpoch 70 [1280/8883 (14.39%)]\t\tLoss: 0.76888\n",
      "Training Progress: \tEpoch 70 [1600/8883 (17.99%)]\t\tLoss: 0.28325\n",
      "Training Progress: \tEpoch 70 [1920/8883 (21.58%)]\t\tLoss: 0.43717\n",
      "Training Progress: \tEpoch 70 [2240/8883 (25.18%)]\t\tLoss: 0.59218\n",
      "Training Progress: \tEpoch 70 [2560/8883 (28.78%)]\t\tLoss: 0.53257\n",
      "Training Progress: \tEpoch 70 [2880/8883 (32.37%)]\t\tLoss: 0.64339\n",
      "Training Progress: \tEpoch 70 [3200/8883 (35.97%)]\t\tLoss: 0.52585\n",
      "Training Progress: \tEpoch 70 [3520/8883 (39.57%)]\t\tLoss: 0.68617\n",
      "Training Progress: \tEpoch 70 [3840/8883 (43.17%)]\t\tLoss: 0.58387\n",
      "Training Progress: \tEpoch 70 [4160/8883 (46.76%)]\t\tLoss: 0.32269\n",
      "Training Progress: \tEpoch 70 [4480/8883 (50.36%)]\t\tLoss: 0.50471\n",
      "Training Progress: \tEpoch 70 [4800/8883 (53.96%)]\t\tLoss: 0.39030\n",
      "Training Progress: \tEpoch 70 [5120/8883 (57.55%)]\t\tLoss: 0.55128\n",
      "Training Progress: \tEpoch 70 [5440/8883 (61.15%)]\t\tLoss: 0.41847\n",
      "Training Progress: \tEpoch 70 [5760/8883 (64.75%)]\t\tLoss: 0.38216\n",
      "Training Progress: \tEpoch 70 [6080/8883 (68.35%)]\t\tLoss: 0.40826\n",
      "Training Progress: \tEpoch 70 [6400/8883 (71.94%)]\t\tLoss: 0.59655\n",
      "Training Progress: \tEpoch 70 [6720/8883 (75.54%)]\t\tLoss: 0.50412\n",
      "Training Progress: \tEpoch 70 [7040/8883 (79.14%)]\t\tLoss: 0.62355\n",
      "Training Progress: \tEpoch 70 [7360/8883 (82.73%)]\t\tLoss: 0.61347\n",
      "Training Progress: \tEpoch 70 [7680/8883 (86.33%)]\t\tLoss: 0.37739\n",
      "Training Progress: \tEpoch 70 [8000/8883 (89.93%)]\t\tLoss: 0.46586\n",
      "Training Progress: \tEpoch 70 [8320/8883 (93.53%)]\t\tLoss: 0.61579\n",
      "Training Progress: \tEpoch 70 [8640/8883 (97.12%)]\t\tLoss: 0.52225\n",
      "\tTrain loss: 0.01331, Accuracy: 7023/8883 (79.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1461/1692 (86.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 557/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/8883 (0.00%)]\t\tLoss: 0.38694\n",
      "Training Progress: \tEpoch 71 [320/8883 (3.60%)]\t\tLoss: 0.42136\n",
      "Training Progress: \tEpoch 71 [640/8883 (7.19%)]\t\tLoss: 0.34116\n",
      "Training Progress: \tEpoch 71 [960/8883 (10.79%)]\t\tLoss: 0.38675\n",
      "Training Progress: \tEpoch 71 [1280/8883 (14.39%)]\t\tLoss: 0.40777\n",
      "Training Progress: \tEpoch 71 [1600/8883 (17.99%)]\t\tLoss: 0.45180\n",
      "Training Progress: \tEpoch 71 [1920/8883 (21.58%)]\t\tLoss: 0.59083\n",
      "Training Progress: \tEpoch 71 [2240/8883 (25.18%)]\t\tLoss: 0.52953\n",
      "Training Progress: \tEpoch 71 [2560/8883 (28.78%)]\t\tLoss: 0.65273\n",
      "Training Progress: \tEpoch 71 [2880/8883 (32.37%)]\t\tLoss: 0.71989\n",
      "Training Progress: \tEpoch 71 [3200/8883 (35.97%)]\t\tLoss: 0.43393\n",
      "Training Progress: \tEpoch 71 [3520/8883 (39.57%)]\t\tLoss: 0.58387\n",
      "Training Progress: \tEpoch 71 [3840/8883 (43.17%)]\t\tLoss: 0.58834\n",
      "Training Progress: \tEpoch 71 [4160/8883 (46.76%)]\t\tLoss: 0.38941\n",
      "Training Progress: \tEpoch 71 [4480/8883 (50.36%)]\t\tLoss: 0.31252\n",
      "Training Progress: \tEpoch 71 [4800/8883 (53.96%)]\t\tLoss: 0.29276\n",
      "Training Progress: \tEpoch 71 [5120/8883 (57.55%)]\t\tLoss: 0.51065\n",
      "Training Progress: \tEpoch 71 [5440/8883 (61.15%)]\t\tLoss: 0.56674\n",
      "Training Progress: \tEpoch 71 [5760/8883 (64.75%)]\t\tLoss: 0.64726\n",
      "Training Progress: \tEpoch 71 [6080/8883 (68.35%)]\t\tLoss: 0.40350\n",
      "Training Progress: \tEpoch 71 [6400/8883 (71.94%)]\t\tLoss: 0.39446\n",
      "Training Progress: \tEpoch 71 [6720/8883 (75.54%)]\t\tLoss: 0.62022\n",
      "Training Progress: \tEpoch 71 [7040/8883 (79.14%)]\t\tLoss: 0.60695\n",
      "Training Progress: \tEpoch 71 [7360/8883 (82.73%)]\t\tLoss: 0.56264\n",
      "Training Progress: \tEpoch 71 [7680/8883 (86.33%)]\t\tLoss: 0.50417\n",
      "Training Progress: \tEpoch 71 [8000/8883 (89.93%)]\t\tLoss: 0.49250\n",
      "Training Progress: \tEpoch 71 [8320/8883 (93.53%)]\t\tLoss: 0.66180\n",
      "Training Progress: \tEpoch 71 [8640/8883 (97.12%)]\t\tLoss: 0.54263\n",
      "\tTrain loss: 0.01328, Accuracy: 7003/8883 (78.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1449/1692 (85.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/8883 (0.00%)]\t\tLoss: 0.57150\n",
      "Training Progress: \tEpoch 72 [320/8883 (3.60%)]\t\tLoss: 0.50216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 72 [640/8883 (7.19%)]\t\tLoss: 0.33386\n",
      "Training Progress: \tEpoch 72 [960/8883 (10.79%)]\t\tLoss: 0.38895\n",
      "Training Progress: \tEpoch 72 [1280/8883 (14.39%)]\t\tLoss: 0.53839\n",
      "Training Progress: \tEpoch 72 [1600/8883 (17.99%)]\t\tLoss: 0.51072\n",
      "Training Progress: \tEpoch 72 [1920/8883 (21.58%)]\t\tLoss: 0.32969\n",
      "Training Progress: \tEpoch 72 [2240/8883 (25.18%)]\t\tLoss: 0.53479\n",
      "Training Progress: \tEpoch 72 [2560/8883 (28.78%)]\t\tLoss: 0.56951\n",
      "Training Progress: \tEpoch 72 [2880/8883 (32.37%)]\t\tLoss: 0.52156\n",
      "Training Progress: \tEpoch 72 [3200/8883 (35.97%)]\t\tLoss: 0.51600\n",
      "Training Progress: \tEpoch 72 [3520/8883 (39.57%)]\t\tLoss: 0.55716\n",
      "Training Progress: \tEpoch 72 [3840/8883 (43.17%)]\t\tLoss: 0.79500\n",
      "Training Progress: \tEpoch 72 [4160/8883 (46.76%)]\t\tLoss: 0.29295\n",
      "Training Progress: \tEpoch 72 [4480/8883 (50.36%)]\t\tLoss: 0.54669\n",
      "Training Progress: \tEpoch 72 [4800/8883 (53.96%)]\t\tLoss: 0.44044\n",
      "Training Progress: \tEpoch 72 [5120/8883 (57.55%)]\t\tLoss: 0.56557\n",
      "Training Progress: \tEpoch 72 [5440/8883 (61.15%)]\t\tLoss: 0.54221\n",
      "Training Progress: \tEpoch 72 [5760/8883 (64.75%)]\t\tLoss: 0.68085\n",
      "Training Progress: \tEpoch 72 [6080/8883 (68.35%)]\t\tLoss: 0.48086\n",
      "Training Progress: \tEpoch 72 [6400/8883 (71.94%)]\t\tLoss: 0.37033\n",
      "Training Progress: \tEpoch 72 [6720/8883 (75.54%)]\t\tLoss: 0.62254\n",
      "Training Progress: \tEpoch 72 [7040/8883 (79.14%)]\t\tLoss: 0.55684\n",
      "Training Progress: \tEpoch 72 [7360/8883 (82.73%)]\t\tLoss: 0.53505\n",
      "Training Progress: \tEpoch 72 [7680/8883 (86.33%)]\t\tLoss: 0.47778\n",
      "Training Progress: \tEpoch 72 [8000/8883 (89.93%)]\t\tLoss: 0.42970\n",
      "Training Progress: \tEpoch 72 [8320/8883 (93.53%)]\t\tLoss: 0.67404\n",
      "Training Progress: \tEpoch 72 [8640/8883 (97.12%)]\t\tLoss: 0.50019\n",
      "\tTrain loss: 0.01303, Accuracy: 7058/8883 (79.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1461/1692 (86.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/8883 (0.00%)]\t\tLoss: 0.63094\n",
      "Training Progress: \tEpoch 73 [320/8883 (3.60%)]\t\tLoss: 0.62545\n",
      "Training Progress: \tEpoch 73 [640/8883 (7.19%)]\t\tLoss: 0.41901\n",
      "Training Progress: \tEpoch 73 [960/8883 (10.79%)]\t\tLoss: 0.62710\n",
      "Training Progress: \tEpoch 73 [1280/8883 (14.39%)]\t\tLoss: 0.61476\n",
      "Training Progress: \tEpoch 73 [1600/8883 (17.99%)]\t\tLoss: 0.61534\n",
      "Training Progress: \tEpoch 73 [1920/8883 (21.58%)]\t\tLoss: 0.41399\n",
      "Training Progress: \tEpoch 73 [2240/8883 (25.18%)]\t\tLoss: 0.58758\n",
      "Training Progress: \tEpoch 73 [2560/8883 (28.78%)]\t\tLoss: 0.65330\n",
      "Training Progress: \tEpoch 73 [2880/8883 (32.37%)]\t\tLoss: 0.43654\n",
      "Training Progress: \tEpoch 73 [3200/8883 (35.97%)]\t\tLoss: 0.60967\n",
      "Training Progress: \tEpoch 73 [3520/8883 (39.57%)]\t\tLoss: 0.98305\n",
      "Training Progress: \tEpoch 73 [3840/8883 (43.17%)]\t\tLoss: 0.66551\n",
      "Training Progress: \tEpoch 73 [4160/8883 (46.76%)]\t\tLoss: 0.34180\n",
      "Training Progress: \tEpoch 73 [4480/8883 (50.36%)]\t\tLoss: 0.48210\n",
      "Training Progress: \tEpoch 73 [4800/8883 (53.96%)]\t\tLoss: 0.44901\n",
      "Training Progress: \tEpoch 73 [5120/8883 (57.55%)]\t\tLoss: 0.78376\n",
      "Training Progress: \tEpoch 73 [5440/8883 (61.15%)]\t\tLoss: 0.54075\n",
      "Training Progress: \tEpoch 73 [5760/8883 (64.75%)]\t\tLoss: 0.44994\n",
      "Training Progress: \tEpoch 73 [6080/8883 (68.35%)]\t\tLoss: 0.39716\n",
      "Training Progress: \tEpoch 73 [6400/8883 (71.94%)]\t\tLoss: 0.37260\n",
      "Training Progress: \tEpoch 73 [6720/8883 (75.54%)]\t\tLoss: 0.81983\n",
      "Training Progress: \tEpoch 73 [7040/8883 (79.14%)]\t\tLoss: 0.63467\n",
      "Training Progress: \tEpoch 73 [7360/8883 (82.73%)]\t\tLoss: 0.65488\n",
      "Training Progress: \tEpoch 73 [7680/8883 (86.33%)]\t\tLoss: 0.42343\n",
      "Training Progress: \tEpoch 73 [8000/8883 (89.93%)]\t\tLoss: 0.60803\n",
      "Training Progress: \tEpoch 73 [8320/8883 (93.53%)]\t\tLoss: 0.66100\n",
      "Training Progress: \tEpoch 73 [8640/8883 (97.12%)]\t\tLoss: 0.62234\n",
      "\tTrain loss: 0.01325, Accuracy: 7016/8883 (78.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1461/1692 (86.00%)\n",
      "\tTest loss: 0.00167, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/8883 (0.00%)]\t\tLoss: 0.59153\n",
      "Training Progress: \tEpoch 74 [320/8883 (3.60%)]\t\tLoss: 0.61457\n",
      "Training Progress: \tEpoch 74 [640/8883 (7.19%)]\t\tLoss: 0.49191\n",
      "Training Progress: \tEpoch 74 [960/8883 (10.79%)]\t\tLoss: 0.43104\n",
      "Training Progress: \tEpoch 74 [1280/8883 (14.39%)]\t\tLoss: 0.49831\n",
      "Training Progress: \tEpoch 74 [1600/8883 (17.99%)]\t\tLoss: 0.65564\n",
      "Training Progress: \tEpoch 74 [1920/8883 (21.58%)]\t\tLoss: 0.58536\n",
      "Training Progress: \tEpoch 74 [2240/8883 (25.18%)]\t\tLoss: 0.70198\n",
      "Training Progress: \tEpoch 74 [2560/8883 (28.78%)]\t\tLoss: 0.59009\n",
      "Training Progress: \tEpoch 74 [2880/8883 (32.37%)]\t\tLoss: 0.63704\n",
      "Training Progress: \tEpoch 74 [3200/8883 (35.97%)]\t\tLoss: 0.53027\n",
      "Training Progress: \tEpoch 74 [3520/8883 (39.57%)]\t\tLoss: 0.66314\n",
      "Training Progress: \tEpoch 74 [3840/8883 (43.17%)]\t\tLoss: 0.63138\n",
      "Training Progress: \tEpoch 74 [4160/8883 (46.76%)]\t\tLoss: 0.51023\n",
      "Training Progress: \tEpoch 74 [4480/8883 (50.36%)]\t\tLoss: 0.41132\n",
      "Training Progress: \tEpoch 74 [4800/8883 (53.96%)]\t\tLoss: 0.31425\n",
      "Training Progress: \tEpoch 74 [5120/8883 (57.55%)]\t\tLoss: 0.49214\n",
      "Training Progress: \tEpoch 74 [5440/8883 (61.15%)]\t\tLoss: 0.44267\n",
      "Training Progress: \tEpoch 74 [5760/8883 (64.75%)]\t\tLoss: 0.33807\n",
      "Training Progress: \tEpoch 74 [6080/8883 (68.35%)]\t\tLoss: 0.35089\n",
      "Training Progress: \tEpoch 74 [6400/8883 (71.94%)]\t\tLoss: 0.55248\n",
      "Training Progress: \tEpoch 74 [6720/8883 (75.54%)]\t\tLoss: 0.70221\n",
      "Training Progress: \tEpoch 74 [7040/8883 (79.14%)]\t\tLoss: 0.78879\n",
      "Training Progress: \tEpoch 74 [7360/8883 (82.73%)]\t\tLoss: 0.45033\n",
      "Training Progress: \tEpoch 74 [7680/8883 (86.33%)]\t\tLoss: 0.61756\n",
      "Training Progress: \tEpoch 74 [8000/8883 (89.93%)]\t\tLoss: 0.40849\n",
      "Training Progress: \tEpoch 74 [8320/8883 (93.53%)]\t\tLoss: 0.65551\n",
      "Training Progress: \tEpoch 74 [8640/8883 (97.12%)]\t\tLoss: 0.58705\n",
      "\tTrain loss: 0.01284, Accuracy: 7065/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00171, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/8883 (0.00%)]\t\tLoss: 0.56719\n",
      "Training Progress: \tEpoch 75 [320/8883 (3.60%)]\t\tLoss: 0.50599\n",
      "Training Progress: \tEpoch 75 [640/8883 (7.19%)]\t\tLoss: 0.37647\n",
      "Training Progress: \tEpoch 75 [960/8883 (10.79%)]\t\tLoss: 0.64484\n",
      "Training Progress: \tEpoch 75 [1280/8883 (14.39%)]\t\tLoss: 0.68405\n",
      "Training Progress: \tEpoch 75 [1600/8883 (17.99%)]\t\tLoss: 0.43401\n",
      "Training Progress: \tEpoch 75 [1920/8883 (21.58%)]\t\tLoss: 0.37342\n",
      "Training Progress: \tEpoch 75 [2240/8883 (25.18%)]\t\tLoss: 0.45338\n",
      "Training Progress: \tEpoch 75 [2560/8883 (28.78%)]\t\tLoss: 0.60469\n",
      "Training Progress: \tEpoch 75 [2880/8883 (32.37%)]\t\tLoss: 0.58099\n",
      "Training Progress: \tEpoch 75 [3200/8883 (35.97%)]\t\tLoss: 0.58965\n",
      "Training Progress: \tEpoch 75 [3520/8883 (39.57%)]\t\tLoss: 0.68979\n",
      "Training Progress: \tEpoch 75 [3840/8883 (43.17%)]\t\tLoss: 0.52270\n",
      "Training Progress: \tEpoch 75 [4160/8883 (46.76%)]\t\tLoss: 0.46551\n",
      "Training Progress: \tEpoch 75 [4480/8883 (50.36%)]\t\tLoss: 0.37751\n",
      "Training Progress: \tEpoch 75 [4800/8883 (53.96%)]\t\tLoss: 0.48428\n",
      "Training Progress: \tEpoch 75 [5120/8883 (57.55%)]\t\tLoss: 0.66219\n",
      "Training Progress: \tEpoch 75 [5440/8883 (61.15%)]\t\tLoss: 0.50625\n",
      "Training Progress: \tEpoch 75 [5760/8883 (64.75%)]\t\tLoss: 0.54875\n",
      "Training Progress: \tEpoch 75 [6080/8883 (68.35%)]\t\tLoss: 0.45928\n",
      "Training Progress: \tEpoch 75 [6400/8883 (71.94%)]\t\tLoss: 0.59494\n",
      "Training Progress: \tEpoch 75 [6720/8883 (75.54%)]\t\tLoss: 0.61961\n",
      "Training Progress: \tEpoch 75 [7040/8883 (79.14%)]\t\tLoss: 0.61325\n",
      "Training Progress: \tEpoch 75 [7360/8883 (82.73%)]\t\tLoss: 0.48405\n",
      "Training Progress: \tEpoch 75 [7680/8883 (86.33%)]\t\tLoss: 0.40069\n",
      "Training Progress: \tEpoch 75 [8000/8883 (89.93%)]\t\tLoss: 0.43989\n",
      "Training Progress: \tEpoch 75 [8320/8883 (93.53%)]\t\tLoss: 0.80527\n",
      "Training Progress: \tEpoch 75 [8640/8883 (97.12%)]\t\tLoss: 0.66106\n",
      "\tTrain loss: 0.01290, Accuracy: 7064/8883 (79.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1476/1692 (87.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/8883 (0.00%)]\t\tLoss: 0.39162\n",
      "Training Progress: \tEpoch 76 [320/8883 (3.60%)]\t\tLoss: 0.40737\n",
      "Training Progress: \tEpoch 76 [640/8883 (7.19%)]\t\tLoss: 0.30151\n",
      "Training Progress: \tEpoch 76 [960/8883 (10.79%)]\t\tLoss: 0.65359\n",
      "Training Progress: \tEpoch 76 [1280/8883 (14.39%)]\t\tLoss: 0.46252\n",
      "Training Progress: \tEpoch 76 [1600/8883 (17.99%)]\t\tLoss: 0.54913\n",
      "Training Progress: \tEpoch 76 [1920/8883 (21.58%)]\t\tLoss: 0.45788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 76 [2240/8883 (25.18%)]\t\tLoss: 0.51672\n",
      "Training Progress: \tEpoch 76 [2560/8883 (28.78%)]\t\tLoss: 0.72397\n",
      "Training Progress: \tEpoch 76 [2880/8883 (32.37%)]\t\tLoss: 0.79160\n",
      "Training Progress: \tEpoch 76 [3200/8883 (35.97%)]\t\tLoss: 0.46549\n",
      "Training Progress: \tEpoch 76 [3520/8883 (39.57%)]\t\tLoss: 0.65342\n",
      "Training Progress: \tEpoch 76 [3840/8883 (43.17%)]\t\tLoss: 0.59398\n",
      "Training Progress: \tEpoch 76 [4160/8883 (46.76%)]\t\tLoss: 0.32290\n",
      "Training Progress: \tEpoch 76 [4480/8883 (50.36%)]\t\tLoss: 0.33339\n",
      "Training Progress: \tEpoch 76 [4800/8883 (53.96%)]\t\tLoss: 0.30133\n",
      "Training Progress: \tEpoch 76 [5120/8883 (57.55%)]\t\tLoss: 0.45151\n",
      "Training Progress: \tEpoch 76 [5440/8883 (61.15%)]\t\tLoss: 0.75416\n",
      "Training Progress: \tEpoch 76 [5760/8883 (64.75%)]\t\tLoss: 0.43242\n",
      "Training Progress: \tEpoch 76 [6080/8883 (68.35%)]\t\tLoss: 0.25357\n",
      "Training Progress: \tEpoch 76 [6400/8883 (71.94%)]\t\tLoss: 0.36751\n",
      "Training Progress: \tEpoch 76 [6720/8883 (75.54%)]\t\tLoss: 0.65866\n",
      "Training Progress: \tEpoch 76 [7040/8883 (79.14%)]\t\tLoss: 0.84903\n",
      "Training Progress: \tEpoch 76 [7360/8883 (82.73%)]\t\tLoss: 0.60619\n",
      "Training Progress: \tEpoch 76 [7680/8883 (86.33%)]\t\tLoss: 0.34786\n",
      "Training Progress: \tEpoch 76 [8000/8883 (89.93%)]\t\tLoss: 0.40720\n",
      "Training Progress: \tEpoch 76 [8320/8883 (93.53%)]\t\tLoss: 0.50051\n",
      "Training Progress: \tEpoch 76 [8640/8883 (97.12%)]\t\tLoss: 0.55785\n",
      "\tTrain loss: 0.01244, Accuracy: 7088/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1492/1692 (88.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/8883 (0.00%)]\t\tLoss: 0.46940\n",
      "Training Progress: \tEpoch 77 [320/8883 (3.60%)]\t\tLoss: 0.51982\n",
      "Training Progress: \tEpoch 77 [640/8883 (7.19%)]\t\tLoss: 0.54885\n",
      "Training Progress: \tEpoch 77 [960/8883 (10.79%)]\t\tLoss: 0.46906\n",
      "Training Progress: \tEpoch 77 [1280/8883 (14.39%)]\t\tLoss: 0.64703\n",
      "Training Progress: \tEpoch 77 [1600/8883 (17.99%)]\t\tLoss: 0.59565\n",
      "Training Progress: \tEpoch 77 [1920/8883 (21.58%)]\t\tLoss: 0.59208\n",
      "Training Progress: \tEpoch 77 [2240/8883 (25.18%)]\t\tLoss: 0.48836\n",
      "Training Progress: \tEpoch 77 [2560/8883 (28.78%)]\t\tLoss: 0.58613\n",
      "Training Progress: \tEpoch 77 [2880/8883 (32.37%)]\t\tLoss: 0.47200\n",
      "Training Progress: \tEpoch 77 [3200/8883 (35.97%)]\t\tLoss: 0.53483\n",
      "Training Progress: \tEpoch 77 [3520/8883 (39.57%)]\t\tLoss: 0.83474\n",
      "Training Progress: \tEpoch 77 [3840/8883 (43.17%)]\t\tLoss: 0.63849\n",
      "Training Progress: \tEpoch 77 [4160/8883 (46.76%)]\t\tLoss: 0.31279\n",
      "Training Progress: \tEpoch 77 [4480/8883 (50.36%)]\t\tLoss: 0.47980\n",
      "Training Progress: \tEpoch 77 [4800/8883 (53.96%)]\t\tLoss: 0.32675\n",
      "Training Progress: \tEpoch 77 [5120/8883 (57.55%)]\t\tLoss: 0.53039\n",
      "Training Progress: \tEpoch 77 [5440/8883 (61.15%)]\t\tLoss: 0.51397\n",
      "Training Progress: \tEpoch 77 [5760/8883 (64.75%)]\t\tLoss: 0.43962\n",
      "Training Progress: \tEpoch 77 [6080/8883 (68.35%)]\t\tLoss: 0.22915\n",
      "Training Progress: \tEpoch 77 [6400/8883 (71.94%)]\t\tLoss: 0.46140\n",
      "Training Progress: \tEpoch 77 [6720/8883 (75.54%)]\t\tLoss: 0.58849\n",
      "Training Progress: \tEpoch 77 [7040/8883 (79.14%)]\t\tLoss: 0.71389\n",
      "Training Progress: \tEpoch 77 [7360/8883 (82.73%)]\t\tLoss: 0.56676\n",
      "Training Progress: \tEpoch 77 [7680/8883 (86.33%)]\t\tLoss: 0.43589\n",
      "Training Progress: \tEpoch 77 [8000/8883 (89.93%)]\t\tLoss: 0.56435\n",
      "Training Progress: \tEpoch 77 [8320/8883 (93.53%)]\t\tLoss: 0.66095\n",
      "Training Progress: \tEpoch 77 [8640/8883 (97.12%)]\t\tLoss: 0.74412\n",
      "\tTrain loss: 0.01279, Accuracy: 7082/8883 (79.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1478/1692 (87.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 500/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/8883 (0.00%)]\t\tLoss: 0.45727\n",
      "Training Progress: \tEpoch 78 [320/8883 (3.60%)]\t\tLoss: 0.39006\n",
      "Training Progress: \tEpoch 78 [640/8883 (7.19%)]\t\tLoss: 0.46181\n",
      "Training Progress: \tEpoch 78 [960/8883 (10.79%)]\t\tLoss: 0.51738\n",
      "Training Progress: \tEpoch 78 [1280/8883 (14.39%)]\t\tLoss: 0.70329\n",
      "Training Progress: \tEpoch 78 [1600/8883 (17.99%)]\t\tLoss: 0.36438\n",
      "Training Progress: \tEpoch 78 [1920/8883 (21.58%)]\t\tLoss: 0.40134\n",
      "Training Progress: \tEpoch 78 [2240/8883 (25.18%)]\t\tLoss: 0.47888\n",
      "Training Progress: \tEpoch 78 [2560/8883 (28.78%)]\t\tLoss: 0.48517\n",
      "Training Progress: \tEpoch 78 [2880/8883 (32.37%)]\t\tLoss: 0.59343\n",
      "Training Progress: \tEpoch 78 [3200/8883 (35.97%)]\t\tLoss: 0.70706\n",
      "Training Progress: \tEpoch 78 [3520/8883 (39.57%)]\t\tLoss: 0.86184\n",
      "Training Progress: \tEpoch 78 [3840/8883 (43.17%)]\t\tLoss: 0.63762\n",
      "Training Progress: \tEpoch 78 [4160/8883 (46.76%)]\t\tLoss: 0.40612\n",
      "Training Progress: \tEpoch 78 [4480/8883 (50.36%)]\t\tLoss: 0.45216\n",
      "Training Progress: \tEpoch 78 [4800/8883 (53.96%)]\t\tLoss: 0.33054\n",
      "Training Progress: \tEpoch 78 [5120/8883 (57.55%)]\t\tLoss: 0.56977\n",
      "Training Progress: \tEpoch 78 [5440/8883 (61.15%)]\t\tLoss: 0.61644\n",
      "Training Progress: \tEpoch 78 [5760/8883 (64.75%)]\t\tLoss: 0.44388\n",
      "Training Progress: \tEpoch 78 [6080/8883 (68.35%)]\t\tLoss: 0.43400\n",
      "Training Progress: \tEpoch 78 [6400/8883 (71.94%)]\t\tLoss: 0.45382\n",
      "Training Progress: \tEpoch 78 [6720/8883 (75.54%)]\t\tLoss: 0.59120\n",
      "Training Progress: \tEpoch 78 [7040/8883 (79.14%)]\t\tLoss: 0.61496\n",
      "Training Progress: \tEpoch 78 [7360/8883 (82.73%)]\t\tLoss: 0.57747\n",
      "Training Progress: \tEpoch 78 [7680/8883 (86.33%)]\t\tLoss: 0.40599\n",
      "Training Progress: \tEpoch 78 [8000/8883 (89.93%)]\t\tLoss: 0.54248\n",
      "Training Progress: \tEpoch 78 [8320/8883 (93.53%)]\t\tLoss: 0.53772\n",
      "Training Progress: \tEpoch 78 [8640/8883 (97.12%)]\t\tLoss: 0.49413\n",
      "\tTrain loss: 0.01271, Accuracy: 7080/8883 (79.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1469/1692 (86.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/8883 (0.00%)]\t\tLoss: 0.39316\n",
      "Training Progress: \tEpoch 79 [320/8883 (3.60%)]\t\tLoss: 0.65577\n",
      "Training Progress: \tEpoch 79 [640/8883 (7.19%)]\t\tLoss: 0.47224\n",
      "Training Progress: \tEpoch 79 [960/8883 (10.79%)]\t\tLoss: 0.57234\n",
      "Training Progress: \tEpoch 79 [1280/8883 (14.39%)]\t\tLoss: 0.45279\n",
      "Training Progress: \tEpoch 79 [1600/8883 (17.99%)]\t\tLoss: 0.39769\n",
      "Training Progress: \tEpoch 79 [1920/8883 (21.58%)]\t\tLoss: 0.49275\n",
      "Training Progress: \tEpoch 79 [2240/8883 (25.18%)]\t\tLoss: 0.50648\n",
      "Training Progress: \tEpoch 79 [2560/8883 (28.78%)]\t\tLoss: 0.71600\n",
      "Training Progress: \tEpoch 79 [2880/8883 (32.37%)]\t\tLoss: 0.47073\n",
      "Training Progress: \tEpoch 79 [3200/8883 (35.97%)]\t\tLoss: 0.42931\n",
      "Training Progress: \tEpoch 79 [3520/8883 (39.57%)]\t\tLoss: 0.55678\n",
      "Training Progress: \tEpoch 79 [3840/8883 (43.17%)]\t\tLoss: 0.68147\n",
      "Training Progress: \tEpoch 79 [4160/8883 (46.76%)]\t\tLoss: 0.35942\n",
      "Training Progress: \tEpoch 79 [4480/8883 (50.36%)]\t\tLoss: 0.42968\n",
      "Training Progress: \tEpoch 79 [4800/8883 (53.96%)]\t\tLoss: 0.42090\n",
      "Training Progress: \tEpoch 79 [5120/8883 (57.55%)]\t\tLoss: 0.57962\n",
      "Training Progress: \tEpoch 79 [5440/8883 (61.15%)]\t\tLoss: 0.51759\n",
      "Training Progress: \tEpoch 79 [5760/8883 (64.75%)]\t\tLoss: 0.57520\n",
      "Training Progress: \tEpoch 79 [6080/8883 (68.35%)]\t\tLoss: 0.31024\n",
      "Training Progress: \tEpoch 79 [6400/8883 (71.94%)]\t\tLoss: 0.48090\n",
      "Training Progress: \tEpoch 79 [6720/8883 (75.54%)]\t\tLoss: 0.56198\n",
      "Training Progress: \tEpoch 79 [7040/8883 (79.14%)]\t\tLoss: 0.88455\n",
      "Training Progress: \tEpoch 79 [7360/8883 (82.73%)]\t\tLoss: 0.50442\n",
      "Training Progress: \tEpoch 79 [7680/8883 (86.33%)]\t\tLoss: 0.65477\n",
      "Training Progress: \tEpoch 79 [8000/8883 (89.93%)]\t\tLoss: 0.54683\n",
      "Training Progress: \tEpoch 79 [8320/8883 (93.53%)]\t\tLoss: 0.61402\n",
      "Training Progress: \tEpoch 79 [8640/8883 (97.12%)]\t\tLoss: 0.77084\n",
      "\tTrain loss: 0.01359, Accuracy: 6984/8883 (78.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 503/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/8883 (0.00%)]\t\tLoss: 0.52183\n",
      "Training Progress: \tEpoch 80 [320/8883 (3.60%)]\t\tLoss: 0.43906\n",
      "Training Progress: \tEpoch 80 [640/8883 (7.19%)]\t\tLoss: 0.38688\n",
      "Training Progress: \tEpoch 80 [960/8883 (10.79%)]\t\tLoss: 0.47611\n",
      "Training Progress: \tEpoch 80 [1280/8883 (14.39%)]\t\tLoss: 0.50953\n",
      "Training Progress: \tEpoch 80 [1600/8883 (17.99%)]\t\tLoss: 0.42984\n",
      "Training Progress: \tEpoch 80 [1920/8883 (21.58%)]\t\tLoss: 0.48030\n",
      "Training Progress: \tEpoch 80 [2240/8883 (25.18%)]\t\tLoss: 0.59286\n",
      "Training Progress: \tEpoch 80 [2560/8883 (28.78%)]\t\tLoss: 0.69821\n",
      "Training Progress: \tEpoch 80 [2880/8883 (32.37%)]\t\tLoss: 0.68055\n",
      "Training Progress: \tEpoch 80 [3200/8883 (35.97%)]\t\tLoss: 0.44947\n",
      "Training Progress: \tEpoch 80 [3520/8883 (39.57%)]\t\tLoss: 0.56894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 80 [3840/8883 (43.17%)]\t\tLoss: 0.72765\n",
      "Training Progress: \tEpoch 80 [4160/8883 (46.76%)]\t\tLoss: 0.47291\n",
      "Training Progress: \tEpoch 80 [4480/8883 (50.36%)]\t\tLoss: 0.52488\n",
      "Training Progress: \tEpoch 80 [4800/8883 (53.96%)]\t\tLoss: 0.30381\n",
      "Training Progress: \tEpoch 80 [5120/8883 (57.55%)]\t\tLoss: 0.65599\n",
      "Training Progress: \tEpoch 80 [5440/8883 (61.15%)]\t\tLoss: 0.57483\n",
      "Training Progress: \tEpoch 80 [5760/8883 (64.75%)]\t\tLoss: 0.40000\n",
      "Training Progress: \tEpoch 80 [6080/8883 (68.35%)]\t\tLoss: 0.55376\n",
      "Training Progress: \tEpoch 80 [6400/8883 (71.94%)]\t\tLoss: 0.50484\n",
      "Training Progress: \tEpoch 80 [6720/8883 (75.54%)]\t\tLoss: 1.34109\n",
      "Training Progress: \tEpoch 80 [7040/8883 (79.14%)]\t\tLoss: 0.62144\n",
      "Training Progress: \tEpoch 80 [7360/8883 (82.73%)]\t\tLoss: 0.60024\n",
      "Training Progress: \tEpoch 80 [7680/8883 (86.33%)]\t\tLoss: 0.46148\n",
      "Training Progress: \tEpoch 80 [8000/8883 (89.93%)]\t\tLoss: 0.43482\n",
      "Training Progress: \tEpoch 80 [8320/8883 (93.53%)]\t\tLoss: 0.53014\n",
      "Training Progress: \tEpoch 80 [8640/8883 (97.12%)]\t\tLoss: 0.46703\n",
      "\tTrain loss: 0.01377, Accuracy: 6953/8883 (78.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1441/1692 (85.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 514/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/8883 (0.00%)]\t\tLoss: 0.37426\n",
      "Training Progress: \tEpoch 81 [320/8883 (3.60%)]\t\tLoss: 0.41683\n",
      "Training Progress: \tEpoch 81 [640/8883 (7.19%)]\t\tLoss: 0.30932\n",
      "Training Progress: \tEpoch 81 [960/8883 (10.79%)]\t\tLoss: 0.47623\n",
      "Training Progress: \tEpoch 81 [1280/8883 (14.39%)]\t\tLoss: 0.41810\n",
      "Training Progress: \tEpoch 81 [1600/8883 (17.99%)]\t\tLoss: 0.36509\n",
      "Training Progress: \tEpoch 81 [1920/8883 (21.58%)]\t\tLoss: 0.38027\n",
      "Training Progress: \tEpoch 81 [2240/8883 (25.18%)]\t\tLoss: 0.44059\n",
      "Training Progress: \tEpoch 81 [2560/8883 (28.78%)]\t\tLoss: 0.68777\n",
      "Training Progress: \tEpoch 81 [2880/8883 (32.37%)]\t\tLoss: 0.37549\n",
      "Training Progress: \tEpoch 81 [3200/8883 (35.97%)]\t\tLoss: 0.54997\n",
      "Training Progress: \tEpoch 81 [3520/8883 (39.57%)]\t\tLoss: 0.82471\n",
      "Training Progress: \tEpoch 81 [3840/8883 (43.17%)]\t\tLoss: 0.62217\n",
      "Training Progress: \tEpoch 81 [4160/8883 (46.76%)]\t\tLoss: 0.38742\n",
      "Training Progress: \tEpoch 81 [4480/8883 (50.36%)]\t\tLoss: 0.58919\n",
      "Training Progress: \tEpoch 81 [4800/8883 (53.96%)]\t\tLoss: 0.33897\n",
      "Training Progress: \tEpoch 81 [5120/8883 (57.55%)]\t\tLoss: 0.68734\n",
      "Training Progress: \tEpoch 81 [5440/8883 (61.15%)]\t\tLoss: 0.46312\n",
      "Training Progress: \tEpoch 81 [5760/8883 (64.75%)]\t\tLoss: 0.40595\n",
      "Training Progress: \tEpoch 81 [6080/8883 (68.35%)]\t\tLoss: 0.38342\n",
      "Training Progress: \tEpoch 81 [6400/8883 (71.94%)]\t\tLoss: 0.62363\n",
      "Training Progress: \tEpoch 81 [6720/8883 (75.54%)]\t\tLoss: 0.66861\n",
      "Training Progress: \tEpoch 81 [7040/8883 (79.14%)]\t\tLoss: 0.76858\n",
      "Training Progress: \tEpoch 81 [7360/8883 (82.73%)]\t\tLoss: 0.60123\n",
      "Training Progress: \tEpoch 81 [7680/8883 (86.33%)]\t\tLoss: 0.60076\n",
      "Training Progress: \tEpoch 81 [8000/8883 (89.93%)]\t\tLoss: 0.55620\n",
      "Training Progress: \tEpoch 81 [8320/8883 (93.53%)]\t\tLoss: 0.57555\n",
      "Training Progress: \tEpoch 81 [8640/8883 (97.12%)]\t\tLoss: 0.39101\n",
      "\tTrain loss: 0.01273, Accuracy: 7075/8883 (79.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1496/1692 (88.00%)\n",
      "\tTest loss: 0.00182, Accuracy: 501/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/8883 (0.00%)]\t\tLoss: 0.49210\n",
      "Training Progress: \tEpoch 82 [320/8883 (3.60%)]\t\tLoss: 0.68878\n",
      "Training Progress: \tEpoch 82 [640/8883 (7.19%)]\t\tLoss: 0.26552\n",
      "Training Progress: \tEpoch 82 [960/8883 (10.79%)]\t\tLoss: 0.57158\n",
      "Training Progress: \tEpoch 82 [1280/8883 (14.39%)]\t\tLoss: 0.60940\n",
      "Training Progress: \tEpoch 82 [1600/8883 (17.99%)]\t\tLoss: 0.29265\n",
      "Training Progress: \tEpoch 82 [1920/8883 (21.58%)]\t\tLoss: 0.35387\n",
      "Training Progress: \tEpoch 82 [2240/8883 (25.18%)]\t\tLoss: 0.57812\n",
      "Training Progress: \tEpoch 82 [2560/8883 (28.78%)]\t\tLoss: 0.86960\n",
      "Training Progress: \tEpoch 82 [2880/8883 (32.37%)]\t\tLoss: 0.46789\n",
      "Training Progress: \tEpoch 82 [3200/8883 (35.97%)]\t\tLoss: 0.39314\n",
      "Training Progress: \tEpoch 82 [3520/8883 (39.57%)]\t\tLoss: 0.63881\n",
      "Training Progress: \tEpoch 82 [3840/8883 (43.17%)]\t\tLoss: 0.64229\n",
      "Training Progress: \tEpoch 82 [4160/8883 (46.76%)]\t\tLoss: 0.36721\n",
      "Training Progress: \tEpoch 82 [4480/8883 (50.36%)]\t\tLoss: 0.43672\n",
      "Training Progress: \tEpoch 82 [4800/8883 (53.96%)]\t\tLoss: 0.37509\n",
      "Training Progress: \tEpoch 82 [5120/8883 (57.55%)]\t\tLoss: 0.43164\n",
      "Training Progress: \tEpoch 82 [5440/8883 (61.15%)]\t\tLoss: 0.48274\n",
      "Training Progress: \tEpoch 82 [5760/8883 (64.75%)]\t\tLoss: 0.37574\n",
      "Training Progress: \tEpoch 82 [6080/8883 (68.35%)]\t\tLoss: 0.40944\n",
      "Training Progress: \tEpoch 82 [6400/8883 (71.94%)]\t\tLoss: 0.48065\n",
      "Training Progress: \tEpoch 82 [6720/8883 (75.54%)]\t\tLoss: 0.54005\n",
      "Training Progress: \tEpoch 82 [7040/8883 (79.14%)]\t\tLoss: 0.66521\n",
      "Training Progress: \tEpoch 82 [7360/8883 (82.73%)]\t\tLoss: 0.51834\n",
      "Training Progress: \tEpoch 82 [7680/8883 (86.33%)]\t\tLoss: 0.36296\n",
      "Training Progress: \tEpoch 82 [8000/8883 (89.93%)]\t\tLoss: 0.56256\n",
      "Training Progress: \tEpoch 82 [8320/8883 (93.53%)]\t\tLoss: 0.46063\n",
      "Training Progress: \tEpoch 82 [8640/8883 (97.12%)]\t\tLoss: 0.73022\n",
      "\tTrain loss: 0.01280, Accuracy: 7063/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1496/1692 (88.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 515/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/8883 (0.00%)]\t\tLoss: 0.63909\n",
      "Training Progress: \tEpoch 83 [320/8883 (3.60%)]\t\tLoss: 0.65443\n",
      "Training Progress: \tEpoch 83 [640/8883 (7.19%)]\t\tLoss: 0.33582\n",
      "Training Progress: \tEpoch 83 [960/8883 (10.79%)]\t\tLoss: 0.47644\n",
      "Training Progress: \tEpoch 83 [1280/8883 (14.39%)]\t\tLoss: 0.62887\n",
      "Training Progress: \tEpoch 83 [1600/8883 (17.99%)]\t\tLoss: 0.61214\n",
      "Training Progress: \tEpoch 83 [1920/8883 (21.58%)]\t\tLoss: 0.79453\n",
      "Training Progress: \tEpoch 83 [2240/8883 (25.18%)]\t\tLoss: 0.45709\n",
      "Training Progress: \tEpoch 83 [2560/8883 (28.78%)]\t\tLoss: 0.66083\n",
      "Training Progress: \tEpoch 83 [2880/8883 (32.37%)]\t\tLoss: 0.41459\n",
      "Training Progress: \tEpoch 83 [3200/8883 (35.97%)]\t\tLoss: 0.42715\n",
      "Training Progress: \tEpoch 83 [3520/8883 (39.57%)]\t\tLoss: 0.69688\n",
      "Training Progress: \tEpoch 83 [3840/8883 (43.17%)]\t\tLoss: 0.71613\n",
      "Training Progress: \tEpoch 83 [4160/8883 (46.76%)]\t\tLoss: 0.32512\n",
      "Training Progress: \tEpoch 83 [4480/8883 (50.36%)]\t\tLoss: 0.28033\n",
      "Training Progress: \tEpoch 83 [4800/8883 (53.96%)]\t\tLoss: 0.50194\n",
      "Training Progress: \tEpoch 83 [5120/8883 (57.55%)]\t\tLoss: 0.56844\n",
      "Training Progress: \tEpoch 83 [5440/8883 (61.15%)]\t\tLoss: 0.80440\n",
      "Training Progress: \tEpoch 83 [5760/8883 (64.75%)]\t\tLoss: 0.54596\n",
      "Training Progress: \tEpoch 83 [6080/8883 (68.35%)]\t\tLoss: 0.46522\n",
      "Training Progress: \tEpoch 83 [6400/8883 (71.94%)]\t\tLoss: 0.60714\n",
      "Training Progress: \tEpoch 83 [6720/8883 (75.54%)]\t\tLoss: 0.58976\n",
      "Training Progress: \tEpoch 83 [7040/8883 (79.14%)]\t\tLoss: 0.64568\n",
      "Training Progress: \tEpoch 83 [7360/8883 (82.73%)]\t\tLoss: 0.49595\n",
      "Training Progress: \tEpoch 83 [7680/8883 (86.33%)]\t\tLoss: 0.53845\n",
      "Training Progress: \tEpoch 83 [8000/8883 (89.93%)]\t\tLoss: 0.40773\n",
      "Training Progress: \tEpoch 83 [8320/8883 (93.53%)]\t\tLoss: 0.64481\n",
      "Training Progress: \tEpoch 83 [8640/8883 (97.12%)]\t\tLoss: 0.58328\n",
      "\tTrain loss: 0.01238, Accuracy: 7111/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/8883 (0.00%)]\t\tLoss: 0.61259\n",
      "Training Progress: \tEpoch 84 [320/8883 (3.60%)]\t\tLoss: 0.63460\n",
      "Training Progress: \tEpoch 84 [640/8883 (7.19%)]\t\tLoss: 0.31248\n",
      "Training Progress: \tEpoch 84 [960/8883 (10.79%)]\t\tLoss: 0.51822\n",
      "Training Progress: \tEpoch 84 [1280/8883 (14.39%)]\t\tLoss: 0.45387\n",
      "Training Progress: \tEpoch 84 [1600/8883 (17.99%)]\t\tLoss: 0.47632\n",
      "Training Progress: \tEpoch 84 [1920/8883 (21.58%)]\t\tLoss: 0.31251\n",
      "Training Progress: \tEpoch 84 [2240/8883 (25.18%)]\t\tLoss: 0.38702\n",
      "Training Progress: \tEpoch 84 [2560/8883 (28.78%)]\t\tLoss: 0.73927\n",
      "Training Progress: \tEpoch 84 [2880/8883 (32.37%)]\t\tLoss: 0.38680\n",
      "Training Progress: \tEpoch 84 [3200/8883 (35.97%)]\t\tLoss: 0.75630\n",
      "Training Progress: \tEpoch 84 [3520/8883 (39.57%)]\t\tLoss: 0.60613\n",
      "Training Progress: \tEpoch 84 [3840/8883 (43.17%)]\t\tLoss: 0.56461\n",
      "Training Progress: \tEpoch 84 [4160/8883 (46.76%)]\t\tLoss: 0.31170\n",
      "Training Progress: \tEpoch 84 [4480/8883 (50.36%)]\t\tLoss: 0.34436\n",
      "Training Progress: \tEpoch 84 [4800/8883 (53.96%)]\t\tLoss: 0.30709\n",
      "Training Progress: \tEpoch 84 [5120/8883 (57.55%)]\t\tLoss: 0.50875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [5440/8883 (61.15%)]\t\tLoss: 0.70047\n",
      "Training Progress: \tEpoch 84 [5760/8883 (64.75%)]\t\tLoss: 0.62136\n",
      "Training Progress: \tEpoch 84 [6080/8883 (68.35%)]\t\tLoss: 0.37533\n",
      "Training Progress: \tEpoch 84 [6400/8883 (71.94%)]\t\tLoss: 0.67591\n",
      "Training Progress: \tEpoch 84 [6720/8883 (75.54%)]\t\tLoss: 0.55282\n",
      "Training Progress: \tEpoch 84 [7040/8883 (79.14%)]\t\tLoss: 0.64340\n",
      "Training Progress: \tEpoch 84 [7360/8883 (82.73%)]\t\tLoss: 0.55397\n",
      "Training Progress: \tEpoch 84 [7680/8883 (86.33%)]\t\tLoss: 0.48611\n",
      "Training Progress: \tEpoch 84 [8000/8883 (89.93%)]\t\tLoss: 0.37512\n",
      "Training Progress: \tEpoch 84 [8320/8883 (93.53%)]\t\tLoss: 0.41365\n",
      "Training Progress: \tEpoch 84 [8640/8883 (97.12%)]\t\tLoss: 0.56385\n",
      "\tTrain loss: 0.01289, Accuracy: 7070/8883 (79.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/8883 (0.00%)]\t\tLoss: 0.48015\n",
      "Training Progress: \tEpoch 85 [320/8883 (3.60%)]\t\tLoss: 0.63260\n",
      "Training Progress: \tEpoch 85 [640/8883 (7.19%)]\t\tLoss: 0.39504\n",
      "Training Progress: \tEpoch 85 [960/8883 (10.79%)]\t\tLoss: 0.44003\n",
      "Training Progress: \tEpoch 85 [1280/8883 (14.39%)]\t\tLoss: 0.61184\n",
      "Training Progress: \tEpoch 85 [1600/8883 (17.99%)]\t\tLoss: 0.29770\n",
      "Training Progress: \tEpoch 85 [1920/8883 (21.58%)]\t\tLoss: 0.33146\n",
      "Training Progress: \tEpoch 85 [2240/8883 (25.18%)]\t\tLoss: 0.40700\n",
      "Training Progress: \tEpoch 85 [2560/8883 (28.78%)]\t\tLoss: 0.61279\n",
      "Training Progress: \tEpoch 85 [2880/8883 (32.37%)]\t\tLoss: 0.42339\n",
      "Training Progress: \tEpoch 85 [3200/8883 (35.97%)]\t\tLoss: 0.46878\n",
      "Training Progress: \tEpoch 85 [3520/8883 (39.57%)]\t\tLoss: 0.77269\n",
      "Training Progress: \tEpoch 85 [3840/8883 (43.17%)]\t\tLoss: 0.53683\n",
      "Training Progress: \tEpoch 85 [4160/8883 (46.76%)]\t\tLoss: 0.32873\n",
      "Training Progress: \tEpoch 85 [4480/8883 (50.36%)]\t\tLoss: 0.36243\n",
      "Training Progress: \tEpoch 85 [4800/8883 (53.96%)]\t\tLoss: 0.48631\n",
      "Training Progress: \tEpoch 85 [5120/8883 (57.55%)]\t\tLoss: 0.45249\n",
      "Training Progress: \tEpoch 85 [5440/8883 (61.15%)]\t\tLoss: 0.45950\n",
      "Training Progress: \tEpoch 85 [5760/8883 (64.75%)]\t\tLoss: 0.52496\n",
      "Training Progress: \tEpoch 85 [6080/8883 (68.35%)]\t\tLoss: 0.47529\n",
      "Training Progress: \tEpoch 85 [6400/8883 (71.94%)]\t\tLoss: 0.80277\n",
      "Training Progress: \tEpoch 85 [6720/8883 (75.54%)]\t\tLoss: 0.54872\n",
      "Training Progress: \tEpoch 85 [7040/8883 (79.14%)]\t\tLoss: 0.58183\n",
      "Training Progress: \tEpoch 85 [7360/8883 (82.73%)]\t\tLoss: 0.53092\n",
      "Training Progress: \tEpoch 85 [7680/8883 (86.33%)]\t\tLoss: 0.46036\n",
      "Training Progress: \tEpoch 85 [8000/8883 (89.93%)]\t\tLoss: 0.41420\n",
      "Training Progress: \tEpoch 85 [8320/8883 (93.53%)]\t\tLoss: 0.66074\n",
      "Training Progress: \tEpoch 85 [8640/8883 (97.12%)]\t\tLoss: 0.59990\n",
      "\tTrain loss: 0.01295, Accuracy: 7073/8883 (79.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1447/1692 (85.00%)\n",
      "\tTest loss: 0.00183, Accuracy: 530/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/8883 (0.00%)]\t\tLoss: 0.33464\n",
      "Training Progress: \tEpoch 86 [320/8883 (3.60%)]\t\tLoss: 0.63496\n",
      "Training Progress: \tEpoch 86 [640/8883 (7.19%)]\t\tLoss: 0.35788\n",
      "Training Progress: \tEpoch 86 [960/8883 (10.79%)]\t\tLoss: 0.45194\n",
      "Training Progress: \tEpoch 86 [1280/8883 (14.39%)]\t\tLoss: 0.58645\n",
      "Training Progress: \tEpoch 86 [1600/8883 (17.99%)]\t\tLoss: 0.32190\n",
      "Training Progress: \tEpoch 86 [1920/8883 (21.58%)]\t\tLoss: 0.44932\n",
      "Training Progress: \tEpoch 86 [2240/8883 (25.18%)]\t\tLoss: 0.44420\n",
      "Training Progress: \tEpoch 86 [2560/8883 (28.78%)]\t\tLoss: 0.73085\n",
      "Training Progress: \tEpoch 86 [2880/8883 (32.37%)]\t\tLoss: 0.43558\n",
      "Training Progress: \tEpoch 86 [3200/8883 (35.97%)]\t\tLoss: 0.38503\n",
      "Training Progress: \tEpoch 86 [3520/8883 (39.57%)]\t\tLoss: 0.64128\n",
      "Training Progress: \tEpoch 86 [3840/8883 (43.17%)]\t\tLoss: 0.53204\n",
      "Training Progress: \tEpoch 86 [4160/8883 (46.76%)]\t\tLoss: 0.35996\n",
      "Training Progress: \tEpoch 86 [4480/8883 (50.36%)]\t\tLoss: 0.47883\n",
      "Training Progress: \tEpoch 86 [4800/8883 (53.96%)]\t\tLoss: 0.32079\n",
      "Training Progress: \tEpoch 86 [5120/8883 (57.55%)]\t\tLoss: 0.50206\n",
      "Training Progress: \tEpoch 86 [5440/8883 (61.15%)]\t\tLoss: 0.72123\n",
      "Training Progress: \tEpoch 86 [5760/8883 (64.75%)]\t\tLoss: 0.62676\n",
      "Training Progress: \tEpoch 86 [6080/8883 (68.35%)]\t\tLoss: 0.26910\n",
      "Training Progress: \tEpoch 86 [6400/8883 (71.94%)]\t\tLoss: 0.59066\n",
      "Training Progress: \tEpoch 86 [6720/8883 (75.54%)]\t\tLoss: 0.74065\n",
      "Training Progress: \tEpoch 86 [7040/8883 (79.14%)]\t\tLoss: 0.62709\n",
      "Training Progress: \tEpoch 86 [7360/8883 (82.73%)]\t\tLoss: 0.53144\n",
      "Training Progress: \tEpoch 86 [7680/8883 (86.33%)]\t\tLoss: 0.43828\n",
      "Training Progress: \tEpoch 86 [8000/8883 (89.93%)]\t\tLoss: 0.47248\n",
      "Training Progress: \tEpoch 86 [8320/8883 (93.53%)]\t\tLoss: 0.52857\n",
      "Training Progress: \tEpoch 86 [8640/8883 (97.12%)]\t\tLoss: 0.46049\n",
      "\tTrain loss: 0.01237, Accuracy: 7121/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1498/1692 (88.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/8883 (0.00%)]\t\tLoss: 0.37986\n",
      "Training Progress: \tEpoch 87 [320/8883 (3.60%)]\t\tLoss: 0.42978\n",
      "Training Progress: \tEpoch 87 [640/8883 (7.19%)]\t\tLoss: 0.29723\n",
      "Training Progress: \tEpoch 87 [960/8883 (10.79%)]\t\tLoss: 0.53403\n",
      "Training Progress: \tEpoch 87 [1280/8883 (14.39%)]\t\tLoss: 0.46307\n",
      "Training Progress: \tEpoch 87 [1600/8883 (17.99%)]\t\tLoss: 0.40515\n",
      "Training Progress: \tEpoch 87 [1920/8883 (21.58%)]\t\tLoss: 0.47176\n",
      "Training Progress: \tEpoch 87 [2240/8883 (25.18%)]\t\tLoss: 0.43432\n",
      "Training Progress: \tEpoch 87 [2560/8883 (28.78%)]\t\tLoss: 0.44634\n",
      "Training Progress: \tEpoch 87 [2880/8883 (32.37%)]\t\tLoss: 0.41906\n",
      "Training Progress: \tEpoch 87 [3200/8883 (35.97%)]\t\tLoss: 0.45054\n",
      "Training Progress: \tEpoch 87 [3520/8883 (39.57%)]\t\tLoss: 0.56107\n",
      "Training Progress: \tEpoch 87 [3840/8883 (43.17%)]\t\tLoss: 0.56728\n",
      "Training Progress: \tEpoch 87 [4160/8883 (46.76%)]\t\tLoss: 0.34022\n",
      "Training Progress: \tEpoch 87 [4480/8883 (50.36%)]\t\tLoss: 0.40588\n",
      "Training Progress: \tEpoch 87 [4800/8883 (53.96%)]\t\tLoss: 0.38833\n",
      "Training Progress: \tEpoch 87 [5120/8883 (57.55%)]\t\tLoss: 0.77912\n",
      "Training Progress: \tEpoch 87 [5440/8883 (61.15%)]\t\tLoss: 0.55573\n",
      "Training Progress: \tEpoch 87 [5760/8883 (64.75%)]\t\tLoss: 0.34818\n",
      "Training Progress: \tEpoch 87 [6080/8883 (68.35%)]\t\tLoss: 0.42303\n",
      "Training Progress: \tEpoch 87 [6400/8883 (71.94%)]\t\tLoss: 0.53025\n",
      "Training Progress: \tEpoch 87 [6720/8883 (75.54%)]\t\tLoss: 0.69107\n",
      "Training Progress: \tEpoch 87 [7040/8883 (79.14%)]\t\tLoss: 0.71268\n",
      "Training Progress: \tEpoch 87 [7360/8883 (82.73%)]\t\tLoss: 0.54955\n",
      "Training Progress: \tEpoch 87 [7680/8883 (86.33%)]\t\tLoss: 0.44012\n",
      "Training Progress: \tEpoch 87 [8000/8883 (89.93%)]\t\tLoss: 0.47286\n",
      "Training Progress: \tEpoch 87 [8320/8883 (93.53%)]\t\tLoss: 0.62714\n",
      "Training Progress: \tEpoch 87 [8640/8883 (97.12%)]\t\tLoss: 0.41120\n",
      "\tTrain loss: 0.01313, Accuracy: 7025/8883 (79.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1476/1692 (87.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 530/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/8883 (0.00%)]\t\tLoss: 0.37710\n",
      "Training Progress: \tEpoch 88 [320/8883 (3.60%)]\t\tLoss: 0.68983\n",
      "Training Progress: \tEpoch 88 [640/8883 (7.19%)]\t\tLoss: 0.55286\n",
      "Training Progress: \tEpoch 88 [960/8883 (10.79%)]\t\tLoss: 0.59715\n",
      "Training Progress: \tEpoch 88 [1280/8883 (14.39%)]\t\tLoss: 0.66574\n",
      "Training Progress: \tEpoch 88 [1600/8883 (17.99%)]\t\tLoss: 0.34509\n",
      "Training Progress: \tEpoch 88 [1920/8883 (21.58%)]\t\tLoss: 0.37539\n",
      "Training Progress: \tEpoch 88 [2240/8883 (25.18%)]\t\tLoss: 0.47185\n",
      "Training Progress: \tEpoch 88 [2560/8883 (28.78%)]\t\tLoss: 0.56664\n",
      "Training Progress: \tEpoch 88 [2880/8883 (32.37%)]\t\tLoss: 0.43445\n",
      "Training Progress: \tEpoch 88 [3200/8883 (35.97%)]\t\tLoss: 0.48712\n",
      "Training Progress: \tEpoch 88 [3520/8883 (39.57%)]\t\tLoss: 0.63980\n",
      "Training Progress: \tEpoch 88 [3840/8883 (43.17%)]\t\tLoss: 0.70204\n",
      "Training Progress: \tEpoch 88 [4160/8883 (46.76%)]\t\tLoss: 0.54817\n",
      "Training Progress: \tEpoch 88 [4480/8883 (50.36%)]\t\tLoss: 0.52735\n",
      "Training Progress: \tEpoch 88 [4800/8883 (53.96%)]\t\tLoss: 0.48022\n",
      "Training Progress: \tEpoch 88 [5120/8883 (57.55%)]\t\tLoss: 0.55119\n",
      "Training Progress: \tEpoch 88 [5440/8883 (61.15%)]\t\tLoss: 0.54731\n",
      "Training Progress: \tEpoch 88 [5760/8883 (64.75%)]\t\tLoss: 0.39713\n",
      "Training Progress: \tEpoch 88 [6080/8883 (68.35%)]\t\tLoss: 0.37220\n",
      "Training Progress: \tEpoch 88 [6400/8883 (71.94%)]\t\tLoss: 0.37924\n",
      "Training Progress: \tEpoch 88 [6720/8883 (75.54%)]\t\tLoss: 0.59577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 88 [7040/8883 (79.14%)]\t\tLoss: 0.55931\n",
      "Training Progress: \tEpoch 88 [7360/8883 (82.73%)]\t\tLoss: 0.55334\n",
      "Training Progress: \tEpoch 88 [7680/8883 (86.33%)]\t\tLoss: 0.42321\n",
      "Training Progress: \tEpoch 88 [8000/8883 (89.93%)]\t\tLoss: 0.58385\n",
      "Training Progress: \tEpoch 88 [8320/8883 (93.53%)]\t\tLoss: 0.49874\n",
      "Training Progress: \tEpoch 88 [8640/8883 (97.12%)]\t\tLoss: 0.46532\n",
      "\tTrain loss: 0.01184, Accuracy: 7175/8883 (80.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1517/1692 (89.00%)\n",
      "\tTest loss: 0.00180, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/8883 (0.00%)]\t\tLoss: 0.40179\n",
      "Training Progress: \tEpoch 89 [320/8883 (3.60%)]\t\tLoss: 0.48405\n",
      "Training Progress: \tEpoch 89 [640/8883 (7.19%)]\t\tLoss: 0.39828\n",
      "Training Progress: \tEpoch 89 [960/8883 (10.79%)]\t\tLoss: 0.48164\n",
      "Training Progress: \tEpoch 89 [1280/8883 (14.39%)]\t\tLoss: 0.69343\n",
      "Training Progress: \tEpoch 89 [1600/8883 (17.99%)]\t\tLoss: 0.42192\n",
      "Training Progress: \tEpoch 89 [1920/8883 (21.58%)]\t\tLoss: 0.36917\n",
      "Training Progress: \tEpoch 89 [2240/8883 (25.18%)]\t\tLoss: 0.54361\n",
      "Training Progress: \tEpoch 89 [2560/8883 (28.78%)]\t\tLoss: 0.72744\n",
      "Training Progress: \tEpoch 89 [2880/8883 (32.37%)]\t\tLoss: 0.58360\n",
      "Training Progress: \tEpoch 89 [3200/8883 (35.97%)]\t\tLoss: 0.40795\n",
      "Training Progress: \tEpoch 89 [3520/8883 (39.57%)]\t\tLoss: 0.55234\n",
      "Training Progress: \tEpoch 89 [3840/8883 (43.17%)]\t\tLoss: 0.59761\n",
      "Training Progress: \tEpoch 89 [4160/8883 (46.76%)]\t\tLoss: 0.31431\n",
      "Training Progress: \tEpoch 89 [4480/8883 (50.36%)]\t\tLoss: 0.34376\n",
      "Training Progress: \tEpoch 89 [4800/8883 (53.96%)]\t\tLoss: 0.30780\n",
      "Training Progress: \tEpoch 89 [5120/8883 (57.55%)]\t\tLoss: 0.57174\n",
      "Training Progress: \tEpoch 89 [5440/8883 (61.15%)]\t\tLoss: 0.42997\n",
      "Training Progress: \tEpoch 89 [5760/8883 (64.75%)]\t\tLoss: 0.34395\n",
      "Training Progress: \tEpoch 89 [6080/8883 (68.35%)]\t\tLoss: 0.56866\n",
      "Training Progress: \tEpoch 89 [6400/8883 (71.94%)]\t\tLoss: 0.43259\n",
      "Training Progress: \tEpoch 89 [6720/8883 (75.54%)]\t\tLoss: 0.67220\n",
      "Training Progress: \tEpoch 89 [7040/8883 (79.14%)]\t\tLoss: 0.56785\n",
      "Training Progress: \tEpoch 89 [7360/8883 (82.73%)]\t\tLoss: 0.56052\n",
      "Training Progress: \tEpoch 89 [7680/8883 (86.33%)]\t\tLoss: 0.42260\n",
      "Training Progress: \tEpoch 89 [8000/8883 (89.93%)]\t\tLoss: 0.55833\n",
      "Training Progress: \tEpoch 89 [8320/8883 (93.53%)]\t\tLoss: 0.50810\n",
      "Training Progress: \tEpoch 89 [8640/8883 (97.12%)]\t\tLoss: 0.40867\n",
      "\tTrain loss: 0.01248, Accuracy: 7088/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1496/1692 (88.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/8883 (0.00%)]\t\tLoss: 0.44281\n",
      "Training Progress: \tEpoch 90 [320/8883 (3.60%)]\t\tLoss: 0.47556\n",
      "Training Progress: \tEpoch 90 [640/8883 (7.19%)]\t\tLoss: 0.30332\n",
      "Training Progress: \tEpoch 90 [960/8883 (10.79%)]\t\tLoss: 0.41694\n",
      "Training Progress: \tEpoch 90 [1280/8883 (14.39%)]\t\tLoss: 0.48618\n",
      "Training Progress: \tEpoch 90 [1600/8883 (17.99%)]\t\tLoss: 0.28216\n",
      "Training Progress: \tEpoch 90 [1920/8883 (21.58%)]\t\tLoss: 0.32597\n",
      "Training Progress: \tEpoch 90 [2240/8883 (25.18%)]\t\tLoss: 0.44437\n",
      "Training Progress: \tEpoch 90 [2560/8883 (28.78%)]\t\tLoss: 0.63590\n",
      "Training Progress: \tEpoch 90 [2880/8883 (32.37%)]\t\tLoss: 0.46719\n",
      "Training Progress: \tEpoch 90 [3200/8883 (35.97%)]\t\tLoss: 0.45103\n",
      "Training Progress: \tEpoch 90 [3520/8883 (39.57%)]\t\tLoss: 0.69150\n",
      "Training Progress: \tEpoch 90 [3840/8883 (43.17%)]\t\tLoss: 0.66396\n",
      "Training Progress: \tEpoch 90 [4160/8883 (46.76%)]\t\tLoss: 0.29498\n",
      "Training Progress: \tEpoch 90 [4480/8883 (50.36%)]\t\tLoss: 0.44147\n",
      "Training Progress: \tEpoch 90 [4800/8883 (53.96%)]\t\tLoss: 0.49334\n",
      "Training Progress: \tEpoch 90 [5120/8883 (57.55%)]\t\tLoss: 0.56606\n",
      "Training Progress: \tEpoch 90 [5440/8883 (61.15%)]\t\tLoss: 0.56144\n",
      "Training Progress: \tEpoch 90 [5760/8883 (64.75%)]\t\tLoss: 0.50198\n",
      "Training Progress: \tEpoch 90 [6080/8883 (68.35%)]\t\tLoss: 0.43325\n",
      "Training Progress: \tEpoch 90 [6400/8883 (71.94%)]\t\tLoss: 0.40622\n",
      "Training Progress: \tEpoch 90 [6720/8883 (75.54%)]\t\tLoss: 0.52137\n",
      "Training Progress: \tEpoch 90 [7040/8883 (79.14%)]\t\tLoss: 0.60531\n",
      "Training Progress: \tEpoch 90 [7360/8883 (82.73%)]\t\tLoss: 0.51633\n",
      "Training Progress: \tEpoch 90 [7680/8883 (86.33%)]\t\tLoss: 0.61947\n",
      "Training Progress: \tEpoch 90 [8000/8883 (89.93%)]\t\tLoss: 0.36940\n",
      "Training Progress: \tEpoch 90 [8320/8883 (93.53%)]\t\tLoss: 0.44680\n",
      "Training Progress: \tEpoch 90 [8640/8883 (97.12%)]\t\tLoss: 0.48458\n",
      "\tTrain loss: 0.01202, Accuracy: 7141/8883 (80.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1523/1692 (90.00%)\n",
      "\tTest loss: 0.00184, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/8883 (0.00%)]\t\tLoss: 0.33518\n",
      "Training Progress: \tEpoch 91 [320/8883 (3.60%)]\t\tLoss: 0.39456\n",
      "Training Progress: \tEpoch 91 [640/8883 (7.19%)]\t\tLoss: 0.41812\n",
      "Training Progress: \tEpoch 91 [960/8883 (10.79%)]\t\tLoss: 0.43972\n",
      "Training Progress: \tEpoch 91 [1280/8883 (14.39%)]\t\tLoss: 0.38489\n",
      "Training Progress: \tEpoch 91 [1600/8883 (17.99%)]\t\tLoss: 0.29582\n",
      "Training Progress: \tEpoch 91 [1920/8883 (21.58%)]\t\tLoss: 0.73792\n",
      "Training Progress: \tEpoch 91 [2240/8883 (25.18%)]\t\tLoss: 0.46955\n",
      "Training Progress: \tEpoch 91 [2560/8883 (28.78%)]\t\tLoss: 0.72225\n",
      "Training Progress: \tEpoch 91 [2880/8883 (32.37%)]\t\tLoss: 0.39769\n",
      "Training Progress: \tEpoch 91 [3200/8883 (35.97%)]\t\tLoss: 0.48899\n",
      "Training Progress: \tEpoch 91 [3520/8883 (39.57%)]\t\tLoss: 1.23278\n",
      "Training Progress: \tEpoch 91 [3840/8883 (43.17%)]\t\tLoss: 0.58531\n",
      "Training Progress: \tEpoch 91 [4160/8883 (46.76%)]\t\tLoss: 0.45710\n",
      "Training Progress: \tEpoch 91 [4480/8883 (50.36%)]\t\tLoss: 0.41691\n",
      "Training Progress: \tEpoch 91 [4800/8883 (53.96%)]\t\tLoss: 0.53102\n",
      "Training Progress: \tEpoch 91 [5120/8883 (57.55%)]\t\tLoss: 0.51204\n",
      "Training Progress: \tEpoch 91 [5440/8883 (61.15%)]\t\tLoss: 0.46048\n",
      "Training Progress: \tEpoch 91 [5760/8883 (64.75%)]\t\tLoss: 0.37051\n",
      "Training Progress: \tEpoch 91 [6080/8883 (68.35%)]\t\tLoss: 0.33678\n",
      "Training Progress: \tEpoch 91 [6400/8883 (71.94%)]\t\tLoss: 0.46230\n",
      "Training Progress: \tEpoch 91 [6720/8883 (75.54%)]\t\tLoss: 0.60143\n",
      "Training Progress: \tEpoch 91 [7040/8883 (79.14%)]\t\tLoss: 0.76049\n",
      "Training Progress: \tEpoch 91 [7360/8883 (82.73%)]\t\tLoss: 0.46135\n",
      "Training Progress: \tEpoch 91 [7680/8883 (86.33%)]\t\tLoss: 0.43765\n",
      "Training Progress: \tEpoch 91 [8000/8883 (89.93%)]\t\tLoss: 0.39050\n",
      "Training Progress: \tEpoch 91 [8320/8883 (93.53%)]\t\tLoss: 0.68772\n",
      "Training Progress: \tEpoch 91 [8640/8883 (97.12%)]\t\tLoss: 0.51185\n",
      "\tTrain loss: 0.01212, Accuracy: 7153/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/8883 (0.00%)]\t\tLoss: 0.66044\n",
      "Training Progress: \tEpoch 92 [320/8883 (3.60%)]\t\tLoss: 0.45694\n",
      "Training Progress: \tEpoch 92 [640/8883 (7.19%)]\t\tLoss: 0.31796\n",
      "Training Progress: \tEpoch 92 [960/8883 (10.79%)]\t\tLoss: 0.56385\n",
      "Training Progress: \tEpoch 92 [1280/8883 (14.39%)]\t\tLoss: 0.39738\n",
      "Training Progress: \tEpoch 92 [1600/8883 (17.99%)]\t\tLoss: 0.33312\n",
      "Training Progress: \tEpoch 92 [1920/8883 (21.58%)]\t\tLoss: 0.34701\n",
      "Training Progress: \tEpoch 92 [2240/8883 (25.18%)]\t\tLoss: 0.46950\n",
      "Training Progress: \tEpoch 92 [2560/8883 (28.78%)]\t\tLoss: 0.51186\n",
      "Training Progress: \tEpoch 92 [2880/8883 (32.37%)]\t\tLoss: 0.65372\n",
      "Training Progress: \tEpoch 92 [3200/8883 (35.97%)]\t\tLoss: 0.52922\n",
      "Training Progress: \tEpoch 92 [3520/8883 (39.57%)]\t\tLoss: 0.58559\n",
      "Training Progress: \tEpoch 92 [3840/8883 (43.17%)]\t\tLoss: 0.56359\n",
      "Training Progress: \tEpoch 92 [4160/8883 (46.76%)]\t\tLoss: 0.30090\n",
      "Training Progress: \tEpoch 92 [4480/8883 (50.36%)]\t\tLoss: 0.36890\n",
      "Training Progress: \tEpoch 92 [4800/8883 (53.96%)]\t\tLoss: 0.60523\n",
      "Training Progress: \tEpoch 92 [5120/8883 (57.55%)]\t\tLoss: 0.44346\n",
      "Training Progress: \tEpoch 92 [5440/8883 (61.15%)]\t\tLoss: 0.45781\n",
      "Training Progress: \tEpoch 92 [5760/8883 (64.75%)]\t\tLoss: 0.46568\n",
      "Training Progress: \tEpoch 92 [6080/8883 (68.35%)]\t\tLoss: 0.36151\n",
      "Training Progress: \tEpoch 92 [6400/8883 (71.94%)]\t\tLoss: 0.52940\n",
      "Training Progress: \tEpoch 92 [6720/8883 (75.54%)]\t\tLoss: 0.55776\n",
      "Training Progress: \tEpoch 92 [7040/8883 (79.14%)]\t\tLoss: 0.75430\n",
      "Training Progress: \tEpoch 92 [7360/8883 (82.73%)]\t\tLoss: 0.50562\n",
      "Training Progress: \tEpoch 92 [7680/8883 (86.33%)]\t\tLoss: 0.45191\n",
      "Training Progress: \tEpoch 92 [8000/8883 (89.93%)]\t\tLoss: 0.38554\n",
      "Training Progress: \tEpoch 92 [8320/8883 (93.53%)]\t\tLoss: 0.51765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 92 [8640/8883 (97.12%)]\t\tLoss: 0.37672\n",
      "\tTrain loss: 0.01229, Accuracy: 7117/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1516/1692 (89.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/8883 (0.00%)]\t\tLoss: 0.35502\n",
      "Training Progress: \tEpoch 93 [320/8883 (3.60%)]\t\tLoss: 0.51351\n",
      "Training Progress: \tEpoch 93 [640/8883 (7.19%)]\t\tLoss: 0.24764\n",
      "Training Progress: \tEpoch 93 [960/8883 (10.79%)]\t\tLoss: 0.57722\n",
      "Training Progress: \tEpoch 93 [1280/8883 (14.39%)]\t\tLoss: 0.44431\n",
      "Training Progress: \tEpoch 93 [1600/8883 (17.99%)]\t\tLoss: 0.44065\n",
      "Training Progress: \tEpoch 93 [1920/8883 (21.58%)]\t\tLoss: 0.47803\n",
      "Training Progress: \tEpoch 93 [2240/8883 (25.18%)]\t\tLoss: 0.52149\n",
      "Training Progress: \tEpoch 93 [2560/8883 (28.78%)]\t\tLoss: 0.58211\n",
      "Training Progress: \tEpoch 93 [2880/8883 (32.37%)]\t\tLoss: 0.71318\n",
      "Training Progress: \tEpoch 93 [3200/8883 (35.97%)]\t\tLoss: 0.51849\n",
      "Training Progress: \tEpoch 93 [3520/8883 (39.57%)]\t\tLoss: 0.65383\n",
      "Training Progress: \tEpoch 93 [3840/8883 (43.17%)]\t\tLoss: 0.74965\n",
      "Training Progress: \tEpoch 93 [4160/8883 (46.76%)]\t\tLoss: 0.29198\n",
      "Training Progress: \tEpoch 93 [4480/8883 (50.36%)]\t\tLoss: 0.39374\n",
      "Training Progress: \tEpoch 93 [4800/8883 (53.96%)]\t\tLoss: 0.29610\n",
      "Training Progress: \tEpoch 93 [5120/8883 (57.55%)]\t\tLoss: 0.56569\n",
      "Training Progress: \tEpoch 93 [5440/8883 (61.15%)]\t\tLoss: 0.46634\n",
      "Training Progress: \tEpoch 93 [5760/8883 (64.75%)]\t\tLoss: 0.46008\n",
      "Training Progress: \tEpoch 93 [6080/8883 (68.35%)]\t\tLoss: 0.46781\n",
      "Training Progress: \tEpoch 93 [6400/8883 (71.94%)]\t\tLoss: 0.59014\n",
      "Training Progress: \tEpoch 93 [6720/8883 (75.54%)]\t\tLoss: 0.65833\n",
      "Training Progress: \tEpoch 93 [7040/8883 (79.14%)]\t\tLoss: 0.68148\n",
      "Training Progress: \tEpoch 93 [7360/8883 (82.73%)]\t\tLoss: 0.48966\n",
      "Training Progress: \tEpoch 93 [7680/8883 (86.33%)]\t\tLoss: 0.65595\n",
      "Training Progress: \tEpoch 93 [8000/8883 (89.93%)]\t\tLoss: 0.44622\n",
      "Training Progress: \tEpoch 93 [8320/8883 (93.53%)]\t\tLoss: 0.63519\n",
      "Training Progress: \tEpoch 93 [8640/8883 (97.12%)]\t\tLoss: 0.57992\n",
      "\tTrain loss: 0.01272, Accuracy: 7065/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1519/1692 (89.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/8883 (0.00%)]\t\tLoss: 0.53865\n",
      "Training Progress: \tEpoch 94 [320/8883 (3.60%)]\t\tLoss: 0.54411\n",
      "Training Progress: \tEpoch 94 [640/8883 (7.19%)]\t\tLoss: 0.28837\n",
      "Training Progress: \tEpoch 94 [960/8883 (10.79%)]\t\tLoss: 0.48527\n",
      "Training Progress: \tEpoch 94 [1280/8883 (14.39%)]\t\tLoss: 0.62247\n",
      "Training Progress: \tEpoch 94 [1600/8883 (17.99%)]\t\tLoss: 0.52366\n",
      "Training Progress: \tEpoch 94 [1920/8883 (21.58%)]\t\tLoss: 0.48889\n",
      "Training Progress: \tEpoch 94 [2240/8883 (25.18%)]\t\tLoss: 0.64175\n",
      "Training Progress: \tEpoch 94 [2560/8883 (28.78%)]\t\tLoss: 0.51065\n",
      "Training Progress: \tEpoch 94 [2880/8883 (32.37%)]\t\tLoss: 0.55009\n",
      "Training Progress: \tEpoch 94 [3200/8883 (35.97%)]\t\tLoss: 0.43030\n",
      "Training Progress: \tEpoch 94 [3520/8883 (39.57%)]\t\tLoss: 0.50051\n",
      "Training Progress: \tEpoch 94 [3840/8883 (43.17%)]\t\tLoss: 0.56061\n",
      "Training Progress: \tEpoch 94 [4160/8883 (46.76%)]\t\tLoss: 0.28670\n",
      "Training Progress: \tEpoch 94 [4480/8883 (50.36%)]\t\tLoss: 0.32910\n",
      "Training Progress: \tEpoch 94 [4800/8883 (53.96%)]\t\tLoss: 0.31242\n",
      "Training Progress: \tEpoch 94 [5120/8883 (57.55%)]\t\tLoss: 0.64542\n",
      "Training Progress: \tEpoch 94 [5440/8883 (61.15%)]\t\tLoss: 0.77659\n",
      "Training Progress: \tEpoch 94 [5760/8883 (64.75%)]\t\tLoss: 0.48485\n",
      "Training Progress: \tEpoch 94 [6080/8883 (68.35%)]\t\tLoss: 0.34926\n",
      "Training Progress: \tEpoch 94 [6400/8883 (71.94%)]\t\tLoss: 0.41089\n",
      "Training Progress: \tEpoch 94 [6720/8883 (75.54%)]\t\tLoss: 0.55352\n",
      "Training Progress: \tEpoch 94 [7040/8883 (79.14%)]\t\tLoss: 0.53193\n",
      "Training Progress: \tEpoch 94 [7360/8883 (82.73%)]\t\tLoss: 0.61742\n",
      "Training Progress: \tEpoch 94 [7680/8883 (86.33%)]\t\tLoss: 0.38793\n",
      "Training Progress: \tEpoch 94 [8000/8883 (89.93%)]\t\tLoss: 0.46425\n",
      "Training Progress: \tEpoch 94 [8320/8883 (93.53%)]\t\tLoss: 0.52668\n",
      "Training Progress: \tEpoch 94 [8640/8883 (97.12%)]\t\tLoss: 0.43755\n",
      "\tTrain loss: 0.01180, Accuracy: 7169/8883 (80.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1531/1692 (90.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 540/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/8883 (0.00%)]\t\tLoss: 0.44359\n",
      "Training Progress: \tEpoch 95 [320/8883 (3.60%)]\t\tLoss: 0.44988\n",
      "Training Progress: \tEpoch 95 [640/8883 (7.19%)]\t\tLoss: 0.33709\n",
      "Training Progress: \tEpoch 95 [960/8883 (10.79%)]\t\tLoss: 0.74123\n",
      "Training Progress: \tEpoch 95 [1280/8883 (14.39%)]\t\tLoss: 0.67060\n",
      "Training Progress: \tEpoch 95 [1600/8883 (17.99%)]\t\tLoss: 0.24508\n",
      "Training Progress: \tEpoch 95 [1920/8883 (21.58%)]\t\tLoss: 0.36990\n",
      "Training Progress: \tEpoch 95 [2240/8883 (25.18%)]\t\tLoss: 0.56880\n",
      "Training Progress: \tEpoch 95 [2560/8883 (28.78%)]\t\tLoss: 0.58056\n",
      "Training Progress: \tEpoch 95 [2880/8883 (32.37%)]\t\tLoss: 0.42866\n",
      "Training Progress: \tEpoch 95 [3200/8883 (35.97%)]\t\tLoss: 0.53791\n",
      "Training Progress: \tEpoch 95 [3520/8883 (39.57%)]\t\tLoss: 0.65763\n",
      "Training Progress: \tEpoch 95 [3840/8883 (43.17%)]\t\tLoss: 0.49634\n",
      "Training Progress: \tEpoch 95 [4160/8883 (46.76%)]\t\tLoss: 0.41595\n",
      "Training Progress: \tEpoch 95 [4480/8883 (50.36%)]\t\tLoss: 0.36831\n",
      "Training Progress: \tEpoch 95 [4800/8883 (53.96%)]\t\tLoss: 0.37055\n",
      "Training Progress: \tEpoch 95 [5120/8883 (57.55%)]\t\tLoss: 0.42032\n",
      "Training Progress: \tEpoch 95 [5440/8883 (61.15%)]\t\tLoss: 0.47261\n",
      "Training Progress: \tEpoch 95 [5760/8883 (64.75%)]\t\tLoss: 0.44403\n",
      "Training Progress: \tEpoch 95 [6080/8883 (68.35%)]\t\tLoss: 0.30852\n",
      "Training Progress: \tEpoch 95 [6400/8883 (71.94%)]\t\tLoss: 0.36702\n",
      "Training Progress: \tEpoch 95 [6720/8883 (75.54%)]\t\tLoss: 0.55175\n",
      "Training Progress: \tEpoch 95 [7040/8883 (79.14%)]\t\tLoss: 0.73018\n",
      "Training Progress: \tEpoch 95 [7360/8883 (82.73%)]\t\tLoss: 0.52281\n",
      "Training Progress: \tEpoch 95 [7680/8883 (86.33%)]\t\tLoss: 0.41298\n",
      "Training Progress: \tEpoch 95 [8000/8883 (89.93%)]\t\tLoss: 0.53422\n",
      "Training Progress: \tEpoch 95 [8320/8883 (93.53%)]\t\tLoss: 0.75562\n",
      "Training Progress: \tEpoch 95 [8640/8883 (97.12%)]\t\tLoss: 0.46573\n",
      "\tTrain loss: 0.01290, Accuracy: 7040/8883 (79.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 572/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/8883 (0.00%)]\t\tLoss: 0.54918\n",
      "Training Progress: \tEpoch 96 [320/8883 (3.60%)]\t\tLoss: 0.45855\n",
      "Training Progress: \tEpoch 96 [640/8883 (7.19%)]\t\tLoss: 0.49604\n",
      "Training Progress: \tEpoch 96 [960/8883 (10.79%)]\t\tLoss: 0.43742\n",
      "Training Progress: \tEpoch 96 [1280/8883 (14.39%)]\t\tLoss: 0.49541\n",
      "Training Progress: \tEpoch 96 [1600/8883 (17.99%)]\t\tLoss: 0.33564\n",
      "Training Progress: \tEpoch 96 [1920/8883 (21.58%)]\t\tLoss: 0.35621\n",
      "Training Progress: \tEpoch 96 [2240/8883 (25.18%)]\t\tLoss: 0.41077\n",
      "Training Progress: \tEpoch 96 [2560/8883 (28.78%)]\t\tLoss: 0.60558\n",
      "Training Progress: \tEpoch 96 [2880/8883 (32.37%)]\t\tLoss: 0.58724\n",
      "Training Progress: \tEpoch 96 [3200/8883 (35.97%)]\t\tLoss: 0.43817\n",
      "Training Progress: \tEpoch 96 [3520/8883 (39.57%)]\t\tLoss: 1.17598\n",
      "Training Progress: \tEpoch 96 [3840/8883 (43.17%)]\t\tLoss: 0.60180\n",
      "Training Progress: \tEpoch 96 [4160/8883 (46.76%)]\t\tLoss: 0.35188\n",
      "Training Progress: \tEpoch 96 [4480/8883 (50.36%)]\t\tLoss: 0.31370\n",
      "Training Progress: \tEpoch 96 [4800/8883 (53.96%)]\t\tLoss: 0.38154\n",
      "Training Progress: \tEpoch 96 [5120/8883 (57.55%)]\t\tLoss: 0.56435\n",
      "Training Progress: \tEpoch 96 [5440/8883 (61.15%)]\t\tLoss: 0.60117\n",
      "Training Progress: \tEpoch 96 [5760/8883 (64.75%)]\t\tLoss: 0.46350\n",
      "Training Progress: \tEpoch 96 [6080/8883 (68.35%)]\t\tLoss: 0.47607\n",
      "Training Progress: \tEpoch 96 [6400/8883 (71.94%)]\t\tLoss: 0.55881\n",
      "Training Progress: \tEpoch 96 [6720/8883 (75.54%)]\t\tLoss: 0.53950\n",
      "Training Progress: \tEpoch 96 [7040/8883 (79.14%)]\t\tLoss: 0.61868\n",
      "Training Progress: \tEpoch 96 [7360/8883 (82.73%)]\t\tLoss: 0.49618\n",
      "Training Progress: \tEpoch 96 [7680/8883 (86.33%)]\t\tLoss: 0.51480\n",
      "Training Progress: \tEpoch 96 [8000/8883 (89.93%)]\t\tLoss: 0.48936\n",
      "Training Progress: \tEpoch 96 [8320/8883 (93.53%)]\t\tLoss: 0.65399\n",
      "Training Progress: \tEpoch 96 [8640/8883 (97.12%)]\t\tLoss: 0.46574\n",
      "\tTrain loss: 0.01199, Accuracy: 7148/8883 (80.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1514/1692 (89.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 547/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/8883 (0.00%)]\t\tLoss: 0.53802\n",
      "Training Progress: \tEpoch 97 [320/8883 (3.60%)]\t\tLoss: 0.49319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 97 [640/8883 (7.19%)]\t\tLoss: 0.40207\n",
      "Training Progress: \tEpoch 97 [960/8883 (10.79%)]\t\tLoss: 0.49138\n",
      "Training Progress: \tEpoch 97 [1280/8883 (14.39%)]\t\tLoss: 0.40833\n",
      "Training Progress: \tEpoch 97 [1600/8883 (17.99%)]\t\tLoss: 0.41907\n",
      "Training Progress: \tEpoch 97 [1920/8883 (21.58%)]\t\tLoss: 0.48393\n",
      "Training Progress: \tEpoch 97 [2240/8883 (25.18%)]\t\tLoss: 0.54292\n",
      "Training Progress: \tEpoch 97 [2560/8883 (28.78%)]\t\tLoss: 0.54373\n",
      "Training Progress: \tEpoch 97 [2880/8883 (32.37%)]\t\tLoss: 0.47988\n",
      "Training Progress: \tEpoch 97 [3200/8883 (35.97%)]\t\tLoss: 0.54352\n",
      "Training Progress: \tEpoch 97 [3520/8883 (39.57%)]\t\tLoss: 0.54534\n",
      "Training Progress: \tEpoch 97 [3840/8883 (43.17%)]\t\tLoss: 0.57222\n",
      "Training Progress: \tEpoch 97 [4160/8883 (46.76%)]\t\tLoss: 0.32756\n",
      "Training Progress: \tEpoch 97 [4480/8883 (50.36%)]\t\tLoss: 0.46456\n",
      "Training Progress: \tEpoch 97 [4800/8883 (53.96%)]\t\tLoss: 0.29476\n",
      "Training Progress: \tEpoch 97 [5120/8883 (57.55%)]\t\tLoss: 0.53147\n",
      "Training Progress: \tEpoch 97 [5440/8883 (61.15%)]\t\tLoss: 0.58199\n",
      "Training Progress: \tEpoch 97 [5760/8883 (64.75%)]\t\tLoss: 0.39321\n",
      "Training Progress: \tEpoch 97 [6080/8883 (68.35%)]\t\tLoss: 0.36952\n",
      "Training Progress: \tEpoch 97 [6400/8883 (71.94%)]\t\tLoss: 0.61513\n",
      "Training Progress: \tEpoch 97 [6720/8883 (75.54%)]\t\tLoss: 0.55561\n",
      "Training Progress: \tEpoch 97 [7040/8883 (79.14%)]\t\tLoss: 0.55047\n",
      "Training Progress: \tEpoch 97 [7360/8883 (82.73%)]\t\tLoss: 0.50598\n",
      "Training Progress: \tEpoch 97 [7680/8883 (86.33%)]\t\tLoss: 0.42053\n",
      "Training Progress: \tEpoch 97 [8000/8883 (89.93%)]\t\tLoss: 0.40575\n",
      "Training Progress: \tEpoch 97 [8320/8883 (93.53%)]\t\tLoss: 0.54527\n",
      "Training Progress: \tEpoch 97 [8640/8883 (97.12%)]\t\tLoss: 0.48666\n",
      "\tTrain loss: 0.01262, Accuracy: 7087/8883 (79.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 526/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/8883 (0.00%)]\t\tLoss: 0.64730\n",
      "Training Progress: \tEpoch 98 [320/8883 (3.60%)]\t\tLoss: 0.44264\n",
      "Training Progress: \tEpoch 98 [640/8883 (7.19%)]\t\tLoss: 0.32012\n",
      "Training Progress: \tEpoch 98 [960/8883 (10.79%)]\t\tLoss: 0.42719\n",
      "Training Progress: \tEpoch 98 [1280/8883 (14.39%)]\t\tLoss: 0.49163\n",
      "Training Progress: \tEpoch 98 [1600/8883 (17.99%)]\t\tLoss: 0.19973\n",
      "Training Progress: \tEpoch 98 [1920/8883 (21.58%)]\t\tLoss: 0.65309\n",
      "Training Progress: \tEpoch 98 [2240/8883 (25.18%)]\t\tLoss: 0.38977\n",
      "Training Progress: \tEpoch 98 [2560/8883 (28.78%)]\t\tLoss: 0.47033\n",
      "Training Progress: \tEpoch 98 [2880/8883 (32.37%)]\t\tLoss: 0.51944\n",
      "Training Progress: \tEpoch 98 [3200/8883 (35.97%)]\t\tLoss: 0.40368\n",
      "Training Progress: \tEpoch 98 [3520/8883 (39.57%)]\t\tLoss: 0.84071\n",
      "Training Progress: \tEpoch 98 [3840/8883 (43.17%)]\t\tLoss: 0.59548\n",
      "Training Progress: \tEpoch 98 [4160/8883 (46.76%)]\t\tLoss: 0.43256\n",
      "Training Progress: \tEpoch 98 [4480/8883 (50.36%)]\t\tLoss: 0.49424\n",
      "Training Progress: \tEpoch 98 [4800/8883 (53.96%)]\t\tLoss: 0.34638\n",
      "Training Progress: \tEpoch 98 [5120/8883 (57.55%)]\t\tLoss: 0.53004\n",
      "Training Progress: \tEpoch 98 [5440/8883 (61.15%)]\t\tLoss: 0.58657\n",
      "Training Progress: \tEpoch 98 [5760/8883 (64.75%)]\t\tLoss: 0.52748\n",
      "Training Progress: \tEpoch 98 [6080/8883 (68.35%)]\t\tLoss: 0.31406\n",
      "Training Progress: \tEpoch 98 [6400/8883 (71.94%)]\t\tLoss: 0.41011\n",
      "Training Progress: \tEpoch 98 [6720/8883 (75.54%)]\t\tLoss: 0.52513\n",
      "Training Progress: \tEpoch 98 [7040/8883 (79.14%)]\t\tLoss: 0.65205\n",
      "Training Progress: \tEpoch 98 [7360/8883 (82.73%)]\t\tLoss: 0.55448\n",
      "Training Progress: \tEpoch 98 [7680/8883 (86.33%)]\t\tLoss: 0.38551\n",
      "Training Progress: \tEpoch 98 [8000/8883 (89.93%)]\t\tLoss: 0.78856\n",
      "Training Progress: \tEpoch 98 [8320/8883 (93.53%)]\t\tLoss: 0.69405\n",
      "Training Progress: \tEpoch 98 [8640/8883 (97.12%)]\t\tLoss: 0.63106\n",
      "\tTrain loss: 0.01222, Accuracy: 7128/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00189, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/8883 (0.00%)]\t\tLoss: 0.53079\n",
      "Training Progress: \tEpoch 99 [320/8883 (3.60%)]\t\tLoss: 0.41264\n",
      "Training Progress: \tEpoch 99 [640/8883 (7.19%)]\t\tLoss: 0.21787\n",
      "Training Progress: \tEpoch 99 [960/8883 (10.79%)]\t\tLoss: 0.66778\n",
      "Training Progress: \tEpoch 99 [1280/8883 (14.39%)]\t\tLoss: 0.39074\n",
      "Training Progress: \tEpoch 99 [1600/8883 (17.99%)]\t\tLoss: 0.60672\n",
      "Training Progress: \tEpoch 99 [1920/8883 (21.58%)]\t\tLoss: 0.42335\n",
      "Training Progress: \tEpoch 99 [2240/8883 (25.18%)]\t\tLoss: 0.38284\n",
      "Training Progress: \tEpoch 99 [2560/8883 (28.78%)]\t\tLoss: 0.87911\n",
      "Training Progress: \tEpoch 99 [2880/8883 (32.37%)]\t\tLoss: 0.58526\n",
      "Training Progress: \tEpoch 99 [3200/8883 (35.97%)]\t\tLoss: 0.71252\n",
      "Training Progress: \tEpoch 99 [3520/8883 (39.57%)]\t\tLoss: 0.60023\n",
      "Training Progress: \tEpoch 99 [3840/8883 (43.17%)]\t\tLoss: 0.55345\n",
      "Training Progress: \tEpoch 99 [4160/8883 (46.76%)]\t\tLoss: 0.35983\n",
      "Training Progress: \tEpoch 99 [4480/8883 (50.36%)]\t\tLoss: 0.31606\n",
      "Training Progress: \tEpoch 99 [4800/8883 (53.96%)]\t\tLoss: 0.31579\n",
      "Training Progress: \tEpoch 99 [5120/8883 (57.55%)]\t\tLoss: 0.46234\n",
      "Training Progress: \tEpoch 99 [5440/8883 (61.15%)]\t\tLoss: 0.56899\n",
      "Training Progress: \tEpoch 99 [5760/8883 (64.75%)]\t\tLoss: 0.61984\n",
      "Training Progress: \tEpoch 99 [6080/8883 (68.35%)]\t\tLoss: 0.50584\n",
      "Training Progress: \tEpoch 99 [6400/8883 (71.94%)]\t\tLoss: 0.54424\n",
      "Training Progress: \tEpoch 99 [6720/8883 (75.54%)]\t\tLoss: 0.78682\n",
      "Training Progress: \tEpoch 99 [7040/8883 (79.14%)]\t\tLoss: 0.68649\n",
      "Training Progress: \tEpoch 99 [7360/8883 (82.73%)]\t\tLoss: 0.54064\n",
      "Training Progress: \tEpoch 99 [7680/8883 (86.33%)]\t\tLoss: 0.45871\n",
      "Training Progress: \tEpoch 99 [8000/8883 (89.93%)]\t\tLoss: 0.46523\n",
      "Training Progress: \tEpoch 99 [8320/8883 (93.53%)]\t\tLoss: 0.41133\n",
      "Training Progress: \tEpoch 99 [8640/8883 (97.12%)]\t\tLoss: 0.55486\n",
      "\tTrain loss: 0.01215, Accuracy: 7133/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 553/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/8883 (0.00%)]\t\tLoss: 0.40253\n",
      "Training Progress: \tEpoch 100 [320/8883 (3.60%)]\t\tLoss: 0.48576\n",
      "Training Progress: \tEpoch 100 [640/8883 (7.19%)]\t\tLoss: 0.44232\n",
      "Training Progress: \tEpoch 100 [960/8883 (10.79%)]\t\tLoss: 0.46440\n",
      "Training Progress: \tEpoch 100 [1280/8883 (14.39%)]\t\tLoss: 0.60962\n",
      "Training Progress: \tEpoch 100 [1600/8883 (17.99%)]\t\tLoss: 0.51566\n",
      "Training Progress: \tEpoch 100 [1920/8883 (21.58%)]\t\tLoss: 0.44068\n",
      "Training Progress: \tEpoch 100 [2240/8883 (25.18%)]\t\tLoss: 0.39413\n",
      "Training Progress: \tEpoch 100 [2560/8883 (28.78%)]\t\tLoss: 0.66650\n",
      "Training Progress: \tEpoch 100 [2880/8883 (32.37%)]\t\tLoss: 0.56403\n",
      "Training Progress: \tEpoch 100 [3200/8883 (35.97%)]\t\tLoss: 0.39368\n",
      "Training Progress: \tEpoch 100 [3520/8883 (39.57%)]\t\tLoss: 0.58327\n",
      "Training Progress: \tEpoch 100 [3840/8883 (43.17%)]\t\tLoss: 0.54501\n",
      "Training Progress: \tEpoch 100 [4160/8883 (46.76%)]\t\tLoss: 0.31091\n",
      "Training Progress: \tEpoch 100 [4480/8883 (50.36%)]\t\tLoss: 0.32323\n",
      "Training Progress: \tEpoch 100 [4800/8883 (53.96%)]\t\tLoss: 0.27965\n",
      "Training Progress: \tEpoch 100 [5120/8883 (57.55%)]\t\tLoss: 0.55348\n",
      "Training Progress: \tEpoch 100 [5440/8883 (61.15%)]\t\tLoss: 0.49019\n",
      "Training Progress: \tEpoch 100 [5760/8883 (64.75%)]\t\tLoss: 0.48660\n",
      "Training Progress: \tEpoch 100 [6080/8883 (68.35%)]\t\tLoss: 0.29398\n",
      "Training Progress: \tEpoch 100 [6400/8883 (71.94%)]\t\tLoss: 0.47375\n",
      "Training Progress: \tEpoch 100 [6720/8883 (75.54%)]\t\tLoss: 0.51455\n",
      "Training Progress: \tEpoch 100 [7040/8883 (79.14%)]\t\tLoss: 0.67799\n",
      "Training Progress: \tEpoch 100 [7360/8883 (82.73%)]\t\tLoss: 0.49282\n",
      "Training Progress: \tEpoch 100 [7680/8883 (86.33%)]\t\tLoss: 0.49785\n",
      "Training Progress: \tEpoch 100 [8000/8883 (89.93%)]\t\tLoss: 0.44614\n",
      "Training Progress: \tEpoch 100 [8320/8883 (93.53%)]\t\tLoss: 0.43390\n",
      "Training Progress: \tEpoch 100 [8640/8883 (97.12%)]\t\tLoss: 0.46836\n",
      "\tTrain loss: 0.01211, Accuracy: 7138/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1511/1692 (89.00%)\n",
      "\tTest loss: 0.00188, Accuracy: 525/1772 (29.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9048463356973995\n",
      "Best test accuracy:\n",
      "0.327313769751693\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(lstm, lstm_optimizer, data_loaders_wgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee026a3",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14083be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/8883 (0.00%)]\t\tLoss: 1.45098\n",
      "Training Progress: \tEpoch 1 [320/8883 (3.60%)]\t\tLoss: 1.46814\n",
      "Training Progress: \tEpoch 1 [640/8883 (7.19%)]\t\tLoss: 1.37617\n",
      "Training Progress: \tEpoch 1 [960/8883 (10.79%)]\t\tLoss: 1.40198\n",
      "Training Progress: \tEpoch 1 [1280/8883 (14.39%)]\t\tLoss: 1.51732\n",
      "Training Progress: \tEpoch 1 [1600/8883 (17.99%)]\t\tLoss: 1.38745\n",
      "Training Progress: \tEpoch 1 [1920/8883 (21.58%)]\t\tLoss: 1.37901\n",
      "Training Progress: \tEpoch 1 [2240/8883 (25.18%)]\t\tLoss: 1.43939\n",
      "Training Progress: \tEpoch 1 [2560/8883 (28.78%)]\t\tLoss: 1.43202\n",
      "Training Progress: \tEpoch 1 [2880/8883 (32.37%)]\t\tLoss: 1.36555\n",
      "Training Progress: \tEpoch 1 [3200/8883 (35.97%)]\t\tLoss: 1.38377\n",
      "Training Progress: \tEpoch 1 [3520/8883 (39.57%)]\t\tLoss: 1.39879\n",
      "Training Progress: \tEpoch 1 [3840/8883 (43.17%)]\t\tLoss: 1.34586\n",
      "Training Progress: \tEpoch 1 [4160/8883 (46.76%)]\t\tLoss: 1.41579\n",
      "Training Progress: \tEpoch 1 [4480/8883 (50.36%)]\t\tLoss: 1.37550\n",
      "Training Progress: \tEpoch 1 [4800/8883 (53.96%)]\t\tLoss: 1.36547\n",
      "Training Progress: \tEpoch 1 [5120/8883 (57.55%)]\t\tLoss: 1.41952\n",
      "Training Progress: \tEpoch 1 [5440/8883 (61.15%)]\t\tLoss: 1.39967\n",
      "Training Progress: \tEpoch 1 [5760/8883 (64.75%)]\t\tLoss: 1.37033\n",
      "Training Progress: \tEpoch 1 [6080/8883 (68.35%)]\t\tLoss: 1.39234\n",
      "Training Progress: \tEpoch 1 [6400/8883 (71.94%)]\t\tLoss: 1.43101\n",
      "Training Progress: \tEpoch 1 [6720/8883 (75.54%)]\t\tLoss: 1.40697\n",
      "Training Progress: \tEpoch 1 [7040/8883 (79.14%)]\t\tLoss: 1.37736\n",
      "Training Progress: \tEpoch 1 [7360/8883 (82.73%)]\t\tLoss: 1.40648\n",
      "Training Progress: \tEpoch 1 [7680/8883 (86.33%)]\t\tLoss: 1.34697\n",
      "Training Progress: \tEpoch 1 [8000/8883 (89.93%)]\t\tLoss: 1.42134\n",
      "Training Progress: \tEpoch 1 [8320/8883 (93.53%)]\t\tLoss: 1.41035\n",
      "Training Progress: \tEpoch 1 [8640/8883 (97.12%)]\t\tLoss: 1.42729\n",
      "\tTrain loss: 0.04323, Accuracy: 2510/8883 (28.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 480/1692 (28.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 446/1772 (25.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/8883 (0.00%)]\t\tLoss: 1.39389\n",
      "Training Progress: \tEpoch 2 [320/8883 (3.60%)]\t\tLoss: 1.37890\n",
      "Training Progress: \tEpoch 2 [640/8883 (7.19%)]\t\tLoss: 1.38316\n",
      "Training Progress: \tEpoch 2 [960/8883 (10.79%)]\t\tLoss: 1.37630\n",
      "Training Progress: \tEpoch 2 [1280/8883 (14.39%)]\t\tLoss: 1.43587\n",
      "Training Progress: \tEpoch 2 [1600/8883 (17.99%)]\t\tLoss: 1.42144\n",
      "Training Progress: \tEpoch 2 [1920/8883 (21.58%)]\t\tLoss: 1.35552\n",
      "Training Progress: \tEpoch 2 [2240/8883 (25.18%)]\t\tLoss: 1.33814\n",
      "Training Progress: \tEpoch 2 [2560/8883 (28.78%)]\t\tLoss: 1.37386\n",
      "Training Progress: \tEpoch 2 [2880/8883 (32.37%)]\t\tLoss: 1.41281\n",
      "Training Progress: \tEpoch 2 [3200/8883 (35.97%)]\t\tLoss: 1.38195\n",
      "Training Progress: \tEpoch 2 [3520/8883 (39.57%)]\t\tLoss: 1.35246\n",
      "Training Progress: \tEpoch 2 [3840/8883 (43.17%)]\t\tLoss: 1.40640\n",
      "Training Progress: \tEpoch 2 [4160/8883 (46.76%)]\t\tLoss: 1.36815\n",
      "Training Progress: \tEpoch 2 [4480/8883 (50.36%)]\t\tLoss: 1.36854\n",
      "Training Progress: \tEpoch 2 [4800/8883 (53.96%)]\t\tLoss: 1.37729\n",
      "Training Progress: \tEpoch 2 [5120/8883 (57.55%)]\t\tLoss: 1.42726\n",
      "Training Progress: \tEpoch 2 [5440/8883 (61.15%)]\t\tLoss: 1.39053\n",
      "Training Progress: \tEpoch 2 [5760/8883 (64.75%)]\t\tLoss: 1.40541\n",
      "Training Progress: \tEpoch 2 [6080/8883 (68.35%)]\t\tLoss: 1.38755\n",
      "Training Progress: \tEpoch 2 [6400/8883 (71.94%)]\t\tLoss: 1.38531\n",
      "Training Progress: \tEpoch 2 [6720/8883 (75.54%)]\t\tLoss: 1.44119\n",
      "Training Progress: \tEpoch 2 [7040/8883 (79.14%)]\t\tLoss: 1.41096\n",
      "Training Progress: \tEpoch 2 [7360/8883 (82.73%)]\t\tLoss: 1.37329\n",
      "Training Progress: \tEpoch 2 [7680/8883 (86.33%)]\t\tLoss: 1.41756\n",
      "Training Progress: \tEpoch 2 [8000/8883 (89.93%)]\t\tLoss: 1.45030\n",
      "Training Progress: \tEpoch 2 [8320/8883 (93.53%)]\t\tLoss: 1.36836\n",
      "Training Progress: \tEpoch 2 [8640/8883 (97.12%)]\t\tLoss: 1.37157\n",
      "\tTrain loss: 0.04270, Accuracy: 2744/8883 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 502/1692 (29.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 467/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/8883 (0.00%)]\t\tLoss: 1.39133\n",
      "Training Progress: \tEpoch 3 [320/8883 (3.60%)]\t\tLoss: 1.35018\n",
      "Training Progress: \tEpoch 3 [640/8883 (7.19%)]\t\tLoss: 1.36946\n",
      "Training Progress: \tEpoch 3 [960/8883 (10.79%)]\t\tLoss: 1.40037\n",
      "Training Progress: \tEpoch 3 [1280/8883 (14.39%)]\t\tLoss: 1.43783\n",
      "Training Progress: \tEpoch 3 [1600/8883 (17.99%)]\t\tLoss: 1.36760\n",
      "Training Progress: \tEpoch 3 [1920/8883 (21.58%)]\t\tLoss: 1.33901\n",
      "Training Progress: \tEpoch 3 [2240/8883 (25.18%)]\t\tLoss: 1.36237\n",
      "Training Progress: \tEpoch 3 [2560/8883 (28.78%)]\t\tLoss: 1.35763\n",
      "Training Progress: \tEpoch 3 [2880/8883 (32.37%)]\t\tLoss: 1.37332\n",
      "Training Progress: \tEpoch 3 [3200/8883 (35.97%)]\t\tLoss: 1.31527\n",
      "Training Progress: \tEpoch 3 [3520/8883 (39.57%)]\t\tLoss: 1.37209\n",
      "Training Progress: \tEpoch 3 [3840/8883 (43.17%)]\t\tLoss: 1.47826\n",
      "Training Progress: \tEpoch 3 [4160/8883 (46.76%)]\t\tLoss: 1.41297\n",
      "Training Progress: \tEpoch 3 [4480/8883 (50.36%)]\t\tLoss: 1.28479\n",
      "Training Progress: \tEpoch 3 [4800/8883 (53.96%)]\t\tLoss: 1.40546\n",
      "Training Progress: \tEpoch 3 [5120/8883 (57.55%)]\t\tLoss: 1.38857\n",
      "Training Progress: \tEpoch 3 [5440/8883 (61.15%)]\t\tLoss: 1.39535\n",
      "Training Progress: \tEpoch 3 [5760/8883 (64.75%)]\t\tLoss: 1.36544\n",
      "Training Progress: \tEpoch 3 [6080/8883 (68.35%)]\t\tLoss: 1.36457\n",
      "Training Progress: \tEpoch 3 [6400/8883 (71.94%)]\t\tLoss: 1.35646\n",
      "Training Progress: \tEpoch 3 [6720/8883 (75.54%)]\t\tLoss: 1.40144\n",
      "Training Progress: \tEpoch 3 [7040/8883 (79.14%)]\t\tLoss: 1.37254\n",
      "Training Progress: \tEpoch 3 [7360/8883 (82.73%)]\t\tLoss: 1.40873\n",
      "Training Progress: \tEpoch 3 [7680/8883 (86.33%)]\t\tLoss: 1.35485\n",
      "Training Progress: \tEpoch 3 [8000/8883 (89.93%)]\t\tLoss: 1.38285\n",
      "Training Progress: \tEpoch 3 [8320/8883 (93.53%)]\t\tLoss: 1.41223\n",
      "Training Progress: \tEpoch 3 [8640/8883 (97.12%)]\t\tLoss: 1.37640\n",
      "\tTrain loss: 0.04238, Accuracy: 2842/8883 (31.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 531/1692 (31.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 473/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/8883 (0.00%)]\t\tLoss: 1.36376\n",
      "Training Progress: \tEpoch 4 [320/8883 (3.60%)]\t\tLoss: 1.39284\n",
      "Training Progress: \tEpoch 4 [640/8883 (7.19%)]\t\tLoss: 1.33145\n",
      "Training Progress: \tEpoch 4 [960/8883 (10.79%)]\t\tLoss: 1.30203\n",
      "Training Progress: \tEpoch 4 [1280/8883 (14.39%)]\t\tLoss: 1.39594\n",
      "Training Progress: \tEpoch 4 [1600/8883 (17.99%)]\t\tLoss: 1.37148\n",
      "Training Progress: \tEpoch 4 [1920/8883 (21.58%)]\t\tLoss: 1.39861\n",
      "Training Progress: \tEpoch 4 [2240/8883 (25.18%)]\t\tLoss: 1.34881\n",
      "Training Progress: \tEpoch 4 [2560/8883 (28.78%)]\t\tLoss: 1.34919\n",
      "Training Progress: \tEpoch 4 [2880/8883 (32.37%)]\t\tLoss: 1.32617\n",
      "Training Progress: \tEpoch 4 [3200/8883 (35.97%)]\t\tLoss: 1.37081\n",
      "Training Progress: \tEpoch 4 [3520/8883 (39.57%)]\t\tLoss: 1.28030\n",
      "Training Progress: \tEpoch 4 [3840/8883 (43.17%)]\t\tLoss: 1.43341\n",
      "Training Progress: \tEpoch 4 [4160/8883 (46.76%)]\t\tLoss: 1.39314\n",
      "Training Progress: \tEpoch 4 [4480/8883 (50.36%)]\t\tLoss: 1.29626\n",
      "Training Progress: \tEpoch 4 [4800/8883 (53.96%)]\t\tLoss: 1.42339\n",
      "Training Progress: \tEpoch 4 [5120/8883 (57.55%)]\t\tLoss: 1.31859\n",
      "Training Progress: \tEpoch 4 [5440/8883 (61.15%)]\t\tLoss: 1.43307\n",
      "Training Progress: \tEpoch 4 [5760/8883 (64.75%)]\t\tLoss: 1.38767\n",
      "Training Progress: \tEpoch 4 [6080/8883 (68.35%)]\t\tLoss: 1.36610\n",
      "Training Progress: \tEpoch 4 [6400/8883 (71.94%)]\t\tLoss: 1.39186\n",
      "Training Progress: \tEpoch 4 [6720/8883 (75.54%)]\t\tLoss: 1.41255\n",
      "Training Progress: \tEpoch 4 [7040/8883 (79.14%)]\t\tLoss: 1.33415\n",
      "Training Progress: \tEpoch 4 [7360/8883 (82.73%)]\t\tLoss: 1.40742\n",
      "Training Progress: \tEpoch 4 [7680/8883 (86.33%)]\t\tLoss: 1.33487\n",
      "Training Progress: \tEpoch 4 [8000/8883 (89.93%)]\t\tLoss: 1.31078\n",
      "Training Progress: \tEpoch 4 [8320/8883 (93.53%)]\t\tLoss: 1.33788\n",
      "Training Progress: \tEpoch 4 [8640/8883 (97.12%)]\t\tLoss: 1.33794\n",
      "\tTrain loss: 0.04181, Accuracy: 3068/8883 (34.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 601/1692 (35.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 525/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/8883 (0.00%)]\t\tLoss: 1.34546\n",
      "Training Progress: \tEpoch 5 [320/8883 (3.60%)]\t\tLoss: 1.41739\n",
      "Training Progress: \tEpoch 5 [640/8883 (7.19%)]\t\tLoss: 1.34988\n",
      "Training Progress: \tEpoch 5 [960/8883 (10.79%)]\t\tLoss: 1.29600\n",
      "Training Progress: \tEpoch 5 [1280/8883 (14.39%)]\t\tLoss: 1.40751\n",
      "Training Progress: \tEpoch 5 [1600/8883 (17.99%)]\t\tLoss: 1.38185\n",
      "Training Progress: \tEpoch 5 [1920/8883 (21.58%)]\t\tLoss: 1.40596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 5 [2240/8883 (25.18%)]\t\tLoss: 1.33125\n",
      "Training Progress: \tEpoch 5 [2560/8883 (28.78%)]\t\tLoss: 1.32847\n",
      "Training Progress: \tEpoch 5 [2880/8883 (32.37%)]\t\tLoss: 1.41150\n",
      "Training Progress: \tEpoch 5 [3200/8883 (35.97%)]\t\tLoss: 1.34822\n",
      "Training Progress: \tEpoch 5 [3520/8883 (39.57%)]\t\tLoss: 1.35501\n",
      "Training Progress: \tEpoch 5 [3840/8883 (43.17%)]\t\tLoss: 1.34661\n",
      "Training Progress: \tEpoch 5 [4160/8883 (46.76%)]\t\tLoss: 1.36485\n",
      "Training Progress: \tEpoch 5 [4480/8883 (50.36%)]\t\tLoss: 1.28788\n",
      "Training Progress: \tEpoch 5 [4800/8883 (53.96%)]\t\tLoss: 1.43445\n",
      "Training Progress: \tEpoch 5 [5120/8883 (57.55%)]\t\tLoss: 1.41163\n",
      "Training Progress: \tEpoch 5 [5440/8883 (61.15%)]\t\tLoss: 1.32758\n",
      "Training Progress: \tEpoch 5 [5760/8883 (64.75%)]\t\tLoss: 1.37108\n",
      "Training Progress: \tEpoch 5 [6080/8883 (68.35%)]\t\tLoss: 1.32197\n",
      "Training Progress: \tEpoch 5 [6400/8883 (71.94%)]\t\tLoss: 1.36893\n",
      "Training Progress: \tEpoch 5 [6720/8883 (75.54%)]\t\tLoss: 1.42080\n",
      "Training Progress: \tEpoch 5 [7040/8883 (79.14%)]\t\tLoss: 1.32823\n",
      "Training Progress: \tEpoch 5 [7360/8883 (82.73%)]\t\tLoss: 1.41736\n",
      "Training Progress: \tEpoch 5 [7680/8883 (86.33%)]\t\tLoss: 1.35900\n",
      "Training Progress: \tEpoch 5 [8000/8883 (89.93%)]\t\tLoss: 1.34807\n",
      "Training Progress: \tEpoch 5 [8320/8883 (93.53%)]\t\tLoss: 1.32251\n",
      "Training Progress: \tEpoch 5 [8640/8883 (97.12%)]\t\tLoss: 1.43204\n",
      "\tTrain loss: 0.04149, Accuracy: 3148/8883 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 593/1692 (35.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 503/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/8883 (0.00%)]\t\tLoss: 1.32407\n",
      "Training Progress: \tEpoch 6 [320/8883 (3.60%)]\t\tLoss: 1.43869\n",
      "Training Progress: \tEpoch 6 [640/8883 (7.19%)]\t\tLoss: 1.37773\n",
      "Training Progress: \tEpoch 6 [960/8883 (10.79%)]\t\tLoss: 1.32037\n",
      "Training Progress: \tEpoch 6 [1280/8883 (14.39%)]\t\tLoss: 1.42827\n",
      "Training Progress: \tEpoch 6 [1600/8883 (17.99%)]\t\tLoss: 1.42660\n",
      "Training Progress: \tEpoch 6 [1920/8883 (21.58%)]\t\tLoss: 1.40477\n",
      "Training Progress: \tEpoch 6 [2240/8883 (25.18%)]\t\tLoss: 1.30429\n",
      "Training Progress: \tEpoch 6 [2560/8883 (28.78%)]\t\tLoss: 1.22868\n",
      "Training Progress: \tEpoch 6 [2880/8883 (32.37%)]\t\tLoss: 1.38316\n",
      "Training Progress: \tEpoch 6 [3200/8883 (35.97%)]\t\tLoss: 1.34458\n",
      "Training Progress: \tEpoch 6 [3520/8883 (39.57%)]\t\tLoss: 1.28128\n",
      "Training Progress: \tEpoch 6 [3840/8883 (43.17%)]\t\tLoss: 1.35834\n",
      "Training Progress: \tEpoch 6 [4160/8883 (46.76%)]\t\tLoss: 1.38383\n",
      "Training Progress: \tEpoch 6 [4480/8883 (50.36%)]\t\tLoss: 1.25840\n",
      "Training Progress: \tEpoch 6 [4800/8883 (53.96%)]\t\tLoss: 1.36758\n",
      "Training Progress: \tEpoch 6 [5120/8883 (57.55%)]\t\tLoss: 1.32593\n",
      "Training Progress: \tEpoch 6 [5440/8883 (61.15%)]\t\tLoss: 1.35806\n",
      "Training Progress: \tEpoch 6 [5760/8883 (64.75%)]\t\tLoss: 1.35908\n",
      "Training Progress: \tEpoch 6 [6080/8883 (68.35%)]\t\tLoss: 1.36230\n",
      "Training Progress: \tEpoch 6 [6400/8883 (71.94%)]\t\tLoss: 1.38083\n",
      "Training Progress: \tEpoch 6 [6720/8883 (75.54%)]\t\tLoss: 1.43924\n",
      "Training Progress: \tEpoch 6 [7040/8883 (79.14%)]\t\tLoss: 1.39979\n",
      "Training Progress: \tEpoch 6 [7360/8883 (82.73%)]\t\tLoss: 1.37951\n",
      "Training Progress: \tEpoch 6 [7680/8883 (86.33%)]\t\tLoss: 1.34688\n",
      "Training Progress: \tEpoch 6 [8000/8883 (89.93%)]\t\tLoss: 1.27564\n",
      "Training Progress: \tEpoch 6 [8320/8883 (93.53%)]\t\tLoss: 1.38822\n",
      "Training Progress: \tEpoch 6 [8640/8883 (97.12%)]\t\tLoss: 1.35259\n",
      "\tTrain loss: 0.04102, Accuracy: 3237/8883 (36.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 606/1692 (35.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 480/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/8883 (0.00%)]\t\tLoss: 1.28589\n",
      "Training Progress: \tEpoch 7 [320/8883 (3.60%)]\t\tLoss: 1.29691\n",
      "Training Progress: \tEpoch 7 [640/8883 (7.19%)]\t\tLoss: 1.34406\n",
      "Training Progress: \tEpoch 7 [960/8883 (10.79%)]\t\tLoss: 1.34780\n",
      "Training Progress: \tEpoch 7 [1280/8883 (14.39%)]\t\tLoss: 1.35776\n",
      "Training Progress: \tEpoch 7 [1600/8883 (17.99%)]\t\tLoss: 1.36930\n",
      "Training Progress: \tEpoch 7 [1920/8883 (21.58%)]\t\tLoss: 1.29382\n",
      "Training Progress: \tEpoch 7 [2240/8883 (25.18%)]\t\tLoss: 1.32029\n",
      "Training Progress: \tEpoch 7 [2560/8883 (28.78%)]\t\tLoss: 1.25669\n",
      "Training Progress: \tEpoch 7 [2880/8883 (32.37%)]\t\tLoss: 1.37888\n",
      "Training Progress: \tEpoch 7 [3200/8883 (35.97%)]\t\tLoss: 1.34670\n",
      "Training Progress: \tEpoch 7 [3520/8883 (39.57%)]\t\tLoss: 1.29181\n",
      "Training Progress: \tEpoch 7 [3840/8883 (43.17%)]\t\tLoss: 1.34943\n",
      "Training Progress: \tEpoch 7 [4160/8883 (46.76%)]\t\tLoss: 1.41582\n",
      "Training Progress: \tEpoch 7 [4480/8883 (50.36%)]\t\tLoss: 1.27177\n",
      "Training Progress: \tEpoch 7 [4800/8883 (53.96%)]\t\tLoss: 1.34152\n",
      "Training Progress: \tEpoch 7 [5120/8883 (57.55%)]\t\tLoss: 1.29064\n",
      "Training Progress: \tEpoch 7 [5440/8883 (61.15%)]\t\tLoss: 1.33727\n",
      "Training Progress: \tEpoch 7 [5760/8883 (64.75%)]\t\tLoss: 1.47053\n",
      "Training Progress: \tEpoch 7 [6080/8883 (68.35%)]\t\tLoss: 1.38342\n",
      "Training Progress: \tEpoch 7 [6400/8883 (71.94%)]\t\tLoss: 1.35455\n",
      "Training Progress: \tEpoch 7 [6720/8883 (75.54%)]\t\tLoss: 1.42420\n",
      "Training Progress: \tEpoch 7 [7040/8883 (79.14%)]\t\tLoss: 1.31497\n",
      "Training Progress: \tEpoch 7 [7360/8883 (82.73%)]\t\tLoss: 1.33961\n",
      "Training Progress: \tEpoch 7 [7680/8883 (86.33%)]\t\tLoss: 1.35703\n",
      "Training Progress: \tEpoch 7 [8000/8883 (89.93%)]\t\tLoss: 1.34711\n",
      "Training Progress: \tEpoch 7 [8320/8883 (93.53%)]\t\tLoss: 1.30363\n",
      "Training Progress: \tEpoch 7 [8640/8883 (97.12%)]\t\tLoss: 1.31836\n",
      "\tTrain loss: 0.04046, Accuracy: 3391/8883 (38.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 640/1692 (37.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/8883 (0.00%)]\t\tLoss: 1.34016\n",
      "Training Progress: \tEpoch 8 [320/8883 (3.60%)]\t\tLoss: 1.34043\n",
      "Training Progress: \tEpoch 8 [640/8883 (7.19%)]\t\tLoss: 1.36091\n",
      "Training Progress: \tEpoch 8 [960/8883 (10.79%)]\t\tLoss: 1.29593\n",
      "Training Progress: \tEpoch 8 [1280/8883 (14.39%)]\t\tLoss: 1.39463\n",
      "Training Progress: \tEpoch 8 [1600/8883 (17.99%)]\t\tLoss: 1.35080\n",
      "Training Progress: \tEpoch 8 [1920/8883 (21.58%)]\t\tLoss: 1.35509\n",
      "Training Progress: \tEpoch 8 [2240/8883 (25.18%)]\t\tLoss: 1.32151\n",
      "Training Progress: \tEpoch 8 [2560/8883 (28.78%)]\t\tLoss: 1.31316\n",
      "Training Progress: \tEpoch 8 [2880/8883 (32.37%)]\t\tLoss: 1.33763\n",
      "Training Progress: \tEpoch 8 [3200/8883 (35.97%)]\t\tLoss: 1.31596\n",
      "Training Progress: \tEpoch 8 [3520/8883 (39.57%)]\t\tLoss: 1.21861\n",
      "Training Progress: \tEpoch 8 [3840/8883 (43.17%)]\t\tLoss: 1.34916\n",
      "Training Progress: \tEpoch 8 [4160/8883 (46.76%)]\t\tLoss: 1.35859\n",
      "Training Progress: \tEpoch 8 [4480/8883 (50.36%)]\t\tLoss: 1.27539\n",
      "Training Progress: \tEpoch 8 [4800/8883 (53.96%)]\t\tLoss: 1.37568\n",
      "Training Progress: \tEpoch 8 [5120/8883 (57.55%)]\t\tLoss: 1.31335\n",
      "Training Progress: \tEpoch 8 [5440/8883 (61.15%)]\t\tLoss: 1.33582\n",
      "Training Progress: \tEpoch 8 [5760/8883 (64.75%)]\t\tLoss: 1.37503\n",
      "Training Progress: \tEpoch 8 [6080/8883 (68.35%)]\t\tLoss: 1.33338\n",
      "Training Progress: \tEpoch 8 [6400/8883 (71.94%)]\t\tLoss: 1.33520\n",
      "Training Progress: \tEpoch 8 [6720/8883 (75.54%)]\t\tLoss: 1.44874\n",
      "Training Progress: \tEpoch 8 [7040/8883 (79.14%)]\t\tLoss: 1.29200\n",
      "Training Progress: \tEpoch 8 [7360/8883 (82.73%)]\t\tLoss: 1.39847\n",
      "Training Progress: \tEpoch 8 [7680/8883 (86.33%)]\t\tLoss: 1.37817\n",
      "Training Progress: \tEpoch 8 [8000/8883 (89.93%)]\t\tLoss: 1.25815\n",
      "Training Progress: \tEpoch 8 [8320/8883 (93.53%)]\t\tLoss: 1.30184\n",
      "Training Progress: \tEpoch 8 [8640/8883 (97.12%)]\t\tLoss: 1.28086\n",
      "\tTrain loss: 0.04005, Accuracy: 3500/8883 (39.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 656/1692 (38.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 518/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/8883 (0.00%)]\t\tLoss: 1.28513\n",
      "Training Progress: \tEpoch 9 [320/8883 (3.60%)]\t\tLoss: 1.35116\n",
      "Training Progress: \tEpoch 9 [640/8883 (7.19%)]\t\tLoss: 1.32978\n",
      "Training Progress: \tEpoch 9 [960/8883 (10.79%)]\t\tLoss: 1.22771\n",
      "Training Progress: \tEpoch 9 [1280/8883 (14.39%)]\t\tLoss: 1.37680\n",
      "Training Progress: \tEpoch 9 [1600/8883 (17.99%)]\t\tLoss: 1.31474\n",
      "Training Progress: \tEpoch 9 [1920/8883 (21.58%)]\t\tLoss: 1.27430\n",
      "Training Progress: \tEpoch 9 [2240/8883 (25.18%)]\t\tLoss: 1.25990\n",
      "Training Progress: \tEpoch 9 [2560/8883 (28.78%)]\t\tLoss: 1.31780\n",
      "Training Progress: \tEpoch 9 [2880/8883 (32.37%)]\t\tLoss: 1.26199\n",
      "Training Progress: \tEpoch 9 [3200/8883 (35.97%)]\t\tLoss: 1.33931\n",
      "Training Progress: \tEpoch 9 [3520/8883 (39.57%)]\t\tLoss: 1.22242\n",
      "Training Progress: \tEpoch 9 [3840/8883 (43.17%)]\t\tLoss: 1.34730\n",
      "Training Progress: \tEpoch 9 [4160/8883 (46.76%)]\t\tLoss: 1.25801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 9 [4480/8883 (50.36%)]\t\tLoss: 1.32353\n",
      "Training Progress: \tEpoch 9 [4800/8883 (53.96%)]\t\tLoss: 1.38833\n",
      "Training Progress: \tEpoch 9 [5120/8883 (57.55%)]\t\tLoss: 1.26948\n",
      "Training Progress: \tEpoch 9 [5440/8883 (61.15%)]\t\tLoss: 1.29389\n",
      "Training Progress: \tEpoch 9 [5760/8883 (64.75%)]\t\tLoss: 1.36084\n",
      "Training Progress: \tEpoch 9 [6080/8883 (68.35%)]\t\tLoss: 1.30460\n",
      "Training Progress: \tEpoch 9 [6400/8883 (71.94%)]\t\tLoss: 1.37292\n",
      "Training Progress: \tEpoch 9 [6720/8883 (75.54%)]\t\tLoss: 1.34870\n",
      "Training Progress: \tEpoch 9 [7040/8883 (79.14%)]\t\tLoss: 1.30717\n",
      "Training Progress: \tEpoch 9 [7360/8883 (82.73%)]\t\tLoss: 1.36178\n",
      "Training Progress: \tEpoch 9 [7680/8883 (86.33%)]\t\tLoss: 1.36113\n",
      "Training Progress: \tEpoch 9 [8000/8883 (89.93%)]\t\tLoss: 1.28886\n",
      "Training Progress: \tEpoch 9 [8320/8883 (93.53%)]\t\tLoss: 1.26579\n",
      "Training Progress: \tEpoch 9 [8640/8883 (97.12%)]\t\tLoss: 1.28396\n",
      "\tTrain loss: 0.03921, Accuracy: 3671/8883 (41.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 679/1692 (40.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/8883 (0.00%)]\t\tLoss: 1.31601\n",
      "Training Progress: \tEpoch 10 [320/8883 (3.60%)]\t\tLoss: 1.23732\n",
      "Training Progress: \tEpoch 10 [640/8883 (7.19%)]\t\tLoss: 1.35219\n",
      "Training Progress: \tEpoch 10 [960/8883 (10.79%)]\t\tLoss: 1.24970\n",
      "Training Progress: \tEpoch 10 [1280/8883 (14.39%)]\t\tLoss: 1.35221\n",
      "Training Progress: \tEpoch 10 [1600/8883 (17.99%)]\t\tLoss: 1.28246\n",
      "Training Progress: \tEpoch 10 [1920/8883 (21.58%)]\t\tLoss: 1.22054\n",
      "Training Progress: \tEpoch 10 [2240/8883 (25.18%)]\t\tLoss: 1.28495\n",
      "Training Progress: \tEpoch 10 [2560/8883 (28.78%)]\t\tLoss: 1.22270\n",
      "Training Progress: \tEpoch 10 [2880/8883 (32.37%)]\t\tLoss: 1.35447\n",
      "Training Progress: \tEpoch 10 [3200/8883 (35.97%)]\t\tLoss: 1.27580\n",
      "Training Progress: \tEpoch 10 [3520/8883 (39.57%)]\t\tLoss: 1.19754\n",
      "Training Progress: \tEpoch 10 [3840/8883 (43.17%)]\t\tLoss: 1.25510\n",
      "Training Progress: \tEpoch 10 [4160/8883 (46.76%)]\t\tLoss: 1.31810\n",
      "Training Progress: \tEpoch 10 [4480/8883 (50.36%)]\t\tLoss: 1.31032\n",
      "Training Progress: \tEpoch 10 [4800/8883 (53.96%)]\t\tLoss: 1.40074\n",
      "Training Progress: \tEpoch 10 [5120/8883 (57.55%)]\t\tLoss: 1.32757\n",
      "Training Progress: \tEpoch 10 [5440/8883 (61.15%)]\t\tLoss: 1.43082\n",
      "Training Progress: \tEpoch 10 [5760/8883 (64.75%)]\t\tLoss: 1.35601\n",
      "Training Progress: \tEpoch 10 [6080/8883 (68.35%)]\t\tLoss: 1.38487\n",
      "Training Progress: \tEpoch 10 [6400/8883 (71.94%)]\t\tLoss: 1.25614\n",
      "Training Progress: \tEpoch 10 [6720/8883 (75.54%)]\t\tLoss: 1.41315\n",
      "Training Progress: \tEpoch 10 [7040/8883 (79.14%)]\t\tLoss: 1.27745\n",
      "Training Progress: \tEpoch 10 [7360/8883 (82.73%)]\t\tLoss: 1.42826\n",
      "Training Progress: \tEpoch 10 [7680/8883 (86.33%)]\t\tLoss: 1.48097\n",
      "Training Progress: \tEpoch 10 [8000/8883 (89.93%)]\t\tLoss: 1.28818\n",
      "Training Progress: \tEpoch 10 [8320/8883 (93.53%)]\t\tLoss: 1.21814\n",
      "Training Progress: \tEpoch 10 [8640/8883 (97.12%)]\t\tLoss: 1.23099\n",
      "\tTrain loss: 0.03894, Accuracy: 3725/8883 (41.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 699/1692 (41.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/8883 (0.00%)]\t\tLoss: 1.33893\n",
      "Training Progress: \tEpoch 11 [320/8883 (3.60%)]\t\tLoss: 1.31934\n",
      "Training Progress: \tEpoch 11 [640/8883 (7.19%)]\t\tLoss: 1.34354\n",
      "Training Progress: \tEpoch 11 [960/8883 (10.79%)]\t\tLoss: 1.29615\n",
      "Training Progress: \tEpoch 11 [1280/8883 (14.39%)]\t\tLoss: 1.35395\n",
      "Training Progress: \tEpoch 11 [1600/8883 (17.99%)]\t\tLoss: 1.30739\n",
      "Training Progress: \tEpoch 11 [1920/8883 (21.58%)]\t\tLoss: 1.29573\n",
      "Training Progress: \tEpoch 11 [2240/8883 (25.18%)]\t\tLoss: 1.20297\n",
      "Training Progress: \tEpoch 11 [2560/8883 (28.78%)]\t\tLoss: 1.21941\n",
      "Training Progress: \tEpoch 11 [2880/8883 (32.37%)]\t\tLoss: 1.32649\n",
      "Training Progress: \tEpoch 11 [3200/8883 (35.97%)]\t\tLoss: 1.24405\n",
      "Training Progress: \tEpoch 11 [3520/8883 (39.57%)]\t\tLoss: 1.17309\n",
      "Training Progress: \tEpoch 11 [3840/8883 (43.17%)]\t\tLoss: 1.24259\n",
      "Training Progress: \tEpoch 11 [4160/8883 (46.76%)]\t\tLoss: 1.29933\n",
      "Training Progress: \tEpoch 11 [4480/8883 (50.36%)]\t\tLoss: 1.25055\n",
      "Training Progress: \tEpoch 11 [4800/8883 (53.96%)]\t\tLoss: 1.32177\n",
      "Training Progress: \tEpoch 11 [5120/8883 (57.55%)]\t\tLoss: 1.22425\n",
      "Training Progress: \tEpoch 11 [5440/8883 (61.15%)]\t\tLoss: 1.25248\n",
      "Training Progress: \tEpoch 11 [5760/8883 (64.75%)]\t\tLoss: 1.36171\n",
      "Training Progress: \tEpoch 11 [6080/8883 (68.35%)]\t\tLoss: 1.27565\n",
      "Training Progress: \tEpoch 11 [6400/8883 (71.94%)]\t\tLoss: 1.25887\n",
      "Training Progress: \tEpoch 11 [6720/8883 (75.54%)]\t\tLoss: 1.47652\n",
      "Training Progress: \tEpoch 11 [7040/8883 (79.14%)]\t\tLoss: 1.26848\n",
      "Training Progress: \tEpoch 11 [7360/8883 (82.73%)]\t\tLoss: 1.31909\n",
      "Training Progress: \tEpoch 11 [7680/8883 (86.33%)]\t\tLoss: 1.37553\n",
      "Training Progress: \tEpoch 11 [8000/8883 (89.93%)]\t\tLoss: 1.22052\n",
      "Training Progress: \tEpoch 11 [8320/8883 (93.53%)]\t\tLoss: 1.34713\n",
      "Training Progress: \tEpoch 11 [8640/8883 (97.12%)]\t\tLoss: 1.20138\n",
      "\tTrain loss: 0.03794, Accuracy: 3911/8883 (44.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 732/1692 (43.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 540/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/8883 (0.00%)]\t\tLoss: 1.21211\n",
      "Training Progress: \tEpoch 12 [320/8883 (3.60%)]\t\tLoss: 1.22894\n",
      "Training Progress: \tEpoch 12 [640/8883 (7.19%)]\t\tLoss: 1.31075\n",
      "Training Progress: \tEpoch 12 [960/8883 (10.79%)]\t\tLoss: 1.22514\n",
      "Training Progress: \tEpoch 12 [1280/8883 (14.39%)]\t\tLoss: 1.34820\n",
      "Training Progress: \tEpoch 12 [1600/8883 (17.99%)]\t\tLoss: 1.30227\n",
      "Training Progress: \tEpoch 12 [1920/8883 (21.58%)]\t\tLoss: 1.41820\n",
      "Training Progress: \tEpoch 12 [2240/8883 (25.18%)]\t\tLoss: 1.34253\n",
      "Training Progress: \tEpoch 12 [2560/8883 (28.78%)]\t\tLoss: 1.19157\n",
      "Training Progress: \tEpoch 12 [2880/8883 (32.37%)]\t\tLoss: 1.22729\n",
      "Training Progress: \tEpoch 12 [3200/8883 (35.97%)]\t\tLoss: 1.21857\n",
      "Training Progress: \tEpoch 12 [3520/8883 (39.57%)]\t\tLoss: 1.25935\n",
      "Training Progress: \tEpoch 12 [3840/8883 (43.17%)]\t\tLoss: 1.25902\n",
      "Training Progress: \tEpoch 12 [4160/8883 (46.76%)]\t\tLoss: 1.32929\n",
      "Training Progress: \tEpoch 12 [4480/8883 (50.36%)]\t\tLoss: 1.25756\n",
      "Training Progress: \tEpoch 12 [4800/8883 (53.96%)]\t\tLoss: 1.29103\n",
      "Training Progress: \tEpoch 12 [5120/8883 (57.55%)]\t\tLoss: 1.25472\n",
      "Training Progress: \tEpoch 12 [5440/8883 (61.15%)]\t\tLoss: 1.32252\n",
      "Training Progress: \tEpoch 12 [5760/8883 (64.75%)]\t\tLoss: 1.40297\n",
      "Training Progress: \tEpoch 12 [6080/8883 (68.35%)]\t\tLoss: 1.33685\n",
      "Training Progress: \tEpoch 12 [6400/8883 (71.94%)]\t\tLoss: 1.28400\n",
      "Training Progress: \tEpoch 12 [6720/8883 (75.54%)]\t\tLoss: 1.39454\n",
      "Training Progress: \tEpoch 12 [7040/8883 (79.14%)]\t\tLoss: 1.23763\n",
      "Training Progress: \tEpoch 12 [7360/8883 (82.73%)]\t\tLoss: 1.22970\n",
      "Training Progress: \tEpoch 12 [7680/8883 (86.33%)]\t\tLoss: 1.35655\n",
      "Training Progress: \tEpoch 12 [8000/8883 (89.93%)]\t\tLoss: 1.30995\n",
      "Training Progress: \tEpoch 12 [8320/8883 (93.53%)]\t\tLoss: 1.20193\n",
      "Training Progress: \tEpoch 12 [8640/8883 (97.12%)]\t\tLoss: 1.26946\n",
      "\tTrain loss: 0.03731, Accuracy: 4034/8883 (45.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 751/1692 (44.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 539/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/8883 (0.00%)]\t\tLoss: 1.21211\n",
      "Training Progress: \tEpoch 13 [320/8883 (3.60%)]\t\tLoss: 1.23093\n",
      "Training Progress: \tEpoch 13 [640/8883 (7.19%)]\t\tLoss: 1.31043\n",
      "Training Progress: \tEpoch 13 [960/8883 (10.79%)]\t\tLoss: 1.12227\n",
      "Training Progress: \tEpoch 13 [1280/8883 (14.39%)]\t\tLoss: 1.28092\n",
      "Training Progress: \tEpoch 13 [1600/8883 (17.99%)]\t\tLoss: 1.20747\n",
      "Training Progress: \tEpoch 13 [1920/8883 (21.58%)]\t\tLoss: 1.27094\n",
      "Training Progress: \tEpoch 13 [2240/8883 (25.18%)]\t\tLoss: 1.19922\n",
      "Training Progress: \tEpoch 13 [2560/8883 (28.78%)]\t\tLoss: 1.17806\n",
      "Training Progress: \tEpoch 13 [2880/8883 (32.37%)]\t\tLoss: 1.28557\n",
      "Training Progress: \tEpoch 13 [3200/8883 (35.97%)]\t\tLoss: 1.18347\n",
      "Training Progress: \tEpoch 13 [3520/8883 (39.57%)]\t\tLoss: 1.21394\n",
      "Training Progress: \tEpoch 13 [3840/8883 (43.17%)]\t\tLoss: 1.31592\n",
      "Training Progress: \tEpoch 13 [4160/8883 (46.76%)]\t\tLoss: 1.19515\n",
      "Training Progress: \tEpoch 13 [4480/8883 (50.36%)]\t\tLoss: 1.20481\n",
      "Training Progress: \tEpoch 13 [4800/8883 (53.96%)]\t\tLoss: 1.25642\n",
      "Training Progress: \tEpoch 13 [5120/8883 (57.55%)]\t\tLoss: 1.27514\n",
      "Training Progress: \tEpoch 13 [5440/8883 (61.15%)]\t\tLoss: 1.14867\n",
      "Training Progress: \tEpoch 13 [5760/8883 (64.75%)]\t\tLoss: 1.37203\n",
      "Training Progress: \tEpoch 13 [6080/8883 (68.35%)]\t\tLoss: 1.24756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 13 [6400/8883 (71.94%)]\t\tLoss: 1.35355\n",
      "Training Progress: \tEpoch 13 [6720/8883 (75.54%)]\t\tLoss: 1.37455\n",
      "Training Progress: \tEpoch 13 [7040/8883 (79.14%)]\t\tLoss: 1.19104\n",
      "Training Progress: \tEpoch 13 [7360/8883 (82.73%)]\t\tLoss: 1.24812\n",
      "Training Progress: \tEpoch 13 [7680/8883 (86.33%)]\t\tLoss: 1.33558\n",
      "Training Progress: \tEpoch 13 [8000/8883 (89.93%)]\t\tLoss: 1.02724\n",
      "Training Progress: \tEpoch 13 [8320/8883 (93.53%)]\t\tLoss: 1.16619\n",
      "Training Progress: \tEpoch 13 [8640/8883 (97.12%)]\t\tLoss: 1.27667\n",
      "\tTrain loss: 0.03642, Accuracy: 4258/8883 (47.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 792/1692 (46.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/8883 (0.00%)]\t\tLoss: 1.16617\n",
      "Training Progress: \tEpoch 14 [320/8883 (3.60%)]\t\tLoss: 1.23375\n",
      "Training Progress: \tEpoch 14 [640/8883 (7.19%)]\t\tLoss: 1.47553\n",
      "Training Progress: \tEpoch 14 [960/8883 (10.79%)]\t\tLoss: 1.22793\n",
      "Training Progress: \tEpoch 14 [1280/8883 (14.39%)]\t\tLoss: 1.36966\n",
      "Training Progress: \tEpoch 14 [1600/8883 (17.99%)]\t\tLoss: 1.20951\n",
      "Training Progress: \tEpoch 14 [1920/8883 (21.58%)]\t\tLoss: 1.26222\n",
      "Training Progress: \tEpoch 14 [2240/8883 (25.18%)]\t\tLoss: 1.10047\n",
      "Training Progress: \tEpoch 14 [2560/8883 (28.78%)]\t\tLoss: 1.19800\n",
      "Training Progress: \tEpoch 14 [2880/8883 (32.37%)]\t\tLoss: 1.17711\n",
      "Training Progress: \tEpoch 14 [3200/8883 (35.97%)]\t\tLoss: 1.22133\n",
      "Training Progress: \tEpoch 14 [3520/8883 (39.57%)]\t\tLoss: 1.20292\n",
      "Training Progress: \tEpoch 14 [3840/8883 (43.17%)]\t\tLoss: 1.30300\n",
      "Training Progress: \tEpoch 14 [4160/8883 (46.76%)]\t\tLoss: 1.18768\n",
      "Training Progress: \tEpoch 14 [4480/8883 (50.36%)]\t\tLoss: 1.28943\n",
      "Training Progress: \tEpoch 14 [4800/8883 (53.96%)]\t\tLoss: 1.17860\n",
      "Training Progress: \tEpoch 14 [5120/8883 (57.55%)]\t\tLoss: 1.27204\n",
      "Training Progress: \tEpoch 14 [5440/8883 (61.15%)]\t\tLoss: 1.26059\n",
      "Training Progress: \tEpoch 14 [5760/8883 (64.75%)]\t\tLoss: 1.39802\n",
      "Training Progress: \tEpoch 14 [6080/8883 (68.35%)]\t\tLoss: 1.25182\n",
      "Training Progress: \tEpoch 14 [6400/8883 (71.94%)]\t\tLoss: 1.27260\n",
      "Training Progress: \tEpoch 14 [6720/8883 (75.54%)]\t\tLoss: 1.27191\n",
      "Training Progress: \tEpoch 14 [7040/8883 (79.14%)]\t\tLoss: 1.25566\n",
      "Training Progress: \tEpoch 14 [7360/8883 (82.73%)]\t\tLoss: 1.23564\n",
      "Training Progress: \tEpoch 14 [7680/8883 (86.33%)]\t\tLoss: 1.35573\n",
      "Training Progress: \tEpoch 14 [8000/8883 (89.93%)]\t\tLoss: 1.14840\n",
      "Training Progress: \tEpoch 14 [8320/8883 (93.53%)]\t\tLoss: 1.25626\n",
      "Training Progress: \tEpoch 14 [8640/8883 (97.12%)]\t\tLoss: 1.17947\n",
      "\tTrain loss: 0.03548, Accuracy: 4376/8883 (49.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 795/1692 (46.00%)\n",
      "\tTest loss: 0.00085, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/8883 (0.00%)]\t\tLoss: 1.22493\n",
      "Training Progress: \tEpoch 15 [320/8883 (3.60%)]\t\tLoss: 1.19186\n",
      "Training Progress: \tEpoch 15 [640/8883 (7.19%)]\t\tLoss: 1.28710\n",
      "Training Progress: \tEpoch 15 [960/8883 (10.79%)]\t\tLoss: 1.07392\n",
      "Training Progress: \tEpoch 15 [1280/8883 (14.39%)]\t\tLoss: 1.30105\n",
      "Training Progress: \tEpoch 15 [1600/8883 (17.99%)]\t\tLoss: 1.22353\n",
      "Training Progress: \tEpoch 15 [1920/8883 (21.58%)]\t\tLoss: 1.19434\n",
      "Training Progress: \tEpoch 15 [2240/8883 (25.18%)]\t\tLoss: 1.15135\n",
      "Training Progress: \tEpoch 15 [2560/8883 (28.78%)]\t\tLoss: 1.10380\n",
      "Training Progress: \tEpoch 15 [2880/8883 (32.37%)]\t\tLoss: 1.16538\n",
      "Training Progress: \tEpoch 15 [3200/8883 (35.97%)]\t\tLoss: 1.13372\n",
      "Training Progress: \tEpoch 15 [3520/8883 (39.57%)]\t\tLoss: 1.16629\n",
      "Training Progress: \tEpoch 15 [3840/8883 (43.17%)]\t\tLoss: 1.07088\n",
      "Training Progress: \tEpoch 15 [4160/8883 (46.76%)]\t\tLoss: 1.26852\n",
      "Training Progress: \tEpoch 15 [4480/8883 (50.36%)]\t\tLoss: 1.19358\n",
      "Training Progress: \tEpoch 15 [4800/8883 (53.96%)]\t\tLoss: 1.11811\n",
      "Training Progress: \tEpoch 15 [5120/8883 (57.55%)]\t\tLoss: 1.18170\n",
      "Training Progress: \tEpoch 15 [5440/8883 (61.15%)]\t\tLoss: 1.24671\n",
      "Training Progress: \tEpoch 15 [5760/8883 (64.75%)]\t\tLoss: 1.38961\n",
      "Training Progress: \tEpoch 15 [6080/8883 (68.35%)]\t\tLoss: 1.15305\n",
      "Training Progress: \tEpoch 15 [6400/8883 (71.94%)]\t\tLoss: 1.33820\n",
      "Training Progress: \tEpoch 15 [6720/8883 (75.54%)]\t\tLoss: 1.53725\n",
      "Training Progress: \tEpoch 15 [7040/8883 (79.14%)]\t\tLoss: 1.18873\n",
      "Training Progress: \tEpoch 15 [7360/8883 (82.73%)]\t\tLoss: 1.26894\n",
      "Training Progress: \tEpoch 15 [7680/8883 (86.33%)]\t\tLoss: 1.28332\n",
      "Training Progress: \tEpoch 15 [8000/8883 (89.93%)]\t\tLoss: 1.17408\n",
      "Training Progress: \tEpoch 15 [8320/8883 (93.53%)]\t\tLoss: 1.26118\n",
      "Training Progress: \tEpoch 15 [8640/8883 (97.12%)]\t\tLoss: 1.16627\n",
      "\tTrain loss: 0.03480, Accuracy: 4442/8883 (50.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 835/1692 (49.00%)\n",
      "\tTest loss: 0.00087, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/8883 (0.00%)]\t\tLoss: 1.07291\n",
      "Training Progress: \tEpoch 16 [320/8883 (3.60%)]\t\tLoss: 1.17542\n",
      "Training Progress: \tEpoch 16 [640/8883 (7.19%)]\t\tLoss: 1.35759\n",
      "Training Progress: \tEpoch 16 [960/8883 (10.79%)]\t\tLoss: 1.22132\n",
      "Training Progress: \tEpoch 16 [1280/8883 (14.39%)]\t\tLoss: 1.33566\n",
      "Training Progress: \tEpoch 16 [1600/8883 (17.99%)]\t\tLoss: 1.22992\n",
      "Training Progress: \tEpoch 16 [1920/8883 (21.58%)]\t\tLoss: 1.32305\n",
      "Training Progress: \tEpoch 16 [2240/8883 (25.18%)]\t\tLoss: 1.11360\n",
      "Training Progress: \tEpoch 16 [2560/8883 (28.78%)]\t\tLoss: 1.16373\n",
      "Training Progress: \tEpoch 16 [2880/8883 (32.37%)]\t\tLoss: 1.13504\n",
      "Training Progress: \tEpoch 16 [3200/8883 (35.97%)]\t\tLoss: 1.21601\n",
      "Training Progress: \tEpoch 16 [3520/8883 (39.57%)]\t\tLoss: 1.19144\n",
      "Training Progress: \tEpoch 16 [3840/8883 (43.17%)]\t\tLoss: 1.14352\n",
      "Training Progress: \tEpoch 16 [4160/8883 (46.76%)]\t\tLoss: 1.23120\n",
      "Training Progress: \tEpoch 16 [4480/8883 (50.36%)]\t\tLoss: 1.17912\n",
      "Training Progress: \tEpoch 16 [4800/8883 (53.96%)]\t\tLoss: 1.17795\n",
      "Training Progress: \tEpoch 16 [5120/8883 (57.55%)]\t\tLoss: 1.24854\n",
      "Training Progress: \tEpoch 16 [5440/8883 (61.15%)]\t\tLoss: 1.27384\n",
      "Training Progress: \tEpoch 16 [5760/8883 (64.75%)]\t\tLoss: 1.37308\n",
      "Training Progress: \tEpoch 16 [6080/8883 (68.35%)]\t\tLoss: 1.24327\n",
      "Training Progress: \tEpoch 16 [6400/8883 (71.94%)]\t\tLoss: 1.26647\n",
      "Training Progress: \tEpoch 16 [6720/8883 (75.54%)]\t\tLoss: 1.30690\n",
      "Training Progress: \tEpoch 16 [7040/8883 (79.14%)]\t\tLoss: 1.13495\n",
      "Training Progress: \tEpoch 16 [7360/8883 (82.73%)]\t\tLoss: 1.25837\n",
      "Training Progress: \tEpoch 16 [7680/8883 (86.33%)]\t\tLoss: 1.21400\n",
      "Training Progress: \tEpoch 16 [8000/8883 (89.93%)]\t\tLoss: 1.15397\n",
      "Training Progress: \tEpoch 16 [8320/8883 (93.53%)]\t\tLoss: 1.09665\n",
      "Training Progress: \tEpoch 16 [8640/8883 (97.12%)]\t\tLoss: 1.12322\n",
      "\tTrain loss: 0.03414, Accuracy: 4532/8883 (51.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 846/1692 (50.00%)\n",
      "\tTest loss: 0.00087, Accuracy: 537/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/8883 (0.00%)]\t\tLoss: 1.20766\n",
      "Training Progress: \tEpoch 17 [320/8883 (3.60%)]\t\tLoss: 1.12456\n",
      "Training Progress: \tEpoch 17 [640/8883 (7.19%)]\t\tLoss: 1.25013\n",
      "Training Progress: \tEpoch 17 [960/8883 (10.79%)]\t\tLoss: 1.28753\n",
      "Training Progress: \tEpoch 17 [1280/8883 (14.39%)]\t\tLoss: 1.33106\n",
      "Training Progress: \tEpoch 17 [1600/8883 (17.99%)]\t\tLoss: 1.27943\n",
      "Training Progress: \tEpoch 17 [1920/8883 (21.58%)]\t\tLoss: 1.28965\n",
      "Training Progress: \tEpoch 17 [2240/8883 (25.18%)]\t\tLoss: 1.00932\n",
      "Training Progress: \tEpoch 17 [2560/8883 (28.78%)]\t\tLoss: 1.06859\n",
      "Training Progress: \tEpoch 17 [2880/8883 (32.37%)]\t\tLoss: 1.25648\n",
      "Training Progress: \tEpoch 17 [3200/8883 (35.97%)]\t\tLoss: 1.24747\n",
      "Training Progress: \tEpoch 17 [3520/8883 (39.57%)]\t\tLoss: 1.17467\n",
      "Training Progress: \tEpoch 17 [3840/8883 (43.17%)]\t\tLoss: 1.08754\n",
      "Training Progress: \tEpoch 17 [4160/8883 (46.76%)]\t\tLoss: 1.31480\n",
      "Training Progress: \tEpoch 17 [4480/8883 (50.36%)]\t\tLoss: 1.12494\n",
      "Training Progress: \tEpoch 17 [4800/8883 (53.96%)]\t\tLoss: 1.10609\n",
      "Training Progress: \tEpoch 17 [5120/8883 (57.55%)]\t\tLoss: 1.18930\n",
      "Training Progress: \tEpoch 17 [5440/8883 (61.15%)]\t\tLoss: 1.07312\n",
      "Training Progress: \tEpoch 17 [5760/8883 (64.75%)]\t\tLoss: 1.30789\n",
      "Training Progress: \tEpoch 17 [6080/8883 (68.35%)]\t\tLoss: 1.07643\n",
      "Training Progress: \tEpoch 17 [6400/8883 (71.94%)]\t\tLoss: 1.22735\n",
      "Training Progress: \tEpoch 17 [6720/8883 (75.54%)]\t\tLoss: 1.25696\n",
      "Training Progress: \tEpoch 17 [7040/8883 (79.14%)]\t\tLoss: 1.18028\n",
      "Training Progress: \tEpoch 17 [7360/8883 (82.73%)]\t\tLoss: 1.22581\n",
      "Training Progress: \tEpoch 17 [7680/8883 (86.33%)]\t\tLoss: 1.26676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 17 [8000/8883 (89.93%)]\t\tLoss: 1.05534\n",
      "Training Progress: \tEpoch 17 [8320/8883 (93.53%)]\t\tLoss: 1.22835\n",
      "Training Progress: \tEpoch 17 [8640/8883 (97.12%)]\t\tLoss: 1.05594\n",
      "\tTrain loss: 0.03389, Accuracy: 4501/8883 (50.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 823/1692 (48.00%)\n",
      "\tTest loss: 0.00091, Accuracy: 527/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/8883 (0.00%)]\t\tLoss: 1.18861\n",
      "Training Progress: \tEpoch 18 [320/8883 (3.60%)]\t\tLoss: 1.02985\n",
      "Training Progress: \tEpoch 18 [640/8883 (7.19%)]\t\tLoss: 1.15642\n",
      "Training Progress: \tEpoch 18 [960/8883 (10.79%)]\t\tLoss: 1.05306\n",
      "Training Progress: \tEpoch 18 [1280/8883 (14.39%)]\t\tLoss: 1.25359\n",
      "Training Progress: \tEpoch 18 [1600/8883 (17.99%)]\t\tLoss: 1.23795\n",
      "Training Progress: \tEpoch 18 [1920/8883 (21.58%)]\t\tLoss: 1.16211\n",
      "Training Progress: \tEpoch 18 [2240/8883 (25.18%)]\t\tLoss: 1.03055\n",
      "Training Progress: \tEpoch 18 [2560/8883 (28.78%)]\t\tLoss: 1.11394\n",
      "Training Progress: \tEpoch 18 [2880/8883 (32.37%)]\t\tLoss: 1.18103\n",
      "Training Progress: \tEpoch 18 [3200/8883 (35.97%)]\t\tLoss: 1.14422\n",
      "Training Progress: \tEpoch 18 [3520/8883 (39.57%)]\t\tLoss: 1.08176\n",
      "Training Progress: \tEpoch 18 [3840/8883 (43.17%)]\t\tLoss: 1.09580\n",
      "Training Progress: \tEpoch 18 [4160/8883 (46.76%)]\t\tLoss: 1.12728\n",
      "Training Progress: \tEpoch 18 [4480/8883 (50.36%)]\t\tLoss: 0.99969\n",
      "Training Progress: \tEpoch 18 [4800/8883 (53.96%)]\t\tLoss: 1.00134\n",
      "Training Progress: \tEpoch 18 [5120/8883 (57.55%)]\t\tLoss: 1.15711\n",
      "Training Progress: \tEpoch 18 [5440/8883 (61.15%)]\t\tLoss: 1.05498\n",
      "Training Progress: \tEpoch 18 [5760/8883 (64.75%)]\t\tLoss: 1.33911\n",
      "Training Progress: \tEpoch 18 [6080/8883 (68.35%)]\t\tLoss: 1.18414\n",
      "Training Progress: \tEpoch 18 [6400/8883 (71.94%)]\t\tLoss: 1.25017\n",
      "Training Progress: \tEpoch 18 [6720/8883 (75.54%)]\t\tLoss: 1.31336\n",
      "Training Progress: \tEpoch 18 [7040/8883 (79.14%)]\t\tLoss: 1.11101\n",
      "Training Progress: \tEpoch 18 [7360/8883 (82.73%)]\t\tLoss: 1.24101\n",
      "Training Progress: \tEpoch 18 [7680/8883 (86.33%)]\t\tLoss: 1.20529\n",
      "Training Progress: \tEpoch 18 [8000/8883 (89.93%)]\t\tLoss: 0.98293\n",
      "Training Progress: \tEpoch 18 [8320/8883 (93.53%)]\t\tLoss: 1.11091\n",
      "Training Progress: \tEpoch 18 [8640/8883 (97.12%)]\t\tLoss: 1.03196\n",
      "\tTrain loss: 0.03218, Accuracy: 4812/8883 (54.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 914/1692 (54.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/8883 (0.00%)]\t\tLoss: 1.11000\n",
      "Training Progress: \tEpoch 19 [320/8883 (3.60%)]\t\tLoss: 1.00225\n",
      "Training Progress: \tEpoch 19 [640/8883 (7.19%)]\t\tLoss: 1.29982\n",
      "Training Progress: \tEpoch 19 [960/8883 (10.79%)]\t\tLoss: 1.13717\n",
      "Training Progress: \tEpoch 19 [1280/8883 (14.39%)]\t\tLoss: 1.19729\n",
      "Training Progress: \tEpoch 19 [1600/8883 (17.99%)]\t\tLoss: 1.17447\n",
      "Training Progress: \tEpoch 19 [1920/8883 (21.58%)]\t\tLoss: 1.11951\n",
      "Training Progress: \tEpoch 19 [2240/8883 (25.18%)]\t\tLoss: 1.07499\n",
      "Training Progress: \tEpoch 19 [2560/8883 (28.78%)]\t\tLoss: 0.99817\n",
      "Training Progress: \tEpoch 19 [2880/8883 (32.37%)]\t\tLoss: 1.06299\n",
      "Training Progress: \tEpoch 19 [3200/8883 (35.97%)]\t\tLoss: 1.21267\n",
      "Training Progress: \tEpoch 19 [3520/8883 (39.57%)]\t\tLoss: 1.16765\n",
      "Training Progress: \tEpoch 19 [3840/8883 (43.17%)]\t\tLoss: 1.02491\n",
      "Training Progress: \tEpoch 19 [4160/8883 (46.76%)]\t\tLoss: 1.08376\n",
      "Training Progress: \tEpoch 19 [4480/8883 (50.36%)]\t\tLoss: 1.12204\n",
      "Training Progress: \tEpoch 19 [4800/8883 (53.96%)]\t\tLoss: 0.96167\n",
      "Training Progress: \tEpoch 19 [5120/8883 (57.55%)]\t\tLoss: 1.18057\n",
      "Training Progress: \tEpoch 19 [5440/8883 (61.15%)]\t\tLoss: 1.03945\n",
      "Training Progress: \tEpoch 19 [5760/8883 (64.75%)]\t\tLoss: 1.42224\n",
      "Training Progress: \tEpoch 19 [6080/8883 (68.35%)]\t\tLoss: 0.98115\n",
      "Training Progress: \tEpoch 19 [6400/8883 (71.94%)]\t\tLoss: 1.15412\n",
      "Training Progress: \tEpoch 19 [6720/8883 (75.54%)]\t\tLoss: 1.30667\n",
      "Training Progress: \tEpoch 19 [7040/8883 (79.14%)]\t\tLoss: 1.11558\n",
      "Training Progress: \tEpoch 19 [7360/8883 (82.73%)]\t\tLoss: 1.15964\n",
      "Training Progress: \tEpoch 19 [7680/8883 (86.33%)]\t\tLoss: 1.18770\n",
      "Training Progress: \tEpoch 19 [8000/8883 (89.93%)]\t\tLoss: 1.09038\n",
      "Training Progress: \tEpoch 19 [8320/8883 (93.53%)]\t\tLoss: 1.04632\n",
      "Training Progress: \tEpoch 19 [8640/8883 (97.12%)]\t\tLoss: 1.05192\n",
      "\tTrain loss: 0.03195, Accuracy: 4791/8883 (53.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 923/1692 (54.00%)\n",
      "\tTest loss: 0.00093, Accuracy: 534/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/8883 (0.00%)]\t\tLoss: 1.17389\n",
      "Training Progress: \tEpoch 20 [320/8883 (3.60%)]\t\tLoss: 1.03253\n",
      "Training Progress: \tEpoch 20 [640/8883 (7.19%)]\t\tLoss: 1.25304\n",
      "Training Progress: \tEpoch 20 [960/8883 (10.79%)]\t\tLoss: 1.16051\n",
      "Training Progress: \tEpoch 20 [1280/8883 (14.39%)]\t\tLoss: 1.36293\n",
      "Training Progress: \tEpoch 20 [1600/8883 (17.99%)]\t\tLoss: 1.13380\n",
      "Training Progress: \tEpoch 20 [1920/8883 (21.58%)]\t\tLoss: 1.13972\n",
      "Training Progress: \tEpoch 20 [2240/8883 (25.18%)]\t\tLoss: 0.93682\n",
      "Training Progress: \tEpoch 20 [2560/8883 (28.78%)]\t\tLoss: 1.01159\n",
      "Training Progress: \tEpoch 20 [2880/8883 (32.37%)]\t\tLoss: 1.03617\n",
      "Training Progress: \tEpoch 20 [3200/8883 (35.97%)]\t\tLoss: 1.19341\n",
      "Training Progress: \tEpoch 20 [3520/8883 (39.57%)]\t\tLoss: 1.03680\n",
      "Training Progress: \tEpoch 20 [3840/8883 (43.17%)]\t\tLoss: 0.91664\n",
      "Training Progress: \tEpoch 20 [4160/8883 (46.76%)]\t\tLoss: 1.20873\n",
      "Training Progress: \tEpoch 20 [4480/8883 (50.36%)]\t\tLoss: 1.22988\n",
      "Training Progress: \tEpoch 20 [4800/8883 (53.96%)]\t\tLoss: 0.95154\n",
      "Training Progress: \tEpoch 20 [5120/8883 (57.55%)]\t\tLoss: 1.12493\n",
      "Training Progress: \tEpoch 20 [5440/8883 (61.15%)]\t\tLoss: 1.14397\n",
      "Training Progress: \tEpoch 20 [5760/8883 (64.75%)]\t\tLoss: 1.46106\n",
      "Training Progress: \tEpoch 20 [6080/8883 (68.35%)]\t\tLoss: 1.02171\n",
      "Training Progress: \tEpoch 20 [6400/8883 (71.94%)]\t\tLoss: 1.25950\n",
      "Training Progress: \tEpoch 20 [6720/8883 (75.54%)]\t\tLoss: 1.11045\n",
      "Training Progress: \tEpoch 20 [7040/8883 (79.14%)]\t\tLoss: 1.05430\n",
      "Training Progress: \tEpoch 20 [7360/8883 (82.73%)]\t\tLoss: 1.30110\n",
      "Training Progress: \tEpoch 20 [7680/8883 (86.33%)]\t\tLoss: 1.09870\n",
      "Training Progress: \tEpoch 20 [8000/8883 (89.93%)]\t\tLoss: 1.08148\n",
      "Training Progress: \tEpoch 20 [8320/8883 (93.53%)]\t\tLoss: 1.11320\n",
      "Training Progress: \tEpoch 20 [8640/8883 (97.12%)]\t\tLoss: 1.05307\n",
      "\tTrain loss: 0.03101, Accuracy: 4937/8883 (55.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 925/1692 (54.00%)\n",
      "\tTest loss: 0.00095, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/8883 (0.00%)]\t\tLoss: 1.15386\n",
      "Training Progress: \tEpoch 21 [320/8883 (3.60%)]\t\tLoss: 0.93752\n",
      "Training Progress: \tEpoch 21 [640/8883 (7.19%)]\t\tLoss: 1.19218\n",
      "Training Progress: \tEpoch 21 [960/8883 (10.79%)]\t\tLoss: 0.94853\n",
      "Training Progress: \tEpoch 21 [1280/8883 (14.39%)]\t\tLoss: 1.23002\n",
      "Training Progress: \tEpoch 21 [1600/8883 (17.99%)]\t\tLoss: 1.13396\n",
      "Training Progress: \tEpoch 21 [1920/8883 (21.58%)]\t\tLoss: 1.17626\n",
      "Training Progress: \tEpoch 21 [2240/8883 (25.18%)]\t\tLoss: 0.87434\n",
      "Training Progress: \tEpoch 21 [2560/8883 (28.78%)]\t\tLoss: 0.94582\n",
      "Training Progress: \tEpoch 21 [2880/8883 (32.37%)]\t\tLoss: 1.11818\n",
      "Training Progress: \tEpoch 21 [3200/8883 (35.97%)]\t\tLoss: 1.10817\n",
      "Training Progress: \tEpoch 21 [3520/8883 (39.57%)]\t\tLoss: 1.13858\n",
      "Training Progress: \tEpoch 21 [3840/8883 (43.17%)]\t\tLoss: 1.04458\n",
      "Training Progress: \tEpoch 21 [4160/8883 (46.76%)]\t\tLoss: 1.00021\n",
      "Training Progress: \tEpoch 21 [4480/8883 (50.36%)]\t\tLoss: 1.08820\n",
      "Training Progress: \tEpoch 21 [4800/8883 (53.96%)]\t\tLoss: 1.03834\n",
      "Training Progress: \tEpoch 21 [5120/8883 (57.55%)]\t\tLoss: 1.04163\n",
      "Training Progress: \tEpoch 21 [5440/8883 (61.15%)]\t\tLoss: 1.04714\n",
      "Training Progress: \tEpoch 21 [5760/8883 (64.75%)]\t\tLoss: 1.33097\n",
      "Training Progress: \tEpoch 21 [6080/8883 (68.35%)]\t\tLoss: 1.12636\n",
      "Training Progress: \tEpoch 21 [6400/8883 (71.94%)]\t\tLoss: 1.13324\n",
      "Training Progress: \tEpoch 21 [6720/8883 (75.54%)]\t\tLoss: 1.01388\n",
      "Training Progress: \tEpoch 21 [7040/8883 (79.14%)]\t\tLoss: 0.98892\n",
      "Training Progress: \tEpoch 21 [7360/8883 (82.73%)]\t\tLoss: 1.10870\n",
      "Training Progress: \tEpoch 21 [7680/8883 (86.33%)]\t\tLoss: 1.17509\n",
      "Training Progress: \tEpoch 21 [8000/8883 (89.93%)]\t\tLoss: 1.07709\n",
      "Training Progress: \tEpoch 21 [8320/8883 (93.53%)]\t\tLoss: 1.08763\n",
      "Training Progress: \tEpoch 21 [8640/8883 (97.12%)]\t\tLoss: 1.17911\n",
      "\tTrain loss: 0.02999, Accuracy: 5116/8883 (57.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 953/1692 (56.00%)\n",
      "\tTest loss: 0.00097, Accuracy: 574/1772 (32.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 22 [0/8883 (0.00%)]\t\tLoss: 1.07246\n",
      "Training Progress: \tEpoch 22 [320/8883 (3.60%)]\t\tLoss: 0.94423\n",
      "Training Progress: \tEpoch 22 [640/8883 (7.19%)]\t\tLoss: 1.17947\n",
      "Training Progress: \tEpoch 22 [960/8883 (10.79%)]\t\tLoss: 0.95650\n",
      "Training Progress: \tEpoch 22 [1280/8883 (14.39%)]\t\tLoss: 1.15973\n",
      "Training Progress: \tEpoch 22 [1600/8883 (17.99%)]\t\tLoss: 1.22044\n",
      "Training Progress: \tEpoch 22 [1920/8883 (21.58%)]\t\tLoss: 1.40591\n",
      "Training Progress: \tEpoch 22 [2240/8883 (25.18%)]\t\tLoss: 0.99698\n",
      "Training Progress: \tEpoch 22 [2560/8883 (28.78%)]\t\tLoss: 0.99459\n",
      "Training Progress: \tEpoch 22 [2880/8883 (32.37%)]\t\tLoss: 0.87372\n",
      "Training Progress: \tEpoch 22 [3200/8883 (35.97%)]\t\tLoss: 1.05121\n",
      "Training Progress: \tEpoch 22 [3520/8883 (39.57%)]\t\tLoss: 0.99038\n",
      "Training Progress: \tEpoch 22 [3840/8883 (43.17%)]\t\tLoss: 0.74595\n",
      "Training Progress: \tEpoch 22 [4160/8883 (46.76%)]\t\tLoss: 1.18710\n",
      "Training Progress: \tEpoch 22 [4480/8883 (50.36%)]\t\tLoss: 1.12632\n",
      "Training Progress: \tEpoch 22 [4800/8883 (53.96%)]\t\tLoss: 1.04672\n",
      "Training Progress: \tEpoch 22 [5120/8883 (57.55%)]\t\tLoss: 1.03281\n",
      "Training Progress: \tEpoch 22 [5440/8883 (61.15%)]\t\tLoss: 0.98485\n",
      "Training Progress: \tEpoch 22 [5760/8883 (64.75%)]\t\tLoss: 1.23701\n",
      "Training Progress: \tEpoch 22 [6080/8883 (68.35%)]\t\tLoss: 0.93831\n",
      "Training Progress: \tEpoch 22 [6400/8883 (71.94%)]\t\tLoss: 1.27771\n",
      "Training Progress: \tEpoch 22 [6720/8883 (75.54%)]\t\tLoss: 1.21451\n",
      "Training Progress: \tEpoch 22 [7040/8883 (79.14%)]\t\tLoss: 0.99501\n",
      "Training Progress: \tEpoch 22 [7360/8883 (82.73%)]\t\tLoss: 1.14835\n",
      "Training Progress: \tEpoch 22 [7680/8883 (86.33%)]\t\tLoss: 1.09757\n",
      "Training Progress: \tEpoch 22 [8000/8883 (89.93%)]\t\tLoss: 1.07929\n",
      "Training Progress: \tEpoch 22 [8320/8883 (93.53%)]\t\tLoss: 0.91689\n",
      "Training Progress: \tEpoch 22 [8640/8883 (97.12%)]\t\tLoss: 1.00945\n",
      "\tTrain loss: 0.02851, Accuracy: 5314/8883 (59.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1023/1692 (60.00%)\n",
      "\tTest loss: 0.00099, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/8883 (0.00%)]\t\tLoss: 1.05195\n",
      "Training Progress: \tEpoch 23 [320/8883 (3.60%)]\t\tLoss: 1.00407\n",
      "Training Progress: \tEpoch 23 [640/8883 (7.19%)]\t\tLoss: 1.22200\n",
      "Training Progress: \tEpoch 23 [960/8883 (10.79%)]\t\tLoss: 0.94793\n",
      "Training Progress: \tEpoch 23 [1280/8883 (14.39%)]\t\tLoss: 1.13667\n",
      "Training Progress: \tEpoch 23 [1600/8883 (17.99%)]\t\tLoss: 1.22449\n",
      "Training Progress: \tEpoch 23 [1920/8883 (21.58%)]\t\tLoss: 1.20792\n",
      "Training Progress: \tEpoch 23 [2240/8883 (25.18%)]\t\tLoss: 0.91013\n",
      "Training Progress: \tEpoch 23 [2560/8883 (28.78%)]\t\tLoss: 1.02180\n",
      "Training Progress: \tEpoch 23 [2880/8883 (32.37%)]\t\tLoss: 0.99892\n",
      "Training Progress: \tEpoch 23 [3200/8883 (35.97%)]\t\tLoss: 1.09724\n",
      "Training Progress: \tEpoch 23 [3520/8883 (39.57%)]\t\tLoss: 1.08677\n",
      "Training Progress: \tEpoch 23 [3840/8883 (43.17%)]\t\tLoss: 0.83177\n",
      "Training Progress: \tEpoch 23 [4160/8883 (46.76%)]\t\tLoss: 1.01086\n",
      "Training Progress: \tEpoch 23 [4480/8883 (50.36%)]\t\tLoss: 0.86650\n",
      "Training Progress: \tEpoch 23 [4800/8883 (53.96%)]\t\tLoss: 0.87082\n",
      "Training Progress: \tEpoch 23 [5120/8883 (57.55%)]\t\tLoss: 1.08214\n",
      "Training Progress: \tEpoch 23 [5440/8883 (61.15%)]\t\tLoss: 1.10389\n",
      "Training Progress: \tEpoch 23 [5760/8883 (64.75%)]\t\tLoss: 1.20307\n",
      "Training Progress: \tEpoch 23 [6080/8883 (68.35%)]\t\tLoss: 0.84244\n",
      "Training Progress: \tEpoch 23 [6400/8883 (71.94%)]\t\tLoss: 1.21627\n",
      "Training Progress: \tEpoch 23 [6720/8883 (75.54%)]\t\tLoss: 1.06783\n",
      "Training Progress: \tEpoch 23 [7040/8883 (79.14%)]\t\tLoss: 0.94815\n",
      "Training Progress: \tEpoch 23 [7360/8883 (82.73%)]\t\tLoss: 0.98292\n",
      "Training Progress: \tEpoch 23 [7680/8883 (86.33%)]\t\tLoss: 1.03668\n",
      "Training Progress: \tEpoch 23 [8000/8883 (89.93%)]\t\tLoss: 0.92052\n",
      "Training Progress: \tEpoch 23 [8320/8883 (93.53%)]\t\tLoss: 1.04259\n",
      "Training Progress: \tEpoch 23 [8640/8883 (97.12%)]\t\tLoss: 1.00405\n",
      "\tTrain loss: 0.02827, Accuracy: 5314/8883 (59.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1003/1692 (59.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 575/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/8883 (0.00%)]\t\tLoss: 0.95835\n",
      "Training Progress: \tEpoch 24 [320/8883 (3.60%)]\t\tLoss: 0.81421\n",
      "Training Progress: \tEpoch 24 [640/8883 (7.19%)]\t\tLoss: 1.13954\n",
      "Training Progress: \tEpoch 24 [960/8883 (10.79%)]\t\tLoss: 0.91253\n",
      "Training Progress: \tEpoch 24 [1280/8883 (14.39%)]\t\tLoss: 1.10511\n",
      "Training Progress: \tEpoch 24 [1600/8883 (17.99%)]\t\tLoss: 1.26532\n",
      "Training Progress: \tEpoch 24 [1920/8883 (21.58%)]\t\tLoss: 1.08770\n",
      "Training Progress: \tEpoch 24 [2240/8883 (25.18%)]\t\tLoss: 0.96582\n",
      "Training Progress: \tEpoch 24 [2560/8883 (28.78%)]\t\tLoss: 0.98954\n",
      "Training Progress: \tEpoch 24 [2880/8883 (32.37%)]\t\tLoss: 0.98074\n",
      "Training Progress: \tEpoch 24 [3200/8883 (35.97%)]\t\tLoss: 1.13260\n",
      "Training Progress: \tEpoch 24 [3520/8883 (39.57%)]\t\tLoss: 1.09599\n",
      "Training Progress: \tEpoch 24 [3840/8883 (43.17%)]\t\tLoss: 0.81612\n",
      "Training Progress: \tEpoch 24 [4160/8883 (46.76%)]\t\tLoss: 0.96148\n",
      "Training Progress: \tEpoch 24 [4480/8883 (50.36%)]\t\tLoss: 1.02328\n",
      "Training Progress: \tEpoch 24 [4800/8883 (53.96%)]\t\tLoss: 1.06080\n",
      "Training Progress: \tEpoch 24 [5120/8883 (57.55%)]\t\tLoss: 1.06435\n",
      "Training Progress: \tEpoch 24 [5440/8883 (61.15%)]\t\tLoss: 0.90291\n",
      "Training Progress: \tEpoch 24 [5760/8883 (64.75%)]\t\tLoss: 1.38484\n",
      "Training Progress: \tEpoch 24 [6080/8883 (68.35%)]\t\tLoss: 0.95876\n",
      "Training Progress: \tEpoch 24 [6400/8883 (71.94%)]\t\tLoss: 1.10461\n",
      "Training Progress: \tEpoch 24 [6720/8883 (75.54%)]\t\tLoss: 1.06158\n",
      "Training Progress: \tEpoch 24 [7040/8883 (79.14%)]\t\tLoss: 1.13517\n",
      "Training Progress: \tEpoch 24 [7360/8883 (82.73%)]\t\tLoss: 0.98713\n",
      "Training Progress: \tEpoch 24 [7680/8883 (86.33%)]\t\tLoss: 1.03800\n",
      "Training Progress: \tEpoch 24 [8000/8883 (89.93%)]\t\tLoss: 0.82893\n",
      "Training Progress: \tEpoch 24 [8320/8883 (93.53%)]\t\tLoss: 0.92774\n",
      "Training Progress: \tEpoch 24 [8640/8883 (97.12%)]\t\tLoss: 0.90015\n",
      "\tTrain loss: 0.02811, Accuracy: 5374/8883 (60.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1008/1692 (59.00%)\n",
      "\tTest loss: 0.00099, Accuracy: 626/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/8883 (0.00%)]\t\tLoss: 0.96980\n",
      "Training Progress: \tEpoch 25 [320/8883 (3.60%)]\t\tLoss: 0.85434\n",
      "Training Progress: \tEpoch 25 [640/8883 (7.19%)]\t\tLoss: 1.00705\n",
      "Training Progress: \tEpoch 25 [960/8883 (10.79%)]\t\tLoss: 0.88392\n",
      "Training Progress: \tEpoch 25 [1280/8883 (14.39%)]\t\tLoss: 1.22648\n",
      "Training Progress: \tEpoch 25 [1600/8883 (17.99%)]\t\tLoss: 1.03454\n",
      "Training Progress: \tEpoch 25 [1920/8883 (21.58%)]\t\tLoss: 1.24840\n",
      "Training Progress: \tEpoch 25 [2240/8883 (25.18%)]\t\tLoss: 0.78140\n",
      "Training Progress: \tEpoch 25 [2560/8883 (28.78%)]\t\tLoss: 0.99364\n",
      "Training Progress: \tEpoch 25 [2880/8883 (32.37%)]\t\tLoss: 0.93665\n",
      "Training Progress: \tEpoch 25 [3200/8883 (35.97%)]\t\tLoss: 1.04215\n",
      "Training Progress: \tEpoch 25 [3520/8883 (39.57%)]\t\tLoss: 1.00124\n",
      "Training Progress: \tEpoch 25 [3840/8883 (43.17%)]\t\tLoss: 0.73698\n",
      "Training Progress: \tEpoch 25 [4160/8883 (46.76%)]\t\tLoss: 1.06368\n",
      "Training Progress: \tEpoch 25 [4480/8883 (50.36%)]\t\tLoss: 1.02713\n",
      "Training Progress: \tEpoch 25 [4800/8883 (53.96%)]\t\tLoss: 0.93757\n",
      "Training Progress: \tEpoch 25 [5120/8883 (57.55%)]\t\tLoss: 1.03808\n",
      "Training Progress: \tEpoch 25 [5440/8883 (61.15%)]\t\tLoss: 0.95853\n",
      "Training Progress: \tEpoch 25 [5760/8883 (64.75%)]\t\tLoss: 1.08979\n",
      "Training Progress: \tEpoch 25 [6080/8883 (68.35%)]\t\tLoss: 0.85164\n",
      "Training Progress: \tEpoch 25 [6400/8883 (71.94%)]\t\tLoss: 1.11778\n",
      "Training Progress: \tEpoch 25 [6720/8883 (75.54%)]\t\tLoss: 0.92800\n",
      "Training Progress: \tEpoch 25 [7040/8883 (79.14%)]\t\tLoss: 0.89928\n",
      "Training Progress: \tEpoch 25 [7360/8883 (82.73%)]\t\tLoss: 1.17676\n",
      "Training Progress: \tEpoch 25 [7680/8883 (86.33%)]\t\tLoss: 0.99445\n",
      "Training Progress: \tEpoch 25 [8000/8883 (89.93%)]\t\tLoss: 0.96334\n",
      "Training Progress: \tEpoch 25 [8320/8883 (93.53%)]\t\tLoss: 0.87337\n",
      "Training Progress: \tEpoch 25 [8640/8883 (97.12%)]\t\tLoss: 1.01806\n",
      "\tTrain loss: 0.02679, Accuracy: 5489/8883 (61.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1072/1692 (63.00%)\n",
      "\tTest loss: 0.00099, Accuracy: 627/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/8883 (0.00%)]\t\tLoss: 0.84247\n",
      "Training Progress: \tEpoch 26 [320/8883 (3.60%)]\t\tLoss: 0.76071\n",
      "Training Progress: \tEpoch 26 [640/8883 (7.19%)]\t\tLoss: 1.00961\n",
      "Training Progress: \tEpoch 26 [960/8883 (10.79%)]\t\tLoss: 0.87808\n",
      "Training Progress: \tEpoch 26 [1280/8883 (14.39%)]\t\tLoss: 1.35857\n",
      "Training Progress: \tEpoch 26 [1600/8883 (17.99%)]\t\tLoss: 1.04645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 26 [1920/8883 (21.58%)]\t\tLoss: 1.18431\n",
      "Training Progress: \tEpoch 26 [2240/8883 (25.18%)]\t\tLoss: 0.78059\n",
      "Training Progress: \tEpoch 26 [2560/8883 (28.78%)]\t\tLoss: 0.96527\n",
      "Training Progress: \tEpoch 26 [2880/8883 (32.37%)]\t\tLoss: 0.95593\n",
      "Training Progress: \tEpoch 26 [3200/8883 (35.97%)]\t\tLoss: 1.02163\n",
      "Training Progress: \tEpoch 26 [3520/8883 (39.57%)]\t\tLoss: 1.03352\n",
      "Training Progress: \tEpoch 26 [3840/8883 (43.17%)]\t\tLoss: 0.75170\n",
      "Training Progress: \tEpoch 26 [4160/8883 (46.76%)]\t\tLoss: 0.91942\n",
      "Training Progress: \tEpoch 26 [4480/8883 (50.36%)]\t\tLoss: 0.91056\n",
      "Training Progress: \tEpoch 26 [4800/8883 (53.96%)]\t\tLoss: 0.89350\n",
      "Training Progress: \tEpoch 26 [5120/8883 (57.55%)]\t\tLoss: 0.97390\n",
      "Training Progress: \tEpoch 26 [5440/8883 (61.15%)]\t\tLoss: 1.02329\n",
      "Training Progress: \tEpoch 26 [5760/8883 (64.75%)]\t\tLoss: 1.27965\n",
      "Training Progress: \tEpoch 26 [6080/8883 (68.35%)]\t\tLoss: 0.89740\n",
      "Training Progress: \tEpoch 26 [6400/8883 (71.94%)]\t\tLoss: 1.21681\n",
      "Training Progress: \tEpoch 26 [6720/8883 (75.54%)]\t\tLoss: 0.98156\n",
      "Training Progress: \tEpoch 26 [7040/8883 (79.14%)]\t\tLoss: 0.93413\n",
      "Training Progress: \tEpoch 26 [7360/8883 (82.73%)]\t\tLoss: 1.18725\n",
      "Training Progress: \tEpoch 26 [7680/8883 (86.33%)]\t\tLoss: 0.96643\n",
      "Training Progress: \tEpoch 26 [8000/8883 (89.93%)]\t\tLoss: 0.81873\n",
      "Training Progress: \tEpoch 26 [8320/8883 (93.53%)]\t\tLoss: 0.79548\n",
      "Training Progress: \tEpoch 26 [8640/8883 (97.12%)]\t\tLoss: 0.77739\n",
      "\tTrain loss: 0.02617, Accuracy: 5550/8883 (62.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1067/1692 (63.00%)\n",
      "\tTest loss: 0.00100, Accuracy: 647/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/8883 (0.00%)]\t\tLoss: 0.85147\n",
      "Training Progress: \tEpoch 27 [320/8883 (3.60%)]\t\tLoss: 0.78561\n",
      "Training Progress: \tEpoch 27 [640/8883 (7.19%)]\t\tLoss: 0.95905\n",
      "Training Progress: \tEpoch 27 [960/8883 (10.79%)]\t\tLoss: 1.01028\n",
      "Training Progress: \tEpoch 27 [1280/8883 (14.39%)]\t\tLoss: 1.20054\n",
      "Training Progress: \tEpoch 27 [1600/8883 (17.99%)]\t\tLoss: 1.07870\n",
      "Training Progress: \tEpoch 27 [1920/8883 (21.58%)]\t\tLoss: 1.27974\n",
      "Training Progress: \tEpoch 27 [2240/8883 (25.18%)]\t\tLoss: 0.80176\n",
      "Training Progress: \tEpoch 27 [2560/8883 (28.78%)]\t\tLoss: 0.90483\n",
      "Training Progress: \tEpoch 27 [2880/8883 (32.37%)]\t\tLoss: 0.83228\n",
      "Training Progress: \tEpoch 27 [3200/8883 (35.97%)]\t\tLoss: 1.08055\n",
      "Training Progress: \tEpoch 27 [3520/8883 (39.57%)]\t\tLoss: 0.89383\n",
      "Training Progress: \tEpoch 27 [3840/8883 (43.17%)]\t\tLoss: 0.80711\n",
      "Training Progress: \tEpoch 27 [4160/8883 (46.76%)]\t\tLoss: 0.99892\n",
      "Training Progress: \tEpoch 27 [4480/8883 (50.36%)]\t\tLoss: 0.79053\n",
      "Training Progress: \tEpoch 27 [4800/8883 (53.96%)]\t\tLoss: 0.94636\n",
      "Training Progress: \tEpoch 27 [5120/8883 (57.55%)]\t\tLoss: 1.06842\n",
      "Training Progress: \tEpoch 27 [5440/8883 (61.15%)]\t\tLoss: 1.02455\n",
      "Training Progress: \tEpoch 27 [5760/8883 (64.75%)]\t\tLoss: 1.17350\n",
      "Training Progress: \tEpoch 27 [6080/8883 (68.35%)]\t\tLoss: 0.80378\n",
      "Training Progress: \tEpoch 27 [6400/8883 (71.94%)]\t\tLoss: 1.05736\n",
      "Training Progress: \tEpoch 27 [6720/8883 (75.54%)]\t\tLoss: 0.98029\n",
      "Training Progress: \tEpoch 27 [7040/8883 (79.14%)]\t\tLoss: 0.88688\n",
      "Training Progress: \tEpoch 27 [7360/8883 (82.73%)]\t\tLoss: 0.84657\n",
      "Training Progress: \tEpoch 27 [7680/8883 (86.33%)]\t\tLoss: 0.91273\n",
      "Training Progress: \tEpoch 27 [8000/8883 (89.93%)]\t\tLoss: 0.95291\n",
      "Training Progress: \tEpoch 27 [8320/8883 (93.53%)]\t\tLoss: 0.95985\n",
      "Training Progress: \tEpoch 27 [8640/8883 (97.12%)]\t\tLoss: 0.88803\n",
      "\tTrain loss: 0.02505, Accuracy: 5730/8883 (64.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1111/1692 (65.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 637/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/8883 (0.00%)]\t\tLoss: 0.75362\n",
      "Training Progress: \tEpoch 28 [320/8883 (3.60%)]\t\tLoss: 0.71620\n",
      "Training Progress: \tEpoch 28 [640/8883 (7.19%)]\t\tLoss: 1.04039\n",
      "Training Progress: \tEpoch 28 [960/8883 (10.79%)]\t\tLoss: 0.85203\n",
      "Training Progress: \tEpoch 28 [1280/8883 (14.39%)]\t\tLoss: 0.86792\n",
      "Training Progress: \tEpoch 28 [1600/8883 (17.99%)]\t\tLoss: 1.05587\n",
      "Training Progress: \tEpoch 28 [1920/8883 (21.58%)]\t\tLoss: 0.84102\n",
      "Training Progress: \tEpoch 28 [2240/8883 (25.18%)]\t\tLoss: 0.85900\n",
      "Training Progress: \tEpoch 28 [2560/8883 (28.78%)]\t\tLoss: 0.85344\n",
      "Training Progress: \tEpoch 28 [2880/8883 (32.37%)]\t\tLoss: 0.64017\n",
      "Training Progress: \tEpoch 28 [3200/8883 (35.97%)]\t\tLoss: 1.12799\n",
      "Training Progress: \tEpoch 28 [3520/8883 (39.57%)]\t\tLoss: 0.91959\n",
      "Training Progress: \tEpoch 28 [3840/8883 (43.17%)]\t\tLoss: 0.84282\n",
      "Training Progress: \tEpoch 28 [4160/8883 (46.76%)]\t\tLoss: 0.91119\n",
      "Training Progress: \tEpoch 28 [4480/8883 (50.36%)]\t\tLoss: 0.81190\n",
      "Training Progress: \tEpoch 28 [4800/8883 (53.96%)]\t\tLoss: 0.85447\n",
      "Training Progress: \tEpoch 28 [5120/8883 (57.55%)]\t\tLoss: 0.87914\n",
      "Training Progress: \tEpoch 28 [5440/8883 (61.15%)]\t\tLoss: 1.13107\n",
      "Training Progress: \tEpoch 28 [5760/8883 (64.75%)]\t\tLoss: 1.16898\n",
      "Training Progress: \tEpoch 28 [6080/8883 (68.35%)]\t\tLoss: 0.80863\n",
      "Training Progress: \tEpoch 28 [6400/8883 (71.94%)]\t\tLoss: 1.04930\n",
      "Training Progress: \tEpoch 28 [6720/8883 (75.54%)]\t\tLoss: 0.99470\n",
      "Training Progress: \tEpoch 28 [7040/8883 (79.14%)]\t\tLoss: 1.01506\n",
      "Training Progress: \tEpoch 28 [7360/8883 (82.73%)]\t\tLoss: 0.86632\n",
      "Training Progress: \tEpoch 28 [7680/8883 (86.33%)]\t\tLoss: 0.91445\n",
      "Training Progress: \tEpoch 28 [8000/8883 (89.93%)]\t\tLoss: 0.78279\n",
      "Training Progress: \tEpoch 28 [8320/8883 (93.53%)]\t\tLoss: 1.08913\n",
      "Training Progress: \tEpoch 28 [8640/8883 (97.12%)]\t\tLoss: 0.90187\n",
      "\tTrain loss: 0.02529, Accuracy: 5718/8883 (64.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1115/1692 (65.00%)\n",
      "\tTest loss: 0.00105, Accuracy: 614/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/8883 (0.00%)]\t\tLoss: 0.79359\n",
      "Training Progress: \tEpoch 29 [320/8883 (3.60%)]\t\tLoss: 0.86168\n",
      "Training Progress: \tEpoch 29 [640/8883 (7.19%)]\t\tLoss: 0.87240\n",
      "Training Progress: \tEpoch 29 [960/8883 (10.79%)]\t\tLoss: 0.82725\n",
      "Training Progress: \tEpoch 29 [1280/8883 (14.39%)]\t\tLoss: 1.03805\n",
      "Training Progress: \tEpoch 29 [1600/8883 (17.99%)]\t\tLoss: 1.17013\n",
      "Training Progress: \tEpoch 29 [1920/8883 (21.58%)]\t\tLoss: 0.95039\n",
      "Training Progress: \tEpoch 29 [2240/8883 (25.18%)]\t\tLoss: 0.77842\n",
      "Training Progress: \tEpoch 29 [2560/8883 (28.78%)]\t\tLoss: 0.82038\n",
      "Training Progress: \tEpoch 29 [2880/8883 (32.37%)]\t\tLoss: 0.74916\n",
      "Training Progress: \tEpoch 29 [3200/8883 (35.97%)]\t\tLoss: 0.86536\n",
      "Training Progress: \tEpoch 29 [3520/8883 (39.57%)]\t\tLoss: 1.10728\n",
      "Training Progress: \tEpoch 29 [3840/8883 (43.17%)]\t\tLoss: 0.71728\n",
      "Training Progress: \tEpoch 29 [4160/8883 (46.76%)]\t\tLoss: 0.78874\n",
      "Training Progress: \tEpoch 29 [4480/8883 (50.36%)]\t\tLoss: 0.95338\n",
      "Training Progress: \tEpoch 29 [4800/8883 (53.96%)]\t\tLoss: 0.66685\n",
      "Training Progress: \tEpoch 29 [5120/8883 (57.55%)]\t\tLoss: 0.93117\n",
      "Training Progress: \tEpoch 29 [5440/8883 (61.15%)]\t\tLoss: 1.02608\n",
      "Training Progress: \tEpoch 29 [5760/8883 (64.75%)]\t\tLoss: 1.16540\n",
      "Training Progress: \tEpoch 29 [6080/8883 (68.35%)]\t\tLoss: 0.74159\n",
      "Training Progress: \tEpoch 29 [6400/8883 (71.94%)]\t\tLoss: 1.06314\n",
      "Training Progress: \tEpoch 29 [6720/8883 (75.54%)]\t\tLoss: 0.75924\n",
      "Training Progress: \tEpoch 29 [7040/8883 (79.14%)]\t\tLoss: 0.64268\n",
      "Training Progress: \tEpoch 29 [7360/8883 (82.73%)]\t\tLoss: 0.86244\n",
      "Training Progress: \tEpoch 29 [7680/8883 (86.33%)]\t\tLoss: 0.88879\n",
      "Training Progress: \tEpoch 29 [8000/8883 (89.93%)]\t\tLoss: 1.02364\n",
      "Training Progress: \tEpoch 29 [8320/8883 (93.53%)]\t\tLoss: 0.88075\n",
      "Training Progress: \tEpoch 29 [8640/8883 (97.12%)]\t\tLoss: 0.73905\n",
      "\tTrain loss: 0.02381, Accuracy: 5879/8883 (66.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1164/1692 (68.00%)\n",
      "\tTest loss: 0.00106, Accuracy: 659/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/8883 (0.00%)]\t\tLoss: 0.70423\n",
      "Training Progress: \tEpoch 30 [320/8883 (3.60%)]\t\tLoss: 0.77639\n",
      "Training Progress: \tEpoch 30 [640/8883 (7.19%)]\t\tLoss: 0.90171\n",
      "Training Progress: \tEpoch 30 [960/8883 (10.79%)]\t\tLoss: 1.05399\n",
      "Training Progress: \tEpoch 30 [1280/8883 (14.39%)]\t\tLoss: 0.98669\n",
      "Training Progress: \tEpoch 30 [1600/8883 (17.99%)]\t\tLoss: 0.90013\n",
      "Training Progress: \tEpoch 30 [1920/8883 (21.58%)]\t\tLoss: 1.21184\n",
      "Training Progress: \tEpoch 30 [2240/8883 (25.18%)]\t\tLoss: 0.85714\n",
      "Training Progress: \tEpoch 30 [2560/8883 (28.78%)]\t\tLoss: 0.75685\n",
      "Training Progress: \tEpoch 30 [2880/8883 (32.37%)]\t\tLoss: 0.82372\n",
      "Training Progress: \tEpoch 30 [3200/8883 (35.97%)]\t\tLoss: 0.97023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 30 [3520/8883 (39.57%)]\t\tLoss: 1.17855\n",
      "Training Progress: \tEpoch 30 [3840/8883 (43.17%)]\t\tLoss: 0.85543\n",
      "Training Progress: \tEpoch 30 [4160/8883 (46.76%)]\t\tLoss: 0.81795\n",
      "Training Progress: \tEpoch 30 [4480/8883 (50.36%)]\t\tLoss: 0.72781\n",
      "Training Progress: \tEpoch 30 [4800/8883 (53.96%)]\t\tLoss: 0.66992\n",
      "Training Progress: \tEpoch 30 [5120/8883 (57.55%)]\t\tLoss: 0.89187\n",
      "Training Progress: \tEpoch 30 [5440/8883 (61.15%)]\t\tLoss: 1.01587\n",
      "Training Progress: \tEpoch 30 [5760/8883 (64.75%)]\t\tLoss: 1.02973\n",
      "Training Progress: \tEpoch 30 [6080/8883 (68.35%)]\t\tLoss: 0.78837\n",
      "Training Progress: \tEpoch 30 [6400/8883 (71.94%)]\t\tLoss: 1.15024\n",
      "Training Progress: \tEpoch 30 [6720/8883 (75.54%)]\t\tLoss: 0.97120\n",
      "Training Progress: \tEpoch 30 [7040/8883 (79.14%)]\t\tLoss: 0.75010\n",
      "Training Progress: \tEpoch 30 [7360/8883 (82.73%)]\t\tLoss: 1.04182\n",
      "Training Progress: \tEpoch 30 [7680/8883 (86.33%)]\t\tLoss: 0.78424\n",
      "Training Progress: \tEpoch 30 [8000/8883 (89.93%)]\t\tLoss: 1.04833\n",
      "Training Progress: \tEpoch 30 [8320/8883 (93.53%)]\t\tLoss: 0.82974\n",
      "Training Progress: \tEpoch 30 [8640/8883 (97.12%)]\t\tLoss: 0.86486\n",
      "\tTrain loss: 0.02397, Accuracy: 5878/8883 (66.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1165/1692 (68.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 635/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/8883 (0.00%)]\t\tLoss: 0.90371\n",
      "Training Progress: \tEpoch 31 [320/8883 (3.60%)]\t\tLoss: 0.79804\n",
      "Training Progress: \tEpoch 31 [640/8883 (7.19%)]\t\tLoss: 0.78516\n",
      "Training Progress: \tEpoch 31 [960/8883 (10.79%)]\t\tLoss: 0.75152\n",
      "Training Progress: \tEpoch 31 [1280/8883 (14.39%)]\t\tLoss: 0.91146\n",
      "Training Progress: \tEpoch 31 [1600/8883 (17.99%)]\t\tLoss: 1.04724\n",
      "Training Progress: \tEpoch 31 [1920/8883 (21.58%)]\t\tLoss: 1.04473\n",
      "Training Progress: \tEpoch 31 [2240/8883 (25.18%)]\t\tLoss: 0.78711\n",
      "Training Progress: \tEpoch 31 [2560/8883 (28.78%)]\t\tLoss: 0.81250\n",
      "Training Progress: \tEpoch 31 [2880/8883 (32.37%)]\t\tLoss: 0.84902\n",
      "Training Progress: \tEpoch 31 [3200/8883 (35.97%)]\t\tLoss: 0.92510\n",
      "Training Progress: \tEpoch 31 [3520/8883 (39.57%)]\t\tLoss: 1.04155\n",
      "Training Progress: \tEpoch 31 [3840/8883 (43.17%)]\t\tLoss: 0.79247\n",
      "Training Progress: \tEpoch 31 [4160/8883 (46.76%)]\t\tLoss: 0.77030\n",
      "Training Progress: \tEpoch 31 [4480/8883 (50.36%)]\t\tLoss: 0.91648\n",
      "Training Progress: \tEpoch 31 [4800/8883 (53.96%)]\t\tLoss: 0.64588\n",
      "Training Progress: \tEpoch 31 [5120/8883 (57.55%)]\t\tLoss: 1.29604\n",
      "Training Progress: \tEpoch 31 [5440/8883 (61.15%)]\t\tLoss: 0.92867\n",
      "Training Progress: \tEpoch 31 [5760/8883 (64.75%)]\t\tLoss: 1.05437\n",
      "Training Progress: \tEpoch 31 [6080/8883 (68.35%)]\t\tLoss: 0.78469\n",
      "Training Progress: \tEpoch 31 [6400/8883 (71.94%)]\t\tLoss: 1.15480\n",
      "Training Progress: \tEpoch 31 [6720/8883 (75.54%)]\t\tLoss: 0.85210\n",
      "Training Progress: \tEpoch 31 [7040/8883 (79.14%)]\t\tLoss: 0.80516\n",
      "Training Progress: \tEpoch 31 [7360/8883 (82.73%)]\t\tLoss: 0.81824\n",
      "Training Progress: \tEpoch 31 [7680/8883 (86.33%)]\t\tLoss: 0.83507\n",
      "Training Progress: \tEpoch 31 [8000/8883 (89.93%)]\t\tLoss: 0.84516\n",
      "Training Progress: \tEpoch 31 [8320/8883 (93.53%)]\t\tLoss: 0.98343\n",
      "Training Progress: \tEpoch 31 [8640/8883 (97.12%)]\t\tLoss: 0.68457\n",
      "\tTrain loss: 0.02300, Accuracy: 5945/8883 (66.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1164/1692 (68.00%)\n",
      "\tTest loss: 0.00110, Accuracy: 682/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/8883 (0.00%)]\t\tLoss: 0.69843\n",
      "Training Progress: \tEpoch 32 [320/8883 (3.60%)]\t\tLoss: 0.69717\n",
      "Training Progress: \tEpoch 32 [640/8883 (7.19%)]\t\tLoss: 1.06426\n",
      "Training Progress: \tEpoch 32 [960/8883 (10.79%)]\t\tLoss: 0.94690\n",
      "Training Progress: \tEpoch 32 [1280/8883 (14.39%)]\t\tLoss: 1.10186\n",
      "Training Progress: \tEpoch 32 [1600/8883 (17.99%)]\t\tLoss: 1.01820\n",
      "Training Progress: \tEpoch 32 [1920/8883 (21.58%)]\t\tLoss: 1.04296\n",
      "Training Progress: \tEpoch 32 [2240/8883 (25.18%)]\t\tLoss: 0.78493\n",
      "Training Progress: \tEpoch 32 [2560/8883 (28.78%)]\t\tLoss: 0.78285\n",
      "Training Progress: \tEpoch 32 [2880/8883 (32.37%)]\t\tLoss: 0.79548\n",
      "Training Progress: \tEpoch 32 [3200/8883 (35.97%)]\t\tLoss: 0.85195\n",
      "Training Progress: \tEpoch 32 [3520/8883 (39.57%)]\t\tLoss: 1.08427\n",
      "Training Progress: \tEpoch 32 [3840/8883 (43.17%)]\t\tLoss: 0.62993\n",
      "Training Progress: \tEpoch 32 [4160/8883 (46.76%)]\t\tLoss: 0.86352\n",
      "Training Progress: \tEpoch 32 [4480/8883 (50.36%)]\t\tLoss: 0.85023\n",
      "Training Progress: \tEpoch 32 [4800/8883 (53.96%)]\t\tLoss: 0.67567\n",
      "Training Progress: \tEpoch 32 [5120/8883 (57.55%)]\t\tLoss: 0.95609\n",
      "Training Progress: \tEpoch 32 [5440/8883 (61.15%)]\t\tLoss: 0.93316\n",
      "Training Progress: \tEpoch 32 [5760/8883 (64.75%)]\t\tLoss: 1.03504\n",
      "Training Progress: \tEpoch 32 [6080/8883 (68.35%)]\t\tLoss: 0.74654\n",
      "Training Progress: \tEpoch 32 [6400/8883 (71.94%)]\t\tLoss: 0.91441\n",
      "Training Progress: \tEpoch 32 [6720/8883 (75.54%)]\t\tLoss: 0.93993\n",
      "Training Progress: \tEpoch 32 [7040/8883 (79.14%)]\t\tLoss: 0.63030\n",
      "Training Progress: \tEpoch 32 [7360/8883 (82.73%)]\t\tLoss: 0.98750\n",
      "Training Progress: \tEpoch 32 [7680/8883 (86.33%)]\t\tLoss: 0.68315\n",
      "Training Progress: \tEpoch 32 [8000/8883 (89.93%)]\t\tLoss: 0.84972\n",
      "Training Progress: \tEpoch 32 [8320/8883 (93.53%)]\t\tLoss: 0.76835\n",
      "Training Progress: \tEpoch 32 [8640/8883 (97.12%)]\t\tLoss: 0.63119\n",
      "\tTrain loss: 0.02238, Accuracy: 6066/8883 (68.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1200/1692 (70.00%)\n",
      "\tTest loss: 0.00113, Accuracy: 652/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/8883 (0.00%)]\t\tLoss: 0.58578\n",
      "Training Progress: \tEpoch 33 [320/8883 (3.60%)]\t\tLoss: 0.69109\n",
      "Training Progress: \tEpoch 33 [640/8883 (7.19%)]\t\tLoss: 0.77214\n",
      "Training Progress: \tEpoch 33 [960/8883 (10.79%)]\t\tLoss: 0.77861\n",
      "Training Progress: \tEpoch 33 [1280/8883 (14.39%)]\t\tLoss: 0.70829\n",
      "Training Progress: \tEpoch 33 [1600/8883 (17.99%)]\t\tLoss: 1.08175\n",
      "Training Progress: \tEpoch 33 [1920/8883 (21.58%)]\t\tLoss: 0.98712\n",
      "Training Progress: \tEpoch 33 [2240/8883 (25.18%)]\t\tLoss: 0.84447\n",
      "Training Progress: \tEpoch 33 [2560/8883 (28.78%)]\t\tLoss: 0.65640\n",
      "Training Progress: \tEpoch 33 [2880/8883 (32.37%)]\t\tLoss: 0.79681\n",
      "Training Progress: \tEpoch 33 [3200/8883 (35.97%)]\t\tLoss: 0.99558\n",
      "Training Progress: \tEpoch 33 [3520/8883 (39.57%)]\t\tLoss: 0.78936\n",
      "Training Progress: \tEpoch 33 [3840/8883 (43.17%)]\t\tLoss: 0.73592\n",
      "Training Progress: \tEpoch 33 [4160/8883 (46.76%)]\t\tLoss: 0.77294\n",
      "Training Progress: \tEpoch 33 [4480/8883 (50.36%)]\t\tLoss: 0.77401\n",
      "Training Progress: \tEpoch 33 [4800/8883 (53.96%)]\t\tLoss: 0.58714\n",
      "Training Progress: \tEpoch 33 [5120/8883 (57.55%)]\t\tLoss: 0.92872\n",
      "Training Progress: \tEpoch 33 [5440/8883 (61.15%)]\t\tLoss: 0.93325\n",
      "Training Progress: \tEpoch 33 [5760/8883 (64.75%)]\t\tLoss: 1.11926\n",
      "Training Progress: \tEpoch 33 [6080/8883 (68.35%)]\t\tLoss: 0.66895\n",
      "Training Progress: \tEpoch 33 [6400/8883 (71.94%)]\t\tLoss: 1.02399\n",
      "Training Progress: \tEpoch 33 [6720/8883 (75.54%)]\t\tLoss: 0.86128\n",
      "Training Progress: \tEpoch 33 [7040/8883 (79.14%)]\t\tLoss: 0.68603\n",
      "Training Progress: \tEpoch 33 [7360/8883 (82.73%)]\t\tLoss: 0.78962\n",
      "Training Progress: \tEpoch 33 [7680/8883 (86.33%)]\t\tLoss: 0.89200\n",
      "Training Progress: \tEpoch 33 [8000/8883 (89.93%)]\t\tLoss: 0.89154\n",
      "Training Progress: \tEpoch 33 [8320/8883 (93.53%)]\t\tLoss: 0.81641\n",
      "Training Progress: \tEpoch 33 [8640/8883 (97.12%)]\t\tLoss: 0.65752\n",
      "\tTrain loss: 0.02124, Accuracy: 6231/8883 (70.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1252/1692 (73.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 691/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/8883 (0.00%)]\t\tLoss: 0.67787\n",
      "Training Progress: \tEpoch 34 [320/8883 (3.60%)]\t\tLoss: 0.83850\n",
      "Training Progress: \tEpoch 34 [640/8883 (7.19%)]\t\tLoss: 0.96060\n",
      "Training Progress: \tEpoch 34 [960/8883 (10.79%)]\t\tLoss: 0.79779\n",
      "Training Progress: \tEpoch 34 [1280/8883 (14.39%)]\t\tLoss: 0.90732\n",
      "Training Progress: \tEpoch 34 [1600/8883 (17.99%)]\t\tLoss: 0.93978\n",
      "Training Progress: \tEpoch 34 [1920/8883 (21.58%)]\t\tLoss: 0.88693\n",
      "Training Progress: \tEpoch 34 [2240/8883 (25.18%)]\t\tLoss: 0.86133\n",
      "Training Progress: \tEpoch 34 [2560/8883 (28.78%)]\t\tLoss: 0.72010\n",
      "Training Progress: \tEpoch 34 [2880/8883 (32.37%)]\t\tLoss: 0.66370\n",
      "Training Progress: \tEpoch 34 [3200/8883 (35.97%)]\t\tLoss: 0.97244\n",
      "Training Progress: \tEpoch 34 [3520/8883 (39.57%)]\t\tLoss: 0.87791\n",
      "Training Progress: \tEpoch 34 [3840/8883 (43.17%)]\t\tLoss: 0.65082\n",
      "Training Progress: \tEpoch 34 [4160/8883 (46.76%)]\t\tLoss: 0.73490\n",
      "Training Progress: \tEpoch 34 [4480/8883 (50.36%)]\t\tLoss: 0.98721\n",
      "Training Progress: \tEpoch 34 [4800/8883 (53.96%)]\t\tLoss: 0.55990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 34 [5120/8883 (57.55%)]\t\tLoss: 0.97580\n",
      "Training Progress: \tEpoch 34 [5440/8883 (61.15%)]\t\tLoss: 0.79681\n",
      "Training Progress: \tEpoch 34 [5760/8883 (64.75%)]\t\tLoss: 1.16832\n",
      "Training Progress: \tEpoch 34 [6080/8883 (68.35%)]\t\tLoss: 0.68137\n",
      "Training Progress: \tEpoch 34 [6400/8883 (71.94%)]\t\tLoss: 0.88576\n",
      "Training Progress: \tEpoch 34 [6720/8883 (75.54%)]\t\tLoss: 0.76190\n",
      "Training Progress: \tEpoch 34 [7040/8883 (79.14%)]\t\tLoss: 0.57851\n",
      "Training Progress: \tEpoch 34 [7360/8883 (82.73%)]\t\tLoss: 0.78610\n",
      "Training Progress: \tEpoch 34 [7680/8883 (86.33%)]\t\tLoss: 0.68296\n",
      "Training Progress: \tEpoch 34 [8000/8883 (89.93%)]\t\tLoss: 1.00283\n",
      "Training Progress: \tEpoch 34 [8320/8883 (93.53%)]\t\tLoss: 0.87385\n",
      "Training Progress: \tEpoch 34 [8640/8883 (97.12%)]\t\tLoss: 0.59808\n",
      "\tTrain loss: 0.02189, Accuracy: 6073/8883 (68.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1197/1692 (70.00%)\n",
      "\tTest loss: 0.00111, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/8883 (0.00%)]\t\tLoss: 0.76034\n",
      "Training Progress: \tEpoch 35 [320/8883 (3.60%)]\t\tLoss: 0.77876\n",
      "Training Progress: \tEpoch 35 [640/8883 (7.19%)]\t\tLoss: 0.85756\n",
      "Training Progress: \tEpoch 35 [960/8883 (10.79%)]\t\tLoss: 0.78806\n",
      "Training Progress: \tEpoch 35 [1280/8883 (14.39%)]\t\tLoss: 0.88019\n",
      "Training Progress: \tEpoch 35 [1600/8883 (17.99%)]\t\tLoss: 1.02500\n",
      "Training Progress: \tEpoch 35 [1920/8883 (21.58%)]\t\tLoss: 0.89214\n",
      "Training Progress: \tEpoch 35 [2240/8883 (25.18%)]\t\tLoss: 0.71671\n",
      "Training Progress: \tEpoch 35 [2560/8883 (28.78%)]\t\tLoss: 0.71228\n",
      "Training Progress: \tEpoch 35 [2880/8883 (32.37%)]\t\tLoss: 0.70254\n",
      "Training Progress: \tEpoch 35 [3200/8883 (35.97%)]\t\tLoss: 0.90049\n",
      "Training Progress: \tEpoch 35 [3520/8883 (39.57%)]\t\tLoss: 0.81744\n",
      "Training Progress: \tEpoch 35 [3840/8883 (43.17%)]\t\tLoss: 0.51844\n",
      "Training Progress: \tEpoch 35 [4160/8883 (46.76%)]\t\tLoss: 0.68587\n",
      "Training Progress: \tEpoch 35 [4480/8883 (50.36%)]\t\tLoss: 0.84701\n",
      "Training Progress: \tEpoch 35 [4800/8883 (53.96%)]\t\tLoss: 0.57561\n",
      "Training Progress: \tEpoch 35 [5120/8883 (57.55%)]\t\tLoss: 0.94787\n",
      "Training Progress: \tEpoch 35 [5440/8883 (61.15%)]\t\tLoss: 0.95176\n",
      "Training Progress: \tEpoch 35 [5760/8883 (64.75%)]\t\tLoss: 0.95733\n",
      "Training Progress: \tEpoch 35 [6080/8883 (68.35%)]\t\tLoss: 0.58052\n",
      "Training Progress: \tEpoch 35 [6400/8883 (71.94%)]\t\tLoss: 1.03032\n",
      "Training Progress: \tEpoch 35 [6720/8883 (75.54%)]\t\tLoss: 0.70523\n",
      "Training Progress: \tEpoch 35 [7040/8883 (79.14%)]\t\tLoss: 0.58253\n",
      "Training Progress: \tEpoch 35 [7360/8883 (82.73%)]\t\tLoss: 0.80425\n",
      "Training Progress: \tEpoch 35 [7680/8883 (86.33%)]\t\tLoss: 0.65693\n",
      "Training Progress: \tEpoch 35 [8000/8883 (89.93%)]\t\tLoss: 0.77769\n",
      "Training Progress: \tEpoch 35 [8320/8883 (93.53%)]\t\tLoss: 0.75992\n",
      "Training Progress: \tEpoch 35 [8640/8883 (97.12%)]\t\tLoss: 0.69129\n",
      "\tTrain loss: 0.02251, Accuracy: 6003/8883 (67.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1181/1692 (69.00%)\n",
      "\tTest loss: 0.00118, Accuracy: 665/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/8883 (0.00%)]\t\tLoss: 0.74971\n",
      "Training Progress: \tEpoch 36 [320/8883 (3.60%)]\t\tLoss: 0.60357\n",
      "Training Progress: \tEpoch 36 [640/8883 (7.19%)]\t\tLoss: 0.87042\n",
      "Training Progress: \tEpoch 36 [960/8883 (10.79%)]\t\tLoss: 0.64609\n",
      "Training Progress: \tEpoch 36 [1280/8883 (14.39%)]\t\tLoss: 0.80321\n",
      "Training Progress: \tEpoch 36 [1600/8883 (17.99%)]\t\tLoss: 1.10231\n",
      "Training Progress: \tEpoch 36 [1920/8883 (21.58%)]\t\tLoss: 0.83637\n",
      "Training Progress: \tEpoch 36 [2240/8883 (25.18%)]\t\tLoss: 0.68483\n",
      "Training Progress: \tEpoch 36 [2560/8883 (28.78%)]\t\tLoss: 0.82872\n",
      "Training Progress: \tEpoch 36 [2880/8883 (32.37%)]\t\tLoss: 0.68475\n",
      "Training Progress: \tEpoch 36 [3200/8883 (35.97%)]\t\tLoss: 0.89757\n",
      "Training Progress: \tEpoch 36 [3520/8883 (39.57%)]\t\tLoss: 0.86015\n",
      "Training Progress: \tEpoch 36 [3840/8883 (43.17%)]\t\tLoss: 0.63636\n",
      "Training Progress: \tEpoch 36 [4160/8883 (46.76%)]\t\tLoss: 0.81098\n",
      "Training Progress: \tEpoch 36 [4480/8883 (50.36%)]\t\tLoss: 0.56360\n",
      "Training Progress: \tEpoch 36 [4800/8883 (53.96%)]\t\tLoss: 0.62439\n",
      "Training Progress: \tEpoch 36 [5120/8883 (57.55%)]\t\tLoss: 1.00707\n",
      "Training Progress: \tEpoch 36 [5440/8883 (61.15%)]\t\tLoss: 0.92742\n",
      "Training Progress: \tEpoch 36 [5760/8883 (64.75%)]\t\tLoss: 0.88870\n",
      "Training Progress: \tEpoch 36 [6080/8883 (68.35%)]\t\tLoss: 0.67524\n",
      "Training Progress: \tEpoch 36 [6400/8883 (71.94%)]\t\tLoss: 1.03061\n",
      "Training Progress: \tEpoch 36 [6720/8883 (75.54%)]\t\tLoss: 0.76246\n",
      "Training Progress: \tEpoch 36 [7040/8883 (79.14%)]\t\tLoss: 0.57791\n",
      "Training Progress: \tEpoch 36 [7360/8883 (82.73%)]\t\tLoss: 0.71192\n",
      "Training Progress: \tEpoch 36 [7680/8883 (86.33%)]\t\tLoss: 0.61124\n",
      "Training Progress: \tEpoch 36 [8000/8883 (89.93%)]\t\tLoss: 0.80340\n",
      "Training Progress: \tEpoch 36 [8320/8883 (93.53%)]\t\tLoss: 0.94023\n",
      "Training Progress: \tEpoch 36 [8640/8883 (97.12%)]\t\tLoss: 0.75123\n",
      "\tTrain loss: 0.02075, Accuracy: 6198/8883 (69.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1249/1692 (73.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 684/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/8883 (0.00%)]\t\tLoss: 0.67482\n",
      "Training Progress: \tEpoch 37 [320/8883 (3.60%)]\t\tLoss: 0.71215\n",
      "Training Progress: \tEpoch 37 [640/8883 (7.19%)]\t\tLoss: 0.88258\n",
      "Training Progress: \tEpoch 37 [960/8883 (10.79%)]\t\tLoss: 0.58796\n",
      "Training Progress: \tEpoch 37 [1280/8883 (14.39%)]\t\tLoss: 0.77201\n",
      "Training Progress: \tEpoch 37 [1600/8883 (17.99%)]\t\tLoss: 0.81491\n",
      "Training Progress: \tEpoch 37 [1920/8883 (21.58%)]\t\tLoss: 0.86331\n",
      "Training Progress: \tEpoch 37 [2240/8883 (25.18%)]\t\tLoss: 0.79810\n",
      "Training Progress: \tEpoch 37 [2560/8883 (28.78%)]\t\tLoss: 0.65455\n",
      "Training Progress: \tEpoch 37 [2880/8883 (32.37%)]\t\tLoss: 0.58573\n",
      "Training Progress: \tEpoch 37 [3200/8883 (35.97%)]\t\tLoss: 0.83386\n",
      "Training Progress: \tEpoch 37 [3520/8883 (39.57%)]\t\tLoss: 0.85153\n",
      "Training Progress: \tEpoch 37 [3840/8883 (43.17%)]\t\tLoss: 0.66638\n",
      "Training Progress: \tEpoch 37 [4160/8883 (46.76%)]\t\tLoss: 0.77017\n",
      "Training Progress: \tEpoch 37 [4480/8883 (50.36%)]\t\tLoss: 0.90976\n",
      "Training Progress: \tEpoch 37 [4800/8883 (53.96%)]\t\tLoss: 0.48580\n",
      "Training Progress: \tEpoch 37 [5120/8883 (57.55%)]\t\tLoss: 0.86248\n",
      "Training Progress: \tEpoch 37 [5440/8883 (61.15%)]\t\tLoss: 1.01836\n",
      "Training Progress: \tEpoch 37 [5760/8883 (64.75%)]\t\tLoss: 0.98245\n",
      "Training Progress: \tEpoch 37 [6080/8883 (68.35%)]\t\tLoss: 0.77403\n",
      "Training Progress: \tEpoch 37 [6400/8883 (71.94%)]\t\tLoss: 0.91096\n",
      "Training Progress: \tEpoch 37 [6720/8883 (75.54%)]\t\tLoss: 0.69424\n",
      "Training Progress: \tEpoch 37 [7040/8883 (79.14%)]\t\tLoss: 0.65664\n",
      "Training Progress: \tEpoch 37 [7360/8883 (82.73%)]\t\tLoss: 0.82439\n",
      "Training Progress: \tEpoch 37 [7680/8883 (86.33%)]\t\tLoss: 0.77589\n",
      "Training Progress: \tEpoch 37 [8000/8883 (89.93%)]\t\tLoss: 0.83362\n",
      "Training Progress: \tEpoch 37 [8320/8883 (93.53%)]\t\tLoss: 0.87430\n",
      "Training Progress: \tEpoch 37 [8640/8883 (97.12%)]\t\tLoss: 0.76814\n",
      "\tTrain loss: 0.01959, Accuracy: 6357/8883 (71.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1288/1692 (76.00%)\n",
      "\tTest loss: 0.00114, Accuracy: 702/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/8883 (0.00%)]\t\tLoss: 0.63173\n",
      "Training Progress: \tEpoch 38 [320/8883 (3.60%)]\t\tLoss: 0.72305\n",
      "Training Progress: \tEpoch 38 [640/8883 (7.19%)]\t\tLoss: 0.82910\n",
      "Training Progress: \tEpoch 38 [960/8883 (10.79%)]\t\tLoss: 0.56751\n",
      "Training Progress: \tEpoch 38 [1280/8883 (14.39%)]\t\tLoss: 1.04729\n",
      "Training Progress: \tEpoch 38 [1600/8883 (17.99%)]\t\tLoss: 0.78160\n",
      "Training Progress: \tEpoch 38 [1920/8883 (21.58%)]\t\tLoss: 0.85721\n",
      "Training Progress: \tEpoch 38 [2240/8883 (25.18%)]\t\tLoss: 0.83557\n",
      "Training Progress: \tEpoch 38 [2560/8883 (28.78%)]\t\tLoss: 0.85083\n",
      "Training Progress: \tEpoch 38 [2880/8883 (32.37%)]\t\tLoss: 0.70853\n",
      "Training Progress: \tEpoch 38 [3200/8883 (35.97%)]\t\tLoss: 0.81067\n",
      "Training Progress: \tEpoch 38 [3520/8883 (39.57%)]\t\tLoss: 0.76793\n",
      "Training Progress: \tEpoch 38 [3840/8883 (43.17%)]\t\tLoss: 0.43378\n",
      "Training Progress: \tEpoch 38 [4160/8883 (46.76%)]\t\tLoss: 0.72940\n",
      "Training Progress: \tEpoch 38 [4480/8883 (50.36%)]\t\tLoss: 0.62120\n",
      "Training Progress: \tEpoch 38 [4800/8883 (53.96%)]\t\tLoss: 0.61814\n",
      "Training Progress: \tEpoch 38 [5120/8883 (57.55%)]\t\tLoss: 1.01705\n",
      "Training Progress: \tEpoch 38 [5440/8883 (61.15%)]\t\tLoss: 0.83353\n",
      "Training Progress: \tEpoch 38 [5760/8883 (64.75%)]\t\tLoss: 1.05968\n",
      "Training Progress: \tEpoch 38 [6080/8883 (68.35%)]\t\tLoss: 0.62197\n",
      "Training Progress: \tEpoch 38 [6400/8883 (71.94%)]\t\tLoss: 1.01755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 38 [6720/8883 (75.54%)]\t\tLoss: 0.89525\n",
      "Training Progress: \tEpoch 38 [7040/8883 (79.14%)]\t\tLoss: 0.71406\n",
      "Training Progress: \tEpoch 38 [7360/8883 (82.73%)]\t\tLoss: 0.66710\n",
      "Training Progress: \tEpoch 38 [7680/8883 (86.33%)]\t\tLoss: 0.87703\n",
      "Training Progress: \tEpoch 38 [8000/8883 (89.93%)]\t\tLoss: 0.58972\n",
      "Training Progress: \tEpoch 38 [8320/8883 (93.53%)]\t\tLoss: 0.92635\n",
      "Training Progress: \tEpoch 38 [8640/8883 (97.12%)]\t\tLoss: 0.61023\n",
      "\tTrain loss: 0.01987, Accuracy: 6336/8883 (71.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1252/1692 (73.00%)\n",
      "\tTest loss: 0.00128, Accuracy: 673/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/8883 (0.00%)]\t\tLoss: 0.56993\n",
      "Training Progress: \tEpoch 39 [320/8883 (3.60%)]\t\tLoss: 0.63092\n",
      "Training Progress: \tEpoch 39 [640/8883 (7.19%)]\t\tLoss: 0.74271\n",
      "Training Progress: \tEpoch 39 [960/8883 (10.79%)]\t\tLoss: 0.68400\n",
      "Training Progress: \tEpoch 39 [1280/8883 (14.39%)]\t\tLoss: 0.66316\n",
      "Training Progress: \tEpoch 39 [1600/8883 (17.99%)]\t\tLoss: 0.99304\n",
      "Training Progress: \tEpoch 39 [1920/8883 (21.58%)]\t\tLoss: 0.66558\n",
      "Training Progress: \tEpoch 39 [2240/8883 (25.18%)]\t\tLoss: 0.68081\n",
      "Training Progress: \tEpoch 39 [2560/8883 (28.78%)]\t\tLoss: 0.62376\n",
      "Training Progress: \tEpoch 39 [2880/8883 (32.37%)]\t\tLoss: 0.49263\n",
      "Training Progress: \tEpoch 39 [3200/8883 (35.97%)]\t\tLoss: 0.91214\n",
      "Training Progress: \tEpoch 39 [3520/8883 (39.57%)]\t\tLoss: 0.80067\n",
      "Training Progress: \tEpoch 39 [3840/8883 (43.17%)]\t\tLoss: 0.47660\n",
      "Training Progress: \tEpoch 39 [4160/8883 (46.76%)]\t\tLoss: 0.57156\n",
      "Training Progress: \tEpoch 39 [4480/8883 (50.36%)]\t\tLoss: 0.60031\n",
      "Training Progress: \tEpoch 39 [4800/8883 (53.96%)]\t\tLoss: 0.76881\n",
      "Training Progress: \tEpoch 39 [5120/8883 (57.55%)]\t\tLoss: 0.90499\n",
      "Training Progress: \tEpoch 39 [5440/8883 (61.15%)]\t\tLoss: 0.84545\n",
      "Training Progress: \tEpoch 39 [5760/8883 (64.75%)]\t\tLoss: 0.90882\n",
      "Training Progress: \tEpoch 39 [6080/8883 (68.35%)]\t\tLoss: 0.57605\n",
      "Training Progress: \tEpoch 39 [6400/8883 (71.94%)]\t\tLoss: 0.93096\n",
      "Training Progress: \tEpoch 39 [6720/8883 (75.54%)]\t\tLoss: 0.85689\n",
      "Training Progress: \tEpoch 39 [7040/8883 (79.14%)]\t\tLoss: 0.68387\n",
      "Training Progress: \tEpoch 39 [7360/8883 (82.73%)]\t\tLoss: 0.68790\n",
      "Training Progress: \tEpoch 39 [7680/8883 (86.33%)]\t\tLoss: 0.69863\n",
      "Training Progress: \tEpoch 39 [8000/8883 (89.93%)]\t\tLoss: 0.68177\n",
      "Training Progress: \tEpoch 39 [8320/8883 (93.53%)]\t\tLoss: 0.79715\n",
      "Training Progress: \tEpoch 39 [8640/8883 (97.12%)]\t\tLoss: 0.59954\n",
      "\tTrain loss: 0.01929, Accuracy: 6399/8883 (72.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1254/1692 (74.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 717/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/8883 (0.00%)]\t\tLoss: 0.52475\n",
      "Training Progress: \tEpoch 40 [320/8883 (3.60%)]\t\tLoss: 0.53145\n",
      "Training Progress: \tEpoch 40 [640/8883 (7.19%)]\t\tLoss: 0.84154\n",
      "Training Progress: \tEpoch 40 [960/8883 (10.79%)]\t\tLoss: 0.67891\n",
      "Training Progress: \tEpoch 40 [1280/8883 (14.39%)]\t\tLoss: 0.71689\n",
      "Training Progress: \tEpoch 40 [1600/8883 (17.99%)]\t\tLoss: 0.82131\n",
      "Training Progress: \tEpoch 40 [1920/8883 (21.58%)]\t\tLoss: 0.74754\n",
      "Training Progress: \tEpoch 40 [2240/8883 (25.18%)]\t\tLoss: 0.55516\n",
      "Training Progress: \tEpoch 40 [2560/8883 (28.78%)]\t\tLoss: 0.58349\n",
      "Training Progress: \tEpoch 40 [2880/8883 (32.37%)]\t\tLoss: 0.79909\n",
      "Training Progress: \tEpoch 40 [3200/8883 (35.97%)]\t\tLoss: 0.93176\n",
      "Training Progress: \tEpoch 40 [3520/8883 (39.57%)]\t\tLoss: 0.72534\n",
      "Training Progress: \tEpoch 40 [3840/8883 (43.17%)]\t\tLoss: 0.62997\n",
      "Training Progress: \tEpoch 40 [4160/8883 (46.76%)]\t\tLoss: 0.65573\n",
      "Training Progress: \tEpoch 40 [4480/8883 (50.36%)]\t\tLoss: 0.70921\n",
      "Training Progress: \tEpoch 40 [4800/8883 (53.96%)]\t\tLoss: 0.61767\n",
      "Training Progress: \tEpoch 40 [5120/8883 (57.55%)]\t\tLoss: 0.94462\n",
      "Training Progress: \tEpoch 40 [5440/8883 (61.15%)]\t\tLoss: 0.84302\n",
      "Training Progress: \tEpoch 40 [5760/8883 (64.75%)]\t\tLoss: 0.83457\n",
      "Training Progress: \tEpoch 40 [6080/8883 (68.35%)]\t\tLoss: 0.75255\n",
      "Training Progress: \tEpoch 40 [6400/8883 (71.94%)]\t\tLoss: 0.80214\n",
      "Training Progress: \tEpoch 40 [6720/8883 (75.54%)]\t\tLoss: 0.77002\n",
      "Training Progress: \tEpoch 40 [7040/8883 (79.14%)]\t\tLoss: 0.61652\n",
      "Training Progress: \tEpoch 40 [7360/8883 (82.73%)]\t\tLoss: 0.65096\n",
      "Training Progress: \tEpoch 40 [7680/8883 (86.33%)]\t\tLoss: 0.53056\n",
      "Training Progress: \tEpoch 40 [8000/8883 (89.93%)]\t\tLoss: 0.76081\n",
      "Training Progress: \tEpoch 40 [8320/8883 (93.53%)]\t\tLoss: 1.00936\n",
      "Training Progress: \tEpoch 40 [8640/8883 (97.12%)]\t\tLoss: 0.69685\n",
      "\tTrain loss: 0.01883, Accuracy: 6430/8883 (72.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1279/1692 (75.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 700/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/8883 (0.00%)]\t\tLoss: 0.50472\n",
      "Training Progress: \tEpoch 41 [320/8883 (3.60%)]\t\tLoss: 0.69962\n",
      "Training Progress: \tEpoch 41 [640/8883 (7.19%)]\t\tLoss: 0.56759\n",
      "Training Progress: \tEpoch 41 [960/8883 (10.79%)]\t\tLoss: 0.54427\n",
      "Training Progress: \tEpoch 41 [1280/8883 (14.39%)]\t\tLoss: 0.81773\n",
      "Training Progress: \tEpoch 41 [1600/8883 (17.99%)]\t\tLoss: 0.92158\n",
      "Training Progress: \tEpoch 41 [1920/8883 (21.58%)]\t\tLoss: 0.68798\n",
      "Training Progress: \tEpoch 41 [2240/8883 (25.18%)]\t\tLoss: 0.56678\n",
      "Training Progress: \tEpoch 41 [2560/8883 (28.78%)]\t\tLoss: 0.45929\n",
      "Training Progress: \tEpoch 41 [2880/8883 (32.37%)]\t\tLoss: 0.72576\n",
      "Training Progress: \tEpoch 41 [3200/8883 (35.97%)]\t\tLoss: 0.79169\n",
      "Training Progress: \tEpoch 41 [3520/8883 (39.57%)]\t\tLoss: 0.89538\n",
      "Training Progress: \tEpoch 41 [3840/8883 (43.17%)]\t\tLoss: 0.54000\n",
      "Training Progress: \tEpoch 41 [4160/8883 (46.76%)]\t\tLoss: 0.72080\n",
      "Training Progress: \tEpoch 41 [4480/8883 (50.36%)]\t\tLoss: 0.61049\n",
      "Training Progress: \tEpoch 41 [4800/8883 (53.96%)]\t\tLoss: 0.65011\n",
      "Training Progress: \tEpoch 41 [5120/8883 (57.55%)]\t\tLoss: 0.62507\n",
      "Training Progress: \tEpoch 41 [5440/8883 (61.15%)]\t\tLoss: 0.81991\n",
      "Training Progress: \tEpoch 41 [5760/8883 (64.75%)]\t\tLoss: 0.99626\n",
      "Training Progress: \tEpoch 41 [6080/8883 (68.35%)]\t\tLoss: 0.69853\n",
      "Training Progress: \tEpoch 41 [6400/8883 (71.94%)]\t\tLoss: 0.77074\n",
      "Training Progress: \tEpoch 41 [6720/8883 (75.54%)]\t\tLoss: 0.65270\n",
      "Training Progress: \tEpoch 41 [7040/8883 (79.14%)]\t\tLoss: 0.56966\n",
      "Training Progress: \tEpoch 41 [7360/8883 (82.73%)]\t\tLoss: 0.84890\n",
      "Training Progress: \tEpoch 41 [7680/8883 (86.33%)]\t\tLoss: 0.64905\n",
      "Training Progress: \tEpoch 41 [8000/8883 (89.93%)]\t\tLoss: 0.62405\n",
      "Training Progress: \tEpoch 41 [8320/8883 (93.53%)]\t\tLoss: 0.95608\n",
      "Training Progress: \tEpoch 41 [8640/8883 (97.12%)]\t\tLoss: 0.51853\n",
      "\tTrain loss: 0.01743, Accuracy: 6594/8883 (74.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1331/1692 (78.00%)\n",
      "\tTest loss: 0.00118, Accuracy: 706/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/8883 (0.00%)]\t\tLoss: 0.68097\n",
      "Training Progress: \tEpoch 42 [320/8883 (3.60%)]\t\tLoss: 0.69775\n",
      "Training Progress: \tEpoch 42 [640/8883 (7.19%)]\t\tLoss: 0.74233\n",
      "Training Progress: \tEpoch 42 [960/8883 (10.79%)]\t\tLoss: 0.59960\n",
      "Training Progress: \tEpoch 42 [1280/8883 (14.39%)]\t\tLoss: 0.55140\n",
      "Training Progress: \tEpoch 42 [1600/8883 (17.99%)]\t\tLoss: 0.93401\n",
      "Training Progress: \tEpoch 42 [1920/8883 (21.58%)]\t\tLoss: 0.67630\n",
      "Training Progress: \tEpoch 42 [2240/8883 (25.18%)]\t\tLoss: 0.80571\n",
      "Training Progress: \tEpoch 42 [2560/8883 (28.78%)]\t\tLoss: 0.65497\n",
      "Training Progress: \tEpoch 42 [2880/8883 (32.37%)]\t\tLoss: 0.56207\n",
      "Training Progress: \tEpoch 42 [3200/8883 (35.97%)]\t\tLoss: 0.88275\n",
      "Training Progress: \tEpoch 42 [3520/8883 (39.57%)]\t\tLoss: 1.06297\n",
      "Training Progress: \tEpoch 42 [3840/8883 (43.17%)]\t\tLoss: 0.42030\n",
      "Training Progress: \tEpoch 42 [4160/8883 (46.76%)]\t\tLoss: 0.57572\n",
      "Training Progress: \tEpoch 42 [4480/8883 (50.36%)]\t\tLoss: 0.69003\n",
      "Training Progress: \tEpoch 42 [4800/8883 (53.96%)]\t\tLoss: 0.54252\n",
      "Training Progress: \tEpoch 42 [5120/8883 (57.55%)]\t\tLoss: 0.72699\n",
      "Training Progress: \tEpoch 42 [5440/8883 (61.15%)]\t\tLoss: 0.87834\n",
      "Training Progress: \tEpoch 42 [5760/8883 (64.75%)]\t\tLoss: 0.98161\n",
      "Training Progress: \tEpoch 42 [6080/8883 (68.35%)]\t\tLoss: 0.92202\n",
      "Training Progress: \tEpoch 42 [6400/8883 (71.94%)]\t\tLoss: 0.83217\n",
      "Training Progress: \tEpoch 42 [6720/8883 (75.54%)]\t\tLoss: 0.76599\n",
      "Training Progress: \tEpoch 42 [7040/8883 (79.14%)]\t\tLoss: 0.74094\n",
      "Training Progress: \tEpoch 42 [7360/8883 (82.73%)]\t\tLoss: 0.53880\n",
      "Training Progress: \tEpoch 42 [7680/8883 (86.33%)]\t\tLoss: 0.74206\n",
      "Training Progress: \tEpoch 42 [8000/8883 (89.93%)]\t\tLoss: 0.73751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [8320/8883 (93.53%)]\t\tLoss: 0.77782\n",
      "Training Progress: \tEpoch 42 [8640/8883 (97.12%)]\t\tLoss: 0.56303\n",
      "\tTrain loss: 0.01866, Accuracy: 6434/8883 (72.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1304/1692 (77.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 689/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/8883 (0.00%)]\t\tLoss: 0.49126\n",
      "Training Progress: \tEpoch 43 [320/8883 (3.60%)]\t\tLoss: 0.61464\n",
      "Training Progress: \tEpoch 43 [640/8883 (7.19%)]\t\tLoss: 0.64233\n",
      "Training Progress: \tEpoch 43 [960/8883 (10.79%)]\t\tLoss: 0.61957\n",
      "Training Progress: \tEpoch 43 [1280/8883 (14.39%)]\t\tLoss: 0.62020\n",
      "Training Progress: \tEpoch 43 [1600/8883 (17.99%)]\t\tLoss: 0.89929\n",
      "Training Progress: \tEpoch 43 [1920/8883 (21.58%)]\t\tLoss: 0.79611\n",
      "Training Progress: \tEpoch 43 [2240/8883 (25.18%)]\t\tLoss: 0.74503\n",
      "Training Progress: \tEpoch 43 [2560/8883 (28.78%)]\t\tLoss: 0.68054\n",
      "Training Progress: \tEpoch 43 [2880/8883 (32.37%)]\t\tLoss: 0.52065\n",
      "Training Progress: \tEpoch 43 [3200/8883 (35.97%)]\t\tLoss: 0.78507\n",
      "Training Progress: \tEpoch 43 [3520/8883 (39.57%)]\t\tLoss: 0.72849\n",
      "Training Progress: \tEpoch 43 [3840/8883 (43.17%)]\t\tLoss: 0.59741\n",
      "Training Progress: \tEpoch 43 [4160/8883 (46.76%)]\t\tLoss: 0.67713\n",
      "Training Progress: \tEpoch 43 [4480/8883 (50.36%)]\t\tLoss: 0.66694\n",
      "Training Progress: \tEpoch 43 [4800/8883 (53.96%)]\t\tLoss: 0.45586\n",
      "Training Progress: \tEpoch 43 [5120/8883 (57.55%)]\t\tLoss: 0.72506\n",
      "Training Progress: \tEpoch 43 [5440/8883 (61.15%)]\t\tLoss: 0.80895\n",
      "Training Progress: \tEpoch 43 [5760/8883 (64.75%)]\t\tLoss: 0.77259\n",
      "Training Progress: \tEpoch 43 [6080/8883 (68.35%)]\t\tLoss: 0.80352\n",
      "Training Progress: \tEpoch 43 [6400/8883 (71.94%)]\t\tLoss: 0.67973\n",
      "Training Progress: \tEpoch 43 [6720/8883 (75.54%)]\t\tLoss: 0.84913\n",
      "Training Progress: \tEpoch 43 [7040/8883 (79.14%)]\t\tLoss: 0.75417\n",
      "Training Progress: \tEpoch 43 [7360/8883 (82.73%)]\t\tLoss: 0.65745\n",
      "Training Progress: \tEpoch 43 [7680/8883 (86.33%)]\t\tLoss: 0.55405\n",
      "Training Progress: \tEpoch 43 [8000/8883 (89.93%)]\t\tLoss: 0.66199\n",
      "Training Progress: \tEpoch 43 [8320/8883 (93.53%)]\t\tLoss: 0.80443\n",
      "Training Progress: \tEpoch 43 [8640/8883 (97.12%)]\t\tLoss: 0.56153\n",
      "\tTrain loss: 0.01786, Accuracy: 6512/8883 (73.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1318/1692 (77.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 684/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/8883 (0.00%)]\t\tLoss: 0.45440\n",
      "Training Progress: \tEpoch 44 [320/8883 (3.60%)]\t\tLoss: 0.75980\n",
      "Training Progress: \tEpoch 44 [640/8883 (7.19%)]\t\tLoss: 0.68721\n",
      "Training Progress: \tEpoch 44 [960/8883 (10.79%)]\t\tLoss: 0.74681\n",
      "Training Progress: \tEpoch 44 [1280/8883 (14.39%)]\t\tLoss: 0.80850\n",
      "Training Progress: \tEpoch 44 [1600/8883 (17.99%)]\t\tLoss: 0.97169\n",
      "Training Progress: \tEpoch 44 [1920/8883 (21.58%)]\t\tLoss: 0.75051\n",
      "Training Progress: \tEpoch 44 [2240/8883 (25.18%)]\t\tLoss: 0.69134\n",
      "Training Progress: \tEpoch 44 [2560/8883 (28.78%)]\t\tLoss: 0.59227\n",
      "Training Progress: \tEpoch 44 [2880/8883 (32.37%)]\t\tLoss: 0.67369\n",
      "Training Progress: \tEpoch 44 [3200/8883 (35.97%)]\t\tLoss: 0.82411\n",
      "Training Progress: \tEpoch 44 [3520/8883 (39.57%)]\t\tLoss: 0.75902\n",
      "Training Progress: \tEpoch 44 [3840/8883 (43.17%)]\t\tLoss: 0.52040\n",
      "Training Progress: \tEpoch 44 [4160/8883 (46.76%)]\t\tLoss: 0.57728\n",
      "Training Progress: \tEpoch 44 [4480/8883 (50.36%)]\t\tLoss: 0.57676\n",
      "Training Progress: \tEpoch 44 [4800/8883 (53.96%)]\t\tLoss: 0.47660\n",
      "Training Progress: \tEpoch 44 [5120/8883 (57.55%)]\t\tLoss: 0.71780\n",
      "Training Progress: \tEpoch 44 [5440/8883 (61.15%)]\t\tLoss: 0.73735\n",
      "Training Progress: \tEpoch 44 [5760/8883 (64.75%)]\t\tLoss: 1.04225\n",
      "Training Progress: \tEpoch 44 [6080/8883 (68.35%)]\t\tLoss: 0.75560\n",
      "Training Progress: \tEpoch 44 [6400/8883 (71.94%)]\t\tLoss: 0.98260\n",
      "Training Progress: \tEpoch 44 [6720/8883 (75.54%)]\t\tLoss: 0.87093\n",
      "Training Progress: \tEpoch 44 [7040/8883 (79.14%)]\t\tLoss: 0.53643\n",
      "Training Progress: \tEpoch 44 [7360/8883 (82.73%)]\t\tLoss: 0.52026\n",
      "Training Progress: \tEpoch 44 [7680/8883 (86.33%)]\t\tLoss: 0.63833\n",
      "Training Progress: \tEpoch 44 [8000/8883 (89.93%)]\t\tLoss: 0.54828\n",
      "Training Progress: \tEpoch 44 [8320/8883 (93.53%)]\t\tLoss: 0.65796\n",
      "Training Progress: \tEpoch 44 [8640/8883 (97.12%)]\t\tLoss: 0.57409\n",
      "\tTrain loss: 0.01661, Accuracy: 6681/8883 (75.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1335/1692 (78.00%)\n",
      "\tTest loss: 0.00124, Accuracy: 692/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/8883 (0.00%)]\t\tLoss: 0.84482\n",
      "Training Progress: \tEpoch 45 [320/8883 (3.60%)]\t\tLoss: 0.52713\n",
      "Training Progress: \tEpoch 45 [640/8883 (7.19%)]\t\tLoss: 0.53293\n",
      "Training Progress: \tEpoch 45 [960/8883 (10.79%)]\t\tLoss: 0.52080\n",
      "Training Progress: \tEpoch 45 [1280/8883 (14.39%)]\t\tLoss: 0.53911\n",
      "Training Progress: \tEpoch 45 [1600/8883 (17.99%)]\t\tLoss: 0.92432\n",
      "Training Progress: \tEpoch 45 [1920/8883 (21.58%)]\t\tLoss: 0.57543\n",
      "Training Progress: \tEpoch 45 [2240/8883 (25.18%)]\t\tLoss: 0.74416\n",
      "Training Progress: \tEpoch 45 [2560/8883 (28.78%)]\t\tLoss: 0.62547\n",
      "Training Progress: \tEpoch 45 [2880/8883 (32.37%)]\t\tLoss: 0.60163\n",
      "Training Progress: \tEpoch 45 [3200/8883 (35.97%)]\t\tLoss: 0.79853\n",
      "Training Progress: \tEpoch 45 [3520/8883 (39.57%)]\t\tLoss: 1.08165\n",
      "Training Progress: \tEpoch 45 [3840/8883 (43.17%)]\t\tLoss: 0.60316\n",
      "Training Progress: \tEpoch 45 [4160/8883 (46.76%)]\t\tLoss: 0.79252\n",
      "Training Progress: \tEpoch 45 [4480/8883 (50.36%)]\t\tLoss: 0.74158\n",
      "Training Progress: \tEpoch 45 [4800/8883 (53.96%)]\t\tLoss: 0.47986\n",
      "Training Progress: \tEpoch 45 [5120/8883 (57.55%)]\t\tLoss: 1.03262\n",
      "Training Progress: \tEpoch 45 [5440/8883 (61.15%)]\t\tLoss: 0.88556\n",
      "Training Progress: \tEpoch 45 [5760/8883 (64.75%)]\t\tLoss: 1.04598\n",
      "Training Progress: \tEpoch 45 [6080/8883 (68.35%)]\t\tLoss: 0.56390\n",
      "Training Progress: \tEpoch 45 [6400/8883 (71.94%)]\t\tLoss: 0.80050\n",
      "Training Progress: \tEpoch 45 [6720/8883 (75.54%)]\t\tLoss: 0.64635\n",
      "Training Progress: \tEpoch 45 [7040/8883 (79.14%)]\t\tLoss: 0.62913\n",
      "Training Progress: \tEpoch 45 [7360/8883 (82.73%)]\t\tLoss: 0.69287\n",
      "Training Progress: \tEpoch 45 [7680/8883 (86.33%)]\t\tLoss: 0.52162\n",
      "Training Progress: \tEpoch 45 [8000/8883 (89.93%)]\t\tLoss: 0.52172\n",
      "Training Progress: \tEpoch 45 [8320/8883 (93.53%)]\t\tLoss: 0.89297\n",
      "Training Progress: \tEpoch 45 [8640/8883 (97.12%)]\t\tLoss: 0.62302\n",
      "\tTrain loss: 0.01971, Accuracy: 6349/8883 (71.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1267/1692 (74.00%)\n",
      "\tTest loss: 0.00133, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/8883 (0.00%)]\t\tLoss: 0.54439\n",
      "Training Progress: \tEpoch 46 [320/8883 (3.60%)]\t\tLoss: 0.63135\n",
      "Training Progress: \tEpoch 46 [640/8883 (7.19%)]\t\tLoss: 0.73391\n",
      "Training Progress: \tEpoch 46 [960/8883 (10.79%)]\t\tLoss: 0.65444\n",
      "Training Progress: \tEpoch 46 [1280/8883 (14.39%)]\t\tLoss: 0.57524\n",
      "Training Progress: \tEpoch 46 [1600/8883 (17.99%)]\t\tLoss: 0.89295\n",
      "Training Progress: \tEpoch 46 [1920/8883 (21.58%)]\t\tLoss: 0.78026\n",
      "Training Progress: \tEpoch 46 [2240/8883 (25.18%)]\t\tLoss: 0.60170\n",
      "Training Progress: \tEpoch 46 [2560/8883 (28.78%)]\t\tLoss: 0.58530\n",
      "Training Progress: \tEpoch 46 [2880/8883 (32.37%)]\t\tLoss: 0.56296\n",
      "Training Progress: \tEpoch 46 [3200/8883 (35.97%)]\t\tLoss: 0.80496\n",
      "Training Progress: \tEpoch 46 [3520/8883 (39.57%)]\t\tLoss: 0.80225\n",
      "Training Progress: \tEpoch 46 [3840/8883 (43.17%)]\t\tLoss: 0.36922\n",
      "Training Progress: \tEpoch 46 [4160/8883 (46.76%)]\t\tLoss: 0.66950\n",
      "Training Progress: \tEpoch 46 [4480/8883 (50.36%)]\t\tLoss: 0.49110\n",
      "Training Progress: \tEpoch 46 [4800/8883 (53.96%)]\t\tLoss: 0.53992\n",
      "Training Progress: \tEpoch 46 [5120/8883 (57.55%)]\t\tLoss: 0.68501\n",
      "Training Progress: \tEpoch 46 [5440/8883 (61.15%)]\t\tLoss: 0.75855\n",
      "Training Progress: \tEpoch 46 [5760/8883 (64.75%)]\t\tLoss: 0.78198\n",
      "Training Progress: \tEpoch 46 [6080/8883 (68.35%)]\t\tLoss: 0.70893\n",
      "Training Progress: \tEpoch 46 [6400/8883 (71.94%)]\t\tLoss: 0.73261\n",
      "Training Progress: \tEpoch 46 [6720/8883 (75.54%)]\t\tLoss: 0.61923\n",
      "Training Progress: \tEpoch 46 [7040/8883 (79.14%)]\t\tLoss: 0.50223\n",
      "Training Progress: \tEpoch 46 [7360/8883 (82.73%)]\t\tLoss: 0.62057\n",
      "Training Progress: \tEpoch 46 [7680/8883 (86.33%)]\t\tLoss: 0.52912\n",
      "Training Progress: \tEpoch 46 [8000/8883 (89.93%)]\t\tLoss: 0.72211\n",
      "Training Progress: \tEpoch 46 [8320/8883 (93.53%)]\t\tLoss: 0.77561\n",
      "Training Progress: \tEpoch 46 [8640/8883 (97.12%)]\t\tLoss: 0.57564\n",
      "\tTrain loss: 0.01869, Accuracy: 6457/8883 (72.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1328/1692 (78.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/8883 (0.00%)]\t\tLoss: 0.50731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 47 [320/8883 (3.60%)]\t\tLoss: 0.65595\n",
      "Training Progress: \tEpoch 47 [640/8883 (7.19%)]\t\tLoss: 0.88994\n",
      "Training Progress: \tEpoch 47 [960/8883 (10.79%)]\t\tLoss: 0.73071\n",
      "Training Progress: \tEpoch 47 [1280/8883 (14.39%)]\t\tLoss: 0.75886\n",
      "Training Progress: \tEpoch 47 [1600/8883 (17.99%)]\t\tLoss: 0.92873\n",
      "Training Progress: \tEpoch 47 [1920/8883 (21.58%)]\t\tLoss: 0.62482\n",
      "Training Progress: \tEpoch 47 [2240/8883 (25.18%)]\t\tLoss: 0.88188\n",
      "Training Progress: \tEpoch 47 [2560/8883 (28.78%)]\t\tLoss: 0.64249\n",
      "Training Progress: \tEpoch 47 [2880/8883 (32.37%)]\t\tLoss: 0.56301\n",
      "Training Progress: \tEpoch 47 [3200/8883 (35.97%)]\t\tLoss: 0.84228\n",
      "Training Progress: \tEpoch 47 [3520/8883 (39.57%)]\t\tLoss: 0.77099\n",
      "Training Progress: \tEpoch 47 [3840/8883 (43.17%)]\t\tLoss: 0.40756\n",
      "Training Progress: \tEpoch 47 [4160/8883 (46.76%)]\t\tLoss: 0.66654\n",
      "Training Progress: \tEpoch 47 [4480/8883 (50.36%)]\t\tLoss: 0.66631\n",
      "Training Progress: \tEpoch 47 [4800/8883 (53.96%)]\t\tLoss: 0.52832\n",
      "Training Progress: \tEpoch 47 [5120/8883 (57.55%)]\t\tLoss: 0.86301\n",
      "Training Progress: \tEpoch 47 [5440/8883 (61.15%)]\t\tLoss: 0.71454\n",
      "Training Progress: \tEpoch 47 [5760/8883 (64.75%)]\t\tLoss: 0.80854\n",
      "Training Progress: \tEpoch 47 [6080/8883 (68.35%)]\t\tLoss: 0.76212\n",
      "Training Progress: \tEpoch 47 [6400/8883 (71.94%)]\t\tLoss: 0.93010\n",
      "Training Progress: \tEpoch 47 [6720/8883 (75.54%)]\t\tLoss: 0.74812\n",
      "Training Progress: \tEpoch 47 [7040/8883 (79.14%)]\t\tLoss: 0.58911\n",
      "Training Progress: \tEpoch 47 [7360/8883 (82.73%)]\t\tLoss: 0.74809\n",
      "Training Progress: \tEpoch 47 [7680/8883 (86.33%)]\t\tLoss: 0.83132\n",
      "Training Progress: \tEpoch 47 [8000/8883 (89.93%)]\t\tLoss: 0.67784\n",
      "Training Progress: \tEpoch 47 [8320/8883 (93.53%)]\t\tLoss: 1.06152\n",
      "Training Progress: \tEpoch 47 [8640/8883 (97.12%)]\t\tLoss: 0.40390\n",
      "\tTrain loss: 0.01756, Accuracy: 6580/8883 (74.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1315/1692 (77.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 701/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/8883 (0.00%)]\t\tLoss: 0.51594\n",
      "Training Progress: \tEpoch 48 [320/8883 (3.60%)]\t\tLoss: 0.60398\n",
      "Training Progress: \tEpoch 48 [640/8883 (7.19%)]\t\tLoss: 0.49357\n",
      "Training Progress: \tEpoch 48 [960/8883 (10.79%)]\t\tLoss: 0.66925\n",
      "Training Progress: \tEpoch 48 [1280/8883 (14.39%)]\t\tLoss: 0.83209\n",
      "Training Progress: \tEpoch 48 [1600/8883 (17.99%)]\t\tLoss: 0.77625\n",
      "Training Progress: \tEpoch 48 [1920/8883 (21.58%)]\t\tLoss: 0.60225\n",
      "Training Progress: \tEpoch 48 [2240/8883 (25.18%)]\t\tLoss: 0.49062\n",
      "Training Progress: \tEpoch 48 [2560/8883 (28.78%)]\t\tLoss: 0.71071\n",
      "Training Progress: \tEpoch 48 [2880/8883 (32.37%)]\t\tLoss: 0.45604\n",
      "Training Progress: \tEpoch 48 [3200/8883 (35.97%)]\t\tLoss: 1.01385\n",
      "Training Progress: \tEpoch 48 [3520/8883 (39.57%)]\t\tLoss: 0.87219\n",
      "Training Progress: \tEpoch 48 [3840/8883 (43.17%)]\t\tLoss: 0.38489\n",
      "Training Progress: \tEpoch 48 [4160/8883 (46.76%)]\t\tLoss: 0.74394\n",
      "Training Progress: \tEpoch 48 [4480/8883 (50.36%)]\t\tLoss: 0.67757\n",
      "Training Progress: \tEpoch 48 [4800/8883 (53.96%)]\t\tLoss: 0.48926\n",
      "Training Progress: \tEpoch 48 [5120/8883 (57.55%)]\t\tLoss: 0.93966\n",
      "Training Progress: \tEpoch 48 [5440/8883 (61.15%)]\t\tLoss: 0.74461\n",
      "Training Progress: \tEpoch 48 [5760/8883 (64.75%)]\t\tLoss: 0.99436\n",
      "Training Progress: \tEpoch 48 [6080/8883 (68.35%)]\t\tLoss: 0.63496\n",
      "Training Progress: \tEpoch 48 [6400/8883 (71.94%)]\t\tLoss: 0.81006\n",
      "Training Progress: \tEpoch 48 [6720/8883 (75.54%)]\t\tLoss: 0.68477\n",
      "Training Progress: \tEpoch 48 [7040/8883 (79.14%)]\t\tLoss: 0.60147\n",
      "Training Progress: \tEpoch 48 [7360/8883 (82.73%)]\t\tLoss: 0.61838\n",
      "Training Progress: \tEpoch 48 [7680/8883 (86.33%)]\t\tLoss: 0.70508\n",
      "Training Progress: \tEpoch 48 [8000/8883 (89.93%)]\t\tLoss: 0.61470\n",
      "Training Progress: \tEpoch 48 [8320/8883 (93.53%)]\t\tLoss: 0.97633\n",
      "Training Progress: \tEpoch 48 [8640/8883 (97.12%)]\t\tLoss: 0.43543\n",
      "\tTrain loss: 0.01799, Accuracy: 6555/8883 (73.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1325/1692 (78.00%)\n",
      "\tTest loss: 0.00139, Accuracy: 665/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/8883 (0.00%)]\t\tLoss: 0.53484\n",
      "Training Progress: \tEpoch 49 [320/8883 (3.60%)]\t\tLoss: 0.67087\n",
      "Training Progress: \tEpoch 49 [640/8883 (7.19%)]\t\tLoss: 0.51427\n",
      "Training Progress: \tEpoch 49 [960/8883 (10.79%)]\t\tLoss: 0.49032\n",
      "Training Progress: \tEpoch 49 [1280/8883 (14.39%)]\t\tLoss: 0.57334\n",
      "Training Progress: \tEpoch 49 [1600/8883 (17.99%)]\t\tLoss: 0.77261\n",
      "Training Progress: \tEpoch 49 [1920/8883 (21.58%)]\t\tLoss: 0.60928\n",
      "Training Progress: \tEpoch 49 [2240/8883 (25.18%)]\t\tLoss: 0.53488\n",
      "Training Progress: \tEpoch 49 [2560/8883 (28.78%)]\t\tLoss: 0.59118\n",
      "Training Progress: \tEpoch 49 [2880/8883 (32.37%)]\t\tLoss: 0.50314\n",
      "Training Progress: \tEpoch 49 [3200/8883 (35.97%)]\t\tLoss: 0.86275\n",
      "Training Progress: \tEpoch 49 [3520/8883 (39.57%)]\t\tLoss: 0.72985\n",
      "Training Progress: \tEpoch 49 [3840/8883 (43.17%)]\t\tLoss: 0.43684\n",
      "Training Progress: \tEpoch 49 [4160/8883 (46.76%)]\t\tLoss: 0.55979\n",
      "Training Progress: \tEpoch 49 [4480/8883 (50.36%)]\t\tLoss: 0.56475\n",
      "Training Progress: \tEpoch 49 [4800/8883 (53.96%)]\t\tLoss: 0.54681\n",
      "Training Progress: \tEpoch 49 [5120/8883 (57.55%)]\t\tLoss: 0.81171\n",
      "Training Progress: \tEpoch 49 [5440/8883 (61.15%)]\t\tLoss: 0.58039\n",
      "Training Progress: \tEpoch 49 [5760/8883 (64.75%)]\t\tLoss: 0.96462\n",
      "Training Progress: \tEpoch 49 [6080/8883 (68.35%)]\t\tLoss: 0.74453\n",
      "Training Progress: \tEpoch 49 [6400/8883 (71.94%)]\t\tLoss: 0.85348\n",
      "Training Progress: \tEpoch 49 [6720/8883 (75.54%)]\t\tLoss: 0.65634\n",
      "Training Progress: \tEpoch 49 [7040/8883 (79.14%)]\t\tLoss: 0.47107\n",
      "Training Progress: \tEpoch 49 [7360/8883 (82.73%)]\t\tLoss: 0.62370\n",
      "Training Progress: \tEpoch 49 [7680/8883 (86.33%)]\t\tLoss: 0.50154\n",
      "Training Progress: \tEpoch 49 [8000/8883 (89.93%)]\t\tLoss: 0.66664\n",
      "Training Progress: \tEpoch 49 [8320/8883 (93.53%)]\t\tLoss: 0.60105\n",
      "Training Progress: \tEpoch 49 [8640/8883 (97.12%)]\t\tLoss: 0.61063\n",
      "\tTrain loss: 0.01764, Accuracy: 6548/8883 (73.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1327/1692 (78.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 697/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/8883 (0.00%)]\t\tLoss: 0.53497\n",
      "Training Progress: \tEpoch 50 [320/8883 (3.60%)]\t\tLoss: 0.67326\n",
      "Training Progress: \tEpoch 50 [640/8883 (7.19%)]\t\tLoss: 0.59306\n",
      "Training Progress: \tEpoch 50 [960/8883 (10.79%)]\t\tLoss: 0.48458\n",
      "Training Progress: \tEpoch 50 [1280/8883 (14.39%)]\t\tLoss: 0.59240\n",
      "Training Progress: \tEpoch 50 [1600/8883 (17.99%)]\t\tLoss: 0.76035\n",
      "Training Progress: \tEpoch 50 [1920/8883 (21.58%)]\t\tLoss: 0.84099\n",
      "Training Progress: \tEpoch 50 [2240/8883 (25.18%)]\t\tLoss: 0.57731\n",
      "Training Progress: \tEpoch 50 [2560/8883 (28.78%)]\t\tLoss: 0.52063\n",
      "Training Progress: \tEpoch 50 [2880/8883 (32.37%)]\t\tLoss: 0.64430\n",
      "Training Progress: \tEpoch 50 [3200/8883 (35.97%)]\t\tLoss: 0.89059\n",
      "Training Progress: \tEpoch 50 [3520/8883 (39.57%)]\t\tLoss: 0.81027\n",
      "Training Progress: \tEpoch 50 [3840/8883 (43.17%)]\t\tLoss: 0.43257\n",
      "Training Progress: \tEpoch 50 [4160/8883 (46.76%)]\t\tLoss: 0.61066\n",
      "Training Progress: \tEpoch 50 [4480/8883 (50.36%)]\t\tLoss: 0.71636\n",
      "Training Progress: \tEpoch 50 [4800/8883 (53.96%)]\t\tLoss: 0.35418\n",
      "Training Progress: \tEpoch 50 [5120/8883 (57.55%)]\t\tLoss: 0.56520\n",
      "Training Progress: \tEpoch 50 [5440/8883 (61.15%)]\t\tLoss: 0.79688\n",
      "Training Progress: \tEpoch 50 [5760/8883 (64.75%)]\t\tLoss: 0.74010\n",
      "Training Progress: \tEpoch 50 [6080/8883 (68.35%)]\t\tLoss: 0.49499\n",
      "Training Progress: \tEpoch 50 [6400/8883 (71.94%)]\t\tLoss: 0.70352\n",
      "Training Progress: \tEpoch 50 [6720/8883 (75.54%)]\t\tLoss: 0.65690\n",
      "Training Progress: \tEpoch 50 [7040/8883 (79.14%)]\t\tLoss: 0.49151\n",
      "Training Progress: \tEpoch 50 [7360/8883 (82.73%)]\t\tLoss: 0.62831\n",
      "Training Progress: \tEpoch 50 [7680/8883 (86.33%)]\t\tLoss: 0.54412\n",
      "Training Progress: \tEpoch 50 [8000/8883 (89.93%)]\t\tLoss: 0.70583\n",
      "Training Progress: \tEpoch 50 [8320/8883 (93.53%)]\t\tLoss: 0.77454\n",
      "Training Progress: \tEpoch 50 [8640/8883 (97.12%)]\t\tLoss: 0.60464\n",
      "\tTrain loss: 0.01553, Accuracy: 6779/8883 (76.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1386/1692 (81.00%)\n",
      "\tTest loss: 0.00133, Accuracy: 744/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/8883 (0.00%)]\t\tLoss: 0.43020\n",
      "Training Progress: \tEpoch 51 [320/8883 (3.60%)]\t\tLoss: 0.72053\n",
      "Training Progress: \tEpoch 51 [640/8883 (7.19%)]\t\tLoss: 0.66929\n",
      "Training Progress: \tEpoch 51 [960/8883 (10.79%)]\t\tLoss: 0.59292\n",
      "Training Progress: \tEpoch 51 [1280/8883 (14.39%)]\t\tLoss: 0.56964\n",
      "Training Progress: \tEpoch 51 [1600/8883 (17.99%)]\t\tLoss: 0.87408\n",
      "Training Progress: \tEpoch 51 [1920/8883 (21.58%)]\t\tLoss: 0.53173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 51 [2240/8883 (25.18%)]\t\tLoss: 0.63631\n",
      "Training Progress: \tEpoch 51 [2560/8883 (28.78%)]\t\tLoss: 0.45060\n",
      "Training Progress: \tEpoch 51 [2880/8883 (32.37%)]\t\tLoss: 0.54081\n",
      "Training Progress: \tEpoch 51 [3200/8883 (35.97%)]\t\tLoss: 0.82405\n",
      "Training Progress: \tEpoch 51 [3520/8883 (39.57%)]\t\tLoss: 0.80840\n",
      "Training Progress: \tEpoch 51 [3840/8883 (43.17%)]\t\tLoss: 0.46911\n",
      "Training Progress: \tEpoch 51 [4160/8883 (46.76%)]\t\tLoss: 0.79797\n",
      "Training Progress: \tEpoch 51 [4480/8883 (50.36%)]\t\tLoss: 0.43851\n",
      "Training Progress: \tEpoch 51 [4800/8883 (53.96%)]\t\tLoss: 0.50866\n",
      "Training Progress: \tEpoch 51 [5120/8883 (57.55%)]\t\tLoss: 0.59169\n",
      "Training Progress: \tEpoch 51 [5440/8883 (61.15%)]\t\tLoss: 0.62700\n",
      "Training Progress: \tEpoch 51 [5760/8883 (64.75%)]\t\tLoss: 0.69559\n",
      "Training Progress: \tEpoch 51 [6080/8883 (68.35%)]\t\tLoss: 0.57989\n",
      "Training Progress: \tEpoch 51 [6400/8883 (71.94%)]\t\tLoss: 0.69049\n",
      "Training Progress: \tEpoch 51 [6720/8883 (75.54%)]\t\tLoss: 0.64002\n",
      "Training Progress: \tEpoch 51 [7040/8883 (79.14%)]\t\tLoss: 0.56513\n",
      "Training Progress: \tEpoch 51 [7360/8883 (82.73%)]\t\tLoss: 0.69971\n",
      "Training Progress: \tEpoch 51 [7680/8883 (86.33%)]\t\tLoss: 0.53809\n",
      "Training Progress: \tEpoch 51 [8000/8883 (89.93%)]\t\tLoss: 0.53423\n",
      "Training Progress: \tEpoch 51 [8320/8883 (93.53%)]\t\tLoss: 0.83688\n",
      "Training Progress: \tEpoch 51 [8640/8883 (97.12%)]\t\tLoss: 0.58034\n",
      "\tTrain loss: 0.01571, Accuracy: 6775/8883 (76.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1380/1692 (81.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 717/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/8883 (0.00%)]\t\tLoss: 0.50830\n",
      "Training Progress: \tEpoch 52 [320/8883 (3.60%)]\t\tLoss: 0.52281\n",
      "Training Progress: \tEpoch 52 [640/8883 (7.19%)]\t\tLoss: 0.63069\n",
      "Training Progress: \tEpoch 52 [960/8883 (10.79%)]\t\tLoss: 0.55874\n",
      "Training Progress: \tEpoch 52 [1280/8883 (14.39%)]\t\tLoss: 0.87748\n",
      "Training Progress: \tEpoch 52 [1600/8883 (17.99%)]\t\tLoss: 0.69199\n",
      "Training Progress: \tEpoch 52 [1920/8883 (21.58%)]\t\tLoss: 0.58398\n",
      "Training Progress: \tEpoch 52 [2240/8883 (25.18%)]\t\tLoss: 0.55964\n",
      "Training Progress: \tEpoch 52 [2560/8883 (28.78%)]\t\tLoss: 0.70092\n",
      "Training Progress: \tEpoch 52 [2880/8883 (32.37%)]\t\tLoss: 0.53767\n",
      "Training Progress: \tEpoch 52 [3200/8883 (35.97%)]\t\tLoss: 0.81710\n",
      "Training Progress: \tEpoch 52 [3520/8883 (39.57%)]\t\tLoss: 0.51524\n",
      "Training Progress: \tEpoch 52 [3840/8883 (43.17%)]\t\tLoss: 0.38442\n",
      "Training Progress: \tEpoch 52 [4160/8883 (46.76%)]\t\tLoss: 0.49901\n",
      "Training Progress: \tEpoch 52 [4480/8883 (50.36%)]\t\tLoss: 0.48783\n",
      "Training Progress: \tEpoch 52 [4800/8883 (53.96%)]\t\tLoss: 0.38356\n",
      "Training Progress: \tEpoch 52 [5120/8883 (57.55%)]\t\tLoss: 0.69113\n",
      "Training Progress: \tEpoch 52 [5440/8883 (61.15%)]\t\tLoss: 0.68542\n",
      "Training Progress: \tEpoch 52 [5760/8883 (64.75%)]\t\tLoss: 1.17105\n",
      "Training Progress: \tEpoch 52 [6080/8883 (68.35%)]\t\tLoss: 0.57054\n",
      "Training Progress: \tEpoch 52 [6400/8883 (71.94%)]\t\tLoss: 0.73314\n",
      "Training Progress: \tEpoch 52 [6720/8883 (75.54%)]\t\tLoss: 0.74586\n",
      "Training Progress: \tEpoch 52 [7040/8883 (79.14%)]\t\tLoss: 0.43058\n",
      "Training Progress: \tEpoch 52 [7360/8883 (82.73%)]\t\tLoss: 0.62382\n",
      "Training Progress: \tEpoch 52 [7680/8883 (86.33%)]\t\tLoss: 0.62405\n",
      "Training Progress: \tEpoch 52 [8000/8883 (89.93%)]\t\tLoss: 0.57533\n",
      "Training Progress: \tEpoch 52 [8320/8883 (93.53%)]\t\tLoss: 0.61030\n",
      "Training Progress: \tEpoch 52 [8640/8883 (97.12%)]\t\tLoss: 0.54665\n",
      "\tTrain loss: 0.01609, Accuracy: 6739/8883 (75.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1376/1692 (81.00%)\n",
      "\tTest loss: 0.00143, Accuracy: 686/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/8883 (0.00%)]\t\tLoss: 0.65937\n",
      "Training Progress: \tEpoch 53 [320/8883 (3.60%)]\t\tLoss: 0.56448\n",
      "Training Progress: \tEpoch 53 [640/8883 (7.19%)]\t\tLoss: 0.67986\n",
      "Training Progress: \tEpoch 53 [960/8883 (10.79%)]\t\tLoss: 0.45131\n",
      "Training Progress: \tEpoch 53 [1280/8883 (14.39%)]\t\tLoss: 0.55731\n",
      "Training Progress: \tEpoch 53 [1600/8883 (17.99%)]\t\tLoss: 0.79782\n",
      "Training Progress: \tEpoch 53 [1920/8883 (21.58%)]\t\tLoss: 0.55085\n",
      "Training Progress: \tEpoch 53 [2240/8883 (25.18%)]\t\tLoss: 0.66832\n",
      "Training Progress: \tEpoch 53 [2560/8883 (28.78%)]\t\tLoss: 0.48403\n",
      "Training Progress: \tEpoch 53 [2880/8883 (32.37%)]\t\tLoss: 0.56765\n",
      "Training Progress: \tEpoch 53 [3200/8883 (35.97%)]\t\tLoss: 0.76766\n",
      "Training Progress: \tEpoch 53 [3520/8883 (39.57%)]\t\tLoss: 1.20606\n",
      "Training Progress: \tEpoch 53 [3840/8883 (43.17%)]\t\tLoss: 0.37966\n",
      "Training Progress: \tEpoch 53 [4160/8883 (46.76%)]\t\tLoss: 0.74099\n",
      "Training Progress: \tEpoch 53 [4480/8883 (50.36%)]\t\tLoss: 0.55756\n",
      "Training Progress: \tEpoch 53 [4800/8883 (53.96%)]\t\tLoss: 0.57291\n",
      "Training Progress: \tEpoch 53 [5120/8883 (57.55%)]\t\tLoss: 0.75086\n",
      "Training Progress: \tEpoch 53 [5440/8883 (61.15%)]\t\tLoss: 0.83031\n",
      "Training Progress: \tEpoch 53 [5760/8883 (64.75%)]\t\tLoss: 0.94124\n",
      "Training Progress: \tEpoch 53 [6080/8883 (68.35%)]\t\tLoss: 0.67393\n",
      "Training Progress: \tEpoch 53 [6400/8883 (71.94%)]\t\tLoss: 0.66000\n",
      "Training Progress: \tEpoch 53 [6720/8883 (75.54%)]\t\tLoss: 0.55872\n",
      "Training Progress: \tEpoch 53 [7040/8883 (79.14%)]\t\tLoss: 0.43969\n",
      "Training Progress: \tEpoch 53 [7360/8883 (82.73%)]\t\tLoss: 0.91547\n",
      "Training Progress: \tEpoch 53 [7680/8883 (86.33%)]\t\tLoss: 0.53049\n",
      "Training Progress: \tEpoch 53 [8000/8883 (89.93%)]\t\tLoss: 0.84845\n",
      "Training Progress: \tEpoch 53 [8320/8883 (93.53%)]\t\tLoss: 0.61107\n",
      "Training Progress: \tEpoch 53 [8640/8883 (97.12%)]\t\tLoss: 0.54256\n",
      "\tTrain loss: 0.01500, Accuracy: 6855/8883 (77.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1387/1692 (81.00%)\n",
      "\tTest loss: 0.00141, Accuracy: 701/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/8883 (0.00%)]\t\tLoss: 0.41266\n",
      "Training Progress: \tEpoch 54 [320/8883 (3.60%)]\t\tLoss: 0.62182\n",
      "Training Progress: \tEpoch 54 [640/8883 (7.19%)]\t\tLoss: 0.50488\n",
      "Training Progress: \tEpoch 54 [960/8883 (10.79%)]\t\tLoss: 0.49354\n",
      "Training Progress: \tEpoch 54 [1280/8883 (14.39%)]\t\tLoss: 0.59180\n",
      "Training Progress: \tEpoch 54 [1600/8883 (17.99%)]\t\tLoss: 0.70065\n",
      "Training Progress: \tEpoch 54 [1920/8883 (21.58%)]\t\tLoss: 0.80436\n",
      "Training Progress: \tEpoch 54 [2240/8883 (25.18%)]\t\tLoss: 0.62623\n",
      "Training Progress: \tEpoch 54 [2560/8883 (28.78%)]\t\tLoss: 0.48320\n",
      "Training Progress: \tEpoch 54 [2880/8883 (32.37%)]\t\tLoss: 0.48736\n",
      "Training Progress: \tEpoch 54 [3200/8883 (35.97%)]\t\tLoss: 0.97240\n",
      "Training Progress: \tEpoch 54 [3520/8883 (39.57%)]\t\tLoss: 0.60204\n",
      "Training Progress: \tEpoch 54 [3840/8883 (43.17%)]\t\tLoss: 0.30107\n",
      "Training Progress: \tEpoch 54 [4160/8883 (46.76%)]\t\tLoss: 0.56892\n",
      "Training Progress: \tEpoch 54 [4480/8883 (50.36%)]\t\tLoss: 0.36879\n",
      "Training Progress: \tEpoch 54 [4800/8883 (53.96%)]\t\tLoss: 0.51822\n",
      "Training Progress: \tEpoch 54 [5120/8883 (57.55%)]\t\tLoss: 0.72890\n",
      "Training Progress: \tEpoch 54 [5440/8883 (61.15%)]\t\tLoss: 0.73516\n",
      "Training Progress: \tEpoch 54 [5760/8883 (64.75%)]\t\tLoss: 0.74324\n",
      "Training Progress: \tEpoch 54 [6080/8883 (68.35%)]\t\tLoss: 0.68733\n",
      "Training Progress: \tEpoch 54 [6400/8883 (71.94%)]\t\tLoss: 0.63574\n",
      "Training Progress: \tEpoch 54 [6720/8883 (75.54%)]\t\tLoss: 0.66147\n",
      "Training Progress: \tEpoch 54 [7040/8883 (79.14%)]\t\tLoss: 0.55928\n",
      "Training Progress: \tEpoch 54 [7360/8883 (82.73%)]\t\tLoss: 0.63743\n",
      "Training Progress: \tEpoch 54 [7680/8883 (86.33%)]\t\tLoss: 0.51770\n",
      "Training Progress: \tEpoch 54 [8000/8883 (89.93%)]\t\tLoss: 0.48672\n",
      "Training Progress: \tEpoch 54 [8320/8883 (93.53%)]\t\tLoss: 0.64910\n",
      "Training Progress: \tEpoch 54 [8640/8883 (97.12%)]\t\tLoss: 0.72881\n",
      "\tTrain loss: 0.01554, Accuracy: 6811/8883 (76.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1372/1692 (81.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 734/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/8883 (0.00%)]\t\tLoss: 0.38756\n",
      "Training Progress: \tEpoch 55 [320/8883 (3.60%)]\t\tLoss: 0.71023\n",
      "Training Progress: \tEpoch 55 [640/8883 (7.19%)]\t\tLoss: 0.61771\n",
      "Training Progress: \tEpoch 55 [960/8883 (10.79%)]\t\tLoss: 0.51678\n",
      "Training Progress: \tEpoch 55 [1280/8883 (14.39%)]\t\tLoss: 0.65168\n",
      "Training Progress: \tEpoch 55 [1600/8883 (17.99%)]\t\tLoss: 0.81893\n",
      "Training Progress: \tEpoch 55 [1920/8883 (21.58%)]\t\tLoss: 0.55435\n",
      "Training Progress: \tEpoch 55 [2240/8883 (25.18%)]\t\tLoss: 0.51750\n",
      "Training Progress: \tEpoch 55 [2560/8883 (28.78%)]\t\tLoss: 0.52251\n",
      "Training Progress: \tEpoch 55 [2880/8883 (32.37%)]\t\tLoss: 0.55600\n",
      "Training Progress: \tEpoch 55 [3200/8883 (35.97%)]\t\tLoss: 0.72816\n",
      "Training Progress: \tEpoch 55 [3520/8883 (39.57%)]\t\tLoss: 1.12082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 55 [3840/8883 (43.17%)]\t\tLoss: 0.34048\n",
      "Training Progress: \tEpoch 55 [4160/8883 (46.76%)]\t\tLoss: 0.52565\n",
      "Training Progress: \tEpoch 55 [4480/8883 (50.36%)]\t\tLoss: 0.58428\n",
      "Training Progress: \tEpoch 55 [4800/8883 (53.96%)]\t\tLoss: 0.40932\n",
      "Training Progress: \tEpoch 55 [5120/8883 (57.55%)]\t\tLoss: 0.62875\n",
      "Training Progress: \tEpoch 55 [5440/8883 (61.15%)]\t\tLoss: 0.74290\n",
      "Training Progress: \tEpoch 55 [5760/8883 (64.75%)]\t\tLoss: 0.89364\n",
      "Training Progress: \tEpoch 55 [6080/8883 (68.35%)]\t\tLoss: 0.59954\n",
      "Training Progress: \tEpoch 55 [6400/8883 (71.94%)]\t\tLoss: 0.69764\n",
      "Training Progress: \tEpoch 55 [6720/8883 (75.54%)]\t\tLoss: 0.62084\n",
      "Training Progress: \tEpoch 55 [7040/8883 (79.14%)]\t\tLoss: 0.44726\n",
      "Training Progress: \tEpoch 55 [7360/8883 (82.73%)]\t\tLoss: 0.64108\n",
      "Training Progress: \tEpoch 55 [7680/8883 (86.33%)]\t\tLoss: 0.81943\n",
      "Training Progress: \tEpoch 55 [8000/8883 (89.93%)]\t\tLoss: 0.58667\n",
      "Training Progress: \tEpoch 55 [8320/8883 (93.53%)]\t\tLoss: 0.79776\n",
      "Training Progress: \tEpoch 55 [8640/8883 (97.12%)]\t\tLoss: 0.55179\n",
      "\tTrain loss: 0.01754, Accuracy: 6607/8883 (74.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1343/1692 (79.00%)\n",
      "\tTest loss: 0.00152, Accuracy: 701/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/8883 (0.00%)]\t\tLoss: 0.43009\n",
      "Training Progress: \tEpoch 56 [320/8883 (3.60%)]\t\tLoss: 0.53769\n",
      "Training Progress: \tEpoch 56 [640/8883 (7.19%)]\t\tLoss: 0.48279\n",
      "Training Progress: \tEpoch 56 [960/8883 (10.79%)]\t\tLoss: 0.59144\n",
      "Training Progress: \tEpoch 56 [1280/8883 (14.39%)]\t\tLoss: 0.52486\n",
      "Training Progress: \tEpoch 56 [1600/8883 (17.99%)]\t\tLoss: 0.82570\n",
      "Training Progress: \tEpoch 56 [1920/8883 (21.58%)]\t\tLoss: 0.65249\n",
      "Training Progress: \tEpoch 56 [2240/8883 (25.18%)]\t\tLoss: 0.43456\n",
      "Training Progress: \tEpoch 56 [2560/8883 (28.78%)]\t\tLoss: 0.97266\n",
      "Training Progress: \tEpoch 56 [2880/8883 (32.37%)]\t\tLoss: 0.44878\n",
      "Training Progress: \tEpoch 56 [3200/8883 (35.97%)]\t\tLoss: 0.94742\n",
      "Training Progress: \tEpoch 56 [3520/8883 (39.57%)]\t\tLoss: 0.72045\n",
      "Training Progress: \tEpoch 56 [3840/8883 (43.17%)]\t\tLoss: 0.51197\n",
      "Training Progress: \tEpoch 56 [4160/8883 (46.76%)]\t\tLoss: 0.55737\n",
      "Training Progress: \tEpoch 56 [4480/8883 (50.36%)]\t\tLoss: 0.81720\n",
      "Training Progress: \tEpoch 56 [4800/8883 (53.96%)]\t\tLoss: 0.37824\n",
      "Training Progress: \tEpoch 56 [5120/8883 (57.55%)]\t\tLoss: 0.66021\n",
      "Training Progress: \tEpoch 56 [5440/8883 (61.15%)]\t\tLoss: 0.59259\n",
      "Training Progress: \tEpoch 56 [5760/8883 (64.75%)]\t\tLoss: 0.89767\n",
      "Training Progress: \tEpoch 56 [6080/8883 (68.35%)]\t\tLoss: 0.71894\n",
      "Training Progress: \tEpoch 56 [6400/8883 (71.94%)]\t\tLoss: 0.73633\n",
      "Training Progress: \tEpoch 56 [6720/8883 (75.54%)]\t\tLoss: 0.55868\n",
      "Training Progress: \tEpoch 56 [7040/8883 (79.14%)]\t\tLoss: 0.60790\n",
      "Training Progress: \tEpoch 56 [7360/8883 (82.73%)]\t\tLoss: 0.53372\n",
      "Training Progress: \tEpoch 56 [7680/8883 (86.33%)]\t\tLoss: 0.50501\n",
      "Training Progress: \tEpoch 56 [8000/8883 (89.93%)]\t\tLoss: 0.50957\n",
      "Training Progress: \tEpoch 56 [8320/8883 (93.53%)]\t\tLoss: 0.73384\n",
      "Training Progress: \tEpoch 56 [8640/8883 (97.12%)]\t\tLoss: 0.52198\n",
      "\tTrain loss: 0.01604, Accuracy: 6713/8883 (75.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1358/1692 (80.00%)\n",
      "\tTest loss: 0.00149, Accuracy: 705/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/8883 (0.00%)]\t\tLoss: 0.43671\n",
      "Training Progress: \tEpoch 57 [320/8883 (3.60%)]\t\tLoss: 0.60437\n",
      "Training Progress: \tEpoch 57 [640/8883 (7.19%)]\t\tLoss: 0.61726\n",
      "Training Progress: \tEpoch 57 [960/8883 (10.79%)]\t\tLoss: 0.41578\n",
      "Training Progress: \tEpoch 57 [1280/8883 (14.39%)]\t\tLoss: 0.61634\n",
      "Training Progress: \tEpoch 57 [1600/8883 (17.99%)]\t\tLoss: 0.66865\n",
      "Training Progress: \tEpoch 57 [1920/8883 (21.58%)]\t\tLoss: 0.46378\n",
      "Training Progress: \tEpoch 57 [2240/8883 (25.18%)]\t\tLoss: 0.44060\n",
      "Training Progress: \tEpoch 57 [2560/8883 (28.78%)]\t\tLoss: 0.46761\n",
      "Training Progress: \tEpoch 57 [2880/8883 (32.37%)]\t\tLoss: 0.54293\n",
      "Training Progress: \tEpoch 57 [3200/8883 (35.97%)]\t\tLoss: 0.74496\n",
      "Training Progress: \tEpoch 57 [3520/8883 (39.57%)]\t\tLoss: 0.75237\n",
      "Training Progress: \tEpoch 57 [3840/8883 (43.17%)]\t\tLoss: 0.39672\n",
      "Training Progress: \tEpoch 57 [4160/8883 (46.76%)]\t\tLoss: 0.57684\n",
      "Training Progress: \tEpoch 57 [4480/8883 (50.36%)]\t\tLoss: 0.52751\n",
      "Training Progress: \tEpoch 57 [4800/8883 (53.96%)]\t\tLoss: 0.47298\n",
      "Training Progress: \tEpoch 57 [5120/8883 (57.55%)]\t\tLoss: 0.59948\n",
      "Training Progress: \tEpoch 57 [5440/8883 (61.15%)]\t\tLoss: 0.66689\n",
      "Training Progress: \tEpoch 57 [5760/8883 (64.75%)]\t\tLoss: 0.63991\n",
      "Training Progress: \tEpoch 57 [6080/8883 (68.35%)]\t\tLoss: 0.65808\n",
      "Training Progress: \tEpoch 57 [6400/8883 (71.94%)]\t\tLoss: 0.86153\n",
      "Training Progress: \tEpoch 57 [6720/8883 (75.54%)]\t\tLoss: 0.57147\n",
      "Training Progress: \tEpoch 57 [7040/8883 (79.14%)]\t\tLoss: 0.97239\n",
      "Training Progress: \tEpoch 57 [7360/8883 (82.73%)]\t\tLoss: 0.65557\n",
      "Training Progress: \tEpoch 57 [7680/8883 (86.33%)]\t\tLoss: 0.55666\n",
      "Training Progress: \tEpoch 57 [8000/8883 (89.93%)]\t\tLoss: 0.58285\n",
      "Training Progress: \tEpoch 57 [8320/8883 (93.53%)]\t\tLoss: 0.67317\n",
      "Training Progress: \tEpoch 57 [8640/8883 (97.12%)]\t\tLoss: 0.37744\n",
      "\tTrain loss: 0.01660, Accuracy: 6686/8883 (75.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1352/1692 (79.00%)\n",
      "\tTest loss: 0.00152, Accuracy: 691/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/8883 (0.00%)]\t\tLoss: 0.42704\n",
      "Training Progress: \tEpoch 58 [320/8883 (3.60%)]\t\tLoss: 0.63281\n",
      "Training Progress: \tEpoch 58 [640/8883 (7.19%)]\t\tLoss: 0.46990\n",
      "Training Progress: \tEpoch 58 [960/8883 (10.79%)]\t\tLoss: 0.38011\n",
      "Training Progress: \tEpoch 58 [1280/8883 (14.39%)]\t\tLoss: 0.56730\n",
      "Training Progress: \tEpoch 58 [1600/8883 (17.99%)]\t\tLoss: 0.92045\n",
      "Training Progress: \tEpoch 58 [1920/8883 (21.58%)]\t\tLoss: 0.46854\n",
      "Training Progress: \tEpoch 58 [2240/8883 (25.18%)]\t\tLoss: 0.49558\n",
      "Training Progress: \tEpoch 58 [2560/8883 (28.78%)]\t\tLoss: 0.45789\n",
      "Training Progress: \tEpoch 58 [2880/8883 (32.37%)]\t\tLoss: 0.61683\n",
      "Training Progress: \tEpoch 58 [3200/8883 (35.97%)]\t\tLoss: 0.72386\n",
      "Training Progress: \tEpoch 58 [3520/8883 (39.57%)]\t\tLoss: 0.58460\n",
      "Training Progress: \tEpoch 58 [3840/8883 (43.17%)]\t\tLoss: 0.31933\n",
      "Training Progress: \tEpoch 58 [4160/8883 (46.76%)]\t\tLoss: 0.50364\n",
      "Training Progress: \tEpoch 58 [4480/8883 (50.36%)]\t\tLoss: 0.49900\n",
      "Training Progress: \tEpoch 58 [4800/8883 (53.96%)]\t\tLoss: 0.45218\n",
      "Training Progress: \tEpoch 58 [5120/8883 (57.55%)]\t\tLoss: 0.63036\n",
      "Training Progress: \tEpoch 58 [5440/8883 (61.15%)]\t\tLoss: 0.55612\n",
      "Training Progress: \tEpoch 58 [5760/8883 (64.75%)]\t\tLoss: 0.74804\n",
      "Training Progress: \tEpoch 58 [6080/8883 (68.35%)]\t\tLoss: 0.56084\n",
      "Training Progress: \tEpoch 58 [6400/8883 (71.94%)]\t\tLoss: 0.65385\n",
      "Training Progress: \tEpoch 58 [6720/8883 (75.54%)]\t\tLoss: 0.57735\n",
      "Training Progress: \tEpoch 58 [7040/8883 (79.14%)]\t\tLoss: 0.45680\n",
      "Training Progress: \tEpoch 58 [7360/8883 (82.73%)]\t\tLoss: 0.74498\n",
      "Training Progress: \tEpoch 58 [7680/8883 (86.33%)]\t\tLoss: 0.56534\n",
      "Training Progress: \tEpoch 58 [8000/8883 (89.93%)]\t\tLoss: 0.64004\n",
      "Training Progress: \tEpoch 58 [8320/8883 (93.53%)]\t\tLoss: 0.60762\n",
      "Training Progress: \tEpoch 58 [8640/8883 (97.12%)]\t\tLoss: 0.42483\n",
      "\tTrain loss: 0.01562, Accuracy: 6785/8883 (76.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1391/1692 (82.00%)\n",
      "\tTest loss: 0.00143, Accuracy: 720/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/8883 (0.00%)]\t\tLoss: 0.44302\n",
      "Training Progress: \tEpoch 59 [320/8883 (3.60%)]\t\tLoss: 0.64567\n",
      "Training Progress: \tEpoch 59 [640/8883 (7.19%)]\t\tLoss: 0.64232\n",
      "Training Progress: \tEpoch 59 [960/8883 (10.79%)]\t\tLoss: 0.54474\n",
      "Training Progress: \tEpoch 59 [1280/8883 (14.39%)]\t\tLoss: 0.54071\n",
      "Training Progress: \tEpoch 59 [1600/8883 (17.99%)]\t\tLoss: 0.89345\n",
      "Training Progress: \tEpoch 59 [1920/8883 (21.58%)]\t\tLoss: 0.54146\n",
      "Training Progress: \tEpoch 59 [2240/8883 (25.18%)]\t\tLoss: 0.54105\n",
      "Training Progress: \tEpoch 59 [2560/8883 (28.78%)]\t\tLoss: 0.51543\n",
      "Training Progress: \tEpoch 59 [2880/8883 (32.37%)]\t\tLoss: 0.77097\n",
      "Training Progress: \tEpoch 59 [3200/8883 (35.97%)]\t\tLoss: 0.69237\n",
      "Training Progress: \tEpoch 59 [3520/8883 (39.57%)]\t\tLoss: 0.79944\n",
      "Training Progress: \tEpoch 59 [3840/8883 (43.17%)]\t\tLoss: 0.53850\n",
      "Training Progress: \tEpoch 59 [4160/8883 (46.76%)]\t\tLoss: 0.54383\n",
      "Training Progress: \tEpoch 59 [4480/8883 (50.36%)]\t\tLoss: 0.59428\n",
      "Training Progress: \tEpoch 59 [4800/8883 (53.96%)]\t\tLoss: 0.36706\n",
      "Training Progress: \tEpoch 59 [5120/8883 (57.55%)]\t\tLoss: 0.72061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 59 [5440/8883 (61.15%)]\t\tLoss: 0.57695\n",
      "Training Progress: \tEpoch 59 [5760/8883 (64.75%)]\t\tLoss: 0.93018\n",
      "Training Progress: \tEpoch 59 [6080/8883 (68.35%)]\t\tLoss: 0.55044\n",
      "Training Progress: \tEpoch 59 [6400/8883 (71.94%)]\t\tLoss: 0.67014\n",
      "Training Progress: \tEpoch 59 [6720/8883 (75.54%)]\t\tLoss: 0.69825\n",
      "Training Progress: \tEpoch 59 [7040/8883 (79.14%)]\t\tLoss: 0.38652\n",
      "Training Progress: \tEpoch 59 [7360/8883 (82.73%)]\t\tLoss: 0.73810\n",
      "Training Progress: \tEpoch 59 [7680/8883 (86.33%)]\t\tLoss: 0.64075\n",
      "Training Progress: \tEpoch 59 [8000/8883 (89.93%)]\t\tLoss: 0.60255\n",
      "Training Progress: \tEpoch 59 [8320/8883 (93.53%)]\t\tLoss: 0.65561\n",
      "Training Progress: \tEpoch 59 [8640/8883 (97.12%)]\t\tLoss: 0.40269\n",
      "\tTrain loss: 0.01559, Accuracy: 6769/8883 (76.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1385/1692 (81.00%)\n",
      "\tTest loss: 0.00146, Accuracy: 733/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/8883 (0.00%)]\t\tLoss: 0.38539\n",
      "Training Progress: \tEpoch 60 [320/8883 (3.60%)]\t\tLoss: 0.55133\n",
      "Training Progress: \tEpoch 60 [640/8883 (7.19%)]\t\tLoss: 0.47677\n",
      "Training Progress: \tEpoch 60 [960/8883 (10.79%)]\t\tLoss: 0.67049\n",
      "Training Progress: \tEpoch 60 [1280/8883 (14.39%)]\t\tLoss: 0.53231\n",
      "Training Progress: \tEpoch 60 [1600/8883 (17.99%)]\t\tLoss: 0.69449\n",
      "Training Progress: \tEpoch 60 [1920/8883 (21.58%)]\t\tLoss: 0.66200\n",
      "Training Progress: \tEpoch 60 [2240/8883 (25.18%)]\t\tLoss: 0.62584\n",
      "Training Progress: \tEpoch 60 [2560/8883 (28.78%)]\t\tLoss: 0.48198\n",
      "Training Progress: \tEpoch 60 [2880/8883 (32.37%)]\t\tLoss: 0.55346\n",
      "Training Progress: \tEpoch 60 [3200/8883 (35.97%)]\t\tLoss: 0.69093\n",
      "Training Progress: \tEpoch 60 [3520/8883 (39.57%)]\t\tLoss: 0.69351\n",
      "Training Progress: \tEpoch 60 [3840/8883 (43.17%)]\t\tLoss: 0.25973\n",
      "Training Progress: \tEpoch 60 [4160/8883 (46.76%)]\t\tLoss: 0.51884\n",
      "Training Progress: \tEpoch 60 [4480/8883 (50.36%)]\t\tLoss: 0.50803\n",
      "Training Progress: \tEpoch 60 [4800/8883 (53.96%)]\t\tLoss: 0.46155\n",
      "Training Progress: \tEpoch 60 [5120/8883 (57.55%)]\t\tLoss: 0.75905\n",
      "Training Progress: \tEpoch 60 [5440/8883 (61.15%)]\t\tLoss: 0.67949\n",
      "Training Progress: \tEpoch 60 [5760/8883 (64.75%)]\t\tLoss: 1.06445\n",
      "Training Progress: \tEpoch 60 [6080/8883 (68.35%)]\t\tLoss: 0.52870\n",
      "Training Progress: \tEpoch 60 [6400/8883 (71.94%)]\t\tLoss: 0.74846\n",
      "Training Progress: \tEpoch 60 [6720/8883 (75.54%)]\t\tLoss: 0.53159\n",
      "Training Progress: \tEpoch 60 [7040/8883 (79.14%)]\t\tLoss: 0.50709\n",
      "Training Progress: \tEpoch 60 [7360/8883 (82.73%)]\t\tLoss: 0.46256\n",
      "Training Progress: \tEpoch 60 [7680/8883 (86.33%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 60 [8000/8883 (89.93%)]\t\tLoss: 0.53114\n",
      "Training Progress: \tEpoch 60 [8320/8883 (93.53%)]\t\tLoss: 0.69745\n",
      "Training Progress: \tEpoch 60 [8640/8883 (97.12%)]\t\tLoss: 0.78755\n",
      "\tTrain loss: 0.01607, Accuracy: 6698/8883 (75.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1379/1692 (81.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 712/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/8883 (0.00%)]\t\tLoss: 0.43126\n",
      "Training Progress: \tEpoch 61 [320/8883 (3.60%)]\t\tLoss: 0.52106\n",
      "Training Progress: \tEpoch 61 [640/8883 (7.19%)]\t\tLoss: 0.56325\n",
      "Training Progress: \tEpoch 61 [960/8883 (10.79%)]\t\tLoss: 0.55982\n",
      "Training Progress: \tEpoch 61 [1280/8883 (14.39%)]\t\tLoss: 0.87068\n",
      "Training Progress: \tEpoch 61 [1600/8883 (17.99%)]\t\tLoss: 0.75995\n",
      "Training Progress: \tEpoch 61 [1920/8883 (21.58%)]\t\tLoss: 0.44469\n",
      "Training Progress: \tEpoch 61 [2240/8883 (25.18%)]\t\tLoss: 0.57285\n",
      "Training Progress: \tEpoch 61 [2560/8883 (28.78%)]\t\tLoss: 0.51220\n",
      "Training Progress: \tEpoch 61 [2880/8883 (32.37%)]\t\tLoss: 0.42452\n",
      "Training Progress: \tEpoch 61 [3200/8883 (35.97%)]\t\tLoss: 0.79516\n",
      "Training Progress: \tEpoch 61 [3520/8883 (39.57%)]\t\tLoss: 0.49778\n",
      "Training Progress: \tEpoch 61 [3840/8883 (43.17%)]\t\tLoss: 0.30479\n",
      "Training Progress: \tEpoch 61 [4160/8883 (46.76%)]\t\tLoss: 0.47383\n",
      "Training Progress: \tEpoch 61 [4480/8883 (50.36%)]\t\tLoss: 0.65369\n",
      "Training Progress: \tEpoch 61 [4800/8883 (53.96%)]\t\tLoss: 0.41446\n",
      "Training Progress: \tEpoch 61 [5120/8883 (57.55%)]\t\tLoss: 0.71532\n",
      "Training Progress: \tEpoch 61 [5440/8883 (61.15%)]\t\tLoss: 0.51557\n",
      "Training Progress: \tEpoch 61 [5760/8883 (64.75%)]\t\tLoss: 1.05505\n",
      "Training Progress: \tEpoch 61 [6080/8883 (68.35%)]\t\tLoss: 0.50306\n",
      "Training Progress: \tEpoch 61 [6400/8883 (71.94%)]\t\tLoss: 0.66668\n",
      "Training Progress: \tEpoch 61 [6720/8883 (75.54%)]\t\tLoss: 0.66908\n",
      "Training Progress: \tEpoch 61 [7040/8883 (79.14%)]\t\tLoss: 0.45447\n",
      "Training Progress: \tEpoch 61 [7360/8883 (82.73%)]\t\tLoss: 0.47505\n",
      "Training Progress: \tEpoch 61 [7680/8883 (86.33%)]\t\tLoss: 0.52512\n",
      "Training Progress: \tEpoch 61 [8000/8883 (89.93%)]\t\tLoss: 0.49956\n",
      "Training Progress: \tEpoch 61 [8320/8883 (93.53%)]\t\tLoss: 0.68813\n",
      "Training Progress: \tEpoch 61 [8640/8883 (97.12%)]\t\tLoss: 0.40103\n",
      "\tTrain loss: 0.01469, Accuracy: 6874/8883 (77.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1410/1692 (83.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 745/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/8883 (0.00%)]\t\tLoss: 0.46595\n",
      "Training Progress: \tEpoch 62 [320/8883 (3.60%)]\t\tLoss: 0.59178\n",
      "Training Progress: \tEpoch 62 [640/8883 (7.19%)]\t\tLoss: 0.47765\n",
      "Training Progress: \tEpoch 62 [960/8883 (10.79%)]\t\tLoss: 0.44466\n",
      "Training Progress: \tEpoch 62 [1280/8883 (14.39%)]\t\tLoss: 0.57951\n",
      "Training Progress: \tEpoch 62 [1600/8883 (17.99%)]\t\tLoss: 0.64852\n",
      "Training Progress: \tEpoch 62 [1920/8883 (21.58%)]\t\tLoss: 0.64592\n",
      "Training Progress: \tEpoch 62 [2240/8883 (25.18%)]\t\tLoss: 0.45602\n",
      "Training Progress: \tEpoch 62 [2560/8883 (28.78%)]\t\tLoss: 0.50472\n",
      "Training Progress: \tEpoch 62 [2880/8883 (32.37%)]\t\tLoss: 0.43989\n",
      "Training Progress: \tEpoch 62 [3200/8883 (35.97%)]\t\tLoss: 0.63958\n",
      "Training Progress: \tEpoch 62 [3520/8883 (39.57%)]\t\tLoss: 0.59629\n",
      "Training Progress: \tEpoch 62 [3840/8883 (43.17%)]\t\tLoss: 0.33072\n",
      "Training Progress: \tEpoch 62 [4160/8883 (46.76%)]\t\tLoss: 0.52808\n",
      "Training Progress: \tEpoch 62 [4480/8883 (50.36%)]\t\tLoss: 0.49275\n",
      "Training Progress: \tEpoch 62 [4800/8883 (53.96%)]\t\tLoss: 0.47061\n",
      "Training Progress: \tEpoch 62 [5120/8883 (57.55%)]\t\tLoss: 0.69394\n",
      "Training Progress: \tEpoch 62 [5440/8883 (61.15%)]\t\tLoss: 0.69637\n",
      "Training Progress: \tEpoch 62 [5760/8883 (64.75%)]\t\tLoss: 0.62132\n",
      "Training Progress: \tEpoch 62 [6080/8883 (68.35%)]\t\tLoss: 0.69199\n",
      "Training Progress: \tEpoch 62 [6400/8883 (71.94%)]\t\tLoss: 0.73370\n",
      "Training Progress: \tEpoch 62 [6720/8883 (75.54%)]\t\tLoss: 0.64371\n",
      "Training Progress: \tEpoch 62 [7040/8883 (79.14%)]\t\tLoss: 0.42572\n",
      "Training Progress: \tEpoch 62 [7360/8883 (82.73%)]\t\tLoss: 0.55174\n",
      "Training Progress: \tEpoch 62 [7680/8883 (86.33%)]\t\tLoss: 0.42854\n",
      "Training Progress: \tEpoch 62 [8000/8883 (89.93%)]\t\tLoss: 0.44171\n",
      "Training Progress: \tEpoch 62 [8320/8883 (93.53%)]\t\tLoss: 0.68538\n",
      "Training Progress: \tEpoch 62 [8640/8883 (97.12%)]\t\tLoss: 0.45786\n",
      "\tTrain loss: 0.01391, Accuracy: 6957/8883 (78.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1441/1692 (85.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 739/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/8883 (0.00%)]\t\tLoss: 0.53436\n",
      "Training Progress: \tEpoch 63 [320/8883 (3.60%)]\t\tLoss: 0.56948\n",
      "Training Progress: \tEpoch 63 [640/8883 (7.19%)]\t\tLoss: 0.46303\n",
      "Training Progress: \tEpoch 63 [960/8883 (10.79%)]\t\tLoss: 0.50375\n",
      "Training Progress: \tEpoch 63 [1280/8883 (14.39%)]\t\tLoss: 0.53090\n",
      "Training Progress: \tEpoch 63 [1600/8883 (17.99%)]\t\tLoss: 0.77474\n",
      "Training Progress: \tEpoch 63 [1920/8883 (21.58%)]\t\tLoss: 0.38810\n",
      "Training Progress: \tEpoch 63 [2240/8883 (25.18%)]\t\tLoss: 0.64868\n",
      "Training Progress: \tEpoch 63 [2560/8883 (28.78%)]\t\tLoss: 0.67573\n",
      "Training Progress: \tEpoch 63 [2880/8883 (32.37%)]\t\tLoss: 0.40826\n",
      "Training Progress: \tEpoch 63 [3200/8883 (35.97%)]\t\tLoss: 0.73168\n",
      "Training Progress: \tEpoch 63 [3520/8883 (39.57%)]\t\tLoss: 0.50737\n",
      "Training Progress: \tEpoch 63 [3840/8883 (43.17%)]\t\tLoss: 0.27536\n",
      "Training Progress: \tEpoch 63 [4160/8883 (46.76%)]\t\tLoss: 0.86904\n",
      "Training Progress: \tEpoch 63 [4480/8883 (50.36%)]\t\tLoss: 0.48812\n",
      "Training Progress: \tEpoch 63 [4800/8883 (53.96%)]\t\tLoss: 0.46915\n",
      "Training Progress: \tEpoch 63 [5120/8883 (57.55%)]\t\tLoss: 0.51561\n",
      "Training Progress: \tEpoch 63 [5440/8883 (61.15%)]\t\tLoss: 0.56155\n",
      "Training Progress: \tEpoch 63 [5760/8883 (64.75%)]\t\tLoss: 0.64735\n",
      "Training Progress: \tEpoch 63 [6080/8883 (68.35%)]\t\tLoss: 0.70804\n",
      "Training Progress: \tEpoch 63 [6400/8883 (71.94%)]\t\tLoss: 0.60976\n",
      "Training Progress: \tEpoch 63 [6720/8883 (75.54%)]\t\tLoss: 0.57139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [7040/8883 (79.14%)]\t\tLoss: 0.55449\n",
      "Training Progress: \tEpoch 63 [7360/8883 (82.73%)]\t\tLoss: 0.65206\n",
      "Training Progress: \tEpoch 63 [7680/8883 (86.33%)]\t\tLoss: 0.45063\n",
      "Training Progress: \tEpoch 63 [8000/8883 (89.93%)]\t\tLoss: 0.61532\n",
      "Training Progress: \tEpoch 63 [8320/8883 (93.53%)]\t\tLoss: 0.64342\n",
      "Training Progress: \tEpoch 63 [8640/8883 (97.12%)]\t\tLoss: 0.47800\n",
      "\tTrain loss: 0.01357, Accuracy: 6994/8883 (78.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00147, Accuracy: 717/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/8883 (0.00%)]\t\tLoss: 0.66724\n",
      "Training Progress: \tEpoch 64 [320/8883 (3.60%)]\t\tLoss: 0.55241\n",
      "Training Progress: \tEpoch 64 [640/8883 (7.19%)]\t\tLoss: 0.63290\n",
      "Training Progress: \tEpoch 64 [960/8883 (10.79%)]\t\tLoss: 0.50025\n",
      "Training Progress: \tEpoch 64 [1280/8883 (14.39%)]\t\tLoss: 0.58421\n",
      "Training Progress: \tEpoch 64 [1600/8883 (17.99%)]\t\tLoss: 1.03905\n",
      "Training Progress: \tEpoch 64 [1920/8883 (21.58%)]\t\tLoss: 0.44571\n",
      "Training Progress: \tEpoch 64 [2240/8883 (25.18%)]\t\tLoss: 0.53346\n",
      "Training Progress: \tEpoch 64 [2560/8883 (28.78%)]\t\tLoss: 0.47259\n",
      "Training Progress: \tEpoch 64 [2880/8883 (32.37%)]\t\tLoss: 0.32529\n",
      "Training Progress: \tEpoch 64 [3200/8883 (35.97%)]\t\tLoss: 0.71085\n",
      "Training Progress: \tEpoch 64 [3520/8883 (39.57%)]\t\tLoss: 0.64730\n",
      "Training Progress: \tEpoch 64 [3840/8883 (43.17%)]\t\tLoss: 0.26910\n",
      "Training Progress: \tEpoch 64 [4160/8883 (46.76%)]\t\tLoss: 0.87427\n",
      "Training Progress: \tEpoch 64 [4480/8883 (50.36%)]\t\tLoss: 0.49940\n",
      "Training Progress: \tEpoch 64 [4800/8883 (53.96%)]\t\tLoss: 0.28476\n",
      "Training Progress: \tEpoch 64 [5120/8883 (57.55%)]\t\tLoss: 0.72542\n",
      "Training Progress: \tEpoch 64 [5440/8883 (61.15%)]\t\tLoss: 0.56831\n",
      "Training Progress: \tEpoch 64 [5760/8883 (64.75%)]\t\tLoss: 0.79324\n",
      "Training Progress: \tEpoch 64 [6080/8883 (68.35%)]\t\tLoss: 0.51641\n",
      "Training Progress: \tEpoch 64 [6400/8883 (71.94%)]\t\tLoss: 0.67460\n",
      "Training Progress: \tEpoch 64 [6720/8883 (75.54%)]\t\tLoss: 0.58946\n",
      "Training Progress: \tEpoch 64 [7040/8883 (79.14%)]\t\tLoss: 0.58257\n",
      "Training Progress: \tEpoch 64 [7360/8883 (82.73%)]\t\tLoss: 0.57046\n",
      "Training Progress: \tEpoch 64 [7680/8883 (86.33%)]\t\tLoss: 0.48740\n",
      "Training Progress: \tEpoch 64 [8000/8883 (89.93%)]\t\tLoss: 0.54205\n",
      "Training Progress: \tEpoch 64 [8320/8883 (93.53%)]\t\tLoss: 0.60718\n",
      "Training Progress: \tEpoch 64 [8640/8883 (97.12%)]\t\tLoss: 0.42036\n",
      "\tTrain loss: 0.01530, Accuracy: 6814/8883 (76.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1406/1692 (83.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 716/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/8883 (0.00%)]\t\tLoss: 0.53473\n",
      "Training Progress: \tEpoch 65 [320/8883 (3.60%)]\t\tLoss: 0.53851\n",
      "Training Progress: \tEpoch 65 [640/8883 (7.19%)]\t\tLoss: 0.44625\n",
      "Training Progress: \tEpoch 65 [960/8883 (10.79%)]\t\tLoss: 0.55367\n",
      "Training Progress: \tEpoch 65 [1280/8883 (14.39%)]\t\tLoss: 0.50168\n",
      "Training Progress: \tEpoch 65 [1600/8883 (17.99%)]\t\tLoss: 0.75291\n",
      "Training Progress: \tEpoch 65 [1920/8883 (21.58%)]\t\tLoss: 0.68737\n",
      "Training Progress: \tEpoch 65 [2240/8883 (25.18%)]\t\tLoss: 0.42650\n",
      "Training Progress: \tEpoch 65 [2560/8883 (28.78%)]\t\tLoss: 0.40343\n",
      "Training Progress: \tEpoch 65 [2880/8883 (32.37%)]\t\tLoss: 0.47255\n",
      "Training Progress: \tEpoch 65 [3200/8883 (35.97%)]\t\tLoss: 0.71022\n",
      "Training Progress: \tEpoch 65 [3520/8883 (39.57%)]\t\tLoss: 0.54954\n",
      "Training Progress: \tEpoch 65 [3840/8883 (43.17%)]\t\tLoss: 0.46831\n",
      "Training Progress: \tEpoch 65 [4160/8883 (46.76%)]\t\tLoss: 0.52827\n",
      "Training Progress: \tEpoch 65 [4480/8883 (50.36%)]\t\tLoss: 0.34941\n",
      "Training Progress: \tEpoch 65 [4800/8883 (53.96%)]\t\tLoss: 0.32265\n",
      "Training Progress: \tEpoch 65 [5120/8883 (57.55%)]\t\tLoss: 0.58969\n",
      "Training Progress: \tEpoch 65 [5440/8883 (61.15%)]\t\tLoss: 0.59869\n",
      "Training Progress: \tEpoch 65 [5760/8883 (64.75%)]\t\tLoss: 0.91074\n",
      "Training Progress: \tEpoch 65 [6080/8883 (68.35%)]\t\tLoss: 0.52311\n",
      "Training Progress: \tEpoch 65 [6400/8883 (71.94%)]\t\tLoss: 0.75163\n",
      "Training Progress: \tEpoch 65 [6720/8883 (75.54%)]\t\tLoss: 0.57207\n",
      "Training Progress: \tEpoch 65 [7040/8883 (79.14%)]\t\tLoss: 0.55379\n",
      "Training Progress: \tEpoch 65 [7360/8883 (82.73%)]\t\tLoss: 0.43111\n",
      "Training Progress: \tEpoch 65 [7680/8883 (86.33%)]\t\tLoss: 0.50884\n",
      "Training Progress: \tEpoch 65 [8000/8883 (89.93%)]\t\tLoss: 0.53576\n",
      "Training Progress: \tEpoch 65 [8320/8883 (93.53%)]\t\tLoss: 0.72959\n",
      "Training Progress: \tEpoch 65 [8640/8883 (97.12%)]\t\tLoss: 0.47354\n",
      "\tTrain loss: 0.01392, Accuracy: 6947/8883 (78.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/8883 (0.00%)]\t\tLoss: 0.42013\n",
      "Training Progress: \tEpoch 66 [320/8883 (3.60%)]\t\tLoss: 0.55067\n",
      "Training Progress: \tEpoch 66 [640/8883 (7.19%)]\t\tLoss: 0.33698\n",
      "Training Progress: \tEpoch 66 [960/8883 (10.79%)]\t\tLoss: 0.35576\n",
      "Training Progress: \tEpoch 66 [1280/8883 (14.39%)]\t\tLoss: 0.48523\n",
      "Training Progress: \tEpoch 66 [1600/8883 (17.99%)]\t\tLoss: 0.60761\n",
      "Training Progress: \tEpoch 66 [1920/8883 (21.58%)]\t\tLoss: 0.57720\n",
      "Training Progress: \tEpoch 66 [2240/8883 (25.18%)]\t\tLoss: 0.68893\n",
      "Training Progress: \tEpoch 66 [2560/8883 (28.78%)]\t\tLoss: 0.47623\n",
      "Training Progress: \tEpoch 66 [2880/8883 (32.37%)]\t\tLoss: 0.38391\n",
      "Training Progress: \tEpoch 66 [3200/8883 (35.97%)]\t\tLoss: 0.74730\n",
      "Training Progress: \tEpoch 66 [3520/8883 (39.57%)]\t\tLoss: 0.87582\n",
      "Training Progress: \tEpoch 66 [3840/8883 (43.17%)]\t\tLoss: 0.33016\n",
      "Training Progress: \tEpoch 66 [4160/8883 (46.76%)]\t\tLoss: 0.64562\n",
      "Training Progress: \tEpoch 66 [4480/8883 (50.36%)]\t\tLoss: 0.48781\n",
      "Training Progress: \tEpoch 66 [4800/8883 (53.96%)]\t\tLoss: 0.33198\n",
      "Training Progress: \tEpoch 66 [5120/8883 (57.55%)]\t\tLoss: 0.72629\n",
      "Training Progress: \tEpoch 66 [5440/8883 (61.15%)]\t\tLoss: 0.53937\n",
      "Training Progress: \tEpoch 66 [5760/8883 (64.75%)]\t\tLoss: 0.58192\n",
      "Training Progress: \tEpoch 66 [6080/8883 (68.35%)]\t\tLoss: 0.50491\n",
      "Training Progress: \tEpoch 66 [6400/8883 (71.94%)]\t\tLoss: 0.72685\n",
      "Training Progress: \tEpoch 66 [6720/8883 (75.54%)]\t\tLoss: 0.69314\n",
      "Training Progress: \tEpoch 66 [7040/8883 (79.14%)]\t\tLoss: 0.37755\n",
      "Training Progress: \tEpoch 66 [7360/8883 (82.73%)]\t\tLoss: 0.47086\n",
      "Training Progress: \tEpoch 66 [7680/8883 (86.33%)]\t\tLoss: 0.56149\n",
      "Training Progress: \tEpoch 66 [8000/8883 (89.93%)]\t\tLoss: 0.44075\n",
      "Training Progress: \tEpoch 66 [8320/8883 (93.53%)]\t\tLoss: 0.69010\n",
      "Training Progress: \tEpoch 66 [8640/8883 (97.12%)]\t\tLoss: 0.38728\n",
      "\tTrain loss: 0.01406, Accuracy: 6931/8883 (78.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1436/1692 (84.00%)\n",
      "\tTest loss: 0.00149, Accuracy: 737/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/8883 (0.00%)]\t\tLoss: 0.40975\n",
      "Training Progress: \tEpoch 67 [320/8883 (3.60%)]\t\tLoss: 0.50968\n",
      "Training Progress: \tEpoch 67 [640/8883 (7.19%)]\t\tLoss: 0.47044\n",
      "Training Progress: \tEpoch 67 [960/8883 (10.79%)]\t\tLoss: 0.37210\n",
      "Training Progress: \tEpoch 67 [1280/8883 (14.39%)]\t\tLoss: 0.56069\n",
      "Training Progress: \tEpoch 67 [1600/8883 (17.99%)]\t\tLoss: 0.68643\n",
      "Training Progress: \tEpoch 67 [1920/8883 (21.58%)]\t\tLoss: 0.65096\n",
      "Training Progress: \tEpoch 67 [2240/8883 (25.18%)]\t\tLoss: 0.57134\n",
      "Training Progress: \tEpoch 67 [2560/8883 (28.78%)]\t\tLoss: 0.43081\n",
      "Training Progress: \tEpoch 67 [2880/8883 (32.37%)]\t\tLoss: 0.59538\n",
      "Training Progress: \tEpoch 67 [3200/8883 (35.97%)]\t\tLoss: 0.65821\n",
      "Training Progress: \tEpoch 67 [3520/8883 (39.57%)]\t\tLoss: 0.49039\n",
      "Training Progress: \tEpoch 67 [3840/8883 (43.17%)]\t\tLoss: 0.25605\n",
      "Training Progress: \tEpoch 67 [4160/8883 (46.76%)]\t\tLoss: 0.57441\n",
      "Training Progress: \tEpoch 67 [4480/8883 (50.36%)]\t\tLoss: 0.36506\n",
      "Training Progress: \tEpoch 67 [4800/8883 (53.96%)]\t\tLoss: 0.53317\n",
      "Training Progress: \tEpoch 67 [5120/8883 (57.55%)]\t\tLoss: 0.75651\n",
      "Training Progress: \tEpoch 67 [5440/8883 (61.15%)]\t\tLoss: 0.70222\n",
      "Training Progress: \tEpoch 67 [5760/8883 (64.75%)]\t\tLoss: 0.74555\n",
      "Training Progress: \tEpoch 67 [6080/8883 (68.35%)]\t\tLoss: 0.51673\n",
      "Training Progress: \tEpoch 67 [6400/8883 (71.94%)]\t\tLoss: 0.65949\n",
      "Training Progress: \tEpoch 67 [6720/8883 (75.54%)]\t\tLoss: 0.62735\n",
      "Training Progress: \tEpoch 67 [7040/8883 (79.14%)]\t\tLoss: 0.59194\n",
      "Training Progress: \tEpoch 67 [7360/8883 (82.73%)]\t\tLoss: 0.66202\n",
      "Training Progress: \tEpoch 67 [7680/8883 (86.33%)]\t\tLoss: 0.70107\n",
      "Training Progress: \tEpoch 67 [8000/8883 (89.93%)]\t\tLoss: 0.40390\n",
      "Training Progress: \tEpoch 67 [8320/8883 (93.53%)]\t\tLoss: 0.71982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 67 [8640/8883 (97.12%)]\t\tLoss: 0.38378\n",
      "\tTrain loss: 0.01413, Accuracy: 6919/8883 (77.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1433/1692 (84.00%)\n",
      "\tTest loss: 0.00161, Accuracy: 733/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/8883 (0.00%)]\t\tLoss: 0.34700\n",
      "Training Progress: \tEpoch 68 [320/8883 (3.60%)]\t\tLoss: 0.52511\n",
      "Training Progress: \tEpoch 68 [640/8883 (7.19%)]\t\tLoss: 0.46220\n",
      "Training Progress: \tEpoch 68 [960/8883 (10.79%)]\t\tLoss: 0.42714\n",
      "Training Progress: \tEpoch 68 [1280/8883 (14.39%)]\t\tLoss: 0.50061\n",
      "Training Progress: \tEpoch 68 [1600/8883 (17.99%)]\t\tLoss: 0.76222\n",
      "Training Progress: \tEpoch 68 [1920/8883 (21.58%)]\t\tLoss: 0.57772\n",
      "Training Progress: \tEpoch 68 [2240/8883 (25.18%)]\t\tLoss: 0.39401\n",
      "Training Progress: \tEpoch 68 [2560/8883 (28.78%)]\t\tLoss: 0.53818\n",
      "Training Progress: \tEpoch 68 [2880/8883 (32.37%)]\t\tLoss: 0.55842\n",
      "Training Progress: \tEpoch 68 [3200/8883 (35.97%)]\t\tLoss: 0.74880\n",
      "Training Progress: \tEpoch 68 [3520/8883 (39.57%)]\t\tLoss: 0.58700\n",
      "Training Progress: \tEpoch 68 [3840/8883 (43.17%)]\t\tLoss: 0.38925\n",
      "Training Progress: \tEpoch 68 [4160/8883 (46.76%)]\t\tLoss: 0.65956\n",
      "Training Progress: \tEpoch 68 [4480/8883 (50.36%)]\t\tLoss: 0.44826\n",
      "Training Progress: \tEpoch 68 [4800/8883 (53.96%)]\t\tLoss: 0.35995\n",
      "Training Progress: \tEpoch 68 [5120/8883 (57.55%)]\t\tLoss: 0.46624\n",
      "Training Progress: \tEpoch 68 [5440/8883 (61.15%)]\t\tLoss: 0.49561\n",
      "Training Progress: \tEpoch 68 [5760/8883 (64.75%)]\t\tLoss: 0.89986\n",
      "Training Progress: \tEpoch 68 [6080/8883 (68.35%)]\t\tLoss: 0.55973\n",
      "Training Progress: \tEpoch 68 [6400/8883 (71.94%)]\t\tLoss: 0.74353\n",
      "Training Progress: \tEpoch 68 [6720/8883 (75.54%)]\t\tLoss: 0.62378\n",
      "Training Progress: \tEpoch 68 [7040/8883 (79.14%)]\t\tLoss: 0.40558\n",
      "Training Progress: \tEpoch 68 [7360/8883 (82.73%)]\t\tLoss: 0.47590\n",
      "Training Progress: \tEpoch 68 [7680/8883 (86.33%)]\t\tLoss: 0.51139\n",
      "Training Progress: \tEpoch 68 [8000/8883 (89.93%)]\t\tLoss: 0.61180\n",
      "Training Progress: \tEpoch 68 [8320/8883 (93.53%)]\t\tLoss: 0.53000\n",
      "Training Progress: \tEpoch 68 [8640/8883 (97.12%)]\t\tLoss: 0.56578\n",
      "\tTrain loss: 0.01417, Accuracy: 6925/8883 (77.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1431/1692 (84.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 713/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/8883 (0.00%)]\t\tLoss: 0.52355\n",
      "Training Progress: \tEpoch 69 [320/8883 (3.60%)]\t\tLoss: 0.52466\n",
      "Training Progress: \tEpoch 69 [640/8883 (7.19%)]\t\tLoss: 0.47723\n",
      "Training Progress: \tEpoch 69 [960/8883 (10.79%)]\t\tLoss: 0.43574\n",
      "Training Progress: \tEpoch 69 [1280/8883 (14.39%)]\t\tLoss: 0.54978\n",
      "Training Progress: \tEpoch 69 [1600/8883 (17.99%)]\t\tLoss: 0.66352\n",
      "Training Progress: \tEpoch 69 [1920/8883 (21.58%)]\t\tLoss: 0.41578\n",
      "Training Progress: \tEpoch 69 [2240/8883 (25.18%)]\t\tLoss: 0.54804\n",
      "Training Progress: \tEpoch 69 [2560/8883 (28.78%)]\t\tLoss: 0.45102\n",
      "Training Progress: \tEpoch 69 [2880/8883 (32.37%)]\t\tLoss: 0.47348\n",
      "Training Progress: \tEpoch 69 [3200/8883 (35.97%)]\t\tLoss: 0.68082\n",
      "Training Progress: \tEpoch 69 [3520/8883 (39.57%)]\t\tLoss: 0.52265\n",
      "Training Progress: \tEpoch 69 [3840/8883 (43.17%)]\t\tLoss: 0.30550\n",
      "Training Progress: \tEpoch 69 [4160/8883 (46.76%)]\t\tLoss: 0.75280\n",
      "Training Progress: \tEpoch 69 [4480/8883 (50.36%)]\t\tLoss: 0.38862\n",
      "Training Progress: \tEpoch 69 [4800/8883 (53.96%)]\t\tLoss: 0.37038\n",
      "Training Progress: \tEpoch 69 [5120/8883 (57.55%)]\t\tLoss: 0.61844\n",
      "Training Progress: \tEpoch 69 [5440/8883 (61.15%)]\t\tLoss: 0.56930\n",
      "Training Progress: \tEpoch 69 [5760/8883 (64.75%)]\t\tLoss: 0.69093\n",
      "Training Progress: \tEpoch 69 [6080/8883 (68.35%)]\t\tLoss: 0.48788\n",
      "Training Progress: \tEpoch 69 [6400/8883 (71.94%)]\t\tLoss: 0.74018\n",
      "Training Progress: \tEpoch 69 [6720/8883 (75.54%)]\t\tLoss: 0.69308\n",
      "Training Progress: \tEpoch 69 [7040/8883 (79.14%)]\t\tLoss: 0.38511\n",
      "Training Progress: \tEpoch 69 [7360/8883 (82.73%)]\t\tLoss: 0.44742\n",
      "Training Progress: \tEpoch 69 [7680/8883 (86.33%)]\t\tLoss: 0.55059\n",
      "Training Progress: \tEpoch 69 [8000/8883 (89.93%)]\t\tLoss: 0.39868\n",
      "Training Progress: \tEpoch 69 [8320/8883 (93.53%)]\t\tLoss: 0.82290\n",
      "Training Progress: \tEpoch 69 [8640/8883 (97.12%)]\t\tLoss: 0.49419\n",
      "\tTrain loss: 0.01312, Accuracy: 7038/8883 (79.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1448/1692 (85.00%)\n",
      "\tTest loss: 0.00149, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/8883 (0.00%)]\t\tLoss: 0.43192\n",
      "Training Progress: \tEpoch 70 [320/8883 (3.60%)]\t\tLoss: 0.54523\n",
      "Training Progress: \tEpoch 70 [640/8883 (7.19%)]\t\tLoss: 0.51251\n",
      "Training Progress: \tEpoch 70 [960/8883 (10.79%)]\t\tLoss: 0.37280\n",
      "Training Progress: \tEpoch 70 [1280/8883 (14.39%)]\t\tLoss: 0.67401\n",
      "Training Progress: \tEpoch 70 [1600/8883 (17.99%)]\t\tLoss: 0.60355\n",
      "Training Progress: \tEpoch 70 [1920/8883 (21.58%)]\t\tLoss: 0.53208\n",
      "Training Progress: \tEpoch 70 [2240/8883 (25.18%)]\t\tLoss: 0.62932\n",
      "Training Progress: \tEpoch 70 [2560/8883 (28.78%)]\t\tLoss: 0.45659\n",
      "Training Progress: \tEpoch 70 [2880/8883 (32.37%)]\t\tLoss: 0.40535\n",
      "Training Progress: \tEpoch 70 [3200/8883 (35.97%)]\t\tLoss: 0.60751\n",
      "Training Progress: \tEpoch 70 [3520/8883 (39.57%)]\t\tLoss: 0.38653\n",
      "Training Progress: \tEpoch 70 [3840/8883 (43.17%)]\t\tLoss: 0.31118\n",
      "Training Progress: \tEpoch 70 [4160/8883 (46.76%)]\t\tLoss: 0.45791\n",
      "Training Progress: \tEpoch 70 [4480/8883 (50.36%)]\t\tLoss: 0.61329\n",
      "Training Progress: \tEpoch 70 [4800/8883 (53.96%)]\t\tLoss: 0.37943\n",
      "Training Progress: \tEpoch 70 [5120/8883 (57.55%)]\t\tLoss: 0.51557\n",
      "Training Progress: \tEpoch 70 [5440/8883 (61.15%)]\t\tLoss: 0.54997\n",
      "Training Progress: \tEpoch 70 [5760/8883 (64.75%)]\t\tLoss: 0.66164\n",
      "Training Progress: \tEpoch 70 [6080/8883 (68.35%)]\t\tLoss: 0.59552\n",
      "Training Progress: \tEpoch 70 [6400/8883 (71.94%)]\t\tLoss: 0.67522\n",
      "Training Progress: \tEpoch 70 [6720/8883 (75.54%)]\t\tLoss: 0.77097\n",
      "Training Progress: \tEpoch 70 [7040/8883 (79.14%)]\t\tLoss: 0.47426\n",
      "Training Progress: \tEpoch 70 [7360/8883 (82.73%)]\t\tLoss: 0.61541\n",
      "Training Progress: \tEpoch 70 [7680/8883 (86.33%)]\t\tLoss: 0.44011\n",
      "Training Progress: \tEpoch 70 [8000/8883 (89.93%)]\t\tLoss: 0.50223\n",
      "Training Progress: \tEpoch 70 [8320/8883 (93.53%)]\t\tLoss: 0.57587\n",
      "Training Progress: \tEpoch 70 [8640/8883 (97.12%)]\t\tLoss: 0.59182\n",
      "\tTrain loss: 0.01359, Accuracy: 6967/8883 (78.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1438/1692 (84.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 712/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/8883 (0.00%)]\t\tLoss: 0.39711\n",
      "Training Progress: \tEpoch 71 [320/8883 (3.60%)]\t\tLoss: 0.51724\n",
      "Training Progress: \tEpoch 71 [640/8883 (7.19%)]\t\tLoss: 0.34717\n",
      "Training Progress: \tEpoch 71 [960/8883 (10.79%)]\t\tLoss: 0.35364\n",
      "Training Progress: \tEpoch 71 [1280/8883 (14.39%)]\t\tLoss: 0.61977\n",
      "Training Progress: \tEpoch 71 [1600/8883 (17.99%)]\t\tLoss: 0.57797\n",
      "Training Progress: \tEpoch 71 [1920/8883 (21.58%)]\t\tLoss: 0.70985\n",
      "Training Progress: \tEpoch 71 [2240/8883 (25.18%)]\t\tLoss: 0.45919\n",
      "Training Progress: \tEpoch 71 [2560/8883 (28.78%)]\t\tLoss: 0.40962\n",
      "Training Progress: \tEpoch 71 [2880/8883 (32.37%)]\t\tLoss: 0.43678\n",
      "Training Progress: \tEpoch 71 [3200/8883 (35.97%)]\t\tLoss: 0.81867\n",
      "Training Progress: \tEpoch 71 [3520/8883 (39.57%)]\t\tLoss: 0.42899\n",
      "Training Progress: \tEpoch 71 [3840/8883 (43.17%)]\t\tLoss: 0.33197\n",
      "Training Progress: \tEpoch 71 [4160/8883 (46.76%)]\t\tLoss: 0.46143\n",
      "Training Progress: \tEpoch 71 [4480/8883 (50.36%)]\t\tLoss: 0.42118\n",
      "Training Progress: \tEpoch 71 [4800/8883 (53.96%)]\t\tLoss: 0.33408\n",
      "Training Progress: \tEpoch 71 [5120/8883 (57.55%)]\t\tLoss: 0.51822\n",
      "Training Progress: \tEpoch 71 [5440/8883 (61.15%)]\t\tLoss: 0.48724\n",
      "Training Progress: \tEpoch 71 [5760/8883 (64.75%)]\t\tLoss: 0.56647\n",
      "Training Progress: \tEpoch 71 [6080/8883 (68.35%)]\t\tLoss: 0.56327\n",
      "Training Progress: \tEpoch 71 [6400/8883 (71.94%)]\t\tLoss: 0.65741\n",
      "Training Progress: \tEpoch 71 [6720/8883 (75.54%)]\t\tLoss: 0.61412\n",
      "Training Progress: \tEpoch 71 [7040/8883 (79.14%)]\t\tLoss: 0.41657\n",
      "Training Progress: \tEpoch 71 [7360/8883 (82.73%)]\t\tLoss: 0.46249\n",
      "Training Progress: \tEpoch 71 [7680/8883 (86.33%)]\t\tLoss: 0.50167\n",
      "Training Progress: \tEpoch 71 [8000/8883 (89.93%)]\t\tLoss: 0.60568\n",
      "Training Progress: \tEpoch 71 [8320/8883 (93.53%)]\t\tLoss: 0.85840\n",
      "Training Progress: \tEpoch 71 [8640/8883 (97.12%)]\t\tLoss: 0.43501\n",
      "\tTrain loss: 0.01566, Accuracy: 6759/8883 (76.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1357/1692 (80.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 703/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/8883 (0.00%)]\t\tLoss: 0.41605\n",
      "Training Progress: \tEpoch 72 [320/8883 (3.60%)]\t\tLoss: 0.51102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 72 [640/8883 (7.19%)]\t\tLoss: 0.47167\n",
      "Training Progress: \tEpoch 72 [960/8883 (10.79%)]\t\tLoss: 0.37003\n",
      "Training Progress: \tEpoch 72 [1280/8883 (14.39%)]\t\tLoss: 0.65022\n",
      "Training Progress: \tEpoch 72 [1600/8883 (17.99%)]\t\tLoss: 0.71442\n",
      "Training Progress: \tEpoch 72 [1920/8883 (21.58%)]\t\tLoss: 0.75857\n",
      "Training Progress: \tEpoch 72 [2240/8883 (25.18%)]\t\tLoss: 0.49299\n",
      "Training Progress: \tEpoch 72 [2560/8883 (28.78%)]\t\tLoss: 0.52340\n",
      "Training Progress: \tEpoch 72 [2880/8883 (32.37%)]\t\tLoss: 0.52057\n",
      "Training Progress: \tEpoch 72 [3200/8883 (35.97%)]\t\tLoss: 0.76657\n",
      "Training Progress: \tEpoch 72 [3520/8883 (39.57%)]\t\tLoss: 0.40006\n",
      "Training Progress: \tEpoch 72 [3840/8883 (43.17%)]\t\tLoss: 0.20366\n",
      "Training Progress: \tEpoch 72 [4160/8883 (46.76%)]\t\tLoss: 0.85189\n",
      "Training Progress: \tEpoch 72 [4480/8883 (50.36%)]\t\tLoss: 0.41258\n",
      "Training Progress: \tEpoch 72 [4800/8883 (53.96%)]\t\tLoss: 0.44640\n",
      "Training Progress: \tEpoch 72 [5120/8883 (57.55%)]\t\tLoss: 0.56344\n",
      "Training Progress: \tEpoch 72 [5440/8883 (61.15%)]\t\tLoss: 0.58003\n",
      "Training Progress: \tEpoch 72 [5760/8883 (64.75%)]\t\tLoss: 0.61423\n",
      "Training Progress: \tEpoch 72 [6080/8883 (68.35%)]\t\tLoss: 0.53663\n",
      "Training Progress: \tEpoch 72 [6400/8883 (71.94%)]\t\tLoss: 0.50525\n",
      "Training Progress: \tEpoch 72 [6720/8883 (75.54%)]\t\tLoss: 0.54027\n",
      "Training Progress: \tEpoch 72 [7040/8883 (79.14%)]\t\tLoss: 0.40539\n",
      "Training Progress: \tEpoch 72 [7360/8883 (82.73%)]\t\tLoss: 0.48612\n",
      "Training Progress: \tEpoch 72 [7680/8883 (86.33%)]\t\tLoss: 0.44012\n",
      "Training Progress: \tEpoch 72 [8000/8883 (89.93%)]\t\tLoss: 0.50953\n",
      "Training Progress: \tEpoch 72 [8320/8883 (93.53%)]\t\tLoss: 0.54339\n",
      "Training Progress: \tEpoch 72 [8640/8883 (97.12%)]\t\tLoss: 0.30454\n",
      "\tTrain loss: 0.01287, Accuracy: 7044/8883 (79.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 700/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/8883 (0.00%)]\t\tLoss: 0.37657\n",
      "Training Progress: \tEpoch 73 [320/8883 (3.60%)]\t\tLoss: 0.52192\n",
      "Training Progress: \tEpoch 73 [640/8883 (7.19%)]\t\tLoss: 0.37974\n",
      "Training Progress: \tEpoch 73 [960/8883 (10.79%)]\t\tLoss: 0.58328\n",
      "Training Progress: \tEpoch 73 [1280/8883 (14.39%)]\t\tLoss: 0.59077\n",
      "Training Progress: \tEpoch 73 [1600/8883 (17.99%)]\t\tLoss: 0.80277\n",
      "Training Progress: \tEpoch 73 [1920/8883 (21.58%)]\t\tLoss: 0.46592\n",
      "Training Progress: \tEpoch 73 [2240/8883 (25.18%)]\t\tLoss: 0.44719\n",
      "Training Progress: \tEpoch 73 [2560/8883 (28.78%)]\t\tLoss: 0.66539\n",
      "Training Progress: \tEpoch 73 [2880/8883 (32.37%)]\t\tLoss: 0.50218\n",
      "Training Progress: \tEpoch 73 [3200/8883 (35.97%)]\t\tLoss: 0.57699\n",
      "Training Progress: \tEpoch 73 [3520/8883 (39.57%)]\t\tLoss: 0.56741\n",
      "Training Progress: \tEpoch 73 [3840/8883 (43.17%)]\t\tLoss: 0.24544\n",
      "Training Progress: \tEpoch 73 [4160/8883 (46.76%)]\t\tLoss: 0.56245\n",
      "Training Progress: \tEpoch 73 [4480/8883 (50.36%)]\t\tLoss: 0.38782\n",
      "Training Progress: \tEpoch 73 [4800/8883 (53.96%)]\t\tLoss: 0.36445\n",
      "Training Progress: \tEpoch 73 [5120/8883 (57.55%)]\t\tLoss: 0.70434\n",
      "Training Progress: \tEpoch 73 [5440/8883 (61.15%)]\t\tLoss: 0.60086\n",
      "Training Progress: \tEpoch 73 [5760/8883 (64.75%)]\t\tLoss: 0.85467\n",
      "Training Progress: \tEpoch 73 [6080/8883 (68.35%)]\t\tLoss: 0.55310\n",
      "Training Progress: \tEpoch 73 [6400/8883 (71.94%)]\t\tLoss: 0.69109\n",
      "Training Progress: \tEpoch 73 [6720/8883 (75.54%)]\t\tLoss: 0.57070\n",
      "Training Progress: \tEpoch 73 [7040/8883 (79.14%)]\t\tLoss: 0.42822\n",
      "Training Progress: \tEpoch 73 [7360/8883 (82.73%)]\t\tLoss: 0.49660\n",
      "Training Progress: \tEpoch 73 [7680/8883 (86.33%)]\t\tLoss: 0.65814\n",
      "Training Progress: \tEpoch 73 [8000/8883 (89.93%)]\t\tLoss: 0.34623\n",
      "Training Progress: \tEpoch 73 [8320/8883 (93.53%)]\t\tLoss: 0.58075\n",
      "Training Progress: \tEpoch 73 [8640/8883 (97.12%)]\t\tLoss: 0.50206\n",
      "\tTrain loss: 0.01363, Accuracy: 6989/8883 (78.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 706/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/8883 (0.00%)]\t\tLoss: 0.35162\n",
      "Training Progress: \tEpoch 74 [320/8883 (3.60%)]\t\tLoss: 0.49640\n",
      "Training Progress: \tEpoch 74 [640/8883 (7.19%)]\t\tLoss: 0.46271\n",
      "Training Progress: \tEpoch 74 [960/8883 (10.79%)]\t\tLoss: 0.33290\n",
      "Training Progress: \tEpoch 74 [1280/8883 (14.39%)]\t\tLoss: 0.51230\n",
      "Training Progress: \tEpoch 74 [1600/8883 (17.99%)]\t\tLoss: 0.54151\n",
      "Training Progress: \tEpoch 74 [1920/8883 (21.58%)]\t\tLoss: 0.38514\n",
      "Training Progress: \tEpoch 74 [2240/8883 (25.18%)]\t\tLoss: 0.52036\n",
      "Training Progress: \tEpoch 74 [2560/8883 (28.78%)]\t\tLoss: 0.52417\n",
      "Training Progress: \tEpoch 74 [2880/8883 (32.37%)]\t\tLoss: 0.51219\n",
      "Training Progress: \tEpoch 74 [3200/8883 (35.97%)]\t\tLoss: 0.86233\n",
      "Training Progress: \tEpoch 74 [3520/8883 (39.57%)]\t\tLoss: 0.56450\n",
      "Training Progress: \tEpoch 74 [3840/8883 (43.17%)]\t\tLoss: 0.27295\n",
      "Training Progress: \tEpoch 74 [4160/8883 (46.76%)]\t\tLoss: 0.49551\n",
      "Training Progress: \tEpoch 74 [4480/8883 (50.36%)]\t\tLoss: 0.41392\n",
      "Training Progress: \tEpoch 74 [4800/8883 (53.96%)]\t\tLoss: 0.39572\n",
      "Training Progress: \tEpoch 74 [5120/8883 (57.55%)]\t\tLoss: 0.47586\n",
      "Training Progress: \tEpoch 74 [5440/8883 (61.15%)]\t\tLoss: 0.60304\n",
      "Training Progress: \tEpoch 74 [5760/8883 (64.75%)]\t\tLoss: 0.73674\n",
      "Training Progress: \tEpoch 74 [6080/8883 (68.35%)]\t\tLoss: 0.55827\n",
      "Training Progress: \tEpoch 74 [6400/8883 (71.94%)]\t\tLoss: 0.57519\n",
      "Training Progress: \tEpoch 74 [6720/8883 (75.54%)]\t\tLoss: 0.61545\n",
      "Training Progress: \tEpoch 74 [7040/8883 (79.14%)]\t\tLoss: 0.47143\n",
      "Training Progress: \tEpoch 74 [7360/8883 (82.73%)]\t\tLoss: 0.58481\n",
      "Training Progress: \tEpoch 74 [7680/8883 (86.33%)]\t\tLoss: 0.54211\n",
      "Training Progress: \tEpoch 74 [8000/8883 (89.93%)]\t\tLoss: 0.37640\n",
      "Training Progress: \tEpoch 74 [8320/8883 (93.53%)]\t\tLoss: 0.60062\n",
      "Training Progress: \tEpoch 74 [8640/8883 (97.12%)]\t\tLoss: 0.31385\n",
      "\tTrain loss: 0.01220, Accuracy: 7112/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1496/1692 (88.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 745/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/8883 (0.00%)]\t\tLoss: 0.34495\n",
      "Training Progress: \tEpoch 75 [320/8883 (3.60%)]\t\tLoss: 0.50360\n",
      "Training Progress: \tEpoch 75 [640/8883 (7.19%)]\t\tLoss: 0.30434\n",
      "Training Progress: \tEpoch 75 [960/8883 (10.79%)]\t\tLoss: 0.39200\n",
      "Training Progress: \tEpoch 75 [1280/8883 (14.39%)]\t\tLoss: 0.49229\n",
      "Training Progress: \tEpoch 75 [1600/8883 (17.99%)]\t\tLoss: 0.54909\n",
      "Training Progress: \tEpoch 75 [1920/8883 (21.58%)]\t\tLoss: 0.60173\n",
      "Training Progress: \tEpoch 75 [2240/8883 (25.18%)]\t\tLoss: 0.50584\n",
      "Training Progress: \tEpoch 75 [2560/8883 (28.78%)]\t\tLoss: 0.44133\n",
      "Training Progress: \tEpoch 75 [2880/8883 (32.37%)]\t\tLoss: 0.55033\n",
      "Training Progress: \tEpoch 75 [3200/8883 (35.97%)]\t\tLoss: 0.90165\n",
      "Training Progress: \tEpoch 75 [3520/8883 (39.57%)]\t\tLoss: 0.61965\n",
      "Training Progress: \tEpoch 75 [3840/8883 (43.17%)]\t\tLoss: 0.26402\n",
      "Training Progress: \tEpoch 75 [4160/8883 (46.76%)]\t\tLoss: 0.96887\n",
      "Training Progress: \tEpoch 75 [4480/8883 (50.36%)]\t\tLoss: 0.52547\n",
      "Training Progress: \tEpoch 75 [4800/8883 (53.96%)]\t\tLoss: 0.35333\n",
      "Training Progress: \tEpoch 75 [5120/8883 (57.55%)]\t\tLoss: 0.54434\n",
      "Training Progress: \tEpoch 75 [5440/8883 (61.15%)]\t\tLoss: 0.49202\n",
      "Training Progress: \tEpoch 75 [5760/8883 (64.75%)]\t\tLoss: 0.57162\n",
      "Training Progress: \tEpoch 75 [6080/8883 (68.35%)]\t\tLoss: 0.47514\n",
      "Training Progress: \tEpoch 75 [6400/8883 (71.94%)]\t\tLoss: 0.57549\n",
      "Training Progress: \tEpoch 75 [6720/8883 (75.54%)]\t\tLoss: 0.52905\n",
      "Training Progress: \tEpoch 75 [7040/8883 (79.14%)]\t\tLoss: 0.38662\n",
      "Training Progress: \tEpoch 75 [7360/8883 (82.73%)]\t\tLoss: 0.61674\n",
      "Training Progress: \tEpoch 75 [7680/8883 (86.33%)]\t\tLoss: 0.64343\n",
      "Training Progress: \tEpoch 75 [8000/8883 (89.93%)]\t\tLoss: 0.53129\n",
      "Training Progress: \tEpoch 75 [8320/8883 (93.53%)]\t\tLoss: 0.72415\n",
      "Training Progress: \tEpoch 75 [8640/8883 (97.12%)]\t\tLoss: 0.61214\n",
      "\tTrain loss: 0.01585, Accuracy: 6764/8883 (76.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/8883 (0.00%)]\t\tLoss: 0.48143\n",
      "Training Progress: \tEpoch 76 [320/8883 (3.60%)]\t\tLoss: 0.52451\n",
      "Training Progress: \tEpoch 76 [640/8883 (7.19%)]\t\tLoss: 0.40712\n",
      "Training Progress: \tEpoch 76 [960/8883 (10.79%)]\t\tLoss: 0.45770\n",
      "Training Progress: \tEpoch 76 [1280/8883 (14.39%)]\t\tLoss: 0.49925\n",
      "Training Progress: \tEpoch 76 [1600/8883 (17.99%)]\t\tLoss: 0.78120\n",
      "Training Progress: \tEpoch 76 [1920/8883 (21.58%)]\t\tLoss: 0.46999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 76 [2240/8883 (25.18%)]\t\tLoss: 0.38338\n",
      "Training Progress: \tEpoch 76 [2560/8883 (28.78%)]\t\tLoss: 0.38410\n",
      "Training Progress: \tEpoch 76 [2880/8883 (32.37%)]\t\tLoss: 0.57435\n",
      "Training Progress: \tEpoch 76 [3200/8883 (35.97%)]\t\tLoss: 0.68917\n",
      "Training Progress: \tEpoch 76 [3520/8883 (39.57%)]\t\tLoss: 0.76972\n",
      "Training Progress: \tEpoch 76 [3840/8883 (43.17%)]\t\tLoss: 0.38279\n",
      "Training Progress: \tEpoch 76 [4160/8883 (46.76%)]\t\tLoss: 0.51684\n",
      "Training Progress: \tEpoch 76 [4480/8883 (50.36%)]\t\tLoss: 0.46049\n",
      "Training Progress: \tEpoch 76 [4800/8883 (53.96%)]\t\tLoss: 0.24687\n",
      "Training Progress: \tEpoch 76 [5120/8883 (57.55%)]\t\tLoss: 0.56028\n",
      "Training Progress: \tEpoch 76 [5440/8883 (61.15%)]\t\tLoss: 0.63025\n",
      "Training Progress: \tEpoch 76 [5760/8883 (64.75%)]\t\tLoss: 0.75734\n",
      "Training Progress: \tEpoch 76 [6080/8883 (68.35%)]\t\tLoss: 0.49424\n",
      "Training Progress: \tEpoch 76 [6400/8883 (71.94%)]\t\tLoss: 0.56797\n",
      "Training Progress: \tEpoch 76 [6720/8883 (75.54%)]\t\tLoss: 0.56162\n",
      "Training Progress: \tEpoch 76 [7040/8883 (79.14%)]\t\tLoss: 0.53481\n",
      "Training Progress: \tEpoch 76 [7360/8883 (82.73%)]\t\tLoss: 0.66148\n",
      "Training Progress: \tEpoch 76 [7680/8883 (86.33%)]\t\tLoss: 0.49928\n",
      "Training Progress: \tEpoch 76 [8000/8883 (89.93%)]\t\tLoss: 0.37073\n",
      "Training Progress: \tEpoch 76 [8320/8883 (93.53%)]\t\tLoss: 0.43827\n",
      "Training Progress: \tEpoch 76 [8640/8883 (97.12%)]\t\tLoss: 0.43382\n",
      "\tTrain loss: 0.01433, Accuracy: 6904/8883 (77.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1422/1692 (84.00%)\n",
      "\tTest loss: 0.00180, Accuracy: 683/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/8883 (0.00%)]\t\tLoss: 0.40504\n",
      "Training Progress: \tEpoch 77 [320/8883 (3.60%)]\t\tLoss: 0.56596\n",
      "Training Progress: \tEpoch 77 [640/8883 (7.19%)]\t\tLoss: 0.35596\n",
      "Training Progress: \tEpoch 77 [960/8883 (10.79%)]\t\tLoss: 0.41655\n",
      "Training Progress: \tEpoch 77 [1280/8883 (14.39%)]\t\tLoss: 0.49989\n",
      "Training Progress: \tEpoch 77 [1600/8883 (17.99%)]\t\tLoss: 0.70343\n",
      "Training Progress: \tEpoch 77 [1920/8883 (21.58%)]\t\tLoss: 0.50465\n",
      "Training Progress: \tEpoch 77 [2240/8883 (25.18%)]\t\tLoss: 0.43554\n",
      "Training Progress: \tEpoch 77 [2560/8883 (28.78%)]\t\tLoss: 0.57003\n",
      "Training Progress: \tEpoch 77 [2880/8883 (32.37%)]\t\tLoss: 0.46461\n",
      "Training Progress: \tEpoch 77 [3200/8883 (35.97%)]\t\tLoss: 0.81146\n",
      "Training Progress: \tEpoch 77 [3520/8883 (39.57%)]\t\tLoss: 0.66499\n",
      "Training Progress: \tEpoch 77 [3840/8883 (43.17%)]\t\tLoss: 0.26231\n",
      "Training Progress: \tEpoch 77 [4160/8883 (46.76%)]\t\tLoss: 0.49878\n",
      "Training Progress: \tEpoch 77 [4480/8883 (50.36%)]\t\tLoss: 0.36449\n",
      "Training Progress: \tEpoch 77 [4800/8883 (53.96%)]\t\tLoss: 0.34736\n",
      "Training Progress: \tEpoch 77 [5120/8883 (57.55%)]\t\tLoss: 0.67926\n",
      "Training Progress: \tEpoch 77 [5440/8883 (61.15%)]\t\tLoss: 0.53199\n",
      "Training Progress: \tEpoch 77 [5760/8883 (64.75%)]\t\tLoss: 0.57961\n",
      "Training Progress: \tEpoch 77 [6080/8883 (68.35%)]\t\tLoss: 0.49892\n",
      "Training Progress: \tEpoch 77 [6400/8883 (71.94%)]\t\tLoss: 0.62078\n",
      "Training Progress: \tEpoch 77 [6720/8883 (75.54%)]\t\tLoss: 0.63803\n",
      "Training Progress: \tEpoch 77 [7040/8883 (79.14%)]\t\tLoss: 0.38605\n",
      "Training Progress: \tEpoch 77 [7360/8883 (82.73%)]\t\tLoss: 0.66913\n",
      "Training Progress: \tEpoch 77 [7680/8883 (86.33%)]\t\tLoss: 0.46095\n",
      "Training Progress: \tEpoch 77 [8000/8883 (89.93%)]\t\tLoss: 0.50149\n",
      "Training Progress: \tEpoch 77 [8320/8883 (93.53%)]\t\tLoss: 0.74305\n",
      "Training Progress: \tEpoch 77 [8640/8883 (97.12%)]\t\tLoss: 0.40373\n",
      "\tTrain loss: 0.01425, Accuracy: 6884/8883 (77.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1427/1692 (84.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 708/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/8883 (0.00%)]\t\tLoss: 0.49112\n",
      "Training Progress: \tEpoch 78 [320/8883 (3.60%)]\t\tLoss: 0.77540\n",
      "Training Progress: \tEpoch 78 [640/8883 (7.19%)]\t\tLoss: 0.40186\n",
      "Training Progress: \tEpoch 78 [960/8883 (10.79%)]\t\tLoss: 0.34636\n",
      "Training Progress: \tEpoch 78 [1280/8883 (14.39%)]\t\tLoss: 0.60715\n",
      "Training Progress: \tEpoch 78 [1600/8883 (17.99%)]\t\tLoss: 0.57374\n",
      "Training Progress: \tEpoch 78 [1920/8883 (21.58%)]\t\tLoss: 0.70774\n",
      "Training Progress: \tEpoch 78 [2240/8883 (25.18%)]\t\tLoss: 0.47244\n",
      "Training Progress: \tEpoch 78 [2560/8883 (28.78%)]\t\tLoss: 0.42394\n",
      "Training Progress: \tEpoch 78 [2880/8883 (32.37%)]\t\tLoss: 0.41456\n",
      "Training Progress: \tEpoch 78 [3200/8883 (35.97%)]\t\tLoss: 0.66513\n",
      "Training Progress: \tEpoch 78 [3520/8883 (39.57%)]\t\tLoss: 0.53351\n",
      "Training Progress: \tEpoch 78 [3840/8883 (43.17%)]\t\tLoss: 0.30940\n",
      "Training Progress: \tEpoch 78 [4160/8883 (46.76%)]\t\tLoss: 0.56352\n",
      "Training Progress: \tEpoch 78 [4480/8883 (50.36%)]\t\tLoss: 0.42803\n",
      "Training Progress: \tEpoch 78 [4800/8883 (53.96%)]\t\tLoss: 0.25122\n",
      "Training Progress: \tEpoch 78 [5120/8883 (57.55%)]\t\tLoss: 0.49311\n",
      "Training Progress: \tEpoch 78 [5440/8883 (61.15%)]\t\tLoss: 0.51688\n",
      "Training Progress: \tEpoch 78 [5760/8883 (64.75%)]\t\tLoss: 0.73440\n",
      "Training Progress: \tEpoch 78 [6080/8883 (68.35%)]\t\tLoss: 0.47897\n",
      "Training Progress: \tEpoch 78 [6400/8883 (71.94%)]\t\tLoss: 0.51517\n",
      "Training Progress: \tEpoch 78 [6720/8883 (75.54%)]\t\tLoss: 0.60170\n",
      "Training Progress: \tEpoch 78 [7040/8883 (79.14%)]\t\tLoss: 0.33092\n",
      "Training Progress: \tEpoch 78 [7360/8883 (82.73%)]\t\tLoss: 0.42056\n",
      "Training Progress: \tEpoch 78 [7680/8883 (86.33%)]\t\tLoss: 0.43115\n",
      "Training Progress: \tEpoch 78 [8000/8883 (89.93%)]\t\tLoss: 0.43973\n",
      "Training Progress: \tEpoch 78 [8320/8883 (93.53%)]\t\tLoss: 0.53988\n",
      "Training Progress: \tEpoch 78 [8640/8883 (97.12%)]\t\tLoss: 0.32424\n",
      "\tTrain loss: 0.01255, Accuracy: 7076/8883 (79.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 741/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/8883 (0.00%)]\t\tLoss: 0.35337\n",
      "Training Progress: \tEpoch 79 [320/8883 (3.60%)]\t\tLoss: 0.54820\n",
      "Training Progress: \tEpoch 79 [640/8883 (7.19%)]\t\tLoss: 0.43362\n",
      "Training Progress: \tEpoch 79 [960/8883 (10.79%)]\t\tLoss: 0.45957\n",
      "Training Progress: \tEpoch 79 [1280/8883 (14.39%)]\t\tLoss: 0.71025\n",
      "Training Progress: \tEpoch 79 [1600/8883 (17.99%)]\t\tLoss: 0.75299\n",
      "Training Progress: \tEpoch 79 [1920/8883 (21.58%)]\t\tLoss: 0.34834\n",
      "Training Progress: \tEpoch 79 [2240/8883 (25.18%)]\t\tLoss: 0.39688\n",
      "Training Progress: \tEpoch 79 [2560/8883 (28.78%)]\t\tLoss: 0.50332\n",
      "Training Progress: \tEpoch 79 [2880/8883 (32.37%)]\t\tLoss: 0.35038\n",
      "Training Progress: \tEpoch 79 [3200/8883 (35.97%)]\t\tLoss: 0.67397\n",
      "Training Progress: \tEpoch 79 [3520/8883 (39.57%)]\t\tLoss: 0.65252\n",
      "Training Progress: \tEpoch 79 [3840/8883 (43.17%)]\t\tLoss: 0.25483\n",
      "Training Progress: \tEpoch 79 [4160/8883 (46.76%)]\t\tLoss: 0.53508\n",
      "Training Progress: \tEpoch 79 [4480/8883 (50.36%)]\t\tLoss: 0.38009\n",
      "Training Progress: \tEpoch 79 [4800/8883 (53.96%)]\t\tLoss: 0.39241\n",
      "Training Progress: \tEpoch 79 [5120/8883 (57.55%)]\t\tLoss: 0.50932\n",
      "Training Progress: \tEpoch 79 [5440/8883 (61.15%)]\t\tLoss: 0.60091\n",
      "Training Progress: \tEpoch 79 [5760/8883 (64.75%)]\t\tLoss: 0.60632\n",
      "Training Progress: \tEpoch 79 [6080/8883 (68.35%)]\t\tLoss: 0.80923\n",
      "Training Progress: \tEpoch 79 [6400/8883 (71.94%)]\t\tLoss: 0.65339\n",
      "Training Progress: \tEpoch 79 [6720/8883 (75.54%)]\t\tLoss: 0.65974\n",
      "Training Progress: \tEpoch 79 [7040/8883 (79.14%)]\t\tLoss: 0.44151\n",
      "Training Progress: \tEpoch 79 [7360/8883 (82.73%)]\t\tLoss: 0.49230\n",
      "Training Progress: \tEpoch 79 [7680/8883 (86.33%)]\t\tLoss: 0.48182\n",
      "Training Progress: \tEpoch 79 [8000/8883 (89.93%)]\t\tLoss: 0.53890\n",
      "Training Progress: \tEpoch 79 [8320/8883 (93.53%)]\t\tLoss: 0.54644\n",
      "Training Progress: \tEpoch 79 [8640/8883 (97.12%)]\t\tLoss: 0.35208\n",
      "\tTrain loss: 0.01308, Accuracy: 7041/8883 (79.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1462/1692 (86.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 700/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/8883 (0.00%)]\t\tLoss: 0.41597\n",
      "Training Progress: \tEpoch 80 [320/8883 (3.60%)]\t\tLoss: 0.47860\n",
      "Training Progress: \tEpoch 80 [640/8883 (7.19%)]\t\tLoss: 0.37872\n",
      "Training Progress: \tEpoch 80 [960/8883 (10.79%)]\t\tLoss: 0.41328\n",
      "Training Progress: \tEpoch 80 [1280/8883 (14.39%)]\t\tLoss: 0.50227\n",
      "Training Progress: \tEpoch 80 [1600/8883 (17.99%)]\t\tLoss: 0.81280\n",
      "Training Progress: \tEpoch 80 [1920/8883 (21.58%)]\t\tLoss: 0.43096\n",
      "Training Progress: \tEpoch 80 [2240/8883 (25.18%)]\t\tLoss: 0.37180\n",
      "Training Progress: \tEpoch 80 [2560/8883 (28.78%)]\t\tLoss: 0.54323\n",
      "Training Progress: \tEpoch 80 [2880/8883 (32.37%)]\t\tLoss: 0.43801\n",
      "Training Progress: \tEpoch 80 [3200/8883 (35.97%)]\t\tLoss: 0.58717\n",
      "Training Progress: \tEpoch 80 [3520/8883 (39.57%)]\t\tLoss: 0.44196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 80 [3840/8883 (43.17%)]\t\tLoss: 0.19871\n",
      "Training Progress: \tEpoch 80 [4160/8883 (46.76%)]\t\tLoss: 0.51218\n",
      "Training Progress: \tEpoch 80 [4480/8883 (50.36%)]\t\tLoss: 0.45278\n",
      "Training Progress: \tEpoch 80 [4800/8883 (53.96%)]\t\tLoss: 0.36490\n",
      "Training Progress: \tEpoch 80 [5120/8883 (57.55%)]\t\tLoss: 0.49329\n",
      "Training Progress: \tEpoch 80 [5440/8883 (61.15%)]\t\tLoss: 0.48497\n",
      "Training Progress: \tEpoch 80 [5760/8883 (64.75%)]\t\tLoss: 0.79005\n",
      "Training Progress: \tEpoch 80 [6080/8883 (68.35%)]\t\tLoss: 0.48418\n",
      "Training Progress: \tEpoch 80 [6400/8883 (71.94%)]\t\tLoss: 0.50990\n",
      "Training Progress: \tEpoch 80 [6720/8883 (75.54%)]\t\tLoss: 0.61563\n",
      "Training Progress: \tEpoch 80 [7040/8883 (79.14%)]\t\tLoss: 0.36811\n",
      "Training Progress: \tEpoch 80 [7360/8883 (82.73%)]\t\tLoss: 0.44076\n",
      "Training Progress: \tEpoch 80 [7680/8883 (86.33%)]\t\tLoss: 0.42809\n",
      "Training Progress: \tEpoch 80 [8000/8883 (89.93%)]\t\tLoss: 0.46843\n",
      "Training Progress: \tEpoch 80 [8320/8883 (93.53%)]\t\tLoss: 0.39036\n",
      "Training Progress: \tEpoch 80 [8640/8883 (97.12%)]\t\tLoss: 0.34607\n",
      "\tTrain loss: 0.01456, Accuracy: 6919/8883 (77.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1432/1692 (84.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 681/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/8883 (0.00%)]\t\tLoss: 0.56827\n",
      "Training Progress: \tEpoch 81 [320/8883 (3.60%)]\t\tLoss: 0.52361\n",
      "Training Progress: \tEpoch 81 [640/8883 (7.19%)]\t\tLoss: 0.42622\n",
      "Training Progress: \tEpoch 81 [960/8883 (10.79%)]\t\tLoss: 0.33642\n",
      "Training Progress: \tEpoch 81 [1280/8883 (14.39%)]\t\tLoss: 0.57629\n",
      "Training Progress: \tEpoch 81 [1600/8883 (17.99%)]\t\tLoss: 0.74841\n",
      "Training Progress: \tEpoch 81 [1920/8883 (21.58%)]\t\tLoss: 0.35307\n",
      "Training Progress: \tEpoch 81 [2240/8883 (25.18%)]\t\tLoss: 0.43124\n",
      "Training Progress: \tEpoch 81 [2560/8883 (28.78%)]\t\tLoss: 0.48386\n",
      "Training Progress: \tEpoch 81 [2880/8883 (32.37%)]\t\tLoss: 0.51348\n",
      "Training Progress: \tEpoch 81 [3200/8883 (35.97%)]\t\tLoss: 0.72439\n",
      "Training Progress: \tEpoch 81 [3520/8883 (39.57%)]\t\tLoss: 0.63198\n",
      "Training Progress: \tEpoch 81 [3840/8883 (43.17%)]\t\tLoss: 0.39960\n",
      "Training Progress: \tEpoch 81 [4160/8883 (46.76%)]\t\tLoss: 0.62697\n",
      "Training Progress: \tEpoch 81 [4480/8883 (50.36%)]\t\tLoss: 0.41642\n",
      "Training Progress: \tEpoch 81 [4800/8883 (53.96%)]\t\tLoss: 0.31666\n",
      "Training Progress: \tEpoch 81 [5120/8883 (57.55%)]\t\tLoss: 0.53584\n",
      "Training Progress: \tEpoch 81 [5440/8883 (61.15%)]\t\tLoss: 0.48195\n",
      "Training Progress: \tEpoch 81 [5760/8883 (64.75%)]\t\tLoss: 0.61527\n",
      "Training Progress: \tEpoch 81 [6080/8883 (68.35%)]\t\tLoss: 0.71023\n",
      "Training Progress: \tEpoch 81 [6400/8883 (71.94%)]\t\tLoss: 0.72481\n",
      "Training Progress: \tEpoch 81 [6720/8883 (75.54%)]\t\tLoss: 0.51414\n",
      "Training Progress: \tEpoch 81 [7040/8883 (79.14%)]\t\tLoss: 0.53744\n",
      "Training Progress: \tEpoch 81 [7360/8883 (82.73%)]\t\tLoss: 0.58012\n",
      "Training Progress: \tEpoch 81 [7680/8883 (86.33%)]\t\tLoss: 0.56308\n",
      "Training Progress: \tEpoch 81 [8000/8883 (89.93%)]\t\tLoss: 0.61551\n",
      "Training Progress: \tEpoch 81 [8320/8883 (93.53%)]\t\tLoss: 0.60774\n",
      "Training Progress: \tEpoch 81 [8640/8883 (97.12%)]\t\tLoss: 0.28449\n",
      "\tTrain loss: 0.01204, Accuracy: 7141/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00167, Accuracy: 727/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/8883 (0.00%)]\t\tLoss: 0.35774\n",
      "Training Progress: \tEpoch 82 [320/8883 (3.60%)]\t\tLoss: 0.55053\n",
      "Training Progress: \tEpoch 82 [640/8883 (7.19%)]\t\tLoss: 0.40568\n",
      "Training Progress: \tEpoch 82 [960/8883 (10.79%)]\t\tLoss: 0.30672\n",
      "Training Progress: \tEpoch 82 [1280/8883 (14.39%)]\t\tLoss: 0.48600\n",
      "Training Progress: \tEpoch 82 [1600/8883 (17.99%)]\t\tLoss: 0.56760\n",
      "Training Progress: \tEpoch 82 [1920/8883 (21.58%)]\t\tLoss: 0.40405\n",
      "Training Progress: \tEpoch 82 [2240/8883 (25.18%)]\t\tLoss: 0.40515\n",
      "Training Progress: \tEpoch 82 [2560/8883 (28.78%)]\t\tLoss: 0.55744\n",
      "Training Progress: \tEpoch 82 [2880/8883 (32.37%)]\t\tLoss: 0.48217\n",
      "Training Progress: \tEpoch 82 [3200/8883 (35.97%)]\t\tLoss: 0.68738\n",
      "Training Progress: \tEpoch 82 [3520/8883 (39.57%)]\t\tLoss: 0.55285\n",
      "Training Progress: \tEpoch 82 [3840/8883 (43.17%)]\t\tLoss: 0.21120\n",
      "Training Progress: \tEpoch 82 [4160/8883 (46.76%)]\t\tLoss: 0.56799\n",
      "Training Progress: \tEpoch 82 [4480/8883 (50.36%)]\t\tLoss: 0.45036\n",
      "Training Progress: \tEpoch 82 [4800/8883 (53.96%)]\t\tLoss: 0.28195\n",
      "Training Progress: \tEpoch 82 [5120/8883 (57.55%)]\t\tLoss: 0.63391\n",
      "Training Progress: \tEpoch 82 [5440/8883 (61.15%)]\t\tLoss: 0.49304\n",
      "Training Progress: \tEpoch 82 [5760/8883 (64.75%)]\t\tLoss: 0.82837\n",
      "Training Progress: \tEpoch 82 [6080/8883 (68.35%)]\t\tLoss: 0.58261\n",
      "Training Progress: \tEpoch 82 [6400/8883 (71.94%)]\t\tLoss: 0.59251\n",
      "Training Progress: \tEpoch 82 [6720/8883 (75.54%)]\t\tLoss: 0.67626\n",
      "Training Progress: \tEpoch 82 [7040/8883 (79.14%)]\t\tLoss: 0.36452\n",
      "Training Progress: \tEpoch 82 [7360/8883 (82.73%)]\t\tLoss: 0.48236\n",
      "Training Progress: \tEpoch 82 [7680/8883 (86.33%)]\t\tLoss: 0.72090\n",
      "Training Progress: \tEpoch 82 [8000/8883 (89.93%)]\t\tLoss: 0.53256\n",
      "Training Progress: \tEpoch 82 [8320/8883 (93.53%)]\t\tLoss: 0.36570\n",
      "Training Progress: \tEpoch 82 [8640/8883 (97.12%)]\t\tLoss: 0.36740\n",
      "\tTrain loss: 0.01350, Accuracy: 7012/8883 (78.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1468/1692 (86.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 702/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/8883 (0.00%)]\t\tLoss: 0.40912\n",
      "Training Progress: \tEpoch 83 [320/8883 (3.60%)]\t\tLoss: 0.49505\n",
      "Training Progress: \tEpoch 83 [640/8883 (7.19%)]\t\tLoss: 0.30845\n",
      "Training Progress: \tEpoch 83 [960/8883 (10.79%)]\t\tLoss: 0.34733\n",
      "Training Progress: \tEpoch 83 [1280/8883 (14.39%)]\t\tLoss: 0.66529\n",
      "Training Progress: \tEpoch 83 [1600/8883 (17.99%)]\t\tLoss: 0.58179\n",
      "Training Progress: \tEpoch 83 [1920/8883 (21.58%)]\t\tLoss: 0.37433\n",
      "Training Progress: \tEpoch 83 [2240/8883 (25.18%)]\t\tLoss: 0.41389\n",
      "Training Progress: \tEpoch 83 [2560/8883 (28.78%)]\t\tLoss: 0.37548\n",
      "Training Progress: \tEpoch 83 [2880/8883 (32.37%)]\t\tLoss: 0.45383\n",
      "Training Progress: \tEpoch 83 [3200/8883 (35.97%)]\t\tLoss: 0.61876\n",
      "Training Progress: \tEpoch 83 [3520/8883 (39.57%)]\t\tLoss: 0.45326\n",
      "Training Progress: \tEpoch 83 [3840/8883 (43.17%)]\t\tLoss: 0.41099\n",
      "Training Progress: \tEpoch 83 [4160/8883 (46.76%)]\t\tLoss: 0.48615\n",
      "Training Progress: \tEpoch 83 [4480/8883 (50.36%)]\t\tLoss: 0.45239\n",
      "Training Progress: \tEpoch 83 [4800/8883 (53.96%)]\t\tLoss: 0.37770\n",
      "Training Progress: \tEpoch 83 [5120/8883 (57.55%)]\t\tLoss: 0.51991\n",
      "Training Progress: \tEpoch 83 [5440/8883 (61.15%)]\t\tLoss: 0.57805\n",
      "Training Progress: \tEpoch 83 [5760/8883 (64.75%)]\t\tLoss: 0.69975\n",
      "Training Progress: \tEpoch 83 [6080/8883 (68.35%)]\t\tLoss: 0.56187\n",
      "Training Progress: \tEpoch 83 [6400/8883 (71.94%)]\t\tLoss: 0.58279\n",
      "Training Progress: \tEpoch 83 [6720/8883 (75.54%)]\t\tLoss: 0.56424\n",
      "Training Progress: \tEpoch 83 [7040/8883 (79.14%)]\t\tLoss: 0.37581\n",
      "Training Progress: \tEpoch 83 [7360/8883 (82.73%)]\t\tLoss: 0.44289\n",
      "Training Progress: \tEpoch 83 [7680/8883 (86.33%)]\t\tLoss: 0.43529\n",
      "Training Progress: \tEpoch 83 [8000/8883 (89.93%)]\t\tLoss: 0.40974\n",
      "Training Progress: \tEpoch 83 [8320/8883 (93.53%)]\t\tLoss: 0.57108\n",
      "Training Progress: \tEpoch 83 [8640/8883 (97.12%)]\t\tLoss: 0.39084\n",
      "\tTrain loss: 0.01447, Accuracy: 6896/8883 (77.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1435/1692 (84.00%)\n",
      "\tTest loss: 0.00170, Accuracy: 700/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/8883 (0.00%)]\t\tLoss: 0.58803\n",
      "Training Progress: \tEpoch 84 [320/8883 (3.60%)]\t\tLoss: 0.49071\n",
      "Training Progress: \tEpoch 84 [640/8883 (7.19%)]\t\tLoss: 0.39628\n",
      "Training Progress: \tEpoch 84 [960/8883 (10.79%)]\t\tLoss: 0.39087\n",
      "Training Progress: \tEpoch 84 [1280/8883 (14.39%)]\t\tLoss: 0.49591\n",
      "Training Progress: \tEpoch 84 [1600/8883 (17.99%)]\t\tLoss: 0.61856\n",
      "Training Progress: \tEpoch 84 [1920/8883 (21.58%)]\t\tLoss: 0.45628\n",
      "Training Progress: \tEpoch 84 [2240/8883 (25.18%)]\t\tLoss: 0.43893\n",
      "Training Progress: \tEpoch 84 [2560/8883 (28.78%)]\t\tLoss: 0.41435\n",
      "Training Progress: \tEpoch 84 [2880/8883 (32.37%)]\t\tLoss: 0.38247\n",
      "Training Progress: \tEpoch 84 [3200/8883 (35.97%)]\t\tLoss: 0.89488\n",
      "Training Progress: \tEpoch 84 [3520/8883 (39.57%)]\t\tLoss: 0.54138\n",
      "Training Progress: \tEpoch 84 [3840/8883 (43.17%)]\t\tLoss: 0.22515\n",
      "Training Progress: \tEpoch 84 [4160/8883 (46.76%)]\t\tLoss: 0.70718\n",
      "Training Progress: \tEpoch 84 [4480/8883 (50.36%)]\t\tLoss: 0.43995\n",
      "Training Progress: \tEpoch 84 [4800/8883 (53.96%)]\t\tLoss: 0.27905\n",
      "Training Progress: \tEpoch 84 [5120/8883 (57.55%)]\t\tLoss: 0.46885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [5440/8883 (61.15%)]\t\tLoss: 0.51603\n",
      "Training Progress: \tEpoch 84 [5760/8883 (64.75%)]\t\tLoss: 0.53512\n",
      "Training Progress: \tEpoch 84 [6080/8883 (68.35%)]\t\tLoss: 0.47331\n",
      "Training Progress: \tEpoch 84 [6400/8883 (71.94%)]\t\tLoss: 0.67520\n",
      "Training Progress: \tEpoch 84 [6720/8883 (75.54%)]\t\tLoss: 0.61746\n",
      "Training Progress: \tEpoch 84 [7040/8883 (79.14%)]\t\tLoss: 0.47324\n",
      "Training Progress: \tEpoch 84 [7360/8883 (82.73%)]\t\tLoss: 0.51660\n",
      "Training Progress: \tEpoch 84 [7680/8883 (86.33%)]\t\tLoss: 0.42791\n",
      "Training Progress: \tEpoch 84 [8000/8883 (89.93%)]\t\tLoss: 0.38469\n",
      "Training Progress: \tEpoch 84 [8320/8883 (93.53%)]\t\tLoss: 0.59976\n",
      "Training Progress: \tEpoch 84 [8640/8883 (97.12%)]\t\tLoss: 0.32420\n",
      "\tTrain loss: 0.01273, Accuracy: 7070/8883 (79.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1483/1692 (87.00%)\n",
      "\tTest loss: 0.00161, Accuracy: 733/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/8883 (0.00%)]\t\tLoss: 0.33272\n",
      "Training Progress: \tEpoch 85 [320/8883 (3.60%)]\t\tLoss: 0.72938\n",
      "Training Progress: \tEpoch 85 [640/8883 (7.19%)]\t\tLoss: 0.49582\n",
      "Training Progress: \tEpoch 85 [960/8883 (10.79%)]\t\tLoss: 0.38866\n",
      "Training Progress: \tEpoch 85 [1280/8883 (14.39%)]\t\tLoss: 0.68071\n",
      "Training Progress: \tEpoch 85 [1600/8883 (17.99%)]\t\tLoss: 0.65394\n",
      "Training Progress: \tEpoch 85 [1920/8883 (21.58%)]\t\tLoss: 0.44191\n",
      "Training Progress: \tEpoch 85 [2240/8883 (25.18%)]\t\tLoss: 0.40126\n",
      "Training Progress: \tEpoch 85 [2560/8883 (28.78%)]\t\tLoss: 0.39666\n",
      "Training Progress: \tEpoch 85 [2880/8883 (32.37%)]\t\tLoss: 0.33305\n",
      "Training Progress: \tEpoch 85 [3200/8883 (35.97%)]\t\tLoss: 0.59292\n",
      "Training Progress: \tEpoch 85 [3520/8883 (39.57%)]\t\tLoss: 0.50081\n",
      "Training Progress: \tEpoch 85 [3840/8883 (43.17%)]\t\tLoss: 0.39757\n",
      "Training Progress: \tEpoch 85 [4160/8883 (46.76%)]\t\tLoss: 0.52110\n",
      "Training Progress: \tEpoch 85 [4480/8883 (50.36%)]\t\tLoss: 0.34400\n",
      "Training Progress: \tEpoch 85 [4800/8883 (53.96%)]\t\tLoss: 0.32605\n",
      "Training Progress: \tEpoch 85 [5120/8883 (57.55%)]\t\tLoss: 0.47374\n",
      "Training Progress: \tEpoch 85 [5440/8883 (61.15%)]\t\tLoss: 0.57261\n",
      "Training Progress: \tEpoch 85 [5760/8883 (64.75%)]\t\tLoss: 0.56151\n",
      "Training Progress: \tEpoch 85 [6080/8883 (68.35%)]\t\tLoss: 0.70156\n",
      "Training Progress: \tEpoch 85 [6400/8883 (71.94%)]\t\tLoss: 0.58666\n",
      "Training Progress: \tEpoch 85 [6720/8883 (75.54%)]\t\tLoss: 0.54471\n",
      "Training Progress: \tEpoch 85 [7040/8883 (79.14%)]\t\tLoss: 0.53342\n",
      "Training Progress: \tEpoch 85 [7360/8883 (82.73%)]\t\tLoss: 0.51369\n",
      "Training Progress: \tEpoch 85 [7680/8883 (86.33%)]\t\tLoss: 0.48739\n",
      "Training Progress: \tEpoch 85 [8000/8883 (89.93%)]\t\tLoss: 0.33789\n",
      "Training Progress: \tEpoch 85 [8320/8883 (93.53%)]\t\tLoss: 0.42377\n",
      "Training Progress: \tEpoch 85 [8640/8883 (97.12%)]\t\tLoss: 0.37842\n",
      "\tTrain loss: 0.01249, Accuracy: 7099/8883 (79.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 722/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/8883 (0.00%)]\t\tLoss: 0.44785\n",
      "Training Progress: \tEpoch 86 [320/8883 (3.60%)]\t\tLoss: 0.52783\n",
      "Training Progress: \tEpoch 86 [640/8883 (7.19%)]\t\tLoss: 0.32550\n",
      "Training Progress: \tEpoch 86 [960/8883 (10.79%)]\t\tLoss: 0.28785\n",
      "Training Progress: \tEpoch 86 [1280/8883 (14.39%)]\t\tLoss: 0.51735\n",
      "Training Progress: \tEpoch 86 [1600/8883 (17.99%)]\t\tLoss: 0.57615\n",
      "Training Progress: \tEpoch 86 [1920/8883 (21.58%)]\t\tLoss: 0.39625\n",
      "Training Progress: \tEpoch 86 [2240/8883 (25.18%)]\t\tLoss: 0.50259\n",
      "Training Progress: \tEpoch 86 [2560/8883 (28.78%)]\t\tLoss: 0.43143\n",
      "Training Progress: \tEpoch 86 [2880/8883 (32.37%)]\t\tLoss: 0.39031\n",
      "Training Progress: \tEpoch 86 [3200/8883 (35.97%)]\t\tLoss: 0.61044\n",
      "Training Progress: \tEpoch 86 [3520/8883 (39.57%)]\t\tLoss: 0.57843\n",
      "Training Progress: \tEpoch 86 [3840/8883 (43.17%)]\t\tLoss: 0.28967\n",
      "Training Progress: \tEpoch 86 [4160/8883 (46.76%)]\t\tLoss: 0.47659\n",
      "Training Progress: \tEpoch 86 [4480/8883 (50.36%)]\t\tLoss: 0.33104\n",
      "Training Progress: \tEpoch 86 [4800/8883 (53.96%)]\t\tLoss: 0.21850\n",
      "Training Progress: \tEpoch 86 [5120/8883 (57.55%)]\t\tLoss: 0.68344\n",
      "Training Progress: \tEpoch 86 [5440/8883 (61.15%)]\t\tLoss: 0.55327\n",
      "Training Progress: \tEpoch 86 [5760/8883 (64.75%)]\t\tLoss: 0.53537\n",
      "Training Progress: \tEpoch 86 [6080/8883 (68.35%)]\t\tLoss: 0.57243\n",
      "Training Progress: \tEpoch 86 [6400/8883 (71.94%)]\t\tLoss: 0.55151\n",
      "Training Progress: \tEpoch 86 [6720/8883 (75.54%)]\t\tLoss: 0.65781\n",
      "Training Progress: \tEpoch 86 [7040/8883 (79.14%)]\t\tLoss: 0.42852\n",
      "Training Progress: \tEpoch 86 [7360/8883 (82.73%)]\t\tLoss: 0.69230\n",
      "Training Progress: \tEpoch 86 [7680/8883 (86.33%)]\t\tLoss: 0.40185\n",
      "Training Progress: \tEpoch 86 [8000/8883 (89.93%)]\t\tLoss: 0.43141\n",
      "Training Progress: \tEpoch 86 [8320/8883 (93.53%)]\t\tLoss: 0.45391\n",
      "Training Progress: \tEpoch 86 [8640/8883 (97.12%)]\t\tLoss: 0.37658\n",
      "\tTrain loss: 0.01284, Accuracy: 7052/8883 (79.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1479/1692 (87.00%)\n",
      "\tTest loss: 0.00180, Accuracy: 718/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/8883 (0.00%)]\t\tLoss: 0.36213\n",
      "Training Progress: \tEpoch 87 [320/8883 (3.60%)]\t\tLoss: 0.62038\n",
      "Training Progress: \tEpoch 87 [640/8883 (7.19%)]\t\tLoss: 0.76063\n",
      "Training Progress: \tEpoch 87 [960/8883 (10.79%)]\t\tLoss: 0.40381\n",
      "Training Progress: \tEpoch 87 [1280/8883 (14.39%)]\t\tLoss: 0.47465\n",
      "Training Progress: \tEpoch 87 [1600/8883 (17.99%)]\t\tLoss: 0.67410\n",
      "Training Progress: \tEpoch 87 [1920/8883 (21.58%)]\t\tLoss: 0.37903\n",
      "Training Progress: \tEpoch 87 [2240/8883 (25.18%)]\t\tLoss: 0.37865\n",
      "Training Progress: \tEpoch 87 [2560/8883 (28.78%)]\t\tLoss: 0.37750\n",
      "Training Progress: \tEpoch 87 [2880/8883 (32.37%)]\t\tLoss: 0.49595\n",
      "Training Progress: \tEpoch 87 [3200/8883 (35.97%)]\t\tLoss: 0.56648\n",
      "Training Progress: \tEpoch 87 [3520/8883 (39.57%)]\t\tLoss: 0.46892\n",
      "Training Progress: \tEpoch 87 [3840/8883 (43.17%)]\t\tLoss: 0.19004\n",
      "Training Progress: \tEpoch 87 [4160/8883 (46.76%)]\t\tLoss: 0.44578\n",
      "Training Progress: \tEpoch 87 [4480/8883 (50.36%)]\t\tLoss: 0.44738\n",
      "Training Progress: \tEpoch 87 [4800/8883 (53.96%)]\t\tLoss: 0.22236\n",
      "Training Progress: \tEpoch 87 [5120/8883 (57.55%)]\t\tLoss: 0.67618\n",
      "Training Progress: \tEpoch 87 [5440/8883 (61.15%)]\t\tLoss: 0.82504\n",
      "Training Progress: \tEpoch 87 [5760/8883 (64.75%)]\t\tLoss: 0.75625\n",
      "Training Progress: \tEpoch 87 [6080/8883 (68.35%)]\t\tLoss: 0.46338\n",
      "Training Progress: \tEpoch 87 [6400/8883 (71.94%)]\t\tLoss: 0.54326\n",
      "Training Progress: \tEpoch 87 [6720/8883 (75.54%)]\t\tLoss: 0.56087\n",
      "Training Progress: \tEpoch 87 [7040/8883 (79.14%)]\t\tLoss: 0.45024\n",
      "Training Progress: \tEpoch 87 [7360/8883 (82.73%)]\t\tLoss: 0.50566\n",
      "Training Progress: \tEpoch 87 [7680/8883 (86.33%)]\t\tLoss: 0.46255\n",
      "Training Progress: \tEpoch 87 [8000/8883 (89.93%)]\t\tLoss: 0.38039\n",
      "Training Progress: \tEpoch 87 [8320/8883 (93.53%)]\t\tLoss: 0.42799\n",
      "Training Progress: \tEpoch 87 [8640/8883 (97.12%)]\t\tLoss: 0.39814\n",
      "\tTrain loss: 0.01228, Accuracy: 7114/8883 (80.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1485/1692 (87.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 697/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/8883 (0.00%)]\t\tLoss: 0.45964\n",
      "Training Progress: \tEpoch 88 [320/8883 (3.60%)]\t\tLoss: 0.50281\n",
      "Training Progress: \tEpoch 88 [640/8883 (7.19%)]\t\tLoss: 0.36111\n",
      "Training Progress: \tEpoch 88 [960/8883 (10.79%)]\t\tLoss: 0.32672\n",
      "Training Progress: \tEpoch 88 [1280/8883 (14.39%)]\t\tLoss: 0.70936\n",
      "Training Progress: \tEpoch 88 [1600/8883 (17.99%)]\t\tLoss: 0.77454\n",
      "Training Progress: \tEpoch 88 [1920/8883 (21.58%)]\t\tLoss: 0.67474\n",
      "Training Progress: \tEpoch 88 [2240/8883 (25.18%)]\t\tLoss: 0.46918\n",
      "Training Progress: \tEpoch 88 [2560/8883 (28.78%)]\t\tLoss: 0.43964\n",
      "Training Progress: \tEpoch 88 [2880/8883 (32.37%)]\t\tLoss: 0.44409\n",
      "Training Progress: \tEpoch 88 [3200/8883 (35.97%)]\t\tLoss: 0.72662\n",
      "Training Progress: \tEpoch 88 [3520/8883 (39.57%)]\t\tLoss: 0.46624\n",
      "Training Progress: \tEpoch 88 [3840/8883 (43.17%)]\t\tLoss: 0.24325\n",
      "Training Progress: \tEpoch 88 [4160/8883 (46.76%)]\t\tLoss: 0.59656\n",
      "Training Progress: \tEpoch 88 [4480/8883 (50.36%)]\t\tLoss: 0.40604\n",
      "Training Progress: \tEpoch 88 [4800/8883 (53.96%)]\t\tLoss: 0.37112\n",
      "Training Progress: \tEpoch 88 [5120/8883 (57.55%)]\t\tLoss: 0.49440\n",
      "Training Progress: \tEpoch 88 [5440/8883 (61.15%)]\t\tLoss: 0.54830\n",
      "Training Progress: \tEpoch 88 [5760/8883 (64.75%)]\t\tLoss: 0.64815\n",
      "Training Progress: \tEpoch 88 [6080/8883 (68.35%)]\t\tLoss: 0.48244\n",
      "Training Progress: \tEpoch 88 [6400/8883 (71.94%)]\t\tLoss: 0.52319\n",
      "Training Progress: \tEpoch 88 [6720/8883 (75.54%)]\t\tLoss: 0.53353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 88 [7040/8883 (79.14%)]\t\tLoss: 0.45755\n",
      "Training Progress: \tEpoch 88 [7360/8883 (82.73%)]\t\tLoss: 0.55686\n",
      "Training Progress: \tEpoch 88 [7680/8883 (86.33%)]\t\tLoss: 0.45537\n",
      "Training Progress: \tEpoch 88 [8000/8883 (89.93%)]\t\tLoss: 0.51581\n",
      "Training Progress: \tEpoch 88 [8320/8883 (93.53%)]\t\tLoss: 0.55201\n",
      "Training Progress: \tEpoch 88 [8640/8883 (97.12%)]\t\tLoss: 0.32544\n",
      "\tTrain loss: 0.01222, Accuracy: 7104/8883 (79.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 719/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/8883 (0.00%)]\t\tLoss: 0.42429\n",
      "Training Progress: \tEpoch 89 [320/8883 (3.60%)]\t\tLoss: 0.55765\n",
      "Training Progress: \tEpoch 89 [640/8883 (7.19%)]\t\tLoss: 0.33769\n",
      "Training Progress: \tEpoch 89 [960/8883 (10.79%)]\t\tLoss: 0.40357\n",
      "Training Progress: \tEpoch 89 [1280/8883 (14.39%)]\t\tLoss: 0.52245\n",
      "Training Progress: \tEpoch 89 [1600/8883 (17.99%)]\t\tLoss: 0.67935\n",
      "Training Progress: \tEpoch 89 [1920/8883 (21.58%)]\t\tLoss: 0.34956\n",
      "Training Progress: \tEpoch 89 [2240/8883 (25.18%)]\t\tLoss: 0.52619\n",
      "Training Progress: \tEpoch 89 [2560/8883 (28.78%)]\t\tLoss: 0.37762\n",
      "Training Progress: \tEpoch 89 [2880/8883 (32.37%)]\t\tLoss: 0.50527\n",
      "Training Progress: \tEpoch 89 [3200/8883 (35.97%)]\t\tLoss: 0.77102\n",
      "Training Progress: \tEpoch 89 [3520/8883 (39.57%)]\t\tLoss: 0.48316\n",
      "Training Progress: \tEpoch 89 [3840/8883 (43.17%)]\t\tLoss: 0.28430\n",
      "Training Progress: \tEpoch 89 [4160/8883 (46.76%)]\t\tLoss: 0.47539\n",
      "Training Progress: \tEpoch 89 [4480/8883 (50.36%)]\t\tLoss: 0.40128\n",
      "Training Progress: \tEpoch 89 [4800/8883 (53.96%)]\t\tLoss: 0.30249\n",
      "Training Progress: \tEpoch 89 [5120/8883 (57.55%)]\t\tLoss: 0.51527\n",
      "Training Progress: \tEpoch 89 [5440/8883 (61.15%)]\t\tLoss: 0.48059\n",
      "Training Progress: \tEpoch 89 [5760/8883 (64.75%)]\t\tLoss: 0.60209\n",
      "Training Progress: \tEpoch 89 [6080/8883 (68.35%)]\t\tLoss: 0.42978\n",
      "Training Progress: \tEpoch 89 [6400/8883 (71.94%)]\t\tLoss: 0.62916\n",
      "Training Progress: \tEpoch 89 [6720/8883 (75.54%)]\t\tLoss: 0.74276\n",
      "Training Progress: \tEpoch 89 [7040/8883 (79.14%)]\t\tLoss: 0.35449\n",
      "Training Progress: \tEpoch 89 [7360/8883 (82.73%)]\t\tLoss: 0.43269\n",
      "Training Progress: \tEpoch 89 [7680/8883 (86.33%)]\t\tLoss: 0.51764\n",
      "Training Progress: \tEpoch 89 [8000/8883 (89.93%)]\t\tLoss: 0.40037\n",
      "Training Progress: \tEpoch 89 [8320/8883 (93.53%)]\t\tLoss: 0.55402\n",
      "Training Progress: \tEpoch 89 [8640/8883 (97.12%)]\t\tLoss: 0.66165\n",
      "\tTrain loss: 0.01219, Accuracy: 7127/8883 (80.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 709/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/8883 (0.00%)]\t\tLoss: 0.49316\n",
      "Training Progress: \tEpoch 90 [320/8883 (3.60%)]\t\tLoss: 0.55361\n",
      "Training Progress: \tEpoch 90 [640/8883 (7.19%)]\t\tLoss: 0.36682\n",
      "Training Progress: \tEpoch 90 [960/8883 (10.79%)]\t\tLoss: 0.35010\n",
      "Training Progress: \tEpoch 90 [1280/8883 (14.39%)]\t\tLoss: 0.48783\n",
      "Training Progress: \tEpoch 90 [1600/8883 (17.99%)]\t\tLoss: 0.65372\n",
      "Training Progress: \tEpoch 90 [1920/8883 (21.58%)]\t\tLoss: 0.34435\n",
      "Training Progress: \tEpoch 90 [2240/8883 (25.18%)]\t\tLoss: 0.37144\n",
      "Training Progress: \tEpoch 90 [2560/8883 (28.78%)]\t\tLoss: 0.44061\n",
      "Training Progress: \tEpoch 90 [2880/8883 (32.37%)]\t\tLoss: 0.44942\n",
      "Training Progress: \tEpoch 90 [3200/8883 (35.97%)]\t\tLoss: 0.58816\n",
      "Training Progress: \tEpoch 90 [3520/8883 (39.57%)]\t\tLoss: 0.38436\n",
      "Training Progress: \tEpoch 90 [3840/8883 (43.17%)]\t\tLoss: 0.19534\n",
      "Training Progress: \tEpoch 90 [4160/8883 (46.76%)]\t\tLoss: 0.46185\n",
      "Training Progress: \tEpoch 90 [4480/8883 (50.36%)]\t\tLoss: 0.60123\n",
      "Training Progress: \tEpoch 90 [4800/8883 (53.96%)]\t\tLoss: 0.28742\n",
      "Training Progress: \tEpoch 90 [5120/8883 (57.55%)]\t\tLoss: 0.52680\n",
      "Training Progress: \tEpoch 90 [5440/8883 (61.15%)]\t\tLoss: 0.48408\n",
      "Training Progress: \tEpoch 90 [5760/8883 (64.75%)]\t\tLoss: 0.57127\n",
      "Training Progress: \tEpoch 90 [6080/8883 (68.35%)]\t\tLoss: 0.50028\n",
      "Training Progress: \tEpoch 90 [6400/8883 (71.94%)]\t\tLoss: 0.63114\n",
      "Training Progress: \tEpoch 90 [6720/8883 (75.54%)]\t\tLoss: 0.96966\n",
      "Training Progress: \tEpoch 90 [7040/8883 (79.14%)]\t\tLoss: 0.46007\n",
      "Training Progress: \tEpoch 90 [7360/8883 (82.73%)]\t\tLoss: 0.44619\n",
      "Training Progress: \tEpoch 90 [7680/8883 (86.33%)]\t\tLoss: 0.42559\n",
      "Training Progress: \tEpoch 90 [8000/8883 (89.93%)]\t\tLoss: 0.38034\n",
      "Training Progress: \tEpoch 90 [8320/8883 (93.53%)]\t\tLoss: 0.60824\n",
      "Training Progress: \tEpoch 90 [8640/8883 (97.12%)]\t\tLoss: 0.36638\n",
      "\tTrain loss: 0.01169, Accuracy: 7184/8883 (80.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1521/1692 (89.00%)\n",
      "\tTest loss: 0.00177, Accuracy: 707/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/8883 (0.00%)]\t\tLoss: 0.45666\n",
      "Training Progress: \tEpoch 91 [320/8883 (3.60%)]\t\tLoss: 0.49858\n",
      "Training Progress: \tEpoch 91 [640/8883 (7.19%)]\t\tLoss: 0.30763\n",
      "Training Progress: \tEpoch 91 [960/8883 (10.79%)]\t\tLoss: 0.37321\n",
      "Training Progress: \tEpoch 91 [1280/8883 (14.39%)]\t\tLoss: 0.48901\n",
      "Training Progress: \tEpoch 91 [1600/8883 (17.99%)]\t\tLoss: 0.66186\n",
      "Training Progress: \tEpoch 91 [1920/8883 (21.58%)]\t\tLoss: 0.49291\n",
      "Training Progress: \tEpoch 91 [2240/8883 (25.18%)]\t\tLoss: 0.42155\n",
      "Training Progress: \tEpoch 91 [2560/8883 (28.78%)]\t\tLoss: 0.32692\n",
      "Training Progress: \tEpoch 91 [2880/8883 (32.37%)]\t\tLoss: 0.40888\n",
      "Training Progress: \tEpoch 91 [3200/8883 (35.97%)]\t\tLoss: 0.75054\n",
      "Training Progress: \tEpoch 91 [3520/8883 (39.57%)]\t\tLoss: 0.39448\n",
      "Training Progress: \tEpoch 91 [3840/8883 (43.17%)]\t\tLoss: 0.30367\n",
      "Training Progress: \tEpoch 91 [4160/8883 (46.76%)]\t\tLoss: 0.43142\n",
      "Training Progress: \tEpoch 91 [4480/8883 (50.36%)]\t\tLoss: 0.31811\n",
      "Training Progress: \tEpoch 91 [4800/8883 (53.96%)]\t\tLoss: 0.31177\n",
      "Training Progress: \tEpoch 91 [5120/8883 (57.55%)]\t\tLoss: 0.75893\n",
      "Training Progress: \tEpoch 91 [5440/8883 (61.15%)]\t\tLoss: 0.57542\n",
      "Training Progress: \tEpoch 91 [5760/8883 (64.75%)]\t\tLoss: 0.64377\n",
      "Training Progress: \tEpoch 91 [6080/8883 (68.35%)]\t\tLoss: 0.46676\n",
      "Training Progress: \tEpoch 91 [6400/8883 (71.94%)]\t\tLoss: 0.50241\n",
      "Training Progress: \tEpoch 91 [6720/8883 (75.54%)]\t\tLoss: 0.53934\n",
      "Training Progress: \tEpoch 91 [7040/8883 (79.14%)]\t\tLoss: 0.35735\n",
      "Training Progress: \tEpoch 91 [7360/8883 (82.73%)]\t\tLoss: 0.45556\n",
      "Training Progress: \tEpoch 91 [7680/8883 (86.33%)]\t\tLoss: 0.53279\n",
      "Training Progress: \tEpoch 91 [8000/8883 (89.93%)]\t\tLoss: 0.38731\n",
      "Training Progress: \tEpoch 91 [8320/8883 (93.53%)]\t\tLoss: 0.52727\n",
      "Training Progress: \tEpoch 91 [8640/8883 (97.12%)]\t\tLoss: 0.36364\n",
      "\tTrain loss: 0.01156, Accuracy: 7199/8883 (81.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/8883 (0.00%)]\t\tLoss: 0.33159\n",
      "Training Progress: \tEpoch 92 [320/8883 (3.60%)]\t\tLoss: 0.47894\n",
      "Training Progress: \tEpoch 92 [640/8883 (7.19%)]\t\tLoss: 0.35594\n",
      "Training Progress: \tEpoch 92 [960/8883 (10.79%)]\t\tLoss: 0.55992\n",
      "Training Progress: \tEpoch 92 [1280/8883 (14.39%)]\t\tLoss: 0.51136\n",
      "Training Progress: \tEpoch 92 [1600/8883 (17.99%)]\t\tLoss: 0.67364\n",
      "Training Progress: \tEpoch 92 [1920/8883 (21.58%)]\t\tLoss: 0.53921\n",
      "Training Progress: \tEpoch 92 [2240/8883 (25.18%)]\t\tLoss: 0.53076\n",
      "Training Progress: \tEpoch 92 [2560/8883 (28.78%)]\t\tLoss: 0.44720\n",
      "Training Progress: \tEpoch 92 [2880/8883 (32.37%)]\t\tLoss: 0.48685\n",
      "Training Progress: \tEpoch 92 [3200/8883 (35.97%)]\t\tLoss: 0.60077\n",
      "Training Progress: \tEpoch 92 [3520/8883 (39.57%)]\t\tLoss: 0.57435\n",
      "Training Progress: \tEpoch 92 [3840/8883 (43.17%)]\t\tLoss: 0.21845\n",
      "Training Progress: \tEpoch 92 [4160/8883 (46.76%)]\t\tLoss: 0.46181\n",
      "Training Progress: \tEpoch 92 [4480/8883 (50.36%)]\t\tLoss: 0.66084\n",
      "Training Progress: \tEpoch 92 [4800/8883 (53.96%)]\t\tLoss: 0.22959\n",
      "Training Progress: \tEpoch 92 [5120/8883 (57.55%)]\t\tLoss: 0.49827\n",
      "Training Progress: \tEpoch 92 [5440/8883 (61.15%)]\t\tLoss: 0.45546\n",
      "Training Progress: \tEpoch 92 [5760/8883 (64.75%)]\t\tLoss: 0.61115\n",
      "Training Progress: \tEpoch 92 [6080/8883 (68.35%)]\t\tLoss: 0.53366\n",
      "Training Progress: \tEpoch 92 [6400/8883 (71.94%)]\t\tLoss: 0.59012\n",
      "Training Progress: \tEpoch 92 [6720/8883 (75.54%)]\t\tLoss: 0.56841\n",
      "Training Progress: \tEpoch 92 [7040/8883 (79.14%)]\t\tLoss: 0.40493\n",
      "Training Progress: \tEpoch 92 [7360/8883 (82.73%)]\t\tLoss: 0.41767\n",
      "Training Progress: \tEpoch 92 [7680/8883 (86.33%)]\t\tLoss: 0.62451\n",
      "Training Progress: \tEpoch 92 [8000/8883 (89.93%)]\t\tLoss: 0.43348\n",
      "Training Progress: \tEpoch 92 [8320/8883 (93.53%)]\t\tLoss: 0.51942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 92 [8640/8883 (97.12%)]\t\tLoss: 0.51457\n",
      "\tTrain loss: 0.01206, Accuracy: 7123/8883 (80.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1528/1692 (90.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 744/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/8883 (0.00%)]\t\tLoss: 0.34572\n",
      "Training Progress: \tEpoch 93 [320/8883 (3.60%)]\t\tLoss: 0.50185\n",
      "Training Progress: \tEpoch 93 [640/8883 (7.19%)]\t\tLoss: 0.31781\n",
      "Training Progress: \tEpoch 93 [960/8883 (10.79%)]\t\tLoss: 0.29088\n",
      "Training Progress: \tEpoch 93 [1280/8883 (14.39%)]\t\tLoss: 0.51448\n",
      "Training Progress: \tEpoch 93 [1600/8883 (17.99%)]\t\tLoss: 0.78904\n",
      "Training Progress: \tEpoch 93 [1920/8883 (21.58%)]\t\tLoss: 0.35398\n",
      "Training Progress: \tEpoch 93 [2240/8883 (25.18%)]\t\tLoss: 0.43037\n",
      "Training Progress: \tEpoch 93 [2560/8883 (28.78%)]\t\tLoss: 0.43439\n",
      "Training Progress: \tEpoch 93 [2880/8883 (32.37%)]\t\tLoss: 0.35214\n",
      "Training Progress: \tEpoch 93 [3200/8883 (35.97%)]\t\tLoss: 0.68376\n",
      "Training Progress: \tEpoch 93 [3520/8883 (39.57%)]\t\tLoss: 0.43826\n",
      "Training Progress: \tEpoch 93 [3840/8883 (43.17%)]\t\tLoss: 0.17496\n",
      "Training Progress: \tEpoch 93 [4160/8883 (46.76%)]\t\tLoss: 0.43988\n",
      "Training Progress: \tEpoch 93 [4480/8883 (50.36%)]\t\tLoss: 0.45142\n",
      "Training Progress: \tEpoch 93 [4800/8883 (53.96%)]\t\tLoss: 0.53994\n",
      "Training Progress: \tEpoch 93 [5120/8883 (57.55%)]\t\tLoss: 0.56590\n",
      "Training Progress: \tEpoch 93 [5440/8883 (61.15%)]\t\tLoss: 0.56844\n",
      "Training Progress: \tEpoch 93 [5760/8883 (64.75%)]\t\tLoss: 0.50085\n",
      "Training Progress: \tEpoch 93 [6080/8883 (68.35%)]\t\tLoss: 0.55172\n",
      "Training Progress: \tEpoch 93 [6400/8883 (71.94%)]\t\tLoss: 0.66065\n",
      "Training Progress: \tEpoch 93 [6720/8883 (75.54%)]\t\tLoss: 0.75812\n",
      "Training Progress: \tEpoch 93 [7040/8883 (79.14%)]\t\tLoss: 0.33688\n",
      "Training Progress: \tEpoch 93 [7360/8883 (82.73%)]\t\tLoss: 0.52980\n",
      "Training Progress: \tEpoch 93 [7680/8883 (86.33%)]\t\tLoss: 0.45394\n",
      "Training Progress: \tEpoch 93 [8000/8883 (89.93%)]\t\tLoss: 0.42502\n",
      "Training Progress: \tEpoch 93 [8320/8883 (93.53%)]\t\tLoss: 0.52282\n",
      "Training Progress: \tEpoch 93 [8640/8883 (97.12%)]\t\tLoss: 0.66421\n",
      "\tTrain loss: 0.01201, Accuracy: 7141/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1515/1692 (89.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 733/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/8883 (0.00%)]\t\tLoss: 0.34120\n",
      "Training Progress: \tEpoch 94 [320/8883 (3.60%)]\t\tLoss: 0.51867\n",
      "Training Progress: \tEpoch 94 [640/8883 (7.19%)]\t\tLoss: 0.34390\n",
      "Training Progress: \tEpoch 94 [960/8883 (10.79%)]\t\tLoss: 0.30267\n",
      "Training Progress: \tEpoch 94 [1280/8883 (14.39%)]\t\tLoss: 0.53465\n",
      "Training Progress: \tEpoch 94 [1600/8883 (17.99%)]\t\tLoss: 0.56770\n",
      "Training Progress: \tEpoch 94 [1920/8883 (21.58%)]\t\tLoss: 0.52435\n",
      "Training Progress: \tEpoch 94 [2240/8883 (25.18%)]\t\tLoss: 0.41729\n",
      "Training Progress: \tEpoch 94 [2560/8883 (28.78%)]\t\tLoss: 0.55982\n",
      "Training Progress: \tEpoch 94 [2880/8883 (32.37%)]\t\tLoss: 0.38878\n",
      "Training Progress: \tEpoch 94 [3200/8883 (35.97%)]\t\tLoss: 0.63540\n",
      "Training Progress: \tEpoch 94 [3520/8883 (39.57%)]\t\tLoss: 0.77123\n",
      "Training Progress: \tEpoch 94 [3840/8883 (43.17%)]\t\tLoss: 0.20460\n",
      "Training Progress: \tEpoch 94 [4160/8883 (46.76%)]\t\tLoss: 0.55129\n",
      "Training Progress: \tEpoch 94 [4480/8883 (50.36%)]\t\tLoss: 0.36551\n",
      "Training Progress: \tEpoch 94 [4800/8883 (53.96%)]\t\tLoss: 0.28155\n",
      "Training Progress: \tEpoch 94 [5120/8883 (57.55%)]\t\tLoss: 0.55325\n",
      "Training Progress: \tEpoch 94 [5440/8883 (61.15%)]\t\tLoss: 0.47700\n",
      "Training Progress: \tEpoch 94 [5760/8883 (64.75%)]\t\tLoss: 0.68372\n",
      "Training Progress: \tEpoch 94 [6080/8883 (68.35%)]\t\tLoss: 0.45780\n",
      "Training Progress: \tEpoch 94 [6400/8883 (71.94%)]\t\tLoss: 0.63593\n",
      "Training Progress: \tEpoch 94 [6720/8883 (75.54%)]\t\tLoss: 0.53753\n",
      "Training Progress: \tEpoch 94 [7040/8883 (79.14%)]\t\tLoss: 0.50175\n",
      "Training Progress: \tEpoch 94 [7360/8883 (82.73%)]\t\tLoss: 0.45059\n",
      "Training Progress: \tEpoch 94 [7680/8883 (86.33%)]\t\tLoss: 0.39981\n",
      "Training Progress: \tEpoch 94 [8000/8883 (89.93%)]\t\tLoss: 0.54173\n",
      "Training Progress: \tEpoch 94 [8320/8883 (93.53%)]\t\tLoss: 0.42185\n",
      "Training Progress: \tEpoch 94 [8640/8883 (97.12%)]\t\tLoss: 0.37536\n",
      "\tTrain loss: 0.01324, Accuracy: 7029/8883 (79.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1472/1692 (86.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 711/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/8883 (0.00%)]\t\tLoss: 0.44897\n",
      "Training Progress: \tEpoch 95 [320/8883 (3.60%)]\t\tLoss: 0.55225\n",
      "Training Progress: \tEpoch 95 [640/8883 (7.19%)]\t\tLoss: 0.42824\n",
      "Training Progress: \tEpoch 95 [960/8883 (10.79%)]\t\tLoss: 0.35904\n",
      "Training Progress: \tEpoch 95 [1280/8883 (14.39%)]\t\tLoss: 0.59892\n",
      "Training Progress: \tEpoch 95 [1600/8883 (17.99%)]\t\tLoss: 0.60576\n",
      "Training Progress: \tEpoch 95 [1920/8883 (21.58%)]\t\tLoss: 0.41950\n",
      "Training Progress: \tEpoch 95 [2240/8883 (25.18%)]\t\tLoss: 0.36674\n",
      "Training Progress: \tEpoch 95 [2560/8883 (28.78%)]\t\tLoss: 0.52684\n",
      "Training Progress: \tEpoch 95 [2880/8883 (32.37%)]\t\tLoss: 0.48899\n",
      "Training Progress: \tEpoch 95 [3200/8883 (35.97%)]\t\tLoss: 0.67360\n",
      "Training Progress: \tEpoch 95 [3520/8883 (39.57%)]\t\tLoss: 0.50236\n",
      "Training Progress: \tEpoch 95 [3840/8883 (43.17%)]\t\tLoss: 0.16176\n",
      "Training Progress: \tEpoch 95 [4160/8883 (46.76%)]\t\tLoss: 0.70054\n",
      "Training Progress: \tEpoch 95 [4480/8883 (50.36%)]\t\tLoss: 0.40049\n",
      "Training Progress: \tEpoch 95 [4800/8883 (53.96%)]\t\tLoss: 0.36109\n",
      "Training Progress: \tEpoch 95 [5120/8883 (57.55%)]\t\tLoss: 0.55854\n",
      "Training Progress: \tEpoch 95 [5440/8883 (61.15%)]\t\tLoss: 0.51831\n",
      "Training Progress: \tEpoch 95 [5760/8883 (64.75%)]\t\tLoss: 0.61831\n",
      "Training Progress: \tEpoch 95 [6080/8883 (68.35%)]\t\tLoss: 0.57554\n",
      "Training Progress: \tEpoch 95 [6400/8883 (71.94%)]\t\tLoss: 0.65098\n",
      "Training Progress: \tEpoch 95 [6720/8883 (75.54%)]\t\tLoss: 0.59478\n",
      "Training Progress: \tEpoch 95 [7040/8883 (79.14%)]\t\tLoss: 0.63509\n",
      "Training Progress: \tEpoch 95 [7360/8883 (82.73%)]\t\tLoss: 0.49191\n",
      "Training Progress: \tEpoch 95 [7680/8883 (86.33%)]\t\tLoss: 0.48829\n",
      "Training Progress: \tEpoch 95 [8000/8883 (89.93%)]\t\tLoss: 0.37681\n",
      "Training Progress: \tEpoch 95 [8320/8883 (93.53%)]\t\tLoss: 0.52383\n",
      "Training Progress: \tEpoch 95 [8640/8883 (97.12%)]\t\tLoss: 0.46992\n",
      "\tTrain loss: 0.01198, Accuracy: 7150/8883 (80.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1525/1692 (90.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 715/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/8883 (0.00%)]\t\tLoss: 0.39818\n",
      "Training Progress: \tEpoch 96 [320/8883 (3.60%)]\t\tLoss: 0.50069\n",
      "Training Progress: \tEpoch 96 [640/8883 (7.19%)]\t\tLoss: 0.29140\n",
      "Training Progress: \tEpoch 96 [960/8883 (10.79%)]\t\tLoss: 0.34187\n",
      "Training Progress: \tEpoch 96 [1280/8883 (14.39%)]\t\tLoss: 0.58812\n",
      "Training Progress: \tEpoch 96 [1600/8883 (17.99%)]\t\tLoss: 0.60466\n",
      "Training Progress: \tEpoch 96 [1920/8883 (21.58%)]\t\tLoss: 0.33770\n",
      "Training Progress: \tEpoch 96 [2240/8883 (25.18%)]\t\tLoss: 0.41801\n",
      "Training Progress: \tEpoch 96 [2560/8883 (28.78%)]\t\tLoss: 0.51516\n",
      "Training Progress: \tEpoch 96 [2880/8883 (32.37%)]\t\tLoss: 0.41537\n",
      "Training Progress: \tEpoch 96 [3200/8883 (35.97%)]\t\tLoss: 0.56766\n",
      "Training Progress: \tEpoch 96 [3520/8883 (39.57%)]\t\tLoss: 0.49761\n",
      "Training Progress: \tEpoch 96 [3840/8883 (43.17%)]\t\tLoss: 0.19730\n",
      "Training Progress: \tEpoch 96 [4160/8883 (46.76%)]\t\tLoss: 0.45610\n",
      "Training Progress: \tEpoch 96 [4480/8883 (50.36%)]\t\tLoss: 0.37193\n",
      "Training Progress: \tEpoch 96 [4800/8883 (53.96%)]\t\tLoss: 0.35959\n",
      "Training Progress: \tEpoch 96 [5120/8883 (57.55%)]\t\tLoss: 0.56186\n",
      "Training Progress: \tEpoch 96 [5440/8883 (61.15%)]\t\tLoss: 0.72411\n",
      "Training Progress: \tEpoch 96 [5760/8883 (64.75%)]\t\tLoss: 0.54007\n",
      "Training Progress: \tEpoch 96 [6080/8883 (68.35%)]\t\tLoss: 0.52168\n",
      "Training Progress: \tEpoch 96 [6400/8883 (71.94%)]\t\tLoss: 0.53470\n",
      "Training Progress: \tEpoch 96 [6720/8883 (75.54%)]\t\tLoss: 0.53596\n",
      "Training Progress: \tEpoch 96 [7040/8883 (79.14%)]\t\tLoss: 0.44267\n",
      "Training Progress: \tEpoch 96 [7360/8883 (82.73%)]\t\tLoss: 0.50082\n",
      "Training Progress: \tEpoch 96 [7680/8883 (86.33%)]\t\tLoss: 0.64367\n",
      "Training Progress: \tEpoch 96 [8000/8883 (89.93%)]\t\tLoss: 0.35694\n",
      "Training Progress: \tEpoch 96 [8320/8883 (93.53%)]\t\tLoss: 0.80900\n",
      "Training Progress: \tEpoch 96 [8640/8883 (97.12%)]\t\tLoss: 0.42848\n",
      "\tTrain loss: 0.01178, Accuracy: 7160/8883 (80.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1520/1692 (89.00%)\n",
      "\tTest loss: 0.00180, Accuracy: 729/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/8883 (0.00%)]\t\tLoss: 0.34514\n",
      "Training Progress: \tEpoch 97 [320/8883 (3.60%)]\t\tLoss: 0.49968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 97 [640/8883 (7.19%)]\t\tLoss: 0.30531\n",
      "Training Progress: \tEpoch 97 [960/8883 (10.79%)]\t\tLoss: 0.46824\n",
      "Training Progress: \tEpoch 97 [1280/8883 (14.39%)]\t\tLoss: 0.57689\n",
      "Training Progress: \tEpoch 97 [1600/8883 (17.99%)]\t\tLoss: 0.71254\n",
      "Training Progress: \tEpoch 97 [1920/8883 (21.58%)]\t\tLoss: 0.44056\n",
      "Training Progress: \tEpoch 97 [2240/8883 (25.18%)]\t\tLoss: 0.50563\n",
      "Training Progress: \tEpoch 97 [2560/8883 (28.78%)]\t\tLoss: 0.43628\n",
      "Training Progress: \tEpoch 97 [2880/8883 (32.37%)]\t\tLoss: 0.42786\n",
      "Training Progress: \tEpoch 97 [3200/8883 (35.97%)]\t\tLoss: 0.64007\n",
      "Training Progress: \tEpoch 97 [3520/8883 (39.57%)]\t\tLoss: 0.77046\n",
      "Training Progress: \tEpoch 97 [3840/8883 (43.17%)]\t\tLoss: 0.27247\n",
      "Training Progress: \tEpoch 97 [4160/8883 (46.76%)]\t\tLoss: 0.45267\n",
      "Training Progress: \tEpoch 97 [4480/8883 (50.36%)]\t\tLoss: 0.42441\n",
      "Training Progress: \tEpoch 97 [4800/8883 (53.96%)]\t\tLoss: 0.39397\n",
      "Training Progress: \tEpoch 97 [5120/8883 (57.55%)]\t\tLoss: 0.49537\n",
      "Training Progress: \tEpoch 97 [5440/8883 (61.15%)]\t\tLoss: 0.56357\n",
      "Training Progress: \tEpoch 97 [5760/8883 (64.75%)]\t\tLoss: 0.60949\n",
      "Training Progress: \tEpoch 97 [6080/8883 (68.35%)]\t\tLoss: 0.44559\n",
      "Training Progress: \tEpoch 97 [6400/8883 (71.94%)]\t\tLoss: 0.66414\n",
      "Training Progress: \tEpoch 97 [6720/8883 (75.54%)]\t\tLoss: 0.57582\n",
      "Training Progress: \tEpoch 97 [7040/8883 (79.14%)]\t\tLoss: 0.39387\n",
      "Training Progress: \tEpoch 97 [7360/8883 (82.73%)]\t\tLoss: 0.46104\n",
      "Training Progress: \tEpoch 97 [7680/8883 (86.33%)]\t\tLoss: 0.48669\n",
      "Training Progress: \tEpoch 97 [8000/8883 (89.93%)]\t\tLoss: 0.39622\n",
      "Training Progress: \tEpoch 97 [8320/8883 (93.53%)]\t\tLoss: 0.52223\n",
      "Training Progress: \tEpoch 97 [8640/8883 (97.12%)]\t\tLoss: 0.48511\n",
      "\tTrain loss: 0.01178, Accuracy: 7149/8883 (80.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1551/1692 (91.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 726/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/8883 (0.00%)]\t\tLoss: 0.34099\n",
      "Training Progress: \tEpoch 98 [320/8883 (3.60%)]\t\tLoss: 0.53200\n",
      "Training Progress: \tEpoch 98 [640/8883 (7.19%)]\t\tLoss: 0.29793\n",
      "Training Progress: \tEpoch 98 [960/8883 (10.79%)]\t\tLoss: 0.33119\n",
      "Training Progress: \tEpoch 98 [1280/8883 (14.39%)]\t\tLoss: 0.49686\n",
      "Training Progress: \tEpoch 98 [1600/8883 (17.99%)]\t\tLoss: 0.54078\n",
      "Training Progress: \tEpoch 98 [1920/8883 (21.58%)]\t\tLoss: 0.50542\n",
      "Training Progress: \tEpoch 98 [2240/8883 (25.18%)]\t\tLoss: 0.39911\n",
      "Training Progress: \tEpoch 98 [2560/8883 (28.78%)]\t\tLoss: 0.52693\n",
      "Training Progress: \tEpoch 98 [2880/8883 (32.37%)]\t\tLoss: 0.46546\n",
      "Training Progress: \tEpoch 98 [3200/8883 (35.97%)]\t\tLoss: 0.63585\n",
      "Training Progress: \tEpoch 98 [3520/8883 (39.57%)]\t\tLoss: 0.54562\n",
      "Training Progress: \tEpoch 98 [3840/8883 (43.17%)]\t\tLoss: 0.31737\n",
      "Training Progress: \tEpoch 98 [4160/8883 (46.76%)]\t\tLoss: 0.42550\n",
      "Training Progress: \tEpoch 98 [4480/8883 (50.36%)]\t\tLoss: 0.45280\n",
      "Training Progress: \tEpoch 98 [4800/8883 (53.96%)]\t\tLoss: 0.20689\n",
      "Training Progress: \tEpoch 98 [5120/8883 (57.55%)]\t\tLoss: 0.52278\n",
      "Training Progress: \tEpoch 98 [5440/8883 (61.15%)]\t\tLoss: 0.53222\n",
      "Training Progress: \tEpoch 98 [5760/8883 (64.75%)]\t\tLoss: 0.58434\n",
      "Training Progress: \tEpoch 98 [6080/8883 (68.35%)]\t\tLoss: 0.46629\n",
      "Training Progress: \tEpoch 98 [6400/8883 (71.94%)]\t\tLoss: 0.68892\n",
      "Training Progress: \tEpoch 98 [6720/8883 (75.54%)]\t\tLoss: 0.52200\n",
      "Training Progress: \tEpoch 98 [7040/8883 (79.14%)]\t\tLoss: 0.33393\n",
      "Training Progress: \tEpoch 98 [7360/8883 (82.73%)]\t\tLoss: 0.65049\n",
      "Training Progress: \tEpoch 98 [7680/8883 (86.33%)]\t\tLoss: 0.46069\n",
      "Training Progress: \tEpoch 98 [8000/8883 (89.93%)]\t\tLoss: 0.52833\n",
      "Training Progress: \tEpoch 98 [8320/8883 (93.53%)]\t\tLoss: 0.51869\n",
      "Training Progress: \tEpoch 98 [8640/8883 (97.12%)]\t\tLoss: 0.36775\n",
      "\tTrain loss: 0.01199, Accuracy: 7139/8883 (80.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1532/1692 (90.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 723/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/8883 (0.00%)]\t\tLoss: 0.32806\n",
      "Training Progress: \tEpoch 99 [320/8883 (3.60%)]\t\tLoss: 0.53035\n",
      "Training Progress: \tEpoch 99 [640/8883 (7.19%)]\t\tLoss: 0.28459\n",
      "Training Progress: \tEpoch 99 [960/8883 (10.79%)]\t\tLoss: 0.44715\n",
      "Training Progress: \tEpoch 99 [1280/8883 (14.39%)]\t\tLoss: 0.54890\n",
      "Training Progress: \tEpoch 99 [1600/8883 (17.99%)]\t\tLoss: 0.54888\n",
      "Training Progress: \tEpoch 99 [1920/8883 (21.58%)]\t\tLoss: 0.44908\n",
      "Training Progress: \tEpoch 99 [2240/8883 (25.18%)]\t\tLoss: 0.36365\n",
      "Training Progress: \tEpoch 99 [2560/8883 (28.78%)]\t\tLoss: 0.59118\n",
      "Training Progress: \tEpoch 99 [2880/8883 (32.37%)]\t\tLoss: 0.32990\n",
      "Training Progress: \tEpoch 99 [3200/8883 (35.97%)]\t\tLoss: 0.79968\n",
      "Training Progress: \tEpoch 99 [3520/8883 (39.57%)]\t\tLoss: 0.33585\n",
      "Training Progress: \tEpoch 99 [3840/8883 (43.17%)]\t\tLoss: 0.28608\n",
      "Training Progress: \tEpoch 99 [4160/8883 (46.76%)]\t\tLoss: 0.42971\n",
      "Training Progress: \tEpoch 99 [4480/8883 (50.36%)]\t\tLoss: 0.40689\n",
      "Training Progress: \tEpoch 99 [4800/8883 (53.96%)]\t\tLoss: 0.51343\n",
      "Training Progress: \tEpoch 99 [5120/8883 (57.55%)]\t\tLoss: 0.49736\n",
      "Training Progress: \tEpoch 99 [5440/8883 (61.15%)]\t\tLoss: 0.52989\n",
      "Training Progress: \tEpoch 99 [5760/8883 (64.75%)]\t\tLoss: 0.90340\n",
      "Training Progress: \tEpoch 99 [6080/8883 (68.35%)]\t\tLoss: 0.64585\n",
      "Training Progress: \tEpoch 99 [6400/8883 (71.94%)]\t\tLoss: 0.54804\n",
      "Training Progress: \tEpoch 99 [6720/8883 (75.54%)]\t\tLoss: 0.57205\n",
      "Training Progress: \tEpoch 99 [7040/8883 (79.14%)]\t\tLoss: 0.34104\n",
      "Training Progress: \tEpoch 99 [7360/8883 (82.73%)]\t\tLoss: 0.42383\n",
      "Training Progress: \tEpoch 99 [7680/8883 (86.33%)]\t\tLoss: 0.42507\n",
      "Training Progress: \tEpoch 99 [8000/8883 (89.93%)]\t\tLoss: 0.39492\n",
      "Training Progress: \tEpoch 99 [8320/8883 (93.53%)]\t\tLoss: 0.56322\n",
      "Training Progress: \tEpoch 99 [8640/8883 (97.12%)]\t\tLoss: 0.51383\n",
      "\tTrain loss: 0.01189, Accuracy: 7162/8883 (80.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1535/1692 (90.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 704/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/8883 (0.00%)]\t\tLoss: 0.40536\n",
      "Training Progress: \tEpoch 100 [320/8883 (3.60%)]\t\tLoss: 0.49227\n",
      "Training Progress: \tEpoch 100 [640/8883 (7.19%)]\t\tLoss: 0.50103\n",
      "Training Progress: \tEpoch 100 [960/8883 (10.79%)]\t\tLoss: 0.34803\n",
      "Training Progress: \tEpoch 100 [1280/8883 (14.39%)]\t\tLoss: 0.49079\n",
      "Training Progress: \tEpoch 100 [1600/8883 (17.99%)]\t\tLoss: 0.65904\n",
      "Training Progress: \tEpoch 100 [1920/8883 (21.58%)]\t\tLoss: 0.38439\n",
      "Training Progress: \tEpoch 100 [2240/8883 (25.18%)]\t\tLoss: 0.44299\n",
      "Training Progress: \tEpoch 100 [2560/8883 (28.78%)]\t\tLoss: 0.41574\n",
      "Training Progress: \tEpoch 100 [2880/8883 (32.37%)]\t\tLoss: 0.32952\n",
      "Training Progress: \tEpoch 100 [3200/8883 (35.97%)]\t\tLoss: 0.70676\n",
      "Training Progress: \tEpoch 100 [3520/8883 (39.57%)]\t\tLoss: 0.48639\n",
      "Training Progress: \tEpoch 100 [3840/8883 (43.17%)]\t\tLoss: 0.27973\n",
      "Training Progress: \tEpoch 100 [4160/8883 (46.76%)]\t\tLoss: 0.59454\n",
      "Training Progress: \tEpoch 100 [4480/8883 (50.36%)]\t\tLoss: 0.41634\n",
      "Training Progress: \tEpoch 100 [4800/8883 (53.96%)]\t\tLoss: 0.38423\n",
      "Training Progress: \tEpoch 100 [5120/8883 (57.55%)]\t\tLoss: 0.67717\n",
      "Training Progress: \tEpoch 100 [5440/8883 (61.15%)]\t\tLoss: 0.45595\n",
      "Training Progress: \tEpoch 100 [5760/8883 (64.75%)]\t\tLoss: 0.55446\n",
      "Training Progress: \tEpoch 100 [6080/8883 (68.35%)]\t\tLoss: 0.49246\n",
      "Training Progress: \tEpoch 100 [6400/8883 (71.94%)]\t\tLoss: 0.72119\n",
      "Training Progress: \tEpoch 100 [6720/8883 (75.54%)]\t\tLoss: 0.60988\n",
      "Training Progress: \tEpoch 100 [7040/8883 (79.14%)]\t\tLoss: 0.47232\n",
      "Training Progress: \tEpoch 100 [7360/8883 (82.73%)]\t\tLoss: 0.48169\n",
      "Training Progress: \tEpoch 100 [7680/8883 (86.33%)]\t\tLoss: 0.48618\n",
      "Training Progress: \tEpoch 100 [8000/8883 (89.93%)]\t\tLoss: 0.44965\n",
      "Training Progress: \tEpoch 100 [8320/8883 (93.53%)]\t\tLoss: 0.39752\n",
      "Training Progress: \tEpoch 100 [8640/8883 (97.12%)]\t\tLoss: 0.31227\n",
      "\tTrain loss: 0.01165, Accuracy: 7180/8883 (80.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1535/1692 (90.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 731/1772 (41.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9166666666666666\n",
      "Best test accuracy:\n",
      "0.4204288939051919\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(gru, gru_optimizer, data_loaders_wgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7dc10",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "312a3710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/8883 (0.00%)]\t\tLoss: 1.42451\n",
      "Training Progress: \tEpoch 1 [320/8883 (3.60%)]\t\tLoss: 1.47733\n",
      "Training Progress: \tEpoch 1 [640/8883 (7.19%)]\t\tLoss: 1.46839\n",
      "Training Progress: \tEpoch 1 [960/8883 (10.79%)]\t\tLoss: 1.39826\n",
      "Training Progress: \tEpoch 1 [1280/8883 (14.39%)]\t\tLoss: 1.38385\n",
      "Training Progress: \tEpoch 1 [1600/8883 (17.99%)]\t\tLoss: 1.40421\n",
      "Training Progress: \tEpoch 1 [1920/8883 (21.58%)]\t\tLoss: 1.40607\n",
      "Training Progress: \tEpoch 1 [2240/8883 (25.18%)]\t\tLoss: 1.49024\n",
      "Training Progress: \tEpoch 1 [2560/8883 (28.78%)]\t\tLoss: 1.42814\n",
      "Training Progress: \tEpoch 1 [2880/8883 (32.37%)]\t\tLoss: 1.39151\n",
      "Training Progress: \tEpoch 1 [3200/8883 (35.97%)]\t\tLoss: 1.43350\n",
      "Training Progress: \tEpoch 1 [3520/8883 (39.57%)]\t\tLoss: 1.45928\n",
      "Training Progress: \tEpoch 1 [3840/8883 (43.17%)]\t\tLoss: 1.38178\n",
      "Training Progress: \tEpoch 1 [4160/8883 (46.76%)]\t\tLoss: 1.45819\n",
      "Training Progress: \tEpoch 1 [4480/8883 (50.36%)]\t\tLoss: 1.38655\n",
      "Training Progress: \tEpoch 1 [4800/8883 (53.96%)]\t\tLoss: 1.38218\n",
      "Training Progress: \tEpoch 1 [5120/8883 (57.55%)]\t\tLoss: 1.35884\n",
      "Training Progress: \tEpoch 1 [5440/8883 (61.15%)]\t\tLoss: 1.42351\n",
      "Training Progress: \tEpoch 1 [5760/8883 (64.75%)]\t\tLoss: 1.46099\n",
      "Training Progress: \tEpoch 1 [6080/8883 (68.35%)]\t\tLoss: 1.36737\n",
      "Training Progress: \tEpoch 1 [6400/8883 (71.94%)]\t\tLoss: 1.41759\n",
      "Training Progress: \tEpoch 1 [6720/8883 (75.54%)]\t\tLoss: 1.43604\n",
      "Training Progress: \tEpoch 1 [7040/8883 (79.14%)]\t\tLoss: 1.37822\n",
      "Training Progress: \tEpoch 1 [7360/8883 (82.73%)]\t\tLoss: 1.41052\n",
      "Training Progress: \tEpoch 1 [7680/8883 (86.33%)]\t\tLoss: 1.45621\n",
      "Training Progress: \tEpoch 1 [8000/8883 (89.93%)]\t\tLoss: 1.36902\n",
      "Training Progress: \tEpoch 1 [8320/8883 (93.53%)]\t\tLoss: 1.37543\n",
      "Training Progress: \tEpoch 1 [8640/8883 (97.12%)]\t\tLoss: 1.32727\n",
      "\tTrain loss: 0.04317, Accuracy: 2604/8883 (29.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 512/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 470/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/8883 (0.00%)]\t\tLoss: 1.43061\n",
      "Training Progress: \tEpoch 2 [320/8883 (3.60%)]\t\tLoss: 1.35956\n",
      "Training Progress: \tEpoch 2 [640/8883 (7.19%)]\t\tLoss: 1.42678\n",
      "Training Progress: \tEpoch 2 [960/8883 (10.79%)]\t\tLoss: 1.42729\n",
      "Training Progress: \tEpoch 2 [1280/8883 (14.39%)]\t\tLoss: 1.40572\n",
      "Training Progress: \tEpoch 2 [1600/8883 (17.99%)]\t\tLoss: 1.44827\n",
      "Training Progress: \tEpoch 2 [1920/8883 (21.58%)]\t\tLoss: 1.37823\n",
      "Training Progress: \tEpoch 2 [2240/8883 (25.18%)]\t\tLoss: 1.32522\n",
      "Training Progress: \tEpoch 2 [2560/8883 (28.78%)]\t\tLoss: 1.39553\n",
      "Training Progress: \tEpoch 2 [2880/8883 (32.37%)]\t\tLoss: 1.33950\n",
      "Training Progress: \tEpoch 2 [3200/8883 (35.97%)]\t\tLoss: 1.35713\n",
      "Training Progress: \tEpoch 2 [3520/8883 (39.57%)]\t\tLoss: 1.38104\n",
      "Training Progress: \tEpoch 2 [3840/8883 (43.17%)]\t\tLoss: 1.40465\n",
      "Training Progress: \tEpoch 2 [4160/8883 (46.76%)]\t\tLoss: 1.37565\n",
      "Training Progress: \tEpoch 2 [4480/8883 (50.36%)]\t\tLoss: 1.33699\n",
      "Training Progress: \tEpoch 2 [4800/8883 (53.96%)]\t\tLoss: 1.37764\n",
      "Training Progress: \tEpoch 2 [5120/8883 (57.55%)]\t\tLoss: 1.29001\n",
      "Training Progress: \tEpoch 2 [5440/8883 (61.15%)]\t\tLoss: 1.37716\n",
      "Training Progress: \tEpoch 2 [5760/8883 (64.75%)]\t\tLoss: 1.37242\n",
      "Training Progress: \tEpoch 2 [6080/8883 (68.35%)]\t\tLoss: 1.31217\n",
      "Training Progress: \tEpoch 2 [6400/8883 (71.94%)]\t\tLoss: 1.41799\n",
      "Training Progress: \tEpoch 2 [6720/8883 (75.54%)]\t\tLoss: 1.38159\n",
      "Training Progress: \tEpoch 2 [7040/8883 (79.14%)]\t\tLoss: 1.37743\n",
      "Training Progress: \tEpoch 2 [7360/8883 (82.73%)]\t\tLoss: 1.35082\n",
      "Training Progress: \tEpoch 2 [7680/8883 (86.33%)]\t\tLoss: 1.36713\n",
      "Training Progress: \tEpoch 2 [8000/8883 (89.93%)]\t\tLoss: 1.37188\n",
      "Training Progress: \tEpoch 2 [8320/8883 (93.53%)]\t\tLoss: 1.43094\n",
      "Training Progress: \tEpoch 2 [8640/8883 (97.12%)]\t\tLoss: 1.39860\n",
      "\tTrain loss: 0.04154, Accuracy: 3105/8883 (34.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 602/1692 (35.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 608/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/8883 (0.00%)]\t\tLoss: 1.37147\n",
      "Training Progress: \tEpoch 3 [320/8883 (3.60%)]\t\tLoss: 1.32026\n",
      "Training Progress: \tEpoch 3 [640/8883 (7.19%)]\t\tLoss: 1.37712\n",
      "Training Progress: \tEpoch 3 [960/8883 (10.79%)]\t\tLoss: 1.38082\n",
      "Training Progress: \tEpoch 3 [1280/8883 (14.39%)]\t\tLoss: 1.36133\n",
      "Training Progress: \tEpoch 3 [1600/8883 (17.99%)]\t\tLoss: 1.42517\n",
      "Training Progress: \tEpoch 3 [1920/8883 (21.58%)]\t\tLoss: 1.39741\n",
      "Training Progress: \tEpoch 3 [2240/8883 (25.18%)]\t\tLoss: 1.32648\n",
      "Training Progress: \tEpoch 3 [2560/8883 (28.78%)]\t\tLoss: 1.38658\n",
      "Training Progress: \tEpoch 3 [2880/8883 (32.37%)]\t\tLoss: 1.31902\n",
      "Training Progress: \tEpoch 3 [3200/8883 (35.97%)]\t\tLoss: 1.29797\n",
      "Training Progress: \tEpoch 3 [3520/8883 (39.57%)]\t\tLoss: 1.46685\n",
      "Training Progress: \tEpoch 3 [3840/8883 (43.17%)]\t\tLoss: 1.46885\n",
      "Training Progress: \tEpoch 3 [4160/8883 (46.76%)]\t\tLoss: 1.33664\n",
      "Training Progress: \tEpoch 3 [4480/8883 (50.36%)]\t\tLoss: 1.28551\n",
      "Training Progress: \tEpoch 3 [4800/8883 (53.96%)]\t\tLoss: 1.29801\n",
      "Training Progress: \tEpoch 3 [5120/8883 (57.55%)]\t\tLoss: 1.26048\n",
      "Training Progress: \tEpoch 3 [5440/8883 (61.15%)]\t\tLoss: 1.35299\n",
      "Training Progress: \tEpoch 3 [5760/8883 (64.75%)]\t\tLoss: 1.32939\n",
      "Training Progress: \tEpoch 3 [6080/8883 (68.35%)]\t\tLoss: 1.19389\n",
      "Training Progress: \tEpoch 3 [6400/8883 (71.94%)]\t\tLoss: 1.34771\n",
      "Training Progress: \tEpoch 3 [6720/8883 (75.54%)]\t\tLoss: 1.22860\n",
      "Training Progress: \tEpoch 3 [7040/8883 (79.14%)]\t\tLoss: 1.35154\n",
      "Training Progress: \tEpoch 3 [7360/8883 (82.73%)]\t\tLoss: 1.27409\n",
      "Training Progress: \tEpoch 3 [7680/8883 (86.33%)]\t\tLoss: 1.26947\n",
      "Training Progress: \tEpoch 3 [8000/8883 (89.93%)]\t\tLoss: 1.30755\n",
      "Training Progress: \tEpoch 3 [8320/8883 (93.53%)]\t\tLoss: 1.19900\n",
      "Training Progress: \tEpoch 3 [8640/8883 (97.12%)]\t\tLoss: 1.27448\n",
      "\tTrain loss: 0.03922, Accuracy: 3508/8883 (39.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 692/1692 (40.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 683/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/8883 (0.00%)]\t\tLoss: 1.23213\n",
      "Training Progress: \tEpoch 4 [320/8883 (3.60%)]\t\tLoss: 1.22327\n",
      "Training Progress: \tEpoch 4 [640/8883 (7.19%)]\t\tLoss: 1.25210\n",
      "Training Progress: \tEpoch 4 [960/8883 (10.79%)]\t\tLoss: 1.28909\n",
      "Training Progress: \tEpoch 4 [1280/8883 (14.39%)]\t\tLoss: 1.24624\n",
      "Training Progress: \tEpoch 4 [1600/8883 (17.99%)]\t\tLoss: 1.46787\n",
      "Training Progress: \tEpoch 4 [1920/8883 (21.58%)]\t\tLoss: 1.31188\n",
      "Training Progress: \tEpoch 4 [2240/8883 (25.18%)]\t\tLoss: 1.29715\n",
      "Training Progress: \tEpoch 4 [2560/8883 (28.78%)]\t\tLoss: 1.39980\n",
      "Training Progress: \tEpoch 4 [2880/8883 (32.37%)]\t\tLoss: 1.23491\n",
      "Training Progress: \tEpoch 4 [3200/8883 (35.97%)]\t\tLoss: 1.29194\n",
      "Training Progress: \tEpoch 4 [3520/8883 (39.57%)]\t\tLoss: 1.46111\n",
      "Training Progress: \tEpoch 4 [3840/8883 (43.17%)]\t\tLoss: 1.42942\n",
      "Training Progress: \tEpoch 4 [4160/8883 (46.76%)]\t\tLoss: 1.32227\n",
      "Training Progress: \tEpoch 4 [4480/8883 (50.36%)]\t\tLoss: 1.29438\n",
      "Training Progress: \tEpoch 4 [4800/8883 (53.96%)]\t\tLoss: 1.26267\n",
      "Training Progress: \tEpoch 4 [5120/8883 (57.55%)]\t\tLoss: 1.26614\n",
      "Training Progress: \tEpoch 4 [5440/8883 (61.15%)]\t\tLoss: 1.34576\n",
      "Training Progress: \tEpoch 4 [5760/8883 (64.75%)]\t\tLoss: 1.42872\n",
      "Training Progress: \tEpoch 4 [6080/8883 (68.35%)]\t\tLoss: 1.23538\n",
      "Training Progress: \tEpoch 4 [6400/8883 (71.94%)]\t\tLoss: 1.16735\n",
      "Training Progress: \tEpoch 4 [6720/8883 (75.54%)]\t\tLoss: 1.19880\n",
      "Training Progress: \tEpoch 4 [7040/8883 (79.14%)]\t\tLoss: 1.29817\n",
      "Training Progress: \tEpoch 4 [7360/8883 (82.73%)]\t\tLoss: 1.33913\n",
      "Training Progress: \tEpoch 4 [7680/8883 (86.33%)]\t\tLoss: 1.29605\n",
      "Training Progress: \tEpoch 4 [8000/8883 (89.93%)]\t\tLoss: 1.29174\n",
      "Training Progress: \tEpoch 4 [8320/8883 (93.53%)]\t\tLoss: 1.17185\n",
      "Training Progress: \tEpoch 4 [8640/8883 (97.12%)]\t\tLoss: 1.19739\n",
      "\tTrain loss: 0.03786, Accuracy: 3802/8883 (42.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 775/1692 (45.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 760/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/8883 (0.00%)]\t\tLoss: 1.07368\n",
      "Training Progress: \tEpoch 5 [320/8883 (3.60%)]\t\tLoss: 1.22939\n",
      "Training Progress: \tEpoch 5 [640/8883 (7.19%)]\t\tLoss: 1.16517\n",
      "Training Progress: \tEpoch 5 [960/8883 (10.79%)]\t\tLoss: 1.16786\n",
      "Training Progress: \tEpoch 5 [1280/8883 (14.39%)]\t\tLoss: 1.28720\n",
      "Training Progress: \tEpoch 5 [1600/8883 (17.99%)]\t\tLoss: 1.39869\n",
      "Training Progress: \tEpoch 5 [1920/8883 (21.58%)]\t\tLoss: 1.26676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 5 [2240/8883 (25.18%)]\t\tLoss: 1.25087\n",
      "Training Progress: \tEpoch 5 [2560/8883 (28.78%)]\t\tLoss: 1.33487\n",
      "Training Progress: \tEpoch 5 [2880/8883 (32.37%)]\t\tLoss: 1.28512\n",
      "Training Progress: \tEpoch 5 [3200/8883 (35.97%)]\t\tLoss: 1.25225\n",
      "Training Progress: \tEpoch 5 [3520/8883 (39.57%)]\t\tLoss: 1.37163\n",
      "Training Progress: \tEpoch 5 [3840/8883 (43.17%)]\t\tLoss: 1.42906\n",
      "Training Progress: \tEpoch 5 [4160/8883 (46.76%)]\t\tLoss: 1.30774\n",
      "Training Progress: \tEpoch 5 [4480/8883 (50.36%)]\t\tLoss: 1.25303\n",
      "Training Progress: \tEpoch 5 [4800/8883 (53.96%)]\t\tLoss: 1.21760\n",
      "Training Progress: \tEpoch 5 [5120/8883 (57.55%)]\t\tLoss: 1.18041\n",
      "Training Progress: \tEpoch 5 [5440/8883 (61.15%)]\t\tLoss: 1.30690\n",
      "Training Progress: \tEpoch 5 [5760/8883 (64.75%)]\t\tLoss: 1.29318\n",
      "Training Progress: \tEpoch 5 [6080/8883 (68.35%)]\t\tLoss: 1.27261\n",
      "Training Progress: \tEpoch 5 [6400/8883 (71.94%)]\t\tLoss: 1.18441\n",
      "Training Progress: \tEpoch 5 [6720/8883 (75.54%)]\t\tLoss: 1.10193\n",
      "Training Progress: \tEpoch 5 [7040/8883 (79.14%)]\t\tLoss: 1.17226\n",
      "Training Progress: \tEpoch 5 [7360/8883 (82.73%)]\t\tLoss: 1.41430\n",
      "Training Progress: \tEpoch 5 [7680/8883 (86.33%)]\t\tLoss: 1.14100\n",
      "Training Progress: \tEpoch 5 [8000/8883 (89.93%)]\t\tLoss: 1.20255\n",
      "Training Progress: \tEpoch 5 [8320/8883 (93.53%)]\t\tLoss: 1.20423\n",
      "Training Progress: \tEpoch 5 [8640/8883 (97.12%)]\t\tLoss: 1.24117\n",
      "\tTrain loss: 0.03681, Accuracy: 4068/8883 (45.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 816/1692 (48.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 792/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/8883 (0.00%)]\t\tLoss: 1.15691\n",
      "Training Progress: \tEpoch 6 [320/8883 (3.60%)]\t\tLoss: 1.22872\n",
      "Training Progress: \tEpoch 6 [640/8883 (7.19%)]\t\tLoss: 1.19763\n",
      "Training Progress: \tEpoch 6 [960/8883 (10.79%)]\t\tLoss: 1.25885\n",
      "Training Progress: \tEpoch 6 [1280/8883 (14.39%)]\t\tLoss: 1.32614\n",
      "Training Progress: \tEpoch 6 [1600/8883 (17.99%)]\t\tLoss: 1.29420\n",
      "Training Progress: \tEpoch 6 [1920/8883 (21.58%)]\t\tLoss: 1.26052\n",
      "Training Progress: \tEpoch 6 [2240/8883 (25.18%)]\t\tLoss: 1.29524\n",
      "Training Progress: \tEpoch 6 [2560/8883 (28.78%)]\t\tLoss: 1.30198\n",
      "Training Progress: \tEpoch 6 [2880/8883 (32.37%)]\t\tLoss: 1.24786\n",
      "Training Progress: \tEpoch 6 [3200/8883 (35.97%)]\t\tLoss: 1.09934\n",
      "Training Progress: \tEpoch 6 [3520/8883 (39.57%)]\t\tLoss: 1.41314\n",
      "Training Progress: \tEpoch 6 [3840/8883 (43.17%)]\t\tLoss: 1.36264\n",
      "Training Progress: \tEpoch 6 [4160/8883 (46.76%)]\t\tLoss: 1.15659\n",
      "Training Progress: \tEpoch 6 [4480/8883 (50.36%)]\t\tLoss: 1.17820\n",
      "Training Progress: \tEpoch 6 [4800/8883 (53.96%)]\t\tLoss: 1.13876\n",
      "Training Progress: \tEpoch 6 [5120/8883 (57.55%)]\t\tLoss: 1.21696\n",
      "Training Progress: \tEpoch 6 [5440/8883 (61.15%)]\t\tLoss: 1.43895\n",
      "Training Progress: \tEpoch 6 [5760/8883 (64.75%)]\t\tLoss: 1.22648\n",
      "Training Progress: \tEpoch 6 [6080/8883 (68.35%)]\t\tLoss: 1.11830\n",
      "Training Progress: \tEpoch 6 [6400/8883 (71.94%)]\t\tLoss: 1.16342\n",
      "Training Progress: \tEpoch 6 [6720/8883 (75.54%)]\t\tLoss: 1.18338\n",
      "Training Progress: \tEpoch 6 [7040/8883 (79.14%)]\t\tLoss: 1.20756\n",
      "Training Progress: \tEpoch 6 [7360/8883 (82.73%)]\t\tLoss: 1.38204\n",
      "Training Progress: \tEpoch 6 [7680/8883 (86.33%)]\t\tLoss: 1.12569\n",
      "Training Progress: \tEpoch 6 [8000/8883 (89.93%)]\t\tLoss: 1.35868\n",
      "Training Progress: \tEpoch 6 [8320/8883 (93.53%)]\t\tLoss: 1.08697\n",
      "Training Progress: \tEpoch 6 [8640/8883 (97.12%)]\t\tLoss: 1.28688\n",
      "\tTrain loss: 0.03691, Accuracy: 3873/8883 (43.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 800/1692 (47.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/8883 (0.00%)]\t\tLoss: 1.11117\n",
      "Training Progress: \tEpoch 7 [320/8883 (3.60%)]\t\tLoss: 1.12965\n",
      "Training Progress: \tEpoch 7 [640/8883 (7.19%)]\t\tLoss: 1.18136\n",
      "Training Progress: \tEpoch 7 [960/8883 (10.79%)]\t\tLoss: 1.20947\n",
      "Training Progress: \tEpoch 7 [1280/8883 (14.39%)]\t\tLoss: 1.29852\n",
      "Training Progress: \tEpoch 7 [1600/8883 (17.99%)]\t\tLoss: 1.30282\n",
      "Training Progress: \tEpoch 7 [1920/8883 (21.58%)]\t\tLoss: 1.31421\n",
      "Training Progress: \tEpoch 7 [2240/8883 (25.18%)]\t\tLoss: 1.22329\n",
      "Training Progress: \tEpoch 7 [2560/8883 (28.78%)]\t\tLoss: 1.21513\n",
      "Training Progress: \tEpoch 7 [2880/8883 (32.37%)]\t\tLoss: 1.21911\n",
      "Training Progress: \tEpoch 7 [3200/8883 (35.97%)]\t\tLoss: 1.44497\n",
      "Training Progress: \tEpoch 7 [3520/8883 (39.57%)]\t\tLoss: 1.39866\n",
      "Training Progress: \tEpoch 7 [3840/8883 (43.17%)]\t\tLoss: 1.47698\n",
      "Training Progress: \tEpoch 7 [4160/8883 (46.76%)]\t\tLoss: 1.28578\n",
      "Training Progress: \tEpoch 7 [4480/8883 (50.36%)]\t\tLoss: 1.24367\n",
      "Training Progress: \tEpoch 7 [4800/8883 (53.96%)]\t\tLoss: 1.17147\n",
      "Training Progress: \tEpoch 7 [5120/8883 (57.55%)]\t\tLoss: 1.25553\n",
      "Training Progress: \tEpoch 7 [5440/8883 (61.15%)]\t\tLoss: 1.34948\n",
      "Training Progress: \tEpoch 7 [5760/8883 (64.75%)]\t\tLoss: 1.27215\n",
      "Training Progress: \tEpoch 7 [6080/8883 (68.35%)]\t\tLoss: 1.08118\n",
      "Training Progress: \tEpoch 7 [6400/8883 (71.94%)]\t\tLoss: 1.11097\n",
      "Training Progress: \tEpoch 7 [6720/8883 (75.54%)]\t\tLoss: 1.18518\n",
      "Training Progress: \tEpoch 7 [7040/8883 (79.14%)]\t\tLoss: 1.22577\n",
      "Training Progress: \tEpoch 7 [7360/8883 (82.73%)]\t\tLoss: 1.28102\n",
      "Training Progress: \tEpoch 7 [7680/8883 (86.33%)]\t\tLoss: 1.15783\n",
      "Training Progress: \tEpoch 7 [8000/8883 (89.93%)]\t\tLoss: 1.23316\n",
      "Training Progress: \tEpoch 7 [8320/8883 (93.53%)]\t\tLoss: 1.03421\n",
      "Training Progress: \tEpoch 7 [8640/8883 (97.12%)]\t\tLoss: 1.19189\n",
      "\tTrain loss: 0.03575, Accuracy: 4081/8883 (45.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 837/1692 (49.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/8883 (0.00%)]\t\tLoss: 1.08875\n",
      "Training Progress: \tEpoch 8 [320/8883 (3.60%)]\t\tLoss: 1.20501\n",
      "Training Progress: \tEpoch 8 [640/8883 (7.19%)]\t\tLoss: 1.29502\n",
      "Training Progress: \tEpoch 8 [960/8883 (10.79%)]\t\tLoss: 1.37231\n",
      "Training Progress: \tEpoch 8 [1280/8883 (14.39%)]\t\tLoss: 1.18515\n",
      "Training Progress: \tEpoch 8 [1600/8883 (17.99%)]\t\tLoss: 1.30101\n",
      "Training Progress: \tEpoch 8 [1920/8883 (21.58%)]\t\tLoss: 1.31143\n",
      "Training Progress: \tEpoch 8 [2240/8883 (25.18%)]\t\tLoss: 1.22072\n",
      "Training Progress: \tEpoch 8 [2560/8883 (28.78%)]\t\tLoss: 1.29656\n",
      "Training Progress: \tEpoch 8 [2880/8883 (32.37%)]\t\tLoss: 1.21146\n",
      "Training Progress: \tEpoch 8 [3200/8883 (35.97%)]\t\tLoss: 1.35356\n",
      "Training Progress: \tEpoch 8 [3520/8883 (39.57%)]\t\tLoss: 1.41627\n",
      "Training Progress: \tEpoch 8 [3840/8883 (43.17%)]\t\tLoss: 1.35789\n",
      "Training Progress: \tEpoch 8 [4160/8883 (46.76%)]\t\tLoss: 1.18588\n",
      "Training Progress: \tEpoch 8 [4480/8883 (50.36%)]\t\tLoss: 1.13102\n",
      "Training Progress: \tEpoch 8 [4800/8883 (53.96%)]\t\tLoss: 1.09185\n",
      "Training Progress: \tEpoch 8 [5120/8883 (57.55%)]\t\tLoss: 1.15039\n",
      "Training Progress: \tEpoch 8 [5440/8883 (61.15%)]\t\tLoss: 1.31135\n",
      "Training Progress: \tEpoch 8 [5760/8883 (64.75%)]\t\tLoss: 1.10472\n",
      "Training Progress: \tEpoch 8 [6080/8883 (68.35%)]\t\tLoss: 1.09954\n",
      "Training Progress: \tEpoch 8 [6400/8883 (71.94%)]\t\tLoss: 1.12099\n",
      "Training Progress: \tEpoch 8 [6720/8883 (75.54%)]\t\tLoss: 1.17464\n",
      "Training Progress: \tEpoch 8 [7040/8883 (79.14%)]\t\tLoss: 1.24918\n",
      "Training Progress: \tEpoch 8 [7360/8883 (82.73%)]\t\tLoss: 1.22616\n",
      "Training Progress: \tEpoch 8 [7680/8883 (86.33%)]\t\tLoss: 1.19063\n",
      "Training Progress: \tEpoch 8 [8000/8883 (89.93%)]\t\tLoss: 1.14148\n",
      "Training Progress: \tEpoch 8 [8320/8883 (93.53%)]\t\tLoss: 1.12493\n",
      "Training Progress: \tEpoch 8 [8640/8883 (97.12%)]\t\tLoss: 1.15001\n",
      "\tTrain loss: 0.03489, Accuracy: 4322/8883 (48.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 871/1692 (51.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 846/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/8883 (0.00%)]\t\tLoss: 1.11638\n",
      "Training Progress: \tEpoch 9 [320/8883 (3.60%)]\t\tLoss: 1.17381\n",
      "Training Progress: \tEpoch 9 [640/8883 (7.19%)]\t\tLoss: 1.11221\n",
      "Training Progress: \tEpoch 9 [960/8883 (10.79%)]\t\tLoss: 1.12439\n",
      "Training Progress: \tEpoch 9 [1280/8883 (14.39%)]\t\tLoss: 1.38542\n",
      "Training Progress: \tEpoch 9 [1600/8883 (17.99%)]\t\tLoss: 1.26971\n",
      "Training Progress: \tEpoch 9 [1920/8883 (21.58%)]\t\tLoss: 1.25795\n",
      "Training Progress: \tEpoch 9 [2240/8883 (25.18%)]\t\tLoss: 1.31857\n",
      "Training Progress: \tEpoch 9 [2560/8883 (28.78%)]\t\tLoss: 1.17052\n",
      "Training Progress: \tEpoch 9 [2880/8883 (32.37%)]\t\tLoss: 1.14729\n",
      "Training Progress: \tEpoch 9 [3200/8883 (35.97%)]\t\tLoss: 1.33000\n",
      "Training Progress: \tEpoch 9 [3520/8883 (39.57%)]\t\tLoss: 1.21511\n",
      "Training Progress: \tEpoch 9 [3840/8883 (43.17%)]\t\tLoss: 1.43641\n",
      "Training Progress: \tEpoch 9 [4160/8883 (46.76%)]\t\tLoss: 1.25344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 9 [4480/8883 (50.36%)]\t\tLoss: 1.11843\n",
      "Training Progress: \tEpoch 9 [4800/8883 (53.96%)]\t\tLoss: 1.15523\n",
      "Training Progress: \tEpoch 9 [5120/8883 (57.55%)]\t\tLoss: 1.17110\n",
      "Training Progress: \tEpoch 9 [5440/8883 (61.15%)]\t\tLoss: 1.41723\n",
      "Training Progress: \tEpoch 9 [5760/8883 (64.75%)]\t\tLoss: 1.20661\n",
      "Training Progress: \tEpoch 9 [6080/8883 (68.35%)]\t\tLoss: 1.04153\n",
      "Training Progress: \tEpoch 9 [6400/8883 (71.94%)]\t\tLoss: 1.09999\n",
      "Training Progress: \tEpoch 9 [6720/8883 (75.54%)]\t\tLoss: 1.06944\n",
      "Training Progress: \tEpoch 9 [7040/8883 (79.14%)]\t\tLoss: 1.28196\n",
      "Training Progress: \tEpoch 9 [7360/8883 (82.73%)]\t\tLoss: 1.20755\n",
      "Training Progress: \tEpoch 9 [7680/8883 (86.33%)]\t\tLoss: 1.07629\n",
      "Training Progress: \tEpoch 9 [8000/8883 (89.93%)]\t\tLoss: 1.17035\n",
      "Training Progress: \tEpoch 9 [8320/8883 (93.53%)]\t\tLoss: 1.17704\n",
      "Training Progress: \tEpoch 9 [8640/8883 (97.12%)]\t\tLoss: 1.30985\n",
      "\tTrain loss: 0.03497, Accuracy: 4282/8883 (48.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 887/1692 (52.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/8883 (0.00%)]\t\tLoss: 1.07849\n",
      "Training Progress: \tEpoch 10 [320/8883 (3.60%)]\t\tLoss: 1.18691\n",
      "Training Progress: \tEpoch 10 [640/8883 (7.19%)]\t\tLoss: 1.18914\n",
      "Training Progress: \tEpoch 10 [960/8883 (10.79%)]\t\tLoss: 1.21905\n",
      "Training Progress: \tEpoch 10 [1280/8883 (14.39%)]\t\tLoss: 1.25747\n",
      "Training Progress: \tEpoch 10 [1600/8883 (17.99%)]\t\tLoss: 1.12288\n",
      "Training Progress: \tEpoch 10 [1920/8883 (21.58%)]\t\tLoss: 1.18039\n",
      "Training Progress: \tEpoch 10 [2240/8883 (25.18%)]\t\tLoss: 1.22639\n",
      "Training Progress: \tEpoch 10 [2560/8883 (28.78%)]\t\tLoss: 1.17028\n",
      "Training Progress: \tEpoch 10 [2880/8883 (32.37%)]\t\tLoss: 1.20864\n",
      "Training Progress: \tEpoch 10 [3200/8883 (35.97%)]\t\tLoss: 1.26532\n",
      "Training Progress: \tEpoch 10 [3520/8883 (39.57%)]\t\tLoss: 1.27209\n",
      "Training Progress: \tEpoch 10 [3840/8883 (43.17%)]\t\tLoss: 1.33741\n",
      "Training Progress: \tEpoch 10 [4160/8883 (46.76%)]\t\tLoss: 1.30885\n",
      "Training Progress: \tEpoch 10 [4480/8883 (50.36%)]\t\tLoss: 1.13066\n",
      "Training Progress: \tEpoch 10 [4800/8883 (53.96%)]\t\tLoss: 1.13157\n",
      "Training Progress: \tEpoch 10 [5120/8883 (57.55%)]\t\tLoss: 1.13332\n",
      "Training Progress: \tEpoch 10 [5440/8883 (61.15%)]\t\tLoss: 1.39181\n",
      "Training Progress: \tEpoch 10 [5760/8883 (64.75%)]\t\tLoss: 1.21789\n",
      "Training Progress: \tEpoch 10 [6080/8883 (68.35%)]\t\tLoss: 1.02097\n",
      "Training Progress: \tEpoch 10 [6400/8883 (71.94%)]\t\tLoss: 1.19991\n",
      "Training Progress: \tEpoch 10 [6720/8883 (75.54%)]\t\tLoss: 1.03304\n",
      "Training Progress: \tEpoch 10 [7040/8883 (79.14%)]\t\tLoss: 1.28939\n",
      "Training Progress: \tEpoch 10 [7360/8883 (82.73%)]\t\tLoss: 1.22047\n",
      "Training Progress: \tEpoch 10 [7680/8883 (86.33%)]\t\tLoss: 1.11943\n",
      "Training Progress: \tEpoch 10 [8000/8883 (89.93%)]\t\tLoss: 1.17132\n",
      "Training Progress: \tEpoch 10 [8320/8883 (93.53%)]\t\tLoss: 1.09497\n",
      "Training Progress: \tEpoch 10 [8640/8883 (97.12%)]\t\tLoss: 1.22709\n",
      "\tTrain loss: 0.03384, Accuracy: 4482/8883 (50.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 925/1692 (54.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 837/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/8883 (0.00%)]\t\tLoss: 1.01458\n",
      "Training Progress: \tEpoch 11 [320/8883 (3.60%)]\t\tLoss: 1.32251\n",
      "Training Progress: \tEpoch 11 [640/8883 (7.19%)]\t\tLoss: 1.14235\n",
      "Training Progress: \tEpoch 11 [960/8883 (10.79%)]\t\tLoss: 1.19091\n",
      "Training Progress: \tEpoch 11 [1280/8883 (14.39%)]\t\tLoss: 1.29576\n",
      "Training Progress: \tEpoch 11 [1600/8883 (17.99%)]\t\tLoss: 1.43157\n",
      "Training Progress: \tEpoch 11 [1920/8883 (21.58%)]\t\tLoss: 1.21565\n",
      "Training Progress: \tEpoch 11 [2240/8883 (25.18%)]\t\tLoss: 1.01464\n",
      "Training Progress: \tEpoch 11 [2560/8883 (28.78%)]\t\tLoss: 1.22419\n",
      "Training Progress: \tEpoch 11 [2880/8883 (32.37%)]\t\tLoss: 1.13661\n",
      "Training Progress: \tEpoch 11 [3200/8883 (35.97%)]\t\tLoss: 1.38219\n",
      "Training Progress: \tEpoch 11 [3520/8883 (39.57%)]\t\tLoss: 1.40872\n",
      "Training Progress: \tEpoch 11 [3840/8883 (43.17%)]\t\tLoss: 1.37956\n",
      "Training Progress: \tEpoch 11 [4160/8883 (46.76%)]\t\tLoss: 1.16721\n",
      "Training Progress: \tEpoch 11 [4480/8883 (50.36%)]\t\tLoss: 1.22667\n",
      "Training Progress: \tEpoch 11 [4800/8883 (53.96%)]\t\tLoss: 1.03867\n",
      "Training Progress: \tEpoch 11 [5120/8883 (57.55%)]\t\tLoss: 1.29376\n",
      "Training Progress: \tEpoch 11 [5440/8883 (61.15%)]\t\tLoss: 1.40666\n",
      "Training Progress: \tEpoch 11 [5760/8883 (64.75%)]\t\tLoss: 1.19318\n",
      "Training Progress: \tEpoch 11 [6080/8883 (68.35%)]\t\tLoss: 1.06170\n",
      "Training Progress: \tEpoch 11 [6400/8883 (71.94%)]\t\tLoss: 1.17516\n",
      "Training Progress: \tEpoch 11 [6720/8883 (75.54%)]\t\tLoss: 1.18676\n",
      "Training Progress: \tEpoch 11 [7040/8883 (79.14%)]\t\tLoss: 1.41041\n",
      "Training Progress: \tEpoch 11 [7360/8883 (82.73%)]\t\tLoss: 1.29872\n",
      "Training Progress: \tEpoch 11 [7680/8883 (86.33%)]\t\tLoss: 1.15698\n",
      "Training Progress: \tEpoch 11 [8000/8883 (89.93%)]\t\tLoss: 1.10088\n",
      "Training Progress: \tEpoch 11 [8320/8883 (93.53%)]\t\tLoss: 1.08921\n",
      "Training Progress: \tEpoch 11 [8640/8883 (97.12%)]\t\tLoss: 1.16232\n",
      "\tTrain loss: 0.03287, Accuracy: 4594/8883 (51.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 957/1692 (56.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 882/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/8883 (0.00%)]\t\tLoss: 1.14881\n",
      "Training Progress: \tEpoch 12 [320/8883 (3.60%)]\t\tLoss: 1.19058\n",
      "Training Progress: \tEpoch 12 [640/8883 (7.19%)]\t\tLoss: 1.05458\n",
      "Training Progress: \tEpoch 12 [960/8883 (10.79%)]\t\tLoss: 1.09429\n",
      "Training Progress: \tEpoch 12 [1280/8883 (14.39%)]\t\tLoss: 1.27885\n",
      "Training Progress: \tEpoch 12 [1600/8883 (17.99%)]\t\tLoss: 1.37248\n",
      "Training Progress: \tEpoch 12 [1920/8883 (21.58%)]\t\tLoss: 1.34853\n",
      "Training Progress: \tEpoch 12 [2240/8883 (25.18%)]\t\tLoss: 1.09653\n",
      "Training Progress: \tEpoch 12 [2560/8883 (28.78%)]\t\tLoss: 1.21142\n",
      "Training Progress: \tEpoch 12 [2880/8883 (32.37%)]\t\tLoss: 1.08710\n",
      "Training Progress: \tEpoch 12 [3200/8883 (35.97%)]\t\tLoss: 1.20844\n",
      "Training Progress: \tEpoch 12 [3520/8883 (39.57%)]\t\tLoss: 1.29621\n",
      "Training Progress: \tEpoch 12 [3840/8883 (43.17%)]\t\tLoss: 1.31780\n",
      "Training Progress: \tEpoch 12 [4160/8883 (46.76%)]\t\tLoss: 1.13243\n",
      "Training Progress: \tEpoch 12 [4480/8883 (50.36%)]\t\tLoss: 1.03846\n",
      "Training Progress: \tEpoch 12 [4800/8883 (53.96%)]\t\tLoss: 1.04866\n",
      "Training Progress: \tEpoch 12 [5120/8883 (57.55%)]\t\tLoss: 1.22692\n",
      "Training Progress: \tEpoch 12 [5440/8883 (61.15%)]\t\tLoss: 1.38381\n",
      "Training Progress: \tEpoch 12 [5760/8883 (64.75%)]\t\tLoss: 1.22886\n",
      "Training Progress: \tEpoch 12 [6080/8883 (68.35%)]\t\tLoss: 0.99223\n",
      "Training Progress: \tEpoch 12 [6400/8883 (71.94%)]\t\tLoss: 1.23541\n",
      "Training Progress: \tEpoch 12 [6720/8883 (75.54%)]\t\tLoss: 1.10222\n",
      "Training Progress: \tEpoch 12 [7040/8883 (79.14%)]\t\tLoss: 1.16944\n",
      "Training Progress: \tEpoch 12 [7360/8883 (82.73%)]\t\tLoss: 1.45880\n",
      "Training Progress: \tEpoch 12 [7680/8883 (86.33%)]\t\tLoss: 1.16054\n",
      "Training Progress: \tEpoch 12 [8000/8883 (89.93%)]\t\tLoss: 1.11327\n",
      "Training Progress: \tEpoch 12 [8320/8883 (93.53%)]\t\tLoss: 1.11778\n",
      "Training Progress: \tEpoch 12 [8640/8883 (97.12%)]\t\tLoss: 1.09443\n",
      "\tTrain loss: 0.03272, Accuracy: 4708/8883 (53.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 974/1692 (57.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 857/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/8883 (0.00%)]\t\tLoss: 1.08249\n",
      "Training Progress: \tEpoch 13 [320/8883 (3.60%)]\t\tLoss: 1.16072\n",
      "Training Progress: \tEpoch 13 [640/8883 (7.19%)]\t\tLoss: 1.17475\n",
      "Training Progress: \tEpoch 13 [960/8883 (10.79%)]\t\tLoss: 1.08069\n",
      "Training Progress: \tEpoch 13 [1280/8883 (14.39%)]\t\tLoss: 1.24256\n",
      "Training Progress: \tEpoch 13 [1600/8883 (17.99%)]\t\tLoss: 1.31876\n",
      "Training Progress: \tEpoch 13 [1920/8883 (21.58%)]\t\tLoss: 1.32048\n",
      "Training Progress: \tEpoch 13 [2240/8883 (25.18%)]\t\tLoss: 1.21514\n",
      "Training Progress: \tEpoch 13 [2560/8883 (28.78%)]\t\tLoss: 1.07775\n",
      "Training Progress: \tEpoch 13 [2880/8883 (32.37%)]\t\tLoss: 1.19698\n",
      "Training Progress: \tEpoch 13 [3200/8883 (35.97%)]\t\tLoss: 1.39508\n",
      "Training Progress: \tEpoch 13 [3520/8883 (39.57%)]\t\tLoss: 1.29063\n",
      "Training Progress: \tEpoch 13 [3840/8883 (43.17%)]\t\tLoss: 1.29864\n",
      "Training Progress: \tEpoch 13 [4160/8883 (46.76%)]\t\tLoss: 1.13398\n",
      "Training Progress: \tEpoch 13 [4480/8883 (50.36%)]\t\tLoss: 1.05040\n",
      "Training Progress: \tEpoch 13 [4800/8883 (53.96%)]\t\tLoss: 0.99096\n",
      "Training Progress: \tEpoch 13 [5120/8883 (57.55%)]\t\tLoss: 1.20033\n",
      "Training Progress: \tEpoch 13 [5440/8883 (61.15%)]\t\tLoss: 1.39343\n",
      "Training Progress: \tEpoch 13 [5760/8883 (64.75%)]\t\tLoss: 1.15733\n",
      "Training Progress: \tEpoch 13 [6080/8883 (68.35%)]\t\tLoss: 1.05999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 13 [6400/8883 (71.94%)]\t\tLoss: 0.99764\n",
      "Training Progress: \tEpoch 13 [6720/8883 (75.54%)]\t\tLoss: 1.06968\n",
      "Training Progress: \tEpoch 13 [7040/8883 (79.14%)]\t\tLoss: 1.23026\n",
      "Training Progress: \tEpoch 13 [7360/8883 (82.73%)]\t\tLoss: 1.33456\n",
      "Training Progress: \tEpoch 13 [7680/8883 (86.33%)]\t\tLoss: 1.19481\n",
      "Training Progress: \tEpoch 13 [8000/8883 (89.93%)]\t\tLoss: 1.20265\n",
      "Training Progress: \tEpoch 13 [8320/8883 (93.53%)]\t\tLoss: 1.13584\n",
      "Training Progress: \tEpoch 13 [8640/8883 (97.12%)]\t\tLoss: 1.19276\n",
      "\tTrain loss: 0.03216, Accuracy: 4713/8883 (53.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 959/1692 (56.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 853/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/8883 (0.00%)]\t\tLoss: 1.09027\n",
      "Training Progress: \tEpoch 14 [320/8883 (3.60%)]\t\tLoss: 1.33755\n",
      "Training Progress: \tEpoch 14 [640/8883 (7.19%)]\t\tLoss: 1.08016\n",
      "Training Progress: \tEpoch 14 [960/8883 (10.79%)]\t\tLoss: 1.18147\n",
      "Training Progress: \tEpoch 14 [1280/8883 (14.39%)]\t\tLoss: 1.28671\n",
      "Training Progress: \tEpoch 14 [1600/8883 (17.99%)]\t\tLoss: 1.26915\n",
      "Training Progress: \tEpoch 14 [1920/8883 (21.58%)]\t\tLoss: 1.12085\n",
      "Training Progress: \tEpoch 14 [2240/8883 (25.18%)]\t\tLoss: 1.13758\n",
      "Training Progress: \tEpoch 14 [2560/8883 (28.78%)]\t\tLoss: 1.18756\n",
      "Training Progress: \tEpoch 14 [2880/8883 (32.37%)]\t\tLoss: 1.29646\n",
      "Training Progress: \tEpoch 14 [3200/8883 (35.97%)]\t\tLoss: 1.28736\n",
      "Training Progress: \tEpoch 14 [3520/8883 (39.57%)]\t\tLoss: 1.31083\n",
      "Training Progress: \tEpoch 14 [3840/8883 (43.17%)]\t\tLoss: 1.30045\n",
      "Training Progress: \tEpoch 14 [4160/8883 (46.76%)]\t\tLoss: 1.18514\n",
      "Training Progress: \tEpoch 14 [4480/8883 (50.36%)]\t\tLoss: 1.05428\n",
      "Training Progress: \tEpoch 14 [4800/8883 (53.96%)]\t\tLoss: 1.04133\n",
      "Training Progress: \tEpoch 14 [5120/8883 (57.55%)]\t\tLoss: 1.15294\n",
      "Training Progress: \tEpoch 14 [5440/8883 (61.15%)]\t\tLoss: 1.25835\n",
      "Training Progress: \tEpoch 14 [5760/8883 (64.75%)]\t\tLoss: 1.17900\n",
      "Training Progress: \tEpoch 14 [6080/8883 (68.35%)]\t\tLoss: 1.01432\n",
      "Training Progress: \tEpoch 14 [6400/8883 (71.94%)]\t\tLoss: 1.11355\n",
      "Training Progress: \tEpoch 14 [6720/8883 (75.54%)]\t\tLoss: 1.01441\n",
      "Training Progress: \tEpoch 14 [7040/8883 (79.14%)]\t\tLoss: 1.29155\n",
      "Training Progress: \tEpoch 14 [7360/8883 (82.73%)]\t\tLoss: 1.27195\n",
      "Training Progress: \tEpoch 14 [7680/8883 (86.33%)]\t\tLoss: 1.11057\n",
      "Training Progress: \tEpoch 14 [8000/8883 (89.93%)]\t\tLoss: 1.12642\n",
      "Training Progress: \tEpoch 14 [8320/8883 (93.53%)]\t\tLoss: 0.98550\n",
      "Training Progress: \tEpoch 14 [8640/8883 (97.12%)]\t\tLoss: 1.36761\n",
      "\tTrain loss: 0.03187, Accuracy: 4726/8883 (53.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 967/1692 (57.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 854/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/8883 (0.00%)]\t\tLoss: 1.07378\n",
      "Training Progress: \tEpoch 15 [320/8883 (3.60%)]\t\tLoss: 1.04464\n",
      "Training Progress: \tEpoch 15 [640/8883 (7.19%)]\t\tLoss: 1.21149\n",
      "Training Progress: \tEpoch 15 [960/8883 (10.79%)]\t\tLoss: 1.07199\n",
      "Training Progress: \tEpoch 15 [1280/8883 (14.39%)]\t\tLoss: 1.18864\n",
      "Training Progress: \tEpoch 15 [1600/8883 (17.99%)]\t\tLoss: 1.21301\n",
      "Training Progress: \tEpoch 15 [1920/8883 (21.58%)]\t\tLoss: 1.07546\n",
      "Training Progress: \tEpoch 15 [2240/8883 (25.18%)]\t\tLoss: 1.04110\n",
      "Training Progress: \tEpoch 15 [2560/8883 (28.78%)]\t\tLoss: 1.12503\n",
      "Training Progress: \tEpoch 15 [2880/8883 (32.37%)]\t\tLoss: 1.16377\n",
      "Training Progress: \tEpoch 15 [3200/8883 (35.97%)]\t\tLoss: 1.33064\n",
      "Training Progress: \tEpoch 15 [3520/8883 (39.57%)]\t\tLoss: 1.28969\n",
      "Training Progress: \tEpoch 15 [3840/8883 (43.17%)]\t\tLoss: 1.13244\n",
      "Training Progress: \tEpoch 15 [4160/8883 (46.76%)]\t\tLoss: 1.18770\n",
      "Training Progress: \tEpoch 15 [4480/8883 (50.36%)]\t\tLoss: 1.00195\n",
      "Training Progress: \tEpoch 15 [4800/8883 (53.96%)]\t\tLoss: 1.00637\n",
      "Training Progress: \tEpoch 15 [5120/8883 (57.55%)]\t\tLoss: 1.18867\n",
      "Training Progress: \tEpoch 15 [5440/8883 (61.15%)]\t\tLoss: 1.32853\n",
      "Training Progress: \tEpoch 15 [5760/8883 (64.75%)]\t\tLoss: 1.12084\n",
      "Training Progress: \tEpoch 15 [6080/8883 (68.35%)]\t\tLoss: 1.05955\n",
      "Training Progress: \tEpoch 15 [6400/8883 (71.94%)]\t\tLoss: 1.14824\n",
      "Training Progress: \tEpoch 15 [6720/8883 (75.54%)]\t\tLoss: 1.17939\n",
      "Training Progress: \tEpoch 15 [7040/8883 (79.14%)]\t\tLoss: 1.22637\n",
      "Training Progress: \tEpoch 15 [7360/8883 (82.73%)]\t\tLoss: 1.34861\n",
      "Training Progress: \tEpoch 15 [7680/8883 (86.33%)]\t\tLoss: 1.11365\n",
      "Training Progress: \tEpoch 15 [8000/8883 (89.93%)]\t\tLoss: 1.18730\n",
      "Training Progress: \tEpoch 15 [8320/8883 (93.53%)]\t\tLoss: 1.06424\n",
      "Training Progress: \tEpoch 15 [8640/8883 (97.12%)]\t\tLoss: 1.18292\n",
      "\tTrain loss: 0.03161, Accuracy: 4742/8883 (53.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 980/1692 (57.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 869/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/8883 (0.00%)]\t\tLoss: 1.10453\n",
      "Training Progress: \tEpoch 16 [320/8883 (3.60%)]\t\tLoss: 1.13604\n",
      "Training Progress: \tEpoch 16 [640/8883 (7.19%)]\t\tLoss: 1.24932\n",
      "Training Progress: \tEpoch 16 [960/8883 (10.79%)]\t\tLoss: 1.06774\n",
      "Training Progress: \tEpoch 16 [1280/8883 (14.39%)]\t\tLoss: 1.20191\n",
      "Training Progress: \tEpoch 16 [1600/8883 (17.99%)]\t\tLoss: 1.32063\n",
      "Training Progress: \tEpoch 16 [1920/8883 (21.58%)]\t\tLoss: 1.26187\n",
      "Training Progress: \tEpoch 16 [2240/8883 (25.18%)]\t\tLoss: 1.10710\n",
      "Training Progress: \tEpoch 16 [2560/8883 (28.78%)]\t\tLoss: 1.09807\n",
      "Training Progress: \tEpoch 16 [2880/8883 (32.37%)]\t\tLoss: 1.10108\n",
      "Training Progress: \tEpoch 16 [3200/8883 (35.97%)]\t\tLoss: 1.26033\n",
      "Training Progress: \tEpoch 16 [3520/8883 (39.57%)]\t\tLoss: 1.30609\n",
      "Training Progress: \tEpoch 16 [3840/8883 (43.17%)]\t\tLoss: 1.31530\n",
      "Training Progress: \tEpoch 16 [4160/8883 (46.76%)]\t\tLoss: 1.24860\n",
      "Training Progress: \tEpoch 16 [4480/8883 (50.36%)]\t\tLoss: 0.92490\n",
      "Training Progress: \tEpoch 16 [4800/8883 (53.96%)]\t\tLoss: 1.05172\n",
      "Training Progress: \tEpoch 16 [5120/8883 (57.55%)]\t\tLoss: 1.31303\n",
      "Training Progress: \tEpoch 16 [5440/8883 (61.15%)]\t\tLoss: 1.27787\n",
      "Training Progress: \tEpoch 16 [5760/8883 (64.75%)]\t\tLoss: 1.20345\n",
      "Training Progress: \tEpoch 16 [6080/8883 (68.35%)]\t\tLoss: 0.95618\n",
      "Training Progress: \tEpoch 16 [6400/8883 (71.94%)]\t\tLoss: 1.06587\n",
      "Training Progress: \tEpoch 16 [6720/8883 (75.54%)]\t\tLoss: 1.16695\n",
      "Training Progress: \tEpoch 16 [7040/8883 (79.14%)]\t\tLoss: 1.14369\n",
      "Training Progress: \tEpoch 16 [7360/8883 (82.73%)]\t\tLoss: 1.16355\n",
      "Training Progress: \tEpoch 16 [7680/8883 (86.33%)]\t\tLoss: 1.24275\n",
      "Training Progress: \tEpoch 16 [8000/8883 (89.93%)]\t\tLoss: 1.14644\n",
      "Training Progress: \tEpoch 16 [8320/8883 (93.53%)]\t\tLoss: 1.12679\n",
      "Training Progress: \tEpoch 16 [8640/8883 (97.12%)]\t\tLoss: 1.12001\n",
      "\tTrain loss: 0.03126, Accuracy: 4876/8883 (54.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 990/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 871/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/8883 (0.00%)]\t\tLoss: 0.98591\n",
      "Training Progress: \tEpoch 17 [320/8883 (3.60%)]\t\tLoss: 1.02415\n",
      "Training Progress: \tEpoch 17 [640/8883 (7.19%)]\t\tLoss: 1.22004\n",
      "Training Progress: \tEpoch 17 [960/8883 (10.79%)]\t\tLoss: 1.02183\n",
      "Training Progress: \tEpoch 17 [1280/8883 (14.39%)]\t\tLoss: 1.03496\n",
      "Training Progress: \tEpoch 17 [1600/8883 (17.99%)]\t\tLoss: 1.33722\n",
      "Training Progress: \tEpoch 17 [1920/8883 (21.58%)]\t\tLoss: 1.07393\n",
      "Training Progress: \tEpoch 17 [2240/8883 (25.18%)]\t\tLoss: 1.11328\n",
      "Training Progress: \tEpoch 17 [2560/8883 (28.78%)]\t\tLoss: 1.07570\n",
      "Training Progress: \tEpoch 17 [2880/8883 (32.37%)]\t\tLoss: 1.11162\n",
      "Training Progress: \tEpoch 17 [3200/8883 (35.97%)]\t\tLoss: 1.12932\n",
      "Training Progress: \tEpoch 17 [3520/8883 (39.57%)]\t\tLoss: 1.37043\n",
      "Training Progress: \tEpoch 17 [3840/8883 (43.17%)]\t\tLoss: 1.27784\n",
      "Training Progress: \tEpoch 17 [4160/8883 (46.76%)]\t\tLoss: 1.25477\n",
      "Training Progress: \tEpoch 17 [4480/8883 (50.36%)]\t\tLoss: 1.12768\n",
      "Training Progress: \tEpoch 17 [4800/8883 (53.96%)]\t\tLoss: 1.00606\n",
      "Training Progress: \tEpoch 17 [5120/8883 (57.55%)]\t\tLoss: 1.26063\n",
      "Training Progress: \tEpoch 17 [5440/8883 (61.15%)]\t\tLoss: 1.30029\n",
      "Training Progress: \tEpoch 17 [5760/8883 (64.75%)]\t\tLoss: 1.16578\n",
      "Training Progress: \tEpoch 17 [6080/8883 (68.35%)]\t\tLoss: 0.99094\n",
      "Training Progress: \tEpoch 17 [6400/8883 (71.94%)]\t\tLoss: 1.08554\n",
      "Training Progress: \tEpoch 17 [6720/8883 (75.54%)]\t\tLoss: 1.12822\n",
      "Training Progress: \tEpoch 17 [7040/8883 (79.14%)]\t\tLoss: 1.45843\n",
      "Training Progress: \tEpoch 17 [7360/8883 (82.73%)]\t\tLoss: 1.22547\n",
      "Training Progress: \tEpoch 17 [7680/8883 (86.33%)]\t\tLoss: 1.04090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 17 [8000/8883 (89.93%)]\t\tLoss: 1.07770\n",
      "Training Progress: \tEpoch 17 [8320/8883 (93.53%)]\t\tLoss: 1.10038\n",
      "Training Progress: \tEpoch 17 [8640/8883 (97.12%)]\t\tLoss: 1.07552\n",
      "\tTrain loss: 0.03079, Accuracy: 4875/8883 (54.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1006/1692 (59.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 866/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/8883 (0.00%)]\t\tLoss: 1.02389\n",
      "Training Progress: \tEpoch 18 [320/8883 (3.60%)]\t\tLoss: 1.01128\n",
      "Training Progress: \tEpoch 18 [640/8883 (7.19%)]\t\tLoss: 1.12500\n",
      "Training Progress: \tEpoch 18 [960/8883 (10.79%)]\t\tLoss: 1.04578\n",
      "Training Progress: \tEpoch 18 [1280/8883 (14.39%)]\t\tLoss: 1.27118\n",
      "Training Progress: \tEpoch 18 [1600/8883 (17.99%)]\t\tLoss: 1.24997\n",
      "Training Progress: \tEpoch 18 [1920/8883 (21.58%)]\t\tLoss: 1.10149\n",
      "Training Progress: \tEpoch 18 [2240/8883 (25.18%)]\t\tLoss: 1.15213\n",
      "Training Progress: \tEpoch 18 [2560/8883 (28.78%)]\t\tLoss: 1.00260\n",
      "Training Progress: \tEpoch 18 [2880/8883 (32.37%)]\t\tLoss: 1.16089\n",
      "Training Progress: \tEpoch 18 [3200/8883 (35.97%)]\t\tLoss: 1.15173\n",
      "Training Progress: \tEpoch 18 [3520/8883 (39.57%)]\t\tLoss: 1.20661\n",
      "Training Progress: \tEpoch 18 [3840/8883 (43.17%)]\t\tLoss: 1.25066\n",
      "Training Progress: \tEpoch 18 [4160/8883 (46.76%)]\t\tLoss: 1.08492\n",
      "Training Progress: \tEpoch 18 [4480/8883 (50.36%)]\t\tLoss: 1.06765\n",
      "Training Progress: \tEpoch 18 [4800/8883 (53.96%)]\t\tLoss: 1.03922\n",
      "Training Progress: \tEpoch 18 [5120/8883 (57.55%)]\t\tLoss: 1.15193\n",
      "Training Progress: \tEpoch 18 [5440/8883 (61.15%)]\t\tLoss: 1.26139\n",
      "Training Progress: \tEpoch 18 [5760/8883 (64.75%)]\t\tLoss: 1.15426\n",
      "Training Progress: \tEpoch 18 [6080/8883 (68.35%)]\t\tLoss: 1.05657\n",
      "Training Progress: \tEpoch 18 [6400/8883 (71.94%)]\t\tLoss: 1.26259\n",
      "Training Progress: \tEpoch 18 [6720/8883 (75.54%)]\t\tLoss: 1.10711\n",
      "Training Progress: \tEpoch 18 [7040/8883 (79.14%)]\t\tLoss: 1.22457\n",
      "Training Progress: \tEpoch 18 [7360/8883 (82.73%)]\t\tLoss: 1.29777\n",
      "Training Progress: \tEpoch 18 [7680/8883 (86.33%)]\t\tLoss: 1.20949\n",
      "Training Progress: \tEpoch 18 [8000/8883 (89.93%)]\t\tLoss: 1.07851\n",
      "Training Progress: \tEpoch 18 [8320/8883 (93.53%)]\t\tLoss: 1.08136\n",
      "Training Progress: \tEpoch 18 [8640/8883 (97.12%)]\t\tLoss: 1.15695\n",
      "\tTrain loss: 0.03058, Accuracy: 4863/8883 (54.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1011/1692 (59.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 863/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/8883 (0.00%)]\t\tLoss: 1.01619\n",
      "Training Progress: \tEpoch 19 [320/8883 (3.60%)]\t\tLoss: 1.05351\n",
      "Training Progress: \tEpoch 19 [640/8883 (7.19%)]\t\tLoss: 1.12816\n",
      "Training Progress: \tEpoch 19 [960/8883 (10.79%)]\t\tLoss: 1.14251\n",
      "Training Progress: \tEpoch 19 [1280/8883 (14.39%)]\t\tLoss: 1.26347\n",
      "Training Progress: \tEpoch 19 [1600/8883 (17.99%)]\t\tLoss: 1.16618\n",
      "Training Progress: \tEpoch 19 [1920/8883 (21.58%)]\t\tLoss: 1.13928\n",
      "Training Progress: \tEpoch 19 [2240/8883 (25.18%)]\t\tLoss: 1.18072\n",
      "Training Progress: \tEpoch 19 [2560/8883 (28.78%)]\t\tLoss: 1.13969\n",
      "Training Progress: \tEpoch 19 [2880/8883 (32.37%)]\t\tLoss: 1.21040\n",
      "Training Progress: \tEpoch 19 [3200/8883 (35.97%)]\t\tLoss: 1.26827\n",
      "Training Progress: \tEpoch 19 [3520/8883 (39.57%)]\t\tLoss: 1.01095\n",
      "Training Progress: \tEpoch 19 [3840/8883 (43.17%)]\t\tLoss: 1.21247\n",
      "Training Progress: \tEpoch 19 [4160/8883 (46.76%)]\t\tLoss: 1.07848\n",
      "Training Progress: \tEpoch 19 [4480/8883 (50.36%)]\t\tLoss: 1.04630\n",
      "Training Progress: \tEpoch 19 [4800/8883 (53.96%)]\t\tLoss: 1.05889\n",
      "Training Progress: \tEpoch 19 [5120/8883 (57.55%)]\t\tLoss: 1.20850\n",
      "Training Progress: \tEpoch 19 [5440/8883 (61.15%)]\t\tLoss: 1.39308\n",
      "Training Progress: \tEpoch 19 [5760/8883 (64.75%)]\t\tLoss: 1.19246\n",
      "Training Progress: \tEpoch 19 [6080/8883 (68.35%)]\t\tLoss: 1.12267\n",
      "Training Progress: \tEpoch 19 [6400/8883 (71.94%)]\t\tLoss: 1.16792\n",
      "Training Progress: \tEpoch 19 [6720/8883 (75.54%)]\t\tLoss: 1.11829\n",
      "Training Progress: \tEpoch 19 [7040/8883 (79.14%)]\t\tLoss: 1.08467\n",
      "Training Progress: \tEpoch 19 [7360/8883 (82.73%)]\t\tLoss: 1.12573\n",
      "Training Progress: \tEpoch 19 [7680/8883 (86.33%)]\t\tLoss: 1.10300\n",
      "Training Progress: \tEpoch 19 [8000/8883 (89.93%)]\t\tLoss: 1.03687\n",
      "Training Progress: \tEpoch 19 [8320/8883 (93.53%)]\t\tLoss: 1.07970\n",
      "Training Progress: \tEpoch 19 [8640/8883 (97.12%)]\t\tLoss: 1.05213\n",
      "\tTrain loss: 0.02995, Accuracy: 5039/8883 (56.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 893/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/8883 (0.00%)]\t\tLoss: 1.21561\n",
      "Training Progress: \tEpoch 20 [320/8883 (3.60%)]\t\tLoss: 1.01638\n",
      "Training Progress: \tEpoch 20 [640/8883 (7.19%)]\t\tLoss: 1.04703\n",
      "Training Progress: \tEpoch 20 [960/8883 (10.79%)]\t\tLoss: 1.13433\n",
      "Training Progress: \tEpoch 20 [1280/8883 (14.39%)]\t\tLoss: 1.04201\n",
      "Training Progress: \tEpoch 20 [1600/8883 (17.99%)]\t\tLoss: 1.35195\n",
      "Training Progress: \tEpoch 20 [1920/8883 (21.58%)]\t\tLoss: 1.18919\n",
      "Training Progress: \tEpoch 20 [2240/8883 (25.18%)]\t\tLoss: 1.10917\n",
      "Training Progress: \tEpoch 20 [2560/8883 (28.78%)]\t\tLoss: 0.94668\n",
      "Training Progress: \tEpoch 20 [2880/8883 (32.37%)]\t\tLoss: 1.22780\n",
      "Training Progress: \tEpoch 20 [3200/8883 (35.97%)]\t\tLoss: 1.34386\n",
      "Training Progress: \tEpoch 20 [3520/8883 (39.57%)]\t\tLoss: 1.29847\n",
      "Training Progress: \tEpoch 20 [3840/8883 (43.17%)]\t\tLoss: 1.30101\n",
      "Training Progress: \tEpoch 20 [4160/8883 (46.76%)]\t\tLoss: 1.11801\n",
      "Training Progress: \tEpoch 20 [4480/8883 (50.36%)]\t\tLoss: 0.95688\n",
      "Training Progress: \tEpoch 20 [4800/8883 (53.96%)]\t\tLoss: 0.93760\n",
      "Training Progress: \tEpoch 20 [5120/8883 (57.55%)]\t\tLoss: 1.03408\n",
      "Training Progress: \tEpoch 20 [5440/8883 (61.15%)]\t\tLoss: 1.38449\n",
      "Training Progress: \tEpoch 20 [5760/8883 (64.75%)]\t\tLoss: 1.26539\n",
      "Training Progress: \tEpoch 20 [6080/8883 (68.35%)]\t\tLoss: 1.08512\n",
      "Training Progress: \tEpoch 20 [6400/8883 (71.94%)]\t\tLoss: 1.10541\n",
      "Training Progress: \tEpoch 20 [6720/8883 (75.54%)]\t\tLoss: 1.06929\n",
      "Training Progress: \tEpoch 20 [7040/8883 (79.14%)]\t\tLoss: 1.21300\n",
      "Training Progress: \tEpoch 20 [7360/8883 (82.73%)]\t\tLoss: 1.16931\n",
      "Training Progress: \tEpoch 20 [7680/8883 (86.33%)]\t\tLoss: 1.07090\n",
      "Training Progress: \tEpoch 20 [8000/8883 (89.93%)]\t\tLoss: 1.14564\n",
      "Training Progress: \tEpoch 20 [8320/8883 (93.53%)]\t\tLoss: 1.05061\n",
      "Training Progress: \tEpoch 20 [8640/8883 (97.12%)]\t\tLoss: 1.10631\n",
      "\tTrain loss: 0.02947, Accuracy: 5081/8883 (57.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1035/1692 (61.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 868/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/8883 (0.00%)]\t\tLoss: 1.08721\n",
      "Training Progress: \tEpoch 21 [320/8883 (3.60%)]\t\tLoss: 1.10613\n",
      "Training Progress: \tEpoch 21 [640/8883 (7.19%)]\t\tLoss: 1.00467\n",
      "Training Progress: \tEpoch 21 [960/8883 (10.79%)]\t\tLoss: 1.26056\n",
      "Training Progress: \tEpoch 21 [1280/8883 (14.39%)]\t\tLoss: 1.18298\n",
      "Training Progress: \tEpoch 21 [1600/8883 (17.99%)]\t\tLoss: 1.26482\n",
      "Training Progress: \tEpoch 21 [1920/8883 (21.58%)]\t\tLoss: 1.23290\n",
      "Training Progress: \tEpoch 21 [2240/8883 (25.18%)]\t\tLoss: 1.16712\n",
      "Training Progress: \tEpoch 21 [2560/8883 (28.78%)]\t\tLoss: 1.09242\n",
      "Training Progress: \tEpoch 21 [2880/8883 (32.37%)]\t\tLoss: 1.22803\n",
      "Training Progress: \tEpoch 21 [3200/8883 (35.97%)]\t\tLoss: 1.27761\n",
      "Training Progress: \tEpoch 21 [3520/8883 (39.57%)]\t\tLoss: 1.19677\n",
      "Training Progress: \tEpoch 21 [3840/8883 (43.17%)]\t\tLoss: 1.11458\n",
      "Training Progress: \tEpoch 21 [4160/8883 (46.76%)]\t\tLoss: 1.18359\n",
      "Training Progress: \tEpoch 21 [4480/8883 (50.36%)]\t\tLoss: 1.09330\n",
      "Training Progress: \tEpoch 21 [4800/8883 (53.96%)]\t\tLoss: 0.98975\n",
      "Training Progress: \tEpoch 21 [5120/8883 (57.55%)]\t\tLoss: 1.22368\n",
      "Training Progress: \tEpoch 21 [5440/8883 (61.15%)]\t\tLoss: 1.41365\n",
      "Training Progress: \tEpoch 21 [5760/8883 (64.75%)]\t\tLoss: 1.01955\n",
      "Training Progress: \tEpoch 21 [6080/8883 (68.35%)]\t\tLoss: 0.91252\n",
      "Training Progress: \tEpoch 21 [6400/8883 (71.94%)]\t\tLoss: 1.26175\n",
      "Training Progress: \tEpoch 21 [6720/8883 (75.54%)]\t\tLoss: 1.08774\n",
      "Training Progress: \tEpoch 21 [7040/8883 (79.14%)]\t\tLoss: 1.28911\n",
      "Training Progress: \tEpoch 21 [7360/8883 (82.73%)]\t\tLoss: 1.10219\n",
      "Training Progress: \tEpoch 21 [7680/8883 (86.33%)]\t\tLoss: 0.95739\n",
      "Training Progress: \tEpoch 21 [8000/8883 (89.93%)]\t\tLoss: 1.06342\n",
      "Training Progress: \tEpoch 21 [8320/8883 (93.53%)]\t\tLoss: 1.03783\n",
      "Training Progress: \tEpoch 21 [8640/8883 (97.12%)]\t\tLoss: 1.05414\n",
      "\tTrain loss: 0.02874, Accuracy: 5194/8883 (58.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1067/1692 (63.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 895/1772 (50.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 22 [0/8883 (0.00%)]\t\tLoss: 1.00329\n",
      "Training Progress: \tEpoch 22 [320/8883 (3.60%)]\t\tLoss: 1.25231\n",
      "Training Progress: \tEpoch 22 [640/8883 (7.19%)]\t\tLoss: 1.07150\n",
      "Training Progress: \tEpoch 22 [960/8883 (10.79%)]\t\tLoss: 1.23940\n",
      "Training Progress: \tEpoch 22 [1280/8883 (14.39%)]\t\tLoss: 1.06040\n",
      "Training Progress: \tEpoch 22 [1600/8883 (17.99%)]\t\tLoss: 1.26211\n",
      "Training Progress: \tEpoch 22 [1920/8883 (21.58%)]\t\tLoss: 1.21011\n",
      "Training Progress: \tEpoch 22 [2240/8883 (25.18%)]\t\tLoss: 1.05144\n",
      "Training Progress: \tEpoch 22 [2560/8883 (28.78%)]\t\tLoss: 1.08920\n",
      "Training Progress: \tEpoch 22 [2880/8883 (32.37%)]\t\tLoss: 1.01004\n",
      "Training Progress: \tEpoch 22 [3200/8883 (35.97%)]\t\tLoss: 1.26823\n",
      "Training Progress: \tEpoch 22 [3520/8883 (39.57%)]\t\tLoss: 1.17655\n",
      "Training Progress: \tEpoch 22 [3840/8883 (43.17%)]\t\tLoss: 1.07863\n",
      "Training Progress: \tEpoch 22 [4160/8883 (46.76%)]\t\tLoss: 1.09672\n",
      "Training Progress: \tEpoch 22 [4480/8883 (50.36%)]\t\tLoss: 1.18541\n",
      "Training Progress: \tEpoch 22 [4800/8883 (53.96%)]\t\tLoss: 0.88297\n",
      "Training Progress: \tEpoch 22 [5120/8883 (57.55%)]\t\tLoss: 1.06159\n",
      "Training Progress: \tEpoch 22 [5440/8883 (61.15%)]\t\tLoss: 1.28225\n",
      "Training Progress: \tEpoch 22 [5760/8883 (64.75%)]\t\tLoss: 1.14912\n",
      "Training Progress: \tEpoch 22 [6080/8883 (68.35%)]\t\tLoss: 1.03509\n",
      "Training Progress: \tEpoch 22 [6400/8883 (71.94%)]\t\tLoss: 1.14986\n",
      "Training Progress: \tEpoch 22 [6720/8883 (75.54%)]\t\tLoss: 1.11340\n",
      "Training Progress: \tEpoch 22 [7040/8883 (79.14%)]\t\tLoss: 1.25404\n",
      "Training Progress: \tEpoch 22 [7360/8883 (82.73%)]\t\tLoss: 1.26480\n",
      "Training Progress: \tEpoch 22 [7680/8883 (86.33%)]\t\tLoss: 1.05830\n",
      "Training Progress: \tEpoch 22 [8000/8883 (89.93%)]\t\tLoss: 1.10894\n",
      "Training Progress: \tEpoch 22 [8320/8883 (93.53%)]\t\tLoss: 0.94520\n",
      "Training Progress: \tEpoch 22 [8640/8883 (97.12%)]\t\tLoss: 1.05289\n",
      "\tTrain loss: 0.02867, Accuracy: 5205/8883 (58.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1071/1692 (63.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 884/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/8883 (0.00%)]\t\tLoss: 1.00423\n",
      "Training Progress: \tEpoch 23 [320/8883 (3.60%)]\t\tLoss: 0.99105\n",
      "Training Progress: \tEpoch 23 [640/8883 (7.19%)]\t\tLoss: 1.09322\n",
      "Training Progress: \tEpoch 23 [960/8883 (10.79%)]\t\tLoss: 1.19473\n",
      "Training Progress: \tEpoch 23 [1280/8883 (14.39%)]\t\tLoss: 1.20461\n",
      "Training Progress: \tEpoch 23 [1600/8883 (17.99%)]\t\tLoss: 1.10537\n",
      "Training Progress: \tEpoch 23 [1920/8883 (21.58%)]\t\tLoss: 1.15983\n",
      "Training Progress: \tEpoch 23 [2240/8883 (25.18%)]\t\tLoss: 1.13445\n",
      "Training Progress: \tEpoch 23 [2560/8883 (28.78%)]\t\tLoss: 1.22249\n",
      "Training Progress: \tEpoch 23 [2880/8883 (32.37%)]\t\tLoss: 1.11093\n",
      "Training Progress: \tEpoch 23 [3200/8883 (35.97%)]\t\tLoss: 1.01309\n",
      "Training Progress: \tEpoch 23 [3520/8883 (39.57%)]\t\tLoss: 1.06991\n",
      "Training Progress: \tEpoch 23 [3840/8883 (43.17%)]\t\tLoss: 1.44741\n",
      "Training Progress: \tEpoch 23 [4160/8883 (46.76%)]\t\tLoss: 1.20914\n",
      "Training Progress: \tEpoch 23 [4480/8883 (50.36%)]\t\tLoss: 0.88032\n",
      "Training Progress: \tEpoch 23 [4800/8883 (53.96%)]\t\tLoss: 0.84910\n",
      "Training Progress: \tEpoch 23 [5120/8883 (57.55%)]\t\tLoss: 1.04171\n",
      "Training Progress: \tEpoch 23 [5440/8883 (61.15%)]\t\tLoss: 1.31800\n",
      "Training Progress: \tEpoch 23 [5760/8883 (64.75%)]\t\tLoss: 1.09760\n",
      "Training Progress: \tEpoch 23 [6080/8883 (68.35%)]\t\tLoss: 0.97360\n",
      "Training Progress: \tEpoch 23 [6400/8883 (71.94%)]\t\tLoss: 1.18243\n",
      "Training Progress: \tEpoch 23 [6720/8883 (75.54%)]\t\tLoss: 1.01695\n",
      "Training Progress: \tEpoch 23 [7040/8883 (79.14%)]\t\tLoss: 1.08353\n",
      "Training Progress: \tEpoch 23 [7360/8883 (82.73%)]\t\tLoss: 1.07007\n",
      "Training Progress: \tEpoch 23 [7680/8883 (86.33%)]\t\tLoss: 1.00737\n",
      "Training Progress: \tEpoch 23 [8000/8883 (89.93%)]\t\tLoss: 1.08328\n",
      "Training Progress: \tEpoch 23 [8320/8883 (93.53%)]\t\tLoss: 0.97618\n",
      "Training Progress: \tEpoch 23 [8640/8883 (97.12%)]\t\tLoss: 1.00652\n",
      "\tTrain loss: 0.02905, Accuracy: 5217/8883 (58.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1092/1692 (64.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/8883 (0.00%)]\t\tLoss: 0.97033\n",
      "Training Progress: \tEpoch 24 [320/8883 (3.60%)]\t\tLoss: 1.12881\n",
      "Training Progress: \tEpoch 24 [640/8883 (7.19%)]\t\tLoss: 1.09302\n",
      "Training Progress: \tEpoch 24 [960/8883 (10.79%)]\t\tLoss: 1.05654\n",
      "Training Progress: \tEpoch 24 [1280/8883 (14.39%)]\t\tLoss: 1.20049\n",
      "Training Progress: \tEpoch 24 [1600/8883 (17.99%)]\t\tLoss: 1.20912\n",
      "Training Progress: \tEpoch 24 [1920/8883 (21.58%)]\t\tLoss: 1.32929\n",
      "Training Progress: \tEpoch 24 [2240/8883 (25.18%)]\t\tLoss: 1.02551\n",
      "Training Progress: \tEpoch 24 [2560/8883 (28.78%)]\t\tLoss: 1.06284\n",
      "Training Progress: \tEpoch 24 [2880/8883 (32.37%)]\t\tLoss: 0.99134\n",
      "Training Progress: \tEpoch 24 [3200/8883 (35.97%)]\t\tLoss: 1.22670\n",
      "Training Progress: \tEpoch 24 [3520/8883 (39.57%)]\t\tLoss: 1.08339\n",
      "Training Progress: \tEpoch 24 [3840/8883 (43.17%)]\t\tLoss: 1.39810\n",
      "Training Progress: \tEpoch 24 [4160/8883 (46.76%)]\t\tLoss: 1.00314\n",
      "Training Progress: \tEpoch 24 [4480/8883 (50.36%)]\t\tLoss: 0.95427\n",
      "Training Progress: \tEpoch 24 [4800/8883 (53.96%)]\t\tLoss: 0.96851\n",
      "Training Progress: \tEpoch 24 [5120/8883 (57.55%)]\t\tLoss: 1.11671\n",
      "Training Progress: \tEpoch 24 [5440/8883 (61.15%)]\t\tLoss: 1.39535\n",
      "Training Progress: \tEpoch 24 [5760/8883 (64.75%)]\t\tLoss: 1.16256\n",
      "Training Progress: \tEpoch 24 [6080/8883 (68.35%)]\t\tLoss: 1.03202\n",
      "Training Progress: \tEpoch 24 [6400/8883 (71.94%)]\t\tLoss: 1.09036\n",
      "Training Progress: \tEpoch 24 [6720/8883 (75.54%)]\t\tLoss: 1.09096\n",
      "Training Progress: \tEpoch 24 [7040/8883 (79.14%)]\t\tLoss: 0.98758\n",
      "Training Progress: \tEpoch 24 [7360/8883 (82.73%)]\t\tLoss: 1.14599\n",
      "Training Progress: \tEpoch 24 [7680/8883 (86.33%)]\t\tLoss: 1.12848\n",
      "Training Progress: \tEpoch 24 [8000/8883 (89.93%)]\t\tLoss: 1.14219\n",
      "Training Progress: \tEpoch 24 [8320/8883 (93.53%)]\t\tLoss: 1.03572\n",
      "Training Progress: \tEpoch 24 [8640/8883 (97.12%)]\t\tLoss: 1.02995\n",
      "\tTrain loss: 0.02853, Accuracy: 5244/8883 (59.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1099/1692 (64.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/8883 (0.00%)]\t\tLoss: 0.97509\n",
      "Training Progress: \tEpoch 25 [320/8883 (3.60%)]\t\tLoss: 1.01856\n",
      "Training Progress: \tEpoch 25 [640/8883 (7.19%)]\t\tLoss: 1.01090\n",
      "Training Progress: \tEpoch 25 [960/8883 (10.79%)]\t\tLoss: 1.01715\n",
      "Training Progress: \tEpoch 25 [1280/8883 (14.39%)]\t\tLoss: 1.10478\n",
      "Training Progress: \tEpoch 25 [1600/8883 (17.99%)]\t\tLoss: 1.23729\n",
      "Training Progress: \tEpoch 25 [1920/8883 (21.58%)]\t\tLoss: 1.12921\n",
      "Training Progress: \tEpoch 25 [2240/8883 (25.18%)]\t\tLoss: 1.17644\n",
      "Training Progress: \tEpoch 25 [2560/8883 (28.78%)]\t\tLoss: 1.04630\n",
      "Training Progress: \tEpoch 25 [2880/8883 (32.37%)]\t\tLoss: 1.17910\n",
      "Training Progress: \tEpoch 25 [3200/8883 (35.97%)]\t\tLoss: 1.16409\n",
      "Training Progress: \tEpoch 25 [3520/8883 (39.57%)]\t\tLoss: 1.16357\n",
      "Training Progress: \tEpoch 25 [3840/8883 (43.17%)]\t\tLoss: 1.32178\n",
      "Training Progress: \tEpoch 25 [4160/8883 (46.76%)]\t\tLoss: 1.11851\n",
      "Training Progress: \tEpoch 25 [4480/8883 (50.36%)]\t\tLoss: 0.91148\n",
      "Training Progress: \tEpoch 25 [4800/8883 (53.96%)]\t\tLoss: 0.92201\n",
      "Training Progress: \tEpoch 25 [5120/8883 (57.55%)]\t\tLoss: 1.04951\n",
      "Training Progress: \tEpoch 25 [5440/8883 (61.15%)]\t\tLoss: 1.31405\n",
      "Training Progress: \tEpoch 25 [5760/8883 (64.75%)]\t\tLoss: 1.02685\n",
      "Training Progress: \tEpoch 25 [6080/8883 (68.35%)]\t\tLoss: 0.96272\n",
      "Training Progress: \tEpoch 25 [6400/8883 (71.94%)]\t\tLoss: 1.06082\n",
      "Training Progress: \tEpoch 25 [6720/8883 (75.54%)]\t\tLoss: 1.16120\n",
      "Training Progress: \tEpoch 25 [7040/8883 (79.14%)]\t\tLoss: 1.16427\n",
      "Training Progress: \tEpoch 25 [7360/8883 (82.73%)]\t\tLoss: 1.09877\n",
      "Training Progress: \tEpoch 25 [7680/8883 (86.33%)]\t\tLoss: 1.03802\n",
      "Training Progress: \tEpoch 25 [8000/8883 (89.93%)]\t\tLoss: 0.94005\n",
      "Training Progress: \tEpoch 25 [8320/8883 (93.53%)]\t\tLoss: 0.95303\n",
      "Training Progress: \tEpoch 25 [8640/8883 (97.12%)]\t\tLoss: 1.18777\n",
      "\tTrain loss: 0.02735, Accuracy: 5326/8883 (59.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1104/1692 (65.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 860/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/8883 (0.00%)]\t\tLoss: 0.94300\n",
      "Training Progress: \tEpoch 26 [320/8883 (3.60%)]\t\tLoss: 1.03121\n",
      "Training Progress: \tEpoch 26 [640/8883 (7.19%)]\t\tLoss: 0.92608\n",
      "Training Progress: \tEpoch 26 [960/8883 (10.79%)]\t\tLoss: 1.07037\n",
      "Training Progress: \tEpoch 26 [1280/8883 (14.39%)]\t\tLoss: 1.19053\n",
      "Training Progress: \tEpoch 26 [1600/8883 (17.99%)]\t\tLoss: 1.22604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 26 [1920/8883 (21.58%)]\t\tLoss: 1.16548\n",
      "Training Progress: \tEpoch 26 [2240/8883 (25.18%)]\t\tLoss: 1.09367\n",
      "Training Progress: \tEpoch 26 [2560/8883 (28.78%)]\t\tLoss: 0.95261\n",
      "Training Progress: \tEpoch 26 [2880/8883 (32.37%)]\t\tLoss: 1.06053\n",
      "Training Progress: \tEpoch 26 [3200/8883 (35.97%)]\t\tLoss: 1.30250\n",
      "Training Progress: \tEpoch 26 [3520/8883 (39.57%)]\t\tLoss: 1.16357\n",
      "Training Progress: \tEpoch 26 [3840/8883 (43.17%)]\t\tLoss: 1.15316\n",
      "Training Progress: \tEpoch 26 [4160/8883 (46.76%)]\t\tLoss: 1.20480\n",
      "Training Progress: \tEpoch 26 [4480/8883 (50.36%)]\t\tLoss: 0.99305\n",
      "Training Progress: \tEpoch 26 [4800/8883 (53.96%)]\t\tLoss: 0.85709\n",
      "Training Progress: \tEpoch 26 [5120/8883 (57.55%)]\t\tLoss: 1.07396\n",
      "Training Progress: \tEpoch 26 [5440/8883 (61.15%)]\t\tLoss: 1.31510\n",
      "Training Progress: \tEpoch 26 [5760/8883 (64.75%)]\t\tLoss: 1.20330\n",
      "Training Progress: \tEpoch 26 [6080/8883 (68.35%)]\t\tLoss: 0.94914\n",
      "Training Progress: \tEpoch 26 [6400/8883 (71.94%)]\t\tLoss: 1.20684\n",
      "Training Progress: \tEpoch 26 [6720/8883 (75.54%)]\t\tLoss: 0.98160\n",
      "Training Progress: \tEpoch 26 [7040/8883 (79.14%)]\t\tLoss: 1.05440\n",
      "Training Progress: \tEpoch 26 [7360/8883 (82.73%)]\t\tLoss: 1.02623\n",
      "Training Progress: \tEpoch 26 [7680/8883 (86.33%)]\t\tLoss: 1.02981\n",
      "Training Progress: \tEpoch 26 [8000/8883 (89.93%)]\t\tLoss: 1.13738\n",
      "Training Progress: \tEpoch 26 [8320/8883 (93.53%)]\t\tLoss: 1.03706\n",
      "Training Progress: \tEpoch 26 [8640/8883 (97.12%)]\t\tLoss: 1.05458\n",
      "\tTrain loss: 0.02756, Accuracy: 5350/8883 (60.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1117/1692 (66.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 873/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/8883 (0.00%)]\t\tLoss: 0.97859\n",
      "Training Progress: \tEpoch 27 [320/8883 (3.60%)]\t\tLoss: 0.85722\n",
      "Training Progress: \tEpoch 27 [640/8883 (7.19%)]\t\tLoss: 0.98372\n",
      "Training Progress: \tEpoch 27 [960/8883 (10.79%)]\t\tLoss: 1.01340\n",
      "Training Progress: \tEpoch 27 [1280/8883 (14.39%)]\t\tLoss: 1.11847\n",
      "Training Progress: \tEpoch 27 [1600/8883 (17.99%)]\t\tLoss: 1.24151\n",
      "Training Progress: \tEpoch 27 [1920/8883 (21.58%)]\t\tLoss: 1.16786\n",
      "Training Progress: \tEpoch 27 [2240/8883 (25.18%)]\t\tLoss: 1.17730\n",
      "Training Progress: \tEpoch 27 [2560/8883 (28.78%)]\t\tLoss: 1.13950\n",
      "Training Progress: \tEpoch 27 [2880/8883 (32.37%)]\t\tLoss: 1.12838\n",
      "Training Progress: \tEpoch 27 [3200/8883 (35.97%)]\t\tLoss: 1.03746\n",
      "Training Progress: \tEpoch 27 [3520/8883 (39.57%)]\t\tLoss: 1.02026\n",
      "Training Progress: \tEpoch 27 [3840/8883 (43.17%)]\t\tLoss: 1.13156\n",
      "Training Progress: \tEpoch 27 [4160/8883 (46.76%)]\t\tLoss: 1.03538\n",
      "Training Progress: \tEpoch 27 [4480/8883 (50.36%)]\t\tLoss: 0.85782\n",
      "Training Progress: \tEpoch 27 [4800/8883 (53.96%)]\t\tLoss: 0.93357\n",
      "Training Progress: \tEpoch 27 [5120/8883 (57.55%)]\t\tLoss: 1.25682\n",
      "Training Progress: \tEpoch 27 [5440/8883 (61.15%)]\t\tLoss: 1.40676\n",
      "Training Progress: \tEpoch 27 [5760/8883 (64.75%)]\t\tLoss: 1.09758\n",
      "Training Progress: \tEpoch 27 [6080/8883 (68.35%)]\t\tLoss: 0.99310\n",
      "Training Progress: \tEpoch 27 [6400/8883 (71.94%)]\t\tLoss: 1.21793\n",
      "Training Progress: \tEpoch 27 [6720/8883 (75.54%)]\t\tLoss: 1.11900\n",
      "Training Progress: \tEpoch 27 [7040/8883 (79.14%)]\t\tLoss: 1.18528\n",
      "Training Progress: \tEpoch 27 [7360/8883 (82.73%)]\t\tLoss: 1.03312\n",
      "Training Progress: \tEpoch 27 [7680/8883 (86.33%)]\t\tLoss: 1.06053\n",
      "Training Progress: \tEpoch 27 [8000/8883 (89.93%)]\t\tLoss: 0.98809\n",
      "Training Progress: \tEpoch 27 [8320/8883 (93.53%)]\t\tLoss: 0.91225\n",
      "Training Progress: \tEpoch 27 [8640/8883 (97.12%)]\t\tLoss: 1.14766\n",
      "\tTrain loss: 0.02669, Accuracy: 5354/8883 (60.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1119/1692 (66.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 881/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/8883 (0.00%)]\t\tLoss: 0.87860\n",
      "Training Progress: \tEpoch 28 [320/8883 (3.60%)]\t\tLoss: 1.19446\n",
      "Training Progress: \tEpoch 28 [640/8883 (7.19%)]\t\tLoss: 1.07528\n",
      "Training Progress: \tEpoch 28 [960/8883 (10.79%)]\t\tLoss: 1.10289\n",
      "Training Progress: \tEpoch 28 [1280/8883 (14.39%)]\t\tLoss: 1.14172\n",
      "Training Progress: \tEpoch 28 [1600/8883 (17.99%)]\t\tLoss: 1.11503\n",
      "Training Progress: \tEpoch 28 [1920/8883 (21.58%)]\t\tLoss: 1.13786\n",
      "Training Progress: \tEpoch 28 [2240/8883 (25.18%)]\t\tLoss: 1.08789\n",
      "Training Progress: \tEpoch 28 [2560/8883 (28.78%)]\t\tLoss: 0.93725\n",
      "Training Progress: \tEpoch 28 [2880/8883 (32.37%)]\t\tLoss: 1.01232\n",
      "Training Progress: \tEpoch 28 [3200/8883 (35.97%)]\t\tLoss: 1.20741\n",
      "Training Progress: \tEpoch 28 [3520/8883 (39.57%)]\t\tLoss: 1.07406\n",
      "Training Progress: \tEpoch 28 [3840/8883 (43.17%)]\t\tLoss: 0.98314\n",
      "Training Progress: \tEpoch 28 [4160/8883 (46.76%)]\t\tLoss: 1.00087\n",
      "Training Progress: \tEpoch 28 [4480/8883 (50.36%)]\t\tLoss: 0.95552\n",
      "Training Progress: \tEpoch 28 [4800/8883 (53.96%)]\t\tLoss: 0.97410\n",
      "Training Progress: \tEpoch 28 [5120/8883 (57.55%)]\t\tLoss: 1.15626\n",
      "Training Progress: \tEpoch 28 [5440/8883 (61.15%)]\t\tLoss: 1.51056\n",
      "Training Progress: \tEpoch 28 [5760/8883 (64.75%)]\t\tLoss: 1.24888\n",
      "Training Progress: \tEpoch 28 [6080/8883 (68.35%)]\t\tLoss: 0.87569\n",
      "Training Progress: \tEpoch 28 [6400/8883 (71.94%)]\t\tLoss: 1.03330\n",
      "Training Progress: \tEpoch 28 [6720/8883 (75.54%)]\t\tLoss: 1.09757\n",
      "Training Progress: \tEpoch 28 [7040/8883 (79.14%)]\t\tLoss: 1.08353\n",
      "Training Progress: \tEpoch 28 [7360/8883 (82.73%)]\t\tLoss: 1.15149\n",
      "Training Progress: \tEpoch 28 [7680/8883 (86.33%)]\t\tLoss: 0.92627\n",
      "Training Progress: \tEpoch 28 [8000/8883 (89.93%)]\t\tLoss: 0.95922\n",
      "Training Progress: \tEpoch 28 [8320/8883 (93.53%)]\t\tLoss: 0.92065\n",
      "Training Progress: \tEpoch 28 [8640/8883 (97.12%)]\t\tLoss: 0.96283\n",
      "\tTrain loss: 0.02684, Accuracy: 5374/8883 (60.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 898/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/8883 (0.00%)]\t\tLoss: 1.03350\n",
      "Training Progress: \tEpoch 29 [320/8883 (3.60%)]\t\tLoss: 1.08983\n",
      "Training Progress: \tEpoch 29 [640/8883 (7.19%)]\t\tLoss: 0.97804\n",
      "Training Progress: \tEpoch 29 [960/8883 (10.79%)]\t\tLoss: 1.01734\n",
      "Training Progress: \tEpoch 29 [1280/8883 (14.39%)]\t\tLoss: 1.09093\n",
      "Training Progress: \tEpoch 29 [1600/8883 (17.99%)]\t\tLoss: 1.15829\n",
      "Training Progress: \tEpoch 29 [1920/8883 (21.58%)]\t\tLoss: 1.14657\n",
      "Training Progress: \tEpoch 29 [2240/8883 (25.18%)]\t\tLoss: 1.28391\n",
      "Training Progress: \tEpoch 29 [2560/8883 (28.78%)]\t\tLoss: 1.00690\n",
      "Training Progress: \tEpoch 29 [2880/8883 (32.37%)]\t\tLoss: 1.11183\n",
      "Training Progress: \tEpoch 29 [3200/8883 (35.97%)]\t\tLoss: 1.10651\n",
      "Training Progress: \tEpoch 29 [3520/8883 (39.57%)]\t\tLoss: 1.15240\n",
      "Training Progress: \tEpoch 29 [3840/8883 (43.17%)]\t\tLoss: 1.16169\n",
      "Training Progress: \tEpoch 29 [4160/8883 (46.76%)]\t\tLoss: 1.13840\n",
      "Training Progress: \tEpoch 29 [4480/8883 (50.36%)]\t\tLoss: 1.07906\n",
      "Training Progress: \tEpoch 29 [4800/8883 (53.96%)]\t\tLoss: 0.97077\n",
      "Training Progress: \tEpoch 29 [5120/8883 (57.55%)]\t\tLoss: 1.19216\n",
      "Training Progress: \tEpoch 29 [5440/8883 (61.15%)]\t\tLoss: 1.34000\n",
      "Training Progress: \tEpoch 29 [5760/8883 (64.75%)]\t\tLoss: 1.07124\n",
      "Training Progress: \tEpoch 29 [6080/8883 (68.35%)]\t\tLoss: 0.90549\n",
      "Training Progress: \tEpoch 29 [6400/8883 (71.94%)]\t\tLoss: 0.94648\n",
      "Training Progress: \tEpoch 29 [6720/8883 (75.54%)]\t\tLoss: 1.08744\n",
      "Training Progress: \tEpoch 29 [7040/8883 (79.14%)]\t\tLoss: 0.95120\n",
      "Training Progress: \tEpoch 29 [7360/8883 (82.73%)]\t\tLoss: 1.13579\n",
      "Training Progress: \tEpoch 29 [7680/8883 (86.33%)]\t\tLoss: 1.00365\n",
      "Training Progress: \tEpoch 29 [8000/8883 (89.93%)]\t\tLoss: 0.94570\n",
      "Training Progress: \tEpoch 29 [8320/8883 (93.53%)]\t\tLoss: 0.99859\n",
      "Training Progress: \tEpoch 29 [8640/8883 (97.12%)]\t\tLoss: 0.85061\n",
      "\tTrain loss: 0.02609, Accuracy: 5519/8883 (62.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1165/1692 (68.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/8883 (0.00%)]\t\tLoss: 0.88002\n",
      "Training Progress: \tEpoch 30 [320/8883 (3.60%)]\t\tLoss: 1.05729\n",
      "Training Progress: \tEpoch 30 [640/8883 (7.19%)]\t\tLoss: 0.97689\n",
      "Training Progress: \tEpoch 30 [960/8883 (10.79%)]\t\tLoss: 1.04446\n",
      "Training Progress: \tEpoch 30 [1280/8883 (14.39%)]\t\tLoss: 1.10328\n",
      "Training Progress: \tEpoch 30 [1600/8883 (17.99%)]\t\tLoss: 1.29444\n",
      "Training Progress: \tEpoch 30 [1920/8883 (21.58%)]\t\tLoss: 0.94930\n",
      "Training Progress: \tEpoch 30 [2240/8883 (25.18%)]\t\tLoss: 1.22122\n",
      "Training Progress: \tEpoch 30 [2560/8883 (28.78%)]\t\tLoss: 1.13217\n",
      "Training Progress: \tEpoch 30 [2880/8883 (32.37%)]\t\tLoss: 1.12551\n",
      "Training Progress: \tEpoch 30 [3200/8883 (35.97%)]\t\tLoss: 1.17133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 30 [3520/8883 (39.57%)]\t\tLoss: 1.09948\n",
      "Training Progress: \tEpoch 30 [3840/8883 (43.17%)]\t\tLoss: 1.31229\n",
      "Training Progress: \tEpoch 30 [4160/8883 (46.76%)]\t\tLoss: 1.18419\n",
      "Training Progress: \tEpoch 30 [4480/8883 (50.36%)]\t\tLoss: 0.99123\n",
      "Training Progress: \tEpoch 30 [4800/8883 (53.96%)]\t\tLoss: 0.86748\n",
      "Training Progress: \tEpoch 30 [5120/8883 (57.55%)]\t\tLoss: 1.22699\n",
      "Training Progress: \tEpoch 30 [5440/8883 (61.15%)]\t\tLoss: 1.32661\n",
      "Training Progress: \tEpoch 30 [5760/8883 (64.75%)]\t\tLoss: 1.05193\n",
      "Training Progress: \tEpoch 30 [6080/8883 (68.35%)]\t\tLoss: 0.99304\n",
      "Training Progress: \tEpoch 30 [6400/8883 (71.94%)]\t\tLoss: 1.00513\n",
      "Training Progress: \tEpoch 30 [6720/8883 (75.54%)]\t\tLoss: 1.08598\n",
      "Training Progress: \tEpoch 30 [7040/8883 (79.14%)]\t\tLoss: 1.04810\n",
      "Training Progress: \tEpoch 30 [7360/8883 (82.73%)]\t\tLoss: 1.04256\n",
      "Training Progress: \tEpoch 30 [7680/8883 (86.33%)]\t\tLoss: 0.93993\n",
      "Training Progress: \tEpoch 30 [8000/8883 (89.93%)]\t\tLoss: 1.01906\n",
      "Training Progress: \tEpoch 30 [8320/8883 (93.53%)]\t\tLoss: 0.91939\n",
      "Training Progress: \tEpoch 30 [8640/8883 (97.12%)]\t\tLoss: 1.24399\n",
      "\tTrain loss: 0.02622, Accuracy: 5450/8883 (61.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1137/1692 (67.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 899/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/8883 (0.00%)]\t\tLoss: 1.04012\n",
      "Training Progress: \tEpoch 31 [320/8883 (3.60%)]\t\tLoss: 1.11580\n",
      "Training Progress: \tEpoch 31 [640/8883 (7.19%)]\t\tLoss: 0.93913\n",
      "Training Progress: \tEpoch 31 [960/8883 (10.79%)]\t\tLoss: 0.97613\n",
      "Training Progress: \tEpoch 31 [1280/8883 (14.39%)]\t\tLoss: 1.13626\n",
      "Training Progress: \tEpoch 31 [1600/8883 (17.99%)]\t\tLoss: 1.21483\n",
      "Training Progress: \tEpoch 31 [1920/8883 (21.58%)]\t\tLoss: 1.38985\n",
      "Training Progress: \tEpoch 31 [2240/8883 (25.18%)]\t\tLoss: 1.09074\n",
      "Training Progress: \tEpoch 31 [2560/8883 (28.78%)]\t\tLoss: 0.91221\n",
      "Training Progress: \tEpoch 31 [2880/8883 (32.37%)]\t\tLoss: 1.10904\n",
      "Training Progress: \tEpoch 31 [3200/8883 (35.97%)]\t\tLoss: 1.27974\n",
      "Training Progress: \tEpoch 31 [3520/8883 (39.57%)]\t\tLoss: 0.94306\n",
      "Training Progress: \tEpoch 31 [3840/8883 (43.17%)]\t\tLoss: 1.17357\n",
      "Training Progress: \tEpoch 31 [4160/8883 (46.76%)]\t\tLoss: 1.09346\n",
      "Training Progress: \tEpoch 31 [4480/8883 (50.36%)]\t\tLoss: 0.91357\n",
      "Training Progress: \tEpoch 31 [4800/8883 (53.96%)]\t\tLoss: 0.94857\n",
      "Training Progress: \tEpoch 31 [5120/8883 (57.55%)]\t\tLoss: 1.46768\n",
      "Training Progress: \tEpoch 31 [5440/8883 (61.15%)]\t\tLoss: 1.27854\n",
      "Training Progress: \tEpoch 31 [5760/8883 (64.75%)]\t\tLoss: 1.09890\n",
      "Training Progress: \tEpoch 31 [6080/8883 (68.35%)]\t\tLoss: 0.84894\n",
      "Training Progress: \tEpoch 31 [6400/8883 (71.94%)]\t\tLoss: 1.22728\n",
      "Training Progress: \tEpoch 31 [6720/8883 (75.54%)]\t\tLoss: 1.08671\n",
      "Training Progress: \tEpoch 31 [7040/8883 (79.14%)]\t\tLoss: 1.10446\n",
      "Training Progress: \tEpoch 31 [7360/8883 (82.73%)]\t\tLoss: 1.30908\n",
      "Training Progress: \tEpoch 31 [7680/8883 (86.33%)]\t\tLoss: 0.94825\n",
      "Training Progress: \tEpoch 31 [8000/8883 (89.93%)]\t\tLoss: 0.97232\n",
      "Training Progress: \tEpoch 31 [8320/8883 (93.53%)]\t\tLoss: 0.97589\n",
      "Training Progress: \tEpoch 31 [8640/8883 (97.12%)]\t\tLoss: 0.97388\n",
      "\tTrain loss: 0.02589, Accuracy: 5567/8883 (62.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1165/1692 (68.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 913/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/8883 (0.00%)]\t\tLoss: 0.95525\n",
      "Training Progress: \tEpoch 32 [320/8883 (3.60%)]\t\tLoss: 0.96098\n",
      "Training Progress: \tEpoch 32 [640/8883 (7.19%)]\t\tLoss: 0.87111\n",
      "Training Progress: \tEpoch 32 [960/8883 (10.79%)]\t\tLoss: 1.00200\n",
      "Training Progress: \tEpoch 32 [1280/8883 (14.39%)]\t\tLoss: 1.03204\n",
      "Training Progress: \tEpoch 32 [1600/8883 (17.99%)]\t\tLoss: 1.30745\n",
      "Training Progress: \tEpoch 32 [1920/8883 (21.58%)]\t\tLoss: 1.01041\n",
      "Training Progress: \tEpoch 32 [2240/8883 (25.18%)]\t\tLoss: 0.97709\n",
      "Training Progress: \tEpoch 32 [2560/8883 (28.78%)]\t\tLoss: 0.99660\n",
      "Training Progress: \tEpoch 32 [2880/8883 (32.37%)]\t\tLoss: 1.12517\n",
      "Training Progress: \tEpoch 32 [3200/8883 (35.97%)]\t\tLoss: 0.97258\n",
      "Training Progress: \tEpoch 32 [3520/8883 (39.57%)]\t\tLoss: 0.96151\n",
      "Training Progress: \tEpoch 32 [3840/8883 (43.17%)]\t\tLoss: 1.18006\n",
      "Training Progress: \tEpoch 32 [4160/8883 (46.76%)]\t\tLoss: 0.99577\n",
      "Training Progress: \tEpoch 32 [4480/8883 (50.36%)]\t\tLoss: 1.00466\n",
      "Training Progress: \tEpoch 32 [4800/8883 (53.96%)]\t\tLoss: 1.04874\n",
      "Training Progress: \tEpoch 32 [5120/8883 (57.55%)]\t\tLoss: 1.17704\n",
      "Training Progress: \tEpoch 32 [5440/8883 (61.15%)]\t\tLoss: 1.23678\n",
      "Training Progress: \tEpoch 32 [5760/8883 (64.75%)]\t\tLoss: 1.07879\n",
      "Training Progress: \tEpoch 32 [6080/8883 (68.35%)]\t\tLoss: 1.12195\n",
      "Training Progress: \tEpoch 32 [6400/8883 (71.94%)]\t\tLoss: 1.08018\n",
      "Training Progress: \tEpoch 32 [6720/8883 (75.54%)]\t\tLoss: 1.00696\n",
      "Training Progress: \tEpoch 32 [7040/8883 (79.14%)]\t\tLoss: 0.95655\n",
      "Training Progress: \tEpoch 32 [7360/8883 (82.73%)]\t\tLoss: 1.14071\n",
      "Training Progress: \tEpoch 32 [7680/8883 (86.33%)]\t\tLoss: 1.06837\n",
      "Training Progress: \tEpoch 32 [8000/8883 (89.93%)]\t\tLoss: 1.09159\n",
      "Training Progress: \tEpoch 32 [8320/8883 (93.53%)]\t\tLoss: 0.97740\n",
      "Training Progress: \tEpoch 32 [8640/8883 (97.12%)]\t\tLoss: 1.03080\n",
      "\tTrain loss: 0.02606, Accuracy: 5569/8883 (62.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/8883 (0.00%)]\t\tLoss: 0.87589\n",
      "Training Progress: \tEpoch 33 [320/8883 (3.60%)]\t\tLoss: 1.02011\n",
      "Training Progress: \tEpoch 33 [640/8883 (7.19%)]\t\tLoss: 1.09281\n",
      "Training Progress: \tEpoch 33 [960/8883 (10.79%)]\t\tLoss: 0.99599\n",
      "Training Progress: \tEpoch 33 [1280/8883 (14.39%)]\t\tLoss: 0.94689\n",
      "Training Progress: \tEpoch 33 [1600/8883 (17.99%)]\t\tLoss: 1.19445\n",
      "Training Progress: \tEpoch 33 [1920/8883 (21.58%)]\t\tLoss: 1.03465\n",
      "Training Progress: \tEpoch 33 [2240/8883 (25.18%)]\t\tLoss: 1.01253\n",
      "Training Progress: \tEpoch 33 [2560/8883 (28.78%)]\t\tLoss: 1.02017\n",
      "Training Progress: \tEpoch 33 [2880/8883 (32.37%)]\t\tLoss: 1.20154\n",
      "Training Progress: \tEpoch 33 [3200/8883 (35.97%)]\t\tLoss: 1.24351\n",
      "Training Progress: \tEpoch 33 [3520/8883 (39.57%)]\t\tLoss: 1.15221\n",
      "Training Progress: \tEpoch 33 [3840/8883 (43.17%)]\t\tLoss: 1.20903\n",
      "Training Progress: \tEpoch 33 [4160/8883 (46.76%)]\t\tLoss: 1.00180\n",
      "Training Progress: \tEpoch 33 [4480/8883 (50.36%)]\t\tLoss: 1.04460\n",
      "Training Progress: \tEpoch 33 [4800/8883 (53.96%)]\t\tLoss: 0.91656\n",
      "Training Progress: \tEpoch 33 [5120/8883 (57.55%)]\t\tLoss: 1.22989\n",
      "Training Progress: \tEpoch 33 [5440/8883 (61.15%)]\t\tLoss: 1.13391\n",
      "Training Progress: \tEpoch 33 [5760/8883 (64.75%)]\t\tLoss: 1.20101\n",
      "Training Progress: \tEpoch 33 [6080/8883 (68.35%)]\t\tLoss: 0.79990\n",
      "Training Progress: \tEpoch 33 [6400/8883 (71.94%)]\t\tLoss: 1.09097\n",
      "Training Progress: \tEpoch 33 [6720/8883 (75.54%)]\t\tLoss: 0.95745\n",
      "Training Progress: \tEpoch 33 [7040/8883 (79.14%)]\t\tLoss: 1.00107\n",
      "Training Progress: \tEpoch 33 [7360/8883 (82.73%)]\t\tLoss: 1.07489\n",
      "Training Progress: \tEpoch 33 [7680/8883 (86.33%)]\t\tLoss: 1.07004\n",
      "Training Progress: \tEpoch 33 [8000/8883 (89.93%)]\t\tLoss: 0.85287\n",
      "Training Progress: \tEpoch 33 [8320/8883 (93.53%)]\t\tLoss: 0.82700\n",
      "Training Progress: \tEpoch 33 [8640/8883 (97.12%)]\t\tLoss: 1.01705\n",
      "\tTrain loss: 0.02569, Accuracy: 5536/8883 (62.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1173/1692 (69.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 861/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/8883 (0.00%)]\t\tLoss: 0.93391\n",
      "Training Progress: \tEpoch 34 [320/8883 (3.60%)]\t\tLoss: 1.06004\n",
      "Training Progress: \tEpoch 34 [640/8883 (7.19%)]\t\tLoss: 0.96554\n",
      "Training Progress: \tEpoch 34 [960/8883 (10.79%)]\t\tLoss: 0.98185\n",
      "Training Progress: \tEpoch 34 [1280/8883 (14.39%)]\t\tLoss: 0.99816\n",
      "Training Progress: \tEpoch 34 [1600/8883 (17.99%)]\t\tLoss: 1.16668\n",
      "Training Progress: \tEpoch 34 [1920/8883 (21.58%)]\t\tLoss: 1.03339\n",
      "Training Progress: \tEpoch 34 [2240/8883 (25.18%)]\t\tLoss: 0.92549\n",
      "Training Progress: \tEpoch 34 [2560/8883 (28.78%)]\t\tLoss: 0.92192\n",
      "Training Progress: \tEpoch 34 [2880/8883 (32.37%)]\t\tLoss: 1.04428\n",
      "Training Progress: \tEpoch 34 [3200/8883 (35.97%)]\t\tLoss: 1.23372\n",
      "Training Progress: \tEpoch 34 [3520/8883 (39.57%)]\t\tLoss: 1.13386\n",
      "Training Progress: \tEpoch 34 [3840/8883 (43.17%)]\t\tLoss: 1.03501\n",
      "Training Progress: \tEpoch 34 [4160/8883 (46.76%)]\t\tLoss: 1.03923\n",
      "Training Progress: \tEpoch 34 [4480/8883 (50.36%)]\t\tLoss: 0.91959\n",
      "Training Progress: \tEpoch 34 [4800/8883 (53.96%)]\t\tLoss: 0.97873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 34 [5120/8883 (57.55%)]\t\tLoss: 1.11874\n",
      "Training Progress: \tEpoch 34 [5440/8883 (61.15%)]\t\tLoss: 1.18823\n",
      "Training Progress: \tEpoch 34 [5760/8883 (64.75%)]\t\tLoss: 1.10820\n",
      "Training Progress: \tEpoch 34 [6080/8883 (68.35%)]\t\tLoss: 0.95099\n",
      "Training Progress: \tEpoch 34 [6400/8883 (71.94%)]\t\tLoss: 1.01092\n",
      "Training Progress: \tEpoch 34 [6720/8883 (75.54%)]\t\tLoss: 0.99057\n",
      "Training Progress: \tEpoch 34 [7040/8883 (79.14%)]\t\tLoss: 1.15882\n",
      "Training Progress: \tEpoch 34 [7360/8883 (82.73%)]\t\tLoss: 1.06814\n",
      "Training Progress: \tEpoch 34 [7680/8883 (86.33%)]\t\tLoss: 0.94532\n",
      "Training Progress: \tEpoch 34 [8000/8883 (89.93%)]\t\tLoss: 0.96515\n",
      "Training Progress: \tEpoch 34 [8320/8883 (93.53%)]\t\tLoss: 0.93701\n",
      "Training Progress: \tEpoch 34 [8640/8883 (97.12%)]\t\tLoss: 1.20449\n",
      "\tTrain loss: 0.02500, Accuracy: 5659/8883 (63.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1188/1692 (70.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 906/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/8883 (0.00%)]\t\tLoss: 0.89672\n",
      "Training Progress: \tEpoch 35 [320/8883 (3.60%)]\t\tLoss: 0.93323\n",
      "Training Progress: \tEpoch 35 [640/8883 (7.19%)]\t\tLoss: 1.03079\n",
      "Training Progress: \tEpoch 35 [960/8883 (10.79%)]\t\tLoss: 1.07853\n",
      "Training Progress: \tEpoch 35 [1280/8883 (14.39%)]\t\tLoss: 1.20162\n",
      "Training Progress: \tEpoch 35 [1600/8883 (17.99%)]\t\tLoss: 1.16000\n",
      "Training Progress: \tEpoch 35 [1920/8883 (21.58%)]\t\tLoss: 0.98578\n",
      "Training Progress: \tEpoch 35 [2240/8883 (25.18%)]\t\tLoss: 1.02843\n",
      "Training Progress: \tEpoch 35 [2560/8883 (28.78%)]\t\tLoss: 0.98223\n",
      "Training Progress: \tEpoch 35 [2880/8883 (32.37%)]\t\tLoss: 1.01062\n",
      "Training Progress: \tEpoch 35 [3200/8883 (35.97%)]\t\tLoss: 1.12338\n",
      "Training Progress: \tEpoch 35 [3520/8883 (39.57%)]\t\tLoss: 1.20735\n",
      "Training Progress: \tEpoch 35 [3840/8883 (43.17%)]\t\tLoss: 1.16455\n",
      "Training Progress: \tEpoch 35 [4160/8883 (46.76%)]\t\tLoss: 1.09447\n",
      "Training Progress: \tEpoch 35 [4480/8883 (50.36%)]\t\tLoss: 0.84575\n",
      "Training Progress: \tEpoch 35 [4800/8883 (53.96%)]\t\tLoss: 0.89443\n",
      "Training Progress: \tEpoch 35 [5120/8883 (57.55%)]\t\tLoss: 1.22196\n",
      "Training Progress: \tEpoch 35 [5440/8883 (61.15%)]\t\tLoss: 1.26126\n",
      "Training Progress: \tEpoch 35 [5760/8883 (64.75%)]\t\tLoss: 0.98895\n",
      "Training Progress: \tEpoch 35 [6080/8883 (68.35%)]\t\tLoss: 0.90468\n",
      "Training Progress: \tEpoch 35 [6400/8883 (71.94%)]\t\tLoss: 1.06993\n",
      "Training Progress: \tEpoch 35 [6720/8883 (75.54%)]\t\tLoss: 1.12197\n",
      "Training Progress: \tEpoch 35 [7040/8883 (79.14%)]\t\tLoss: 1.15643\n",
      "Training Progress: \tEpoch 35 [7360/8883 (82.73%)]\t\tLoss: 1.16697\n",
      "Training Progress: \tEpoch 35 [7680/8883 (86.33%)]\t\tLoss: 1.11010\n",
      "Training Progress: \tEpoch 35 [8000/8883 (89.93%)]\t\tLoss: 1.03202\n",
      "Training Progress: \tEpoch 35 [8320/8883 (93.53%)]\t\tLoss: 0.90453\n",
      "Training Progress: \tEpoch 35 [8640/8883 (97.12%)]\t\tLoss: 1.02406\n",
      "\tTrain loss: 0.02484, Accuracy: 5702/8883 (64.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1192/1692 (70.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 865/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/8883 (0.00%)]\t\tLoss: 0.93606\n",
      "Training Progress: \tEpoch 36 [320/8883 (3.60%)]\t\tLoss: 1.06452\n",
      "Training Progress: \tEpoch 36 [640/8883 (7.19%)]\t\tLoss: 1.17950\n",
      "Training Progress: \tEpoch 36 [960/8883 (10.79%)]\t\tLoss: 1.16838\n",
      "Training Progress: \tEpoch 36 [1280/8883 (14.39%)]\t\tLoss: 1.03448\n",
      "Training Progress: \tEpoch 36 [1600/8883 (17.99%)]\t\tLoss: 1.12370\n",
      "Training Progress: \tEpoch 36 [1920/8883 (21.58%)]\t\tLoss: 0.81337\n",
      "Training Progress: \tEpoch 36 [2240/8883 (25.18%)]\t\tLoss: 1.00442\n",
      "Training Progress: \tEpoch 36 [2560/8883 (28.78%)]\t\tLoss: 0.95657\n",
      "Training Progress: \tEpoch 36 [2880/8883 (32.37%)]\t\tLoss: 0.93454\n",
      "Training Progress: \tEpoch 36 [3200/8883 (35.97%)]\t\tLoss: 1.22321\n",
      "Training Progress: \tEpoch 36 [3520/8883 (39.57%)]\t\tLoss: 1.13751\n",
      "Training Progress: \tEpoch 36 [3840/8883 (43.17%)]\t\tLoss: 1.22422\n",
      "Training Progress: \tEpoch 36 [4160/8883 (46.76%)]\t\tLoss: 1.25732\n",
      "Training Progress: \tEpoch 36 [4480/8883 (50.36%)]\t\tLoss: 0.99163\n",
      "Training Progress: \tEpoch 36 [4800/8883 (53.96%)]\t\tLoss: 0.84416\n",
      "Training Progress: \tEpoch 36 [5120/8883 (57.55%)]\t\tLoss: 1.12949\n",
      "Training Progress: \tEpoch 36 [5440/8883 (61.15%)]\t\tLoss: 1.19973\n",
      "Training Progress: \tEpoch 36 [5760/8883 (64.75%)]\t\tLoss: 1.06824\n",
      "Training Progress: \tEpoch 36 [6080/8883 (68.35%)]\t\tLoss: 0.77700\n",
      "Training Progress: \tEpoch 36 [6400/8883 (71.94%)]\t\tLoss: 1.11305\n",
      "Training Progress: \tEpoch 36 [6720/8883 (75.54%)]\t\tLoss: 1.23290\n",
      "Training Progress: \tEpoch 36 [7040/8883 (79.14%)]\t\tLoss: 1.23806\n",
      "Training Progress: \tEpoch 36 [7360/8883 (82.73%)]\t\tLoss: 1.13642\n",
      "Training Progress: \tEpoch 36 [7680/8883 (86.33%)]\t\tLoss: 1.13511\n",
      "Training Progress: \tEpoch 36 [8000/8883 (89.93%)]\t\tLoss: 1.10173\n",
      "Training Progress: \tEpoch 36 [8320/8883 (93.53%)]\t\tLoss: 1.01194\n",
      "Training Progress: \tEpoch 36 [8640/8883 (97.12%)]\t\tLoss: 0.99273\n",
      "\tTrain loss: 0.02442, Accuracy: 5821/8883 (65.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1214/1692 (71.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 886/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/8883 (0.00%)]\t\tLoss: 1.06236\n",
      "Training Progress: \tEpoch 37 [320/8883 (3.60%)]\t\tLoss: 0.99035\n",
      "Training Progress: \tEpoch 37 [640/8883 (7.19%)]\t\tLoss: 0.90201\n",
      "Training Progress: \tEpoch 37 [960/8883 (10.79%)]\t\tLoss: 0.94425\n",
      "Training Progress: \tEpoch 37 [1280/8883 (14.39%)]\t\tLoss: 1.13440\n",
      "Training Progress: \tEpoch 37 [1600/8883 (17.99%)]\t\tLoss: 1.13770\n",
      "Training Progress: \tEpoch 37 [1920/8883 (21.58%)]\t\tLoss: 1.11809\n",
      "Training Progress: \tEpoch 37 [2240/8883 (25.18%)]\t\tLoss: 1.01397\n",
      "Training Progress: \tEpoch 37 [2560/8883 (28.78%)]\t\tLoss: 1.08070\n",
      "Training Progress: \tEpoch 37 [2880/8883 (32.37%)]\t\tLoss: 1.03709\n",
      "Training Progress: \tEpoch 37 [3200/8883 (35.97%)]\t\tLoss: 1.19549\n",
      "Training Progress: \tEpoch 37 [3520/8883 (39.57%)]\t\tLoss: 1.17877\n",
      "Training Progress: \tEpoch 37 [3840/8883 (43.17%)]\t\tLoss: 1.33450\n",
      "Training Progress: \tEpoch 37 [4160/8883 (46.76%)]\t\tLoss: 1.22176\n",
      "Training Progress: \tEpoch 37 [4480/8883 (50.36%)]\t\tLoss: 0.91229\n",
      "Training Progress: \tEpoch 37 [4800/8883 (53.96%)]\t\tLoss: 0.82512\n",
      "Training Progress: \tEpoch 37 [5120/8883 (57.55%)]\t\tLoss: 1.27974\n",
      "Training Progress: \tEpoch 37 [5440/8883 (61.15%)]\t\tLoss: 1.18792\n",
      "Training Progress: \tEpoch 37 [5760/8883 (64.75%)]\t\tLoss: 0.97730\n",
      "Training Progress: \tEpoch 37 [6080/8883 (68.35%)]\t\tLoss: 1.06200\n",
      "Training Progress: \tEpoch 37 [6400/8883 (71.94%)]\t\tLoss: 1.20046\n",
      "Training Progress: \tEpoch 37 [6720/8883 (75.54%)]\t\tLoss: 1.12506\n",
      "Training Progress: \tEpoch 37 [7040/8883 (79.14%)]\t\tLoss: 0.95131\n",
      "Training Progress: \tEpoch 37 [7360/8883 (82.73%)]\t\tLoss: 1.13020\n",
      "Training Progress: \tEpoch 37 [7680/8883 (86.33%)]\t\tLoss: 1.16259\n",
      "Training Progress: \tEpoch 37 [8000/8883 (89.93%)]\t\tLoss: 0.87273\n",
      "Training Progress: \tEpoch 37 [8320/8883 (93.53%)]\t\tLoss: 1.07500\n",
      "Training Progress: \tEpoch 37 [8640/8883 (97.12%)]\t\tLoss: 1.13080\n",
      "\tTrain loss: 0.02443, Accuracy: 5682/8883 (63.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1183/1692 (69.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 861/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/8883 (0.00%)]\t\tLoss: 0.98058\n",
      "Training Progress: \tEpoch 38 [320/8883 (3.60%)]\t\tLoss: 0.94536\n",
      "Training Progress: \tEpoch 38 [640/8883 (7.19%)]\t\tLoss: 0.83031\n",
      "Training Progress: \tEpoch 38 [960/8883 (10.79%)]\t\tLoss: 1.09588\n",
      "Training Progress: \tEpoch 38 [1280/8883 (14.39%)]\t\tLoss: 1.11339\n",
      "Training Progress: \tEpoch 38 [1600/8883 (17.99%)]\t\tLoss: 1.00779\n",
      "Training Progress: \tEpoch 38 [1920/8883 (21.58%)]\t\tLoss: 1.14845\n",
      "Training Progress: \tEpoch 38 [2240/8883 (25.18%)]\t\tLoss: 1.09827\n",
      "Training Progress: \tEpoch 38 [2560/8883 (28.78%)]\t\tLoss: 1.21814\n",
      "Training Progress: \tEpoch 38 [2880/8883 (32.37%)]\t\tLoss: 1.14544\n",
      "Training Progress: \tEpoch 38 [3200/8883 (35.97%)]\t\tLoss: 1.22200\n",
      "Training Progress: \tEpoch 38 [3520/8883 (39.57%)]\t\tLoss: 1.03073\n",
      "Training Progress: \tEpoch 38 [3840/8883 (43.17%)]\t\tLoss: 1.32826\n",
      "Training Progress: \tEpoch 38 [4160/8883 (46.76%)]\t\tLoss: 1.08178\n",
      "Training Progress: \tEpoch 38 [4480/8883 (50.36%)]\t\tLoss: 0.85565\n",
      "Training Progress: \tEpoch 38 [4800/8883 (53.96%)]\t\tLoss: 0.95574\n",
      "Training Progress: \tEpoch 38 [5120/8883 (57.55%)]\t\tLoss: 1.10288\n",
      "Training Progress: \tEpoch 38 [5440/8883 (61.15%)]\t\tLoss: 1.16833\n",
      "Training Progress: \tEpoch 38 [5760/8883 (64.75%)]\t\tLoss: 0.96064\n",
      "Training Progress: \tEpoch 38 [6080/8883 (68.35%)]\t\tLoss: 0.87864\n",
      "Training Progress: \tEpoch 38 [6400/8883 (71.94%)]\t\tLoss: 1.09419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 38 [6720/8883 (75.54%)]\t\tLoss: 1.05822\n",
      "Training Progress: \tEpoch 38 [7040/8883 (79.14%)]\t\tLoss: 0.96292\n",
      "Training Progress: \tEpoch 38 [7360/8883 (82.73%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 38 [7680/8883 (86.33%)]\t\tLoss: 0.97365\n",
      "Training Progress: \tEpoch 38 [8000/8883 (89.93%)]\t\tLoss: 1.12824\n",
      "Training Progress: \tEpoch 38 [8320/8883 (93.53%)]\t\tLoss: 0.95936\n",
      "Training Progress: \tEpoch 38 [8640/8883 (97.12%)]\t\tLoss: 1.13090\n",
      "\tTrain loss: 0.02424, Accuracy: 5894/8883 (66.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1227/1692 (72.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 920/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/8883 (0.00%)]\t\tLoss: 0.83782\n",
      "Training Progress: \tEpoch 39 [320/8883 (3.60%)]\t\tLoss: 0.88646\n",
      "Training Progress: \tEpoch 39 [640/8883 (7.19%)]\t\tLoss: 0.96840\n",
      "Training Progress: \tEpoch 39 [960/8883 (10.79%)]\t\tLoss: 0.92703\n",
      "Training Progress: \tEpoch 39 [1280/8883 (14.39%)]\t\tLoss: 0.93024\n",
      "Training Progress: \tEpoch 39 [1600/8883 (17.99%)]\t\tLoss: 1.15115\n",
      "Training Progress: \tEpoch 39 [1920/8883 (21.58%)]\t\tLoss: 0.87549\n",
      "Training Progress: \tEpoch 39 [2240/8883 (25.18%)]\t\tLoss: 0.94728\n",
      "Training Progress: \tEpoch 39 [2560/8883 (28.78%)]\t\tLoss: 0.92361\n",
      "Training Progress: \tEpoch 39 [2880/8883 (32.37%)]\t\tLoss: 1.00402\n",
      "Training Progress: \tEpoch 39 [3200/8883 (35.97%)]\t\tLoss: 1.14923\n",
      "Training Progress: \tEpoch 39 [3520/8883 (39.57%)]\t\tLoss: 1.13406\n",
      "Training Progress: \tEpoch 39 [3840/8883 (43.17%)]\t\tLoss: 1.06457\n",
      "Training Progress: \tEpoch 39 [4160/8883 (46.76%)]\t\tLoss: 1.03083\n",
      "Training Progress: \tEpoch 39 [4480/8883 (50.36%)]\t\tLoss: 0.86743\n",
      "Training Progress: \tEpoch 39 [4800/8883 (53.96%)]\t\tLoss: 0.82631\n",
      "Training Progress: \tEpoch 39 [5120/8883 (57.55%)]\t\tLoss: 1.20409\n",
      "Training Progress: \tEpoch 39 [5440/8883 (61.15%)]\t\tLoss: 1.29082\n",
      "Training Progress: \tEpoch 39 [5760/8883 (64.75%)]\t\tLoss: 1.04059\n",
      "Training Progress: \tEpoch 39 [6080/8883 (68.35%)]\t\tLoss: 0.88073\n",
      "Training Progress: \tEpoch 39 [6400/8883 (71.94%)]\t\tLoss: 0.96995\n",
      "Training Progress: \tEpoch 39 [6720/8883 (75.54%)]\t\tLoss: 0.92876\n",
      "Training Progress: \tEpoch 39 [7040/8883 (79.14%)]\t\tLoss: 1.07610\n",
      "Training Progress: \tEpoch 39 [7360/8883 (82.73%)]\t\tLoss: 1.08785\n",
      "Training Progress: \tEpoch 39 [7680/8883 (86.33%)]\t\tLoss: 1.04629\n",
      "Training Progress: \tEpoch 39 [8000/8883 (89.93%)]\t\tLoss: 1.05273\n",
      "Training Progress: \tEpoch 39 [8320/8883 (93.53%)]\t\tLoss: 0.79510\n",
      "Training Progress: \tEpoch 39 [8640/8883 (97.12%)]\t\tLoss: 1.06097\n",
      "\tTrain loss: 0.02315, Accuracy: 5945/8883 (66.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1243/1692 (73.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/8883 (0.00%)]\t\tLoss: 1.03182\n",
      "Training Progress: \tEpoch 40 [320/8883 (3.60%)]\t\tLoss: 0.99319\n",
      "Training Progress: \tEpoch 40 [640/8883 (7.19%)]\t\tLoss: 0.92555\n",
      "Training Progress: \tEpoch 40 [960/8883 (10.79%)]\t\tLoss: 0.93596\n",
      "Training Progress: \tEpoch 40 [1280/8883 (14.39%)]\t\tLoss: 1.08443\n",
      "Training Progress: \tEpoch 40 [1600/8883 (17.99%)]\t\tLoss: 1.12432\n",
      "Training Progress: \tEpoch 40 [1920/8883 (21.58%)]\t\tLoss: 0.88736\n",
      "Training Progress: \tEpoch 40 [2240/8883 (25.18%)]\t\tLoss: 1.01496\n",
      "Training Progress: \tEpoch 40 [2560/8883 (28.78%)]\t\tLoss: 1.00404\n",
      "Training Progress: \tEpoch 40 [2880/8883 (32.37%)]\t\tLoss: 1.06750\n",
      "Training Progress: \tEpoch 40 [3200/8883 (35.97%)]\t\tLoss: 1.24924\n",
      "Training Progress: \tEpoch 40 [3520/8883 (39.57%)]\t\tLoss: 0.88136\n",
      "Training Progress: \tEpoch 40 [3840/8883 (43.17%)]\t\tLoss: 1.06632\n",
      "Training Progress: \tEpoch 40 [4160/8883 (46.76%)]\t\tLoss: 1.12425\n",
      "Training Progress: \tEpoch 40 [4480/8883 (50.36%)]\t\tLoss: 0.89132\n",
      "Training Progress: \tEpoch 40 [4800/8883 (53.96%)]\t\tLoss: 0.89528\n",
      "Training Progress: \tEpoch 40 [5120/8883 (57.55%)]\t\tLoss: 1.08559\n",
      "Training Progress: \tEpoch 40 [5440/8883 (61.15%)]\t\tLoss: 1.30002\n",
      "Training Progress: \tEpoch 40 [5760/8883 (64.75%)]\t\tLoss: 0.98033\n",
      "Training Progress: \tEpoch 40 [6080/8883 (68.35%)]\t\tLoss: 0.78095\n",
      "Training Progress: \tEpoch 40 [6400/8883 (71.94%)]\t\tLoss: 1.05605\n",
      "Training Progress: \tEpoch 40 [6720/8883 (75.54%)]\t\tLoss: 1.17154\n",
      "Training Progress: \tEpoch 40 [7040/8883 (79.14%)]\t\tLoss: 0.92284\n",
      "Training Progress: \tEpoch 40 [7360/8883 (82.73%)]\t\tLoss: 1.01840\n",
      "Training Progress: \tEpoch 40 [7680/8883 (86.33%)]\t\tLoss: 0.83997\n",
      "Training Progress: \tEpoch 40 [8000/8883 (89.93%)]\t\tLoss: 0.82417\n",
      "Training Progress: \tEpoch 40 [8320/8883 (93.53%)]\t\tLoss: 0.90280\n",
      "Training Progress: \tEpoch 40 [8640/8883 (97.12%)]\t\tLoss: 1.13623\n",
      "\tTrain loss: 0.02390, Accuracy: 5815/8883 (65.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 883/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/8883 (0.00%)]\t\tLoss: 0.98313\n",
      "Training Progress: \tEpoch 41 [320/8883 (3.60%)]\t\tLoss: 0.84848\n",
      "Training Progress: \tEpoch 41 [640/8883 (7.19%)]\t\tLoss: 0.92498\n",
      "Training Progress: \tEpoch 41 [960/8883 (10.79%)]\t\tLoss: 0.97115\n",
      "Training Progress: \tEpoch 41 [1280/8883 (14.39%)]\t\tLoss: 1.05058\n",
      "Training Progress: \tEpoch 41 [1600/8883 (17.99%)]\t\tLoss: 1.11052\n",
      "Training Progress: \tEpoch 41 [1920/8883 (21.58%)]\t\tLoss: 0.83245\n",
      "Training Progress: \tEpoch 41 [2240/8883 (25.18%)]\t\tLoss: 1.05953\n",
      "Training Progress: \tEpoch 41 [2560/8883 (28.78%)]\t\tLoss: 0.87266\n",
      "Training Progress: \tEpoch 41 [2880/8883 (32.37%)]\t\tLoss: 1.04560\n",
      "Training Progress: \tEpoch 41 [3200/8883 (35.97%)]\t\tLoss: 1.18755\n",
      "Training Progress: \tEpoch 41 [3520/8883 (39.57%)]\t\tLoss: 1.35816\n",
      "Training Progress: \tEpoch 41 [3840/8883 (43.17%)]\t\tLoss: 1.23429\n",
      "Training Progress: \tEpoch 41 [4160/8883 (46.76%)]\t\tLoss: 1.43072\n",
      "Training Progress: \tEpoch 41 [4480/8883 (50.36%)]\t\tLoss: 0.89291\n",
      "Training Progress: \tEpoch 41 [4800/8883 (53.96%)]\t\tLoss: 0.85811\n",
      "Training Progress: \tEpoch 41 [5120/8883 (57.55%)]\t\tLoss: 1.19469\n",
      "Training Progress: \tEpoch 41 [5440/8883 (61.15%)]\t\tLoss: 1.45244\n",
      "Training Progress: \tEpoch 41 [5760/8883 (64.75%)]\t\tLoss: 0.96902\n",
      "Training Progress: \tEpoch 41 [6080/8883 (68.35%)]\t\tLoss: 0.79312\n",
      "Training Progress: \tEpoch 41 [6400/8883 (71.94%)]\t\tLoss: 0.96866\n",
      "Training Progress: \tEpoch 41 [6720/8883 (75.54%)]\t\tLoss: 1.22150\n",
      "Training Progress: \tEpoch 41 [7040/8883 (79.14%)]\t\tLoss: 0.96559\n",
      "Training Progress: \tEpoch 41 [7360/8883 (82.73%)]\t\tLoss: 1.02511\n",
      "Training Progress: \tEpoch 41 [7680/8883 (86.33%)]\t\tLoss: 0.94461\n",
      "Training Progress: \tEpoch 41 [8000/8883 (89.93%)]\t\tLoss: 0.96451\n",
      "Training Progress: \tEpoch 41 [8320/8883 (93.53%)]\t\tLoss: 0.86514\n",
      "Training Progress: \tEpoch 41 [8640/8883 (97.12%)]\t\tLoss: 1.16728\n",
      "\tTrain loss: 0.02433, Accuracy: 5779/8883 (65.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1193/1692 (70.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 892/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/8883 (0.00%)]\t\tLoss: 0.88181\n",
      "Training Progress: \tEpoch 42 [320/8883 (3.60%)]\t\tLoss: 1.16259\n",
      "Training Progress: \tEpoch 42 [640/8883 (7.19%)]\t\tLoss: 0.93135\n",
      "Training Progress: \tEpoch 42 [960/8883 (10.79%)]\t\tLoss: 0.96554\n",
      "Training Progress: \tEpoch 42 [1280/8883 (14.39%)]\t\tLoss: 1.06874\n",
      "Training Progress: \tEpoch 42 [1600/8883 (17.99%)]\t\tLoss: 1.00711\n",
      "Training Progress: \tEpoch 42 [1920/8883 (21.58%)]\t\tLoss: 0.96560\n",
      "Training Progress: \tEpoch 42 [2240/8883 (25.18%)]\t\tLoss: 1.01011\n",
      "Training Progress: \tEpoch 42 [2560/8883 (28.78%)]\t\tLoss: 0.79097\n",
      "Training Progress: \tEpoch 42 [2880/8883 (32.37%)]\t\tLoss: 1.01195\n",
      "Training Progress: \tEpoch 42 [3200/8883 (35.97%)]\t\tLoss: 0.95435\n",
      "Training Progress: \tEpoch 42 [3520/8883 (39.57%)]\t\tLoss: 1.00901\n",
      "Training Progress: \tEpoch 42 [3840/8883 (43.17%)]\t\tLoss: 1.19590\n",
      "Training Progress: \tEpoch 42 [4160/8883 (46.76%)]\t\tLoss: 1.10638\n",
      "Training Progress: \tEpoch 42 [4480/8883 (50.36%)]\t\tLoss: 0.88354\n",
      "Training Progress: \tEpoch 42 [4800/8883 (53.96%)]\t\tLoss: 0.72190\n",
      "Training Progress: \tEpoch 42 [5120/8883 (57.55%)]\t\tLoss: 1.01049\n",
      "Training Progress: \tEpoch 42 [5440/8883 (61.15%)]\t\tLoss: 1.23800\n",
      "Training Progress: \tEpoch 42 [5760/8883 (64.75%)]\t\tLoss: 0.95574\n",
      "Training Progress: \tEpoch 42 [6080/8883 (68.35%)]\t\tLoss: 0.86930\n",
      "Training Progress: \tEpoch 42 [6400/8883 (71.94%)]\t\tLoss: 1.21297\n",
      "Training Progress: \tEpoch 42 [6720/8883 (75.54%)]\t\tLoss: 1.00277\n",
      "Training Progress: \tEpoch 42 [7040/8883 (79.14%)]\t\tLoss: 0.97209\n",
      "Training Progress: \tEpoch 42 [7360/8883 (82.73%)]\t\tLoss: 1.07007\n",
      "Training Progress: \tEpoch 42 [7680/8883 (86.33%)]\t\tLoss: 1.18067\n",
      "Training Progress: \tEpoch 42 [8000/8883 (89.93%)]\t\tLoss: 0.82733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [8320/8883 (93.53%)]\t\tLoss: 0.82770\n",
      "Training Progress: \tEpoch 42 [8640/8883 (97.12%)]\t\tLoss: 1.19353\n",
      "\tTrain loss: 0.02299, Accuracy: 6037/8883 (67.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1276/1692 (75.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 861/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/8883 (0.00%)]\t\tLoss: 1.07979\n",
      "Training Progress: \tEpoch 43 [320/8883 (3.60%)]\t\tLoss: 0.82028\n",
      "Training Progress: \tEpoch 43 [640/8883 (7.19%)]\t\tLoss: 0.87003\n",
      "Training Progress: \tEpoch 43 [960/8883 (10.79%)]\t\tLoss: 1.11616\n",
      "Training Progress: \tEpoch 43 [1280/8883 (14.39%)]\t\tLoss: 0.92687\n",
      "Training Progress: \tEpoch 43 [1600/8883 (17.99%)]\t\tLoss: 1.10847\n",
      "Training Progress: \tEpoch 43 [1920/8883 (21.58%)]\t\tLoss: 1.17777\n",
      "Training Progress: \tEpoch 43 [2240/8883 (25.18%)]\t\tLoss: 1.06265\n",
      "Training Progress: \tEpoch 43 [2560/8883 (28.78%)]\t\tLoss: 0.89455\n",
      "Training Progress: \tEpoch 43 [2880/8883 (32.37%)]\t\tLoss: 1.01874\n",
      "Training Progress: \tEpoch 43 [3200/8883 (35.97%)]\t\tLoss: 1.17129\n",
      "Training Progress: \tEpoch 43 [3520/8883 (39.57%)]\t\tLoss: 1.00285\n",
      "Training Progress: \tEpoch 43 [3840/8883 (43.17%)]\t\tLoss: 1.13100\n",
      "Training Progress: \tEpoch 43 [4160/8883 (46.76%)]\t\tLoss: 0.98493\n",
      "Training Progress: \tEpoch 43 [4480/8883 (50.36%)]\t\tLoss: 0.81083\n",
      "Training Progress: \tEpoch 43 [4800/8883 (53.96%)]\t\tLoss: 0.73822\n",
      "Training Progress: \tEpoch 43 [5120/8883 (57.55%)]\t\tLoss: 1.26725\n",
      "Training Progress: \tEpoch 43 [5440/8883 (61.15%)]\t\tLoss: 1.38706\n",
      "Training Progress: \tEpoch 43 [5760/8883 (64.75%)]\t\tLoss: 1.06897\n",
      "Training Progress: \tEpoch 43 [6080/8883 (68.35%)]\t\tLoss: 0.91741\n",
      "Training Progress: \tEpoch 43 [6400/8883 (71.94%)]\t\tLoss: 1.24081\n",
      "Training Progress: \tEpoch 43 [6720/8883 (75.54%)]\t\tLoss: 1.08257\n",
      "Training Progress: \tEpoch 43 [7040/8883 (79.14%)]\t\tLoss: 1.17316\n",
      "Training Progress: \tEpoch 43 [7360/8883 (82.73%)]\t\tLoss: 1.14994\n",
      "Training Progress: \tEpoch 43 [7680/8883 (86.33%)]\t\tLoss: 1.23371\n",
      "Training Progress: \tEpoch 43 [8000/8883 (89.93%)]\t\tLoss: 0.82845\n",
      "Training Progress: \tEpoch 43 [8320/8883 (93.53%)]\t\tLoss: 1.04807\n",
      "Training Progress: \tEpoch 43 [8640/8883 (97.12%)]\t\tLoss: 1.09141\n",
      "\tTrain loss: 0.02429, Accuracy: 5947/8883 (66.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 892/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/8883 (0.00%)]\t\tLoss: 1.03048\n",
      "Training Progress: \tEpoch 44 [320/8883 (3.60%)]\t\tLoss: 1.00380\n",
      "Training Progress: \tEpoch 44 [640/8883 (7.19%)]\t\tLoss: 1.12143\n",
      "Training Progress: \tEpoch 44 [960/8883 (10.79%)]\t\tLoss: 0.87369\n",
      "Training Progress: \tEpoch 44 [1280/8883 (14.39%)]\t\tLoss: 1.06574\n",
      "Training Progress: \tEpoch 44 [1600/8883 (17.99%)]\t\tLoss: 1.21076\n",
      "Training Progress: \tEpoch 44 [1920/8883 (21.58%)]\t\tLoss: 0.95000\n",
      "Training Progress: \tEpoch 44 [2240/8883 (25.18%)]\t\tLoss: 0.91742\n",
      "Training Progress: \tEpoch 44 [2560/8883 (28.78%)]\t\tLoss: 1.09628\n",
      "Training Progress: \tEpoch 44 [2880/8883 (32.37%)]\t\tLoss: 0.99220\n",
      "Training Progress: \tEpoch 44 [3200/8883 (35.97%)]\t\tLoss: 1.19793\n",
      "Training Progress: \tEpoch 44 [3520/8883 (39.57%)]\t\tLoss: 0.77641\n",
      "Training Progress: \tEpoch 44 [3840/8883 (43.17%)]\t\tLoss: 1.41875\n",
      "Training Progress: \tEpoch 44 [4160/8883 (46.76%)]\t\tLoss: 1.02903\n",
      "Training Progress: \tEpoch 44 [4480/8883 (50.36%)]\t\tLoss: 0.75759\n",
      "Training Progress: \tEpoch 44 [4800/8883 (53.96%)]\t\tLoss: 1.05840\n",
      "Training Progress: \tEpoch 44 [5120/8883 (57.55%)]\t\tLoss: 1.09796\n",
      "Training Progress: \tEpoch 44 [5440/8883 (61.15%)]\t\tLoss: 0.97740\n",
      "Training Progress: \tEpoch 44 [5760/8883 (64.75%)]\t\tLoss: 0.91561\n",
      "Training Progress: \tEpoch 44 [6080/8883 (68.35%)]\t\tLoss: 0.84700\n",
      "Training Progress: \tEpoch 44 [6400/8883 (71.94%)]\t\tLoss: 0.87460\n",
      "Training Progress: \tEpoch 44 [6720/8883 (75.54%)]\t\tLoss: 0.95330\n",
      "Training Progress: \tEpoch 44 [7040/8883 (79.14%)]\t\tLoss: 0.83609\n",
      "Training Progress: \tEpoch 44 [7360/8883 (82.73%)]\t\tLoss: 0.98287\n",
      "Training Progress: \tEpoch 44 [7680/8883 (86.33%)]\t\tLoss: 0.92730\n",
      "Training Progress: \tEpoch 44 [8000/8883 (89.93%)]\t\tLoss: 1.16673\n",
      "Training Progress: \tEpoch 44 [8320/8883 (93.53%)]\t\tLoss: 0.86476\n",
      "Training Progress: \tEpoch 44 [8640/8883 (97.12%)]\t\tLoss: 1.01275\n",
      "\tTrain loss: 0.02316, Accuracy: 6022/8883 (67.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/8883 (0.00%)]\t\tLoss: 0.94148\n",
      "Training Progress: \tEpoch 45 [320/8883 (3.60%)]\t\tLoss: 0.85718\n",
      "Training Progress: \tEpoch 45 [640/8883 (7.19%)]\t\tLoss: 0.86547\n",
      "Training Progress: \tEpoch 45 [960/8883 (10.79%)]\t\tLoss: 0.85934\n",
      "Training Progress: \tEpoch 45 [1280/8883 (14.39%)]\t\tLoss: 1.09291\n",
      "Training Progress: \tEpoch 45 [1600/8883 (17.99%)]\t\tLoss: 1.18765\n",
      "Training Progress: \tEpoch 45 [1920/8883 (21.58%)]\t\tLoss: 1.17834\n",
      "Training Progress: \tEpoch 45 [2240/8883 (25.18%)]\t\tLoss: 1.08099\n",
      "Training Progress: \tEpoch 45 [2560/8883 (28.78%)]\t\tLoss: 1.09250\n",
      "Training Progress: \tEpoch 45 [2880/8883 (32.37%)]\t\tLoss: 1.16209\n",
      "Training Progress: \tEpoch 45 [3200/8883 (35.97%)]\t\tLoss: 1.36220\n",
      "Training Progress: \tEpoch 45 [3520/8883 (39.57%)]\t\tLoss: 0.98279\n",
      "Training Progress: \tEpoch 45 [3840/8883 (43.17%)]\t\tLoss: 1.18470\n",
      "Training Progress: \tEpoch 45 [4160/8883 (46.76%)]\t\tLoss: 1.07437\n",
      "Training Progress: \tEpoch 45 [4480/8883 (50.36%)]\t\tLoss: 1.00479\n",
      "Training Progress: \tEpoch 45 [4800/8883 (53.96%)]\t\tLoss: 0.88027\n",
      "Training Progress: \tEpoch 45 [5120/8883 (57.55%)]\t\tLoss: 1.19151\n",
      "Training Progress: \tEpoch 45 [5440/8883 (61.15%)]\t\tLoss: 1.20097\n",
      "Training Progress: \tEpoch 45 [5760/8883 (64.75%)]\t\tLoss: 1.09219\n",
      "Training Progress: \tEpoch 45 [6080/8883 (68.35%)]\t\tLoss: 0.89570\n",
      "Training Progress: \tEpoch 45 [6400/8883 (71.94%)]\t\tLoss: 1.00520\n",
      "Training Progress: \tEpoch 45 [6720/8883 (75.54%)]\t\tLoss: 1.12514\n",
      "Training Progress: \tEpoch 45 [7040/8883 (79.14%)]\t\tLoss: 1.16039\n",
      "Training Progress: \tEpoch 45 [7360/8883 (82.73%)]\t\tLoss: 0.93860\n",
      "Training Progress: \tEpoch 45 [7680/8883 (86.33%)]\t\tLoss: 0.91824\n",
      "Training Progress: \tEpoch 45 [8000/8883 (89.93%)]\t\tLoss: 1.23286\n",
      "Training Progress: \tEpoch 45 [8320/8883 (93.53%)]\t\tLoss: 0.85402\n",
      "Training Progress: \tEpoch 45 [8640/8883 (97.12%)]\t\tLoss: 1.09219\n",
      "\tTrain loss: 0.02256, Accuracy: 6105/8883 (68.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1282/1692 (75.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 882/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/8883 (0.00%)]\t\tLoss: 0.97893\n",
      "Training Progress: \tEpoch 46 [320/8883 (3.60%)]\t\tLoss: 1.06749\n",
      "Training Progress: \tEpoch 46 [640/8883 (7.19%)]\t\tLoss: 0.75001\n",
      "Training Progress: \tEpoch 46 [960/8883 (10.79%)]\t\tLoss: 0.99033\n",
      "Training Progress: \tEpoch 46 [1280/8883 (14.39%)]\t\tLoss: 0.88573\n",
      "Training Progress: \tEpoch 46 [1600/8883 (17.99%)]\t\tLoss: 1.09227\n",
      "Training Progress: \tEpoch 46 [1920/8883 (21.58%)]\t\tLoss: 1.15034\n",
      "Training Progress: \tEpoch 46 [2240/8883 (25.18%)]\t\tLoss: 1.07855\n",
      "Training Progress: \tEpoch 46 [2560/8883 (28.78%)]\t\tLoss: 0.88791\n",
      "Training Progress: \tEpoch 46 [2880/8883 (32.37%)]\t\tLoss: 0.94469\n",
      "Training Progress: \tEpoch 46 [3200/8883 (35.97%)]\t\tLoss: 1.00862\n",
      "Training Progress: \tEpoch 46 [3520/8883 (39.57%)]\t\tLoss: 0.93428\n",
      "Training Progress: \tEpoch 46 [3840/8883 (43.17%)]\t\tLoss: 1.14968\n",
      "Training Progress: \tEpoch 46 [4160/8883 (46.76%)]\t\tLoss: 0.95940\n",
      "Training Progress: \tEpoch 46 [4480/8883 (50.36%)]\t\tLoss: 1.06157\n",
      "Training Progress: \tEpoch 46 [4800/8883 (53.96%)]\t\tLoss: 0.87776\n",
      "Training Progress: \tEpoch 46 [5120/8883 (57.55%)]\t\tLoss: 1.18320\n",
      "Training Progress: \tEpoch 46 [5440/8883 (61.15%)]\t\tLoss: 1.12770\n",
      "Training Progress: \tEpoch 46 [5760/8883 (64.75%)]\t\tLoss: 1.19805\n",
      "Training Progress: \tEpoch 46 [6080/8883 (68.35%)]\t\tLoss: 0.90189\n",
      "Training Progress: \tEpoch 46 [6400/8883 (71.94%)]\t\tLoss: 1.01949\n",
      "Training Progress: \tEpoch 46 [6720/8883 (75.54%)]\t\tLoss: 1.00092\n",
      "Training Progress: \tEpoch 46 [7040/8883 (79.14%)]\t\tLoss: 1.09289\n",
      "Training Progress: \tEpoch 46 [7360/8883 (82.73%)]\t\tLoss: 1.17354\n",
      "Training Progress: \tEpoch 46 [7680/8883 (86.33%)]\t\tLoss: 0.89510\n",
      "Training Progress: \tEpoch 46 [8000/8883 (89.93%)]\t\tLoss: 0.95340\n",
      "Training Progress: \tEpoch 46 [8320/8883 (93.53%)]\t\tLoss: 1.09216\n",
      "Training Progress: \tEpoch 46 [8640/8883 (97.12%)]\t\tLoss: 1.04668\n",
      "\tTrain loss: 0.02236, Accuracy: 6176/8883 (69.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1300/1692 (76.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 901/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/8883 (0.00%)]\t\tLoss: 0.80929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 47 [320/8883 (3.60%)]\t\tLoss: 0.84141\n",
      "Training Progress: \tEpoch 47 [640/8883 (7.19%)]\t\tLoss: 1.02895\n",
      "Training Progress: \tEpoch 47 [960/8883 (10.79%)]\t\tLoss: 0.98574\n",
      "Training Progress: \tEpoch 47 [1280/8883 (14.39%)]\t\tLoss: 1.26617\n",
      "Training Progress: \tEpoch 47 [1600/8883 (17.99%)]\t\tLoss: 1.13547\n",
      "Training Progress: \tEpoch 47 [1920/8883 (21.58%)]\t\tLoss: 1.00482\n",
      "Training Progress: \tEpoch 47 [2240/8883 (25.18%)]\t\tLoss: 1.00083\n",
      "Training Progress: \tEpoch 47 [2560/8883 (28.78%)]\t\tLoss: 0.93065\n",
      "Training Progress: \tEpoch 47 [2880/8883 (32.37%)]\t\tLoss: 1.00245\n",
      "Training Progress: \tEpoch 47 [3200/8883 (35.97%)]\t\tLoss: 0.95680\n",
      "Training Progress: \tEpoch 47 [3520/8883 (39.57%)]\t\tLoss: 0.96236\n",
      "Training Progress: \tEpoch 47 [3840/8883 (43.17%)]\t\tLoss: 1.15623\n",
      "Training Progress: \tEpoch 47 [4160/8883 (46.76%)]\t\tLoss: 1.14094\n",
      "Training Progress: \tEpoch 47 [4480/8883 (50.36%)]\t\tLoss: 0.93045\n",
      "Training Progress: \tEpoch 47 [4800/8883 (53.96%)]\t\tLoss: 0.76629\n",
      "Training Progress: \tEpoch 47 [5120/8883 (57.55%)]\t\tLoss: 1.10390\n",
      "Training Progress: \tEpoch 47 [5440/8883 (61.15%)]\t\tLoss: 0.94391\n",
      "Training Progress: \tEpoch 47 [5760/8883 (64.75%)]\t\tLoss: 1.06141\n",
      "Training Progress: \tEpoch 47 [6080/8883 (68.35%)]\t\tLoss: 0.85220\n",
      "Training Progress: \tEpoch 47 [6400/8883 (71.94%)]\t\tLoss: 1.03936\n",
      "Training Progress: \tEpoch 47 [6720/8883 (75.54%)]\t\tLoss: 1.00123\n",
      "Training Progress: \tEpoch 47 [7040/8883 (79.14%)]\t\tLoss: 1.07500\n",
      "Training Progress: \tEpoch 47 [7360/8883 (82.73%)]\t\tLoss: 0.98693\n",
      "Training Progress: \tEpoch 47 [7680/8883 (86.33%)]\t\tLoss: 0.98741\n",
      "Training Progress: \tEpoch 47 [8000/8883 (89.93%)]\t\tLoss: 0.93922\n",
      "Training Progress: \tEpoch 47 [8320/8883 (93.53%)]\t\tLoss: 0.85761\n",
      "Training Progress: \tEpoch 47 [8640/8883 (97.12%)]\t\tLoss: 0.92106\n",
      "\tTrain loss: 0.02209, Accuracy: 6162/8883 (69.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1287/1692 (76.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 875/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/8883 (0.00%)]\t\tLoss: 0.76383\n",
      "Training Progress: \tEpoch 48 [320/8883 (3.60%)]\t\tLoss: 0.92459\n",
      "Training Progress: \tEpoch 48 [640/8883 (7.19%)]\t\tLoss: 0.83272\n",
      "Training Progress: \tEpoch 48 [960/8883 (10.79%)]\t\tLoss: 0.98842\n",
      "Training Progress: \tEpoch 48 [1280/8883 (14.39%)]\t\tLoss: 1.12817\n",
      "Training Progress: \tEpoch 48 [1600/8883 (17.99%)]\t\tLoss: 1.00959\n",
      "Training Progress: \tEpoch 48 [1920/8883 (21.58%)]\t\tLoss: 0.94639\n",
      "Training Progress: \tEpoch 48 [2240/8883 (25.18%)]\t\tLoss: 1.04197\n",
      "Training Progress: \tEpoch 48 [2560/8883 (28.78%)]\t\tLoss: 0.92642\n",
      "Training Progress: \tEpoch 48 [2880/8883 (32.37%)]\t\tLoss: 1.25523\n",
      "Training Progress: \tEpoch 48 [3200/8883 (35.97%)]\t\tLoss: 1.27283\n",
      "Training Progress: \tEpoch 48 [3520/8883 (39.57%)]\t\tLoss: 1.08026\n",
      "Training Progress: \tEpoch 48 [3840/8883 (43.17%)]\t\tLoss: 1.02186\n",
      "Training Progress: \tEpoch 48 [4160/8883 (46.76%)]\t\tLoss: 0.92039\n",
      "Training Progress: \tEpoch 48 [4480/8883 (50.36%)]\t\tLoss: 1.15342\n",
      "Training Progress: \tEpoch 48 [4800/8883 (53.96%)]\t\tLoss: 0.79467\n",
      "Training Progress: \tEpoch 48 [5120/8883 (57.55%)]\t\tLoss: 0.95010\n",
      "Training Progress: \tEpoch 48 [5440/8883 (61.15%)]\t\tLoss: 1.35410\n",
      "Training Progress: \tEpoch 48 [5760/8883 (64.75%)]\t\tLoss: 0.97488\n",
      "Training Progress: \tEpoch 48 [6080/8883 (68.35%)]\t\tLoss: 0.81840\n",
      "Training Progress: \tEpoch 48 [6400/8883 (71.94%)]\t\tLoss: 0.95895\n",
      "Training Progress: \tEpoch 48 [6720/8883 (75.54%)]\t\tLoss: 0.96471\n",
      "Training Progress: \tEpoch 48 [7040/8883 (79.14%)]\t\tLoss: 1.05033\n",
      "Training Progress: \tEpoch 48 [7360/8883 (82.73%)]\t\tLoss: 0.94036\n",
      "Training Progress: \tEpoch 48 [7680/8883 (86.33%)]\t\tLoss: 0.83929\n",
      "Training Progress: \tEpoch 48 [8000/8883 (89.93%)]\t\tLoss: 1.03694\n",
      "Training Progress: \tEpoch 48 [8320/8883 (93.53%)]\t\tLoss: 0.87319\n",
      "Training Progress: \tEpoch 48 [8640/8883 (97.12%)]\t\tLoss: 0.86741\n",
      "\tTrain loss: 0.02179, Accuracy: 6204/8883 (69.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1309/1692 (77.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 898/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/8883 (0.00%)]\t\tLoss: 0.84464\n",
      "Training Progress: \tEpoch 49 [320/8883 (3.60%)]\t\tLoss: 0.79242\n",
      "Training Progress: \tEpoch 49 [640/8883 (7.19%)]\t\tLoss: 0.91552\n",
      "Training Progress: \tEpoch 49 [960/8883 (10.79%)]\t\tLoss: 0.90051\n",
      "Training Progress: \tEpoch 49 [1280/8883 (14.39%)]\t\tLoss: 0.97982\n",
      "Training Progress: \tEpoch 49 [1600/8883 (17.99%)]\t\tLoss: 1.25100\n",
      "Training Progress: \tEpoch 49 [1920/8883 (21.58%)]\t\tLoss: 1.00391\n",
      "Training Progress: \tEpoch 49 [2240/8883 (25.18%)]\t\tLoss: 0.86633\n",
      "Training Progress: \tEpoch 49 [2560/8883 (28.78%)]\t\tLoss: 0.91522\n",
      "Training Progress: \tEpoch 49 [2880/8883 (32.37%)]\t\tLoss: 1.08444\n",
      "Training Progress: \tEpoch 49 [3200/8883 (35.97%)]\t\tLoss: 1.23920\n",
      "Training Progress: \tEpoch 49 [3520/8883 (39.57%)]\t\tLoss: 1.16733\n",
      "Training Progress: \tEpoch 49 [3840/8883 (43.17%)]\t\tLoss: 1.42633\n",
      "Training Progress: \tEpoch 49 [4160/8883 (46.76%)]\t\tLoss: 1.07619\n",
      "Training Progress: \tEpoch 49 [4480/8883 (50.36%)]\t\tLoss: 0.94055\n",
      "Training Progress: \tEpoch 49 [4800/8883 (53.96%)]\t\tLoss: 0.75443\n",
      "Training Progress: \tEpoch 49 [5120/8883 (57.55%)]\t\tLoss: 1.00280\n",
      "Training Progress: \tEpoch 49 [5440/8883 (61.15%)]\t\tLoss: 1.19280\n",
      "Training Progress: \tEpoch 49 [5760/8883 (64.75%)]\t\tLoss: 1.11833\n",
      "Training Progress: \tEpoch 49 [6080/8883 (68.35%)]\t\tLoss: 0.84061\n",
      "Training Progress: \tEpoch 49 [6400/8883 (71.94%)]\t\tLoss: 0.95587\n",
      "Training Progress: \tEpoch 49 [6720/8883 (75.54%)]\t\tLoss: 0.97029\n",
      "Training Progress: \tEpoch 49 [7040/8883 (79.14%)]\t\tLoss: 1.09671\n",
      "Training Progress: \tEpoch 49 [7360/8883 (82.73%)]\t\tLoss: 1.14805\n",
      "Training Progress: \tEpoch 49 [7680/8883 (86.33%)]\t\tLoss: 1.02098\n",
      "Training Progress: \tEpoch 49 [8000/8883 (89.93%)]\t\tLoss: 0.85045\n",
      "Training Progress: \tEpoch 49 [8320/8883 (93.53%)]\t\tLoss: 0.86621\n",
      "Training Progress: \tEpoch 49 [8640/8883 (97.12%)]\t\tLoss: 1.18668\n",
      "\tTrain loss: 0.02187, Accuracy: 6148/8883 (69.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1278/1692 (75.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 896/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/8883 (0.00%)]\t\tLoss: 0.84750\n",
      "Training Progress: \tEpoch 50 [320/8883 (3.60%)]\t\tLoss: 0.99679\n",
      "Training Progress: \tEpoch 50 [640/8883 (7.19%)]\t\tLoss: 1.08008\n",
      "Training Progress: \tEpoch 50 [960/8883 (10.79%)]\t\tLoss: 0.90798\n",
      "Training Progress: \tEpoch 50 [1280/8883 (14.39%)]\t\tLoss: 0.99956\n",
      "Training Progress: \tEpoch 50 [1600/8883 (17.99%)]\t\tLoss: 0.99884\n",
      "Training Progress: \tEpoch 50 [1920/8883 (21.58%)]\t\tLoss: 0.95067\n",
      "Training Progress: \tEpoch 50 [2240/8883 (25.18%)]\t\tLoss: 0.93425\n",
      "Training Progress: \tEpoch 50 [2560/8883 (28.78%)]\t\tLoss: 0.92353\n",
      "Training Progress: \tEpoch 50 [2880/8883 (32.37%)]\t\tLoss: 1.11178\n",
      "Training Progress: \tEpoch 50 [3200/8883 (35.97%)]\t\tLoss: 1.32655\n",
      "Training Progress: \tEpoch 50 [3520/8883 (39.57%)]\t\tLoss: 0.90048\n",
      "Training Progress: \tEpoch 50 [3840/8883 (43.17%)]\t\tLoss: 1.17993\n",
      "Training Progress: \tEpoch 50 [4160/8883 (46.76%)]\t\tLoss: 1.00478\n",
      "Training Progress: \tEpoch 50 [4480/8883 (50.36%)]\t\tLoss: 0.87228\n",
      "Training Progress: \tEpoch 50 [4800/8883 (53.96%)]\t\tLoss: 0.82056\n",
      "Training Progress: \tEpoch 50 [5120/8883 (57.55%)]\t\tLoss: 0.95551\n",
      "Training Progress: \tEpoch 50 [5440/8883 (61.15%)]\t\tLoss: 1.11045\n",
      "Training Progress: \tEpoch 50 [5760/8883 (64.75%)]\t\tLoss: 0.88601\n",
      "Training Progress: \tEpoch 50 [6080/8883 (68.35%)]\t\tLoss: 0.83272\n",
      "Training Progress: \tEpoch 50 [6400/8883 (71.94%)]\t\tLoss: 1.20978\n",
      "Training Progress: \tEpoch 50 [6720/8883 (75.54%)]\t\tLoss: 0.85508\n",
      "Training Progress: \tEpoch 50 [7040/8883 (79.14%)]\t\tLoss: 0.79804\n",
      "Training Progress: \tEpoch 50 [7360/8883 (82.73%)]\t\tLoss: 1.02271\n",
      "Training Progress: \tEpoch 50 [7680/8883 (86.33%)]\t\tLoss: 0.89224\n",
      "Training Progress: \tEpoch 50 [8000/8883 (89.93%)]\t\tLoss: 0.76564\n",
      "Training Progress: \tEpoch 50 [8320/8883 (93.53%)]\t\tLoss: 0.72046\n",
      "Training Progress: \tEpoch 50 [8640/8883 (97.12%)]\t\tLoss: 1.02042\n",
      "\tTrain loss: 0.02085, Accuracy: 6302/8883 (70.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1344/1692 (79.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 910/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/8883 (0.00%)]\t\tLoss: 0.89085\n",
      "Training Progress: \tEpoch 51 [320/8883 (3.60%)]\t\tLoss: 0.96514\n",
      "Training Progress: \tEpoch 51 [640/8883 (7.19%)]\t\tLoss: 0.75544\n",
      "Training Progress: \tEpoch 51 [960/8883 (10.79%)]\t\tLoss: 0.82227\n",
      "Training Progress: \tEpoch 51 [1280/8883 (14.39%)]\t\tLoss: 0.96268\n",
      "Training Progress: \tEpoch 51 [1600/8883 (17.99%)]\t\tLoss: 1.43737\n",
      "Training Progress: \tEpoch 51 [1920/8883 (21.58%)]\t\tLoss: 1.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 51 [2240/8883 (25.18%)]\t\tLoss: 0.86201\n",
      "Training Progress: \tEpoch 51 [2560/8883 (28.78%)]\t\tLoss: 1.17210\n",
      "Training Progress: \tEpoch 51 [2880/8883 (32.37%)]\t\tLoss: 1.00399\n",
      "Training Progress: \tEpoch 51 [3200/8883 (35.97%)]\t\tLoss: 1.04048\n",
      "Training Progress: \tEpoch 51 [3520/8883 (39.57%)]\t\tLoss: 0.91697\n",
      "Training Progress: \tEpoch 51 [3840/8883 (43.17%)]\t\tLoss: 1.05741\n",
      "Training Progress: \tEpoch 51 [4160/8883 (46.76%)]\t\tLoss: 1.09559\n",
      "Training Progress: \tEpoch 51 [4480/8883 (50.36%)]\t\tLoss: 0.94785\n",
      "Training Progress: \tEpoch 51 [4800/8883 (53.96%)]\t\tLoss: 0.70397\n",
      "Training Progress: \tEpoch 51 [5120/8883 (57.55%)]\t\tLoss: 1.28571\n",
      "Training Progress: \tEpoch 51 [5440/8883 (61.15%)]\t\tLoss: 0.95228\n",
      "Training Progress: \tEpoch 51 [5760/8883 (64.75%)]\t\tLoss: 1.06669\n",
      "Training Progress: \tEpoch 51 [6080/8883 (68.35%)]\t\tLoss: 0.85468\n",
      "Training Progress: \tEpoch 51 [6400/8883 (71.94%)]\t\tLoss: 1.01605\n",
      "Training Progress: \tEpoch 51 [6720/8883 (75.54%)]\t\tLoss: 1.06161\n",
      "Training Progress: \tEpoch 51 [7040/8883 (79.14%)]\t\tLoss: 1.01868\n",
      "Training Progress: \tEpoch 51 [7360/8883 (82.73%)]\t\tLoss: 0.95744\n",
      "Training Progress: \tEpoch 51 [7680/8883 (86.33%)]\t\tLoss: 1.07300\n",
      "Training Progress: \tEpoch 51 [8000/8883 (89.93%)]\t\tLoss: 1.05125\n",
      "Training Progress: \tEpoch 51 [8320/8883 (93.53%)]\t\tLoss: 0.94795\n",
      "Training Progress: \tEpoch 51 [8640/8883 (97.12%)]\t\tLoss: 1.06454\n",
      "\tTrain loss: 0.02087, Accuracy: 6336/8883 (71.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1351/1692 (79.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 905/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/8883 (0.00%)]\t\tLoss: 0.92245\n",
      "Training Progress: \tEpoch 52 [320/8883 (3.60%)]\t\tLoss: 0.87470\n",
      "Training Progress: \tEpoch 52 [640/8883 (7.19%)]\t\tLoss: 0.96695\n",
      "Training Progress: \tEpoch 52 [960/8883 (10.79%)]\t\tLoss: 0.83075\n",
      "Training Progress: \tEpoch 52 [1280/8883 (14.39%)]\t\tLoss: 0.93745\n",
      "Training Progress: \tEpoch 52 [1600/8883 (17.99%)]\t\tLoss: 1.08534\n",
      "Training Progress: \tEpoch 52 [1920/8883 (21.58%)]\t\tLoss: 0.94681\n",
      "Training Progress: \tEpoch 52 [2240/8883 (25.18%)]\t\tLoss: 0.84161\n",
      "Training Progress: \tEpoch 52 [2560/8883 (28.78%)]\t\tLoss: 1.01491\n",
      "Training Progress: \tEpoch 52 [2880/8883 (32.37%)]\t\tLoss: 0.87798\n",
      "Training Progress: \tEpoch 52 [3200/8883 (35.97%)]\t\tLoss: 1.02886\n",
      "Training Progress: \tEpoch 52 [3520/8883 (39.57%)]\t\tLoss: 1.05264\n",
      "Training Progress: \tEpoch 52 [3840/8883 (43.17%)]\t\tLoss: 1.21159\n",
      "Training Progress: \tEpoch 52 [4160/8883 (46.76%)]\t\tLoss: 0.99197\n",
      "Training Progress: \tEpoch 52 [4480/8883 (50.36%)]\t\tLoss: 1.06503\n",
      "Training Progress: \tEpoch 52 [4800/8883 (53.96%)]\t\tLoss: 0.90193\n",
      "Training Progress: \tEpoch 52 [5120/8883 (57.55%)]\t\tLoss: 0.93738\n",
      "Training Progress: \tEpoch 52 [5440/8883 (61.15%)]\t\tLoss: 1.24023\n",
      "Training Progress: \tEpoch 52 [5760/8883 (64.75%)]\t\tLoss: 1.07806\n",
      "Training Progress: \tEpoch 52 [6080/8883 (68.35%)]\t\tLoss: 0.76668\n",
      "Training Progress: \tEpoch 52 [6400/8883 (71.94%)]\t\tLoss: 0.86111\n",
      "Training Progress: \tEpoch 52 [6720/8883 (75.54%)]\t\tLoss: 0.94298\n",
      "Training Progress: \tEpoch 52 [7040/8883 (79.14%)]\t\tLoss: 0.91044\n",
      "Training Progress: \tEpoch 52 [7360/8883 (82.73%)]\t\tLoss: 0.98578\n",
      "Training Progress: \tEpoch 52 [7680/8883 (86.33%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 52 [8000/8883 (89.93%)]\t\tLoss: 0.72991\n",
      "Training Progress: \tEpoch 52 [8320/8883 (93.53%)]\t\tLoss: 0.80995\n",
      "Training Progress: \tEpoch 52 [8640/8883 (97.12%)]\t\tLoss: 1.01655\n",
      "\tTrain loss: 0.02025, Accuracy: 6382/8883 (71.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1356/1692 (80.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 899/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/8883 (0.00%)]\t\tLoss: 0.84318\n",
      "Training Progress: \tEpoch 53 [320/8883 (3.60%)]\t\tLoss: 0.92124\n",
      "Training Progress: \tEpoch 53 [640/8883 (7.19%)]\t\tLoss: 0.79407\n",
      "Training Progress: \tEpoch 53 [960/8883 (10.79%)]\t\tLoss: 1.14844\n",
      "Training Progress: \tEpoch 53 [1280/8883 (14.39%)]\t\tLoss: 1.16551\n",
      "Training Progress: \tEpoch 53 [1600/8883 (17.99%)]\t\tLoss: 0.97932\n",
      "Training Progress: \tEpoch 53 [1920/8883 (21.58%)]\t\tLoss: 1.03294\n",
      "Training Progress: \tEpoch 53 [2240/8883 (25.18%)]\t\tLoss: 1.00627\n",
      "Training Progress: \tEpoch 53 [2560/8883 (28.78%)]\t\tLoss: 0.83686\n",
      "Training Progress: \tEpoch 53 [2880/8883 (32.37%)]\t\tLoss: 0.88854\n",
      "Training Progress: \tEpoch 53 [3200/8883 (35.97%)]\t\tLoss: 1.10649\n",
      "Training Progress: \tEpoch 53 [3520/8883 (39.57%)]\t\tLoss: 1.02358\n",
      "Training Progress: \tEpoch 53 [3840/8883 (43.17%)]\t\tLoss: 1.10865\n",
      "Training Progress: \tEpoch 53 [4160/8883 (46.76%)]\t\tLoss: 1.02580\n",
      "Training Progress: \tEpoch 53 [4480/8883 (50.36%)]\t\tLoss: 0.95009\n",
      "Training Progress: \tEpoch 53 [4800/8883 (53.96%)]\t\tLoss: 0.84766\n",
      "Training Progress: \tEpoch 53 [5120/8883 (57.55%)]\t\tLoss: 1.06708\n",
      "Training Progress: \tEpoch 53 [5440/8883 (61.15%)]\t\tLoss: 1.06654\n",
      "Training Progress: \tEpoch 53 [5760/8883 (64.75%)]\t\tLoss: 0.94960\n",
      "Training Progress: \tEpoch 53 [6080/8883 (68.35%)]\t\tLoss: 0.87162\n",
      "Training Progress: \tEpoch 53 [6400/8883 (71.94%)]\t\tLoss: 1.03827\n",
      "Training Progress: \tEpoch 53 [6720/8883 (75.54%)]\t\tLoss: 0.89406\n",
      "Training Progress: \tEpoch 53 [7040/8883 (79.14%)]\t\tLoss: 1.15208\n",
      "Training Progress: \tEpoch 53 [7360/8883 (82.73%)]\t\tLoss: 1.08437\n",
      "Training Progress: \tEpoch 53 [7680/8883 (86.33%)]\t\tLoss: 1.02768\n",
      "Training Progress: \tEpoch 53 [8000/8883 (89.93%)]\t\tLoss: 0.89036\n",
      "Training Progress: \tEpoch 53 [8320/8883 (93.53%)]\t\tLoss: 0.82724\n",
      "Training Progress: \tEpoch 53 [8640/8883 (97.12%)]\t\tLoss: 1.19869\n",
      "\tTrain loss: 0.02015, Accuracy: 6452/8883 (72.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1362/1692 (80.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 893/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/8883 (0.00%)]\t\tLoss: 0.82442\n",
      "Training Progress: \tEpoch 54 [320/8883 (3.60%)]\t\tLoss: 1.12472\n",
      "Training Progress: \tEpoch 54 [640/8883 (7.19%)]\t\tLoss: 0.76217\n",
      "Training Progress: \tEpoch 54 [960/8883 (10.79%)]\t\tLoss: 0.90203\n",
      "Training Progress: \tEpoch 54 [1280/8883 (14.39%)]\t\tLoss: 0.88144\n",
      "Training Progress: \tEpoch 54 [1600/8883 (17.99%)]\t\tLoss: 1.22495\n",
      "Training Progress: \tEpoch 54 [1920/8883 (21.58%)]\t\tLoss: 0.79429\n",
      "Training Progress: \tEpoch 54 [2240/8883 (25.18%)]\t\tLoss: 1.03280\n",
      "Training Progress: \tEpoch 54 [2560/8883 (28.78%)]\t\tLoss: 1.15471\n",
      "Training Progress: \tEpoch 54 [2880/8883 (32.37%)]\t\tLoss: 1.04248\n",
      "Training Progress: \tEpoch 54 [3200/8883 (35.97%)]\t\tLoss: 1.19562\n",
      "Training Progress: \tEpoch 54 [3520/8883 (39.57%)]\t\tLoss: 0.96178\n",
      "Training Progress: \tEpoch 54 [3840/8883 (43.17%)]\t\tLoss: 1.01938\n",
      "Training Progress: \tEpoch 54 [4160/8883 (46.76%)]\t\tLoss: 1.05401\n",
      "Training Progress: \tEpoch 54 [4480/8883 (50.36%)]\t\tLoss: 0.79135\n",
      "Training Progress: \tEpoch 54 [4800/8883 (53.96%)]\t\tLoss: 0.83474\n",
      "Training Progress: \tEpoch 54 [5120/8883 (57.55%)]\t\tLoss: 0.97667\n",
      "Training Progress: \tEpoch 54 [5440/8883 (61.15%)]\t\tLoss: 0.93730\n",
      "Training Progress: \tEpoch 54 [5760/8883 (64.75%)]\t\tLoss: 0.96039\n",
      "Training Progress: \tEpoch 54 [6080/8883 (68.35%)]\t\tLoss: 0.94553\n",
      "Training Progress: \tEpoch 54 [6400/8883 (71.94%)]\t\tLoss: 1.14374\n",
      "Training Progress: \tEpoch 54 [6720/8883 (75.54%)]\t\tLoss: 0.89494\n",
      "Training Progress: \tEpoch 54 [7040/8883 (79.14%)]\t\tLoss: 0.90027\n",
      "Training Progress: \tEpoch 54 [7360/8883 (82.73%)]\t\tLoss: 1.04840\n",
      "Training Progress: \tEpoch 54 [7680/8883 (86.33%)]\t\tLoss: 0.99521\n",
      "Training Progress: \tEpoch 54 [8000/8883 (89.93%)]\t\tLoss: 0.93977\n",
      "Training Progress: \tEpoch 54 [8320/8883 (93.53%)]\t\tLoss: 0.78277\n",
      "Training Progress: \tEpoch 54 [8640/8883 (97.12%)]\t\tLoss: 1.20466\n",
      "\tTrain loss: 0.01998, Accuracy: 6510/8883 (73.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1385/1692 (81.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 898/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/8883 (0.00%)]\t\tLoss: 0.83844\n",
      "Training Progress: \tEpoch 55 [320/8883 (3.60%)]\t\tLoss: 0.94343\n",
      "Training Progress: \tEpoch 55 [640/8883 (7.19%)]\t\tLoss: 0.78674\n",
      "Training Progress: \tEpoch 55 [960/8883 (10.79%)]\t\tLoss: 0.86740\n",
      "Training Progress: \tEpoch 55 [1280/8883 (14.39%)]\t\tLoss: 0.94052\n",
      "Training Progress: \tEpoch 55 [1600/8883 (17.99%)]\t\tLoss: 0.94811\n",
      "Training Progress: \tEpoch 55 [1920/8883 (21.58%)]\t\tLoss: 0.97498\n",
      "Training Progress: \tEpoch 55 [2240/8883 (25.18%)]\t\tLoss: 0.84962\n",
      "Training Progress: \tEpoch 55 [2560/8883 (28.78%)]\t\tLoss: 0.81969\n",
      "Training Progress: \tEpoch 55 [2880/8883 (32.37%)]\t\tLoss: 1.14965\n",
      "Training Progress: \tEpoch 55 [3200/8883 (35.97%)]\t\tLoss: 1.25319\n",
      "Training Progress: \tEpoch 55 [3520/8883 (39.57%)]\t\tLoss: 0.89084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 55 [3840/8883 (43.17%)]\t\tLoss: 0.97559\n",
      "Training Progress: \tEpoch 55 [4160/8883 (46.76%)]\t\tLoss: 0.82081\n",
      "Training Progress: \tEpoch 55 [4480/8883 (50.36%)]\t\tLoss: 1.01169\n",
      "Training Progress: \tEpoch 55 [4800/8883 (53.96%)]\t\tLoss: 0.79307\n",
      "Training Progress: \tEpoch 55 [5120/8883 (57.55%)]\t\tLoss: 1.21005\n",
      "Training Progress: \tEpoch 55 [5440/8883 (61.15%)]\t\tLoss: 1.10808\n",
      "Training Progress: \tEpoch 55 [5760/8883 (64.75%)]\t\tLoss: 0.87595\n",
      "Training Progress: \tEpoch 55 [6080/8883 (68.35%)]\t\tLoss: 0.81590\n",
      "Training Progress: \tEpoch 55 [6400/8883 (71.94%)]\t\tLoss: 0.99094\n",
      "Training Progress: \tEpoch 55 [6720/8883 (75.54%)]\t\tLoss: 0.98516\n",
      "Training Progress: \tEpoch 55 [7040/8883 (79.14%)]\t\tLoss: 0.92385\n",
      "Training Progress: \tEpoch 55 [7360/8883 (82.73%)]\t\tLoss: 0.93694\n",
      "Training Progress: \tEpoch 55 [7680/8883 (86.33%)]\t\tLoss: 0.80612\n",
      "Training Progress: \tEpoch 55 [8000/8883 (89.93%)]\t\tLoss: 0.92740\n",
      "Training Progress: \tEpoch 55 [8320/8883 (93.53%)]\t\tLoss: 0.85642\n",
      "Training Progress: \tEpoch 55 [8640/8883 (97.12%)]\t\tLoss: 1.07786\n",
      "\tTrain loss: 0.01941, Accuracy: 6475/8883 (72.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1385/1692 (81.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 925/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/8883 (0.00%)]\t\tLoss: 0.85255\n",
      "Training Progress: \tEpoch 56 [320/8883 (3.60%)]\t\tLoss: 0.78676\n",
      "Training Progress: \tEpoch 56 [640/8883 (7.19%)]\t\tLoss: 0.83577\n",
      "Training Progress: \tEpoch 56 [960/8883 (10.79%)]\t\tLoss: 0.99024\n",
      "Training Progress: \tEpoch 56 [1280/8883 (14.39%)]\t\tLoss: 0.88342\n",
      "Training Progress: \tEpoch 56 [1600/8883 (17.99%)]\t\tLoss: 1.16198\n",
      "Training Progress: \tEpoch 56 [1920/8883 (21.58%)]\t\tLoss: 0.94042\n",
      "Training Progress: \tEpoch 56 [2240/8883 (25.18%)]\t\tLoss: 0.90616\n",
      "Training Progress: \tEpoch 56 [2560/8883 (28.78%)]\t\tLoss: 1.03966\n",
      "Training Progress: \tEpoch 56 [2880/8883 (32.37%)]\t\tLoss: 0.87274\n",
      "Training Progress: \tEpoch 56 [3200/8883 (35.97%)]\t\tLoss: 0.94151\n",
      "Training Progress: \tEpoch 56 [3520/8883 (39.57%)]\t\tLoss: 1.10705\n",
      "Training Progress: \tEpoch 56 [3840/8883 (43.17%)]\t\tLoss: 1.28020\n",
      "Training Progress: \tEpoch 56 [4160/8883 (46.76%)]\t\tLoss: 1.11307\n",
      "Training Progress: \tEpoch 56 [4480/8883 (50.36%)]\t\tLoss: 0.72568\n",
      "Training Progress: \tEpoch 56 [4800/8883 (53.96%)]\t\tLoss: 0.85488\n",
      "Training Progress: \tEpoch 56 [5120/8883 (57.55%)]\t\tLoss: 1.04995\n",
      "Training Progress: \tEpoch 56 [5440/8883 (61.15%)]\t\tLoss: 1.12240\n",
      "Training Progress: \tEpoch 56 [5760/8883 (64.75%)]\t\tLoss: 0.74968\n",
      "Training Progress: \tEpoch 56 [6080/8883 (68.35%)]\t\tLoss: 0.81946\n",
      "Training Progress: \tEpoch 56 [6400/8883 (71.94%)]\t\tLoss: 0.87127\n",
      "Training Progress: \tEpoch 56 [6720/8883 (75.54%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 56 [7040/8883 (79.14%)]\t\tLoss: 1.05330\n",
      "Training Progress: \tEpoch 56 [7360/8883 (82.73%)]\t\tLoss: 0.87846\n",
      "Training Progress: \tEpoch 56 [7680/8883 (86.33%)]\t\tLoss: 0.83705\n",
      "Training Progress: \tEpoch 56 [8000/8883 (89.93%)]\t\tLoss: 0.86458\n",
      "Training Progress: \tEpoch 56 [8320/8883 (93.53%)]\t\tLoss: 0.86003\n",
      "Training Progress: \tEpoch 56 [8640/8883 (97.12%)]\t\tLoss: 0.73392\n",
      "\tTrain loss: 0.01953, Accuracy: 6465/8883 (72.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1383/1692 (81.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 933/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/8883 (0.00%)]\t\tLoss: 1.05454\n",
      "Training Progress: \tEpoch 57 [320/8883 (3.60%)]\t\tLoss: 1.05349\n",
      "Training Progress: \tEpoch 57 [640/8883 (7.19%)]\t\tLoss: 0.79582\n",
      "Training Progress: \tEpoch 57 [960/8883 (10.79%)]\t\tLoss: 0.71978\n",
      "Training Progress: \tEpoch 57 [1280/8883 (14.39%)]\t\tLoss: 0.94352\n",
      "Training Progress: \tEpoch 57 [1600/8883 (17.99%)]\t\tLoss: 1.01790\n",
      "Training Progress: \tEpoch 57 [1920/8883 (21.58%)]\t\tLoss: 0.84462\n",
      "Training Progress: \tEpoch 57 [2240/8883 (25.18%)]\t\tLoss: 0.76960\n",
      "Training Progress: \tEpoch 57 [2560/8883 (28.78%)]\t\tLoss: 0.75653\n",
      "Training Progress: \tEpoch 57 [2880/8883 (32.37%)]\t\tLoss: 0.75062\n",
      "Training Progress: \tEpoch 57 [3200/8883 (35.97%)]\t\tLoss: 1.06889\n",
      "Training Progress: \tEpoch 57 [3520/8883 (39.57%)]\t\tLoss: 1.09508\n",
      "Training Progress: \tEpoch 57 [3840/8883 (43.17%)]\t\tLoss: 0.83840\n",
      "Training Progress: \tEpoch 57 [4160/8883 (46.76%)]\t\tLoss: 0.94335\n",
      "Training Progress: \tEpoch 57 [4480/8883 (50.36%)]\t\tLoss: 0.85077\n",
      "Training Progress: \tEpoch 57 [4800/8883 (53.96%)]\t\tLoss: 0.88985\n",
      "Training Progress: \tEpoch 57 [5120/8883 (57.55%)]\t\tLoss: 1.07009\n",
      "Training Progress: \tEpoch 57 [5440/8883 (61.15%)]\t\tLoss: 1.13122\n",
      "Training Progress: \tEpoch 57 [5760/8883 (64.75%)]\t\tLoss: 0.89439\n",
      "Training Progress: \tEpoch 57 [6080/8883 (68.35%)]\t\tLoss: 0.78712\n",
      "Training Progress: \tEpoch 57 [6400/8883 (71.94%)]\t\tLoss: 1.01936\n",
      "Training Progress: \tEpoch 57 [6720/8883 (75.54%)]\t\tLoss: 1.02859\n",
      "Training Progress: \tEpoch 57 [7040/8883 (79.14%)]\t\tLoss: 1.01796\n",
      "Training Progress: \tEpoch 57 [7360/8883 (82.73%)]\t\tLoss: 1.02505\n",
      "Training Progress: \tEpoch 57 [7680/8883 (86.33%)]\t\tLoss: 0.86634\n",
      "Training Progress: \tEpoch 57 [8000/8883 (89.93%)]\t\tLoss: 0.82709\n",
      "Training Progress: \tEpoch 57 [8320/8883 (93.53%)]\t\tLoss: 0.81136\n",
      "Training Progress: \tEpoch 57 [8640/8883 (97.12%)]\t\tLoss: 1.08241\n",
      "\tTrain loss: 0.01971, Accuracy: 6492/8883 (73.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1392/1692 (82.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 951/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/8883 (0.00%)]\t\tLoss: 0.83067\n",
      "Training Progress: \tEpoch 58 [320/8883 (3.60%)]\t\tLoss: 0.93081\n",
      "Training Progress: \tEpoch 58 [640/8883 (7.19%)]\t\tLoss: 0.77856\n",
      "Training Progress: \tEpoch 58 [960/8883 (10.79%)]\t\tLoss: 0.89926\n",
      "Training Progress: \tEpoch 58 [1280/8883 (14.39%)]\t\tLoss: 0.95507\n",
      "Training Progress: \tEpoch 58 [1600/8883 (17.99%)]\t\tLoss: 1.04472\n",
      "Training Progress: \tEpoch 58 [1920/8883 (21.58%)]\t\tLoss: 0.94782\n",
      "Training Progress: \tEpoch 58 [2240/8883 (25.18%)]\t\tLoss: 0.70644\n",
      "Training Progress: \tEpoch 58 [2560/8883 (28.78%)]\t\tLoss: 0.87493\n",
      "Training Progress: \tEpoch 58 [2880/8883 (32.37%)]\t\tLoss: 1.21325\n",
      "Training Progress: \tEpoch 58 [3200/8883 (35.97%)]\t\tLoss: 1.03016\n",
      "Training Progress: \tEpoch 58 [3520/8883 (39.57%)]\t\tLoss: 0.91672\n",
      "Training Progress: \tEpoch 58 [3840/8883 (43.17%)]\t\tLoss: 1.08250\n",
      "Training Progress: \tEpoch 58 [4160/8883 (46.76%)]\t\tLoss: 0.93450\n",
      "Training Progress: \tEpoch 58 [4480/8883 (50.36%)]\t\tLoss: 1.08048\n",
      "Training Progress: \tEpoch 58 [4800/8883 (53.96%)]\t\tLoss: 0.82647\n",
      "Training Progress: \tEpoch 58 [5120/8883 (57.55%)]\t\tLoss: 0.99472\n",
      "Training Progress: \tEpoch 58 [5440/8883 (61.15%)]\t\tLoss: 1.04035\n",
      "Training Progress: \tEpoch 58 [5760/8883 (64.75%)]\t\tLoss: 1.02408\n",
      "Training Progress: \tEpoch 58 [6080/8883 (68.35%)]\t\tLoss: 0.85163\n",
      "Training Progress: \tEpoch 58 [6400/8883 (71.94%)]\t\tLoss: 0.90941\n",
      "Training Progress: \tEpoch 58 [6720/8883 (75.54%)]\t\tLoss: 0.91178\n",
      "Training Progress: \tEpoch 58 [7040/8883 (79.14%)]\t\tLoss: 0.93489\n",
      "Training Progress: \tEpoch 58 [7360/8883 (82.73%)]\t\tLoss: 0.80659\n",
      "Training Progress: \tEpoch 58 [7680/8883 (86.33%)]\t\tLoss: 0.75688\n",
      "Training Progress: \tEpoch 58 [8000/8883 (89.93%)]\t\tLoss: 1.02373\n",
      "Training Progress: \tEpoch 58 [8320/8883 (93.53%)]\t\tLoss: 0.84972\n",
      "Training Progress: \tEpoch 58 [8640/8883 (97.12%)]\t\tLoss: 0.88353\n",
      "\tTrain loss: 0.01858, Accuracy: 6565/8883 (73.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1413/1692 (83.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 926/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/8883 (0.00%)]\t\tLoss: 1.07420\n",
      "Training Progress: \tEpoch 59 [320/8883 (3.60%)]\t\tLoss: 0.86018\n",
      "Training Progress: \tEpoch 59 [640/8883 (7.19%)]\t\tLoss: 0.88573\n",
      "Training Progress: \tEpoch 59 [960/8883 (10.79%)]\t\tLoss: 1.02244\n",
      "Training Progress: \tEpoch 59 [1280/8883 (14.39%)]\t\tLoss: 0.95508\n",
      "Training Progress: \tEpoch 59 [1600/8883 (17.99%)]\t\tLoss: 1.01608\n",
      "Training Progress: \tEpoch 59 [1920/8883 (21.58%)]\t\tLoss: 0.78952\n",
      "Training Progress: \tEpoch 59 [2240/8883 (25.18%)]\t\tLoss: 0.79494\n",
      "Training Progress: \tEpoch 59 [2560/8883 (28.78%)]\t\tLoss: 0.77198\n",
      "Training Progress: \tEpoch 59 [2880/8883 (32.37%)]\t\tLoss: 1.01730\n",
      "Training Progress: \tEpoch 59 [3200/8883 (35.97%)]\t\tLoss: 0.99121\n",
      "Training Progress: \tEpoch 59 [3520/8883 (39.57%)]\t\tLoss: 1.02482\n",
      "Training Progress: \tEpoch 59 [3840/8883 (43.17%)]\t\tLoss: 1.10994\n",
      "Training Progress: \tEpoch 59 [4160/8883 (46.76%)]\t\tLoss: 0.80538\n",
      "Training Progress: \tEpoch 59 [4480/8883 (50.36%)]\t\tLoss: 0.76526\n",
      "Training Progress: \tEpoch 59 [4800/8883 (53.96%)]\t\tLoss: 0.70043\n",
      "Training Progress: \tEpoch 59 [5120/8883 (57.55%)]\t\tLoss: 1.06565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 59 [5440/8883 (61.15%)]\t\tLoss: 1.31289\n",
      "Training Progress: \tEpoch 59 [5760/8883 (64.75%)]\t\tLoss: 0.82369\n",
      "Training Progress: \tEpoch 59 [6080/8883 (68.35%)]\t\tLoss: 0.99132\n",
      "Training Progress: \tEpoch 59 [6400/8883 (71.94%)]\t\tLoss: 0.90713\n",
      "Training Progress: \tEpoch 59 [6720/8883 (75.54%)]\t\tLoss: 0.90924\n",
      "Training Progress: \tEpoch 59 [7040/8883 (79.14%)]\t\tLoss: 0.95886\n",
      "Training Progress: \tEpoch 59 [7360/8883 (82.73%)]\t\tLoss: 0.81880\n",
      "Training Progress: \tEpoch 59 [7680/8883 (86.33%)]\t\tLoss: 0.91682\n",
      "Training Progress: \tEpoch 59 [8000/8883 (89.93%)]\t\tLoss: 0.77236\n",
      "Training Progress: \tEpoch 59 [8320/8883 (93.53%)]\t\tLoss: 0.80435\n",
      "Training Progress: \tEpoch 59 [8640/8883 (97.12%)]\t\tLoss: 0.75833\n",
      "\tTrain loss: 0.01851, Accuracy: 6597/8883 (74.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1411/1692 (83.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 948/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/8883 (0.00%)]\t\tLoss: 0.85372\n",
      "Training Progress: \tEpoch 60 [320/8883 (3.60%)]\t\tLoss: 0.96137\n",
      "Training Progress: \tEpoch 60 [640/8883 (7.19%)]\t\tLoss: 0.77342\n",
      "Training Progress: \tEpoch 60 [960/8883 (10.79%)]\t\tLoss: 0.86909\n",
      "Training Progress: \tEpoch 60 [1280/8883 (14.39%)]\t\tLoss: 0.94867\n",
      "Training Progress: \tEpoch 60 [1600/8883 (17.99%)]\t\tLoss: 0.98638\n",
      "Training Progress: \tEpoch 60 [1920/8883 (21.58%)]\t\tLoss: 0.99168\n",
      "Training Progress: \tEpoch 60 [2240/8883 (25.18%)]\t\tLoss: 0.80177\n",
      "Training Progress: \tEpoch 60 [2560/8883 (28.78%)]\t\tLoss: 0.94573\n",
      "Training Progress: \tEpoch 60 [2880/8883 (32.37%)]\t\tLoss: 0.99423\n",
      "Training Progress: \tEpoch 60 [3200/8883 (35.97%)]\t\tLoss: 0.97412\n",
      "Training Progress: \tEpoch 60 [3520/8883 (39.57%)]\t\tLoss: 0.97284\n",
      "Training Progress: \tEpoch 60 [3840/8883 (43.17%)]\t\tLoss: 1.09883\n",
      "Training Progress: \tEpoch 60 [4160/8883 (46.76%)]\t\tLoss: 0.78475\n",
      "Training Progress: \tEpoch 60 [4480/8883 (50.36%)]\t\tLoss: 0.77972\n",
      "Training Progress: \tEpoch 60 [4800/8883 (53.96%)]\t\tLoss: 0.85248\n",
      "Training Progress: \tEpoch 60 [5120/8883 (57.55%)]\t\tLoss: 0.82926\n",
      "Training Progress: \tEpoch 60 [5440/8883 (61.15%)]\t\tLoss: 1.24356\n",
      "Training Progress: \tEpoch 60 [5760/8883 (64.75%)]\t\tLoss: 1.07503\n",
      "Training Progress: \tEpoch 60 [6080/8883 (68.35%)]\t\tLoss: 0.84221\n",
      "Training Progress: \tEpoch 60 [6400/8883 (71.94%)]\t\tLoss: 0.96747\n",
      "Training Progress: \tEpoch 60 [6720/8883 (75.54%)]\t\tLoss: 0.93788\n",
      "Training Progress: \tEpoch 60 [7040/8883 (79.14%)]\t\tLoss: 0.91548\n",
      "Training Progress: \tEpoch 60 [7360/8883 (82.73%)]\t\tLoss: 0.88995\n",
      "Training Progress: \tEpoch 60 [7680/8883 (86.33%)]\t\tLoss: 0.78132\n",
      "Training Progress: \tEpoch 60 [8000/8883 (89.93%)]\t\tLoss: 0.76472\n",
      "Training Progress: \tEpoch 60 [8320/8883 (93.53%)]\t\tLoss: 0.96218\n",
      "Training Progress: \tEpoch 60 [8640/8883 (97.12%)]\t\tLoss: 1.08652\n",
      "\tTrain loss: 0.01865, Accuracy: 6633/8883 (74.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1414/1692 (83.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 940/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/8883 (0.00%)]\t\tLoss: 0.94070\n",
      "Training Progress: \tEpoch 61 [320/8883 (3.60%)]\t\tLoss: 0.87798\n",
      "Training Progress: \tEpoch 61 [640/8883 (7.19%)]\t\tLoss: 0.84212\n",
      "Training Progress: \tEpoch 61 [960/8883 (10.79%)]\t\tLoss: 0.83366\n",
      "Training Progress: \tEpoch 61 [1280/8883 (14.39%)]\t\tLoss: 1.10139\n",
      "Training Progress: \tEpoch 61 [1600/8883 (17.99%)]\t\tLoss: 0.87249\n",
      "Training Progress: \tEpoch 61 [1920/8883 (21.58%)]\t\tLoss: 0.99337\n",
      "Training Progress: \tEpoch 61 [2240/8883 (25.18%)]\t\tLoss: 0.87072\n",
      "Training Progress: \tEpoch 61 [2560/8883 (28.78%)]\t\tLoss: 1.05113\n",
      "Training Progress: \tEpoch 61 [2880/8883 (32.37%)]\t\tLoss: 0.79358\n",
      "Training Progress: \tEpoch 61 [3200/8883 (35.97%)]\t\tLoss: 0.92586\n",
      "Training Progress: \tEpoch 61 [3520/8883 (39.57%)]\t\tLoss: 0.88582\n",
      "Training Progress: \tEpoch 61 [3840/8883 (43.17%)]\t\tLoss: 1.11753\n",
      "Training Progress: \tEpoch 61 [4160/8883 (46.76%)]\t\tLoss: 0.87839\n",
      "Training Progress: \tEpoch 61 [4480/8883 (50.36%)]\t\tLoss: 0.84960\n",
      "Training Progress: \tEpoch 61 [4800/8883 (53.96%)]\t\tLoss: 0.73123\n",
      "Training Progress: \tEpoch 61 [5120/8883 (57.55%)]\t\tLoss: 0.96922\n",
      "Training Progress: \tEpoch 61 [5440/8883 (61.15%)]\t\tLoss: 1.19264\n",
      "Training Progress: \tEpoch 61 [5760/8883 (64.75%)]\t\tLoss: 1.04716\n",
      "Training Progress: \tEpoch 61 [6080/8883 (68.35%)]\t\tLoss: 0.88892\n",
      "Training Progress: \tEpoch 61 [6400/8883 (71.94%)]\t\tLoss: 0.86057\n",
      "Training Progress: \tEpoch 61 [6720/8883 (75.54%)]\t\tLoss: 0.89581\n",
      "Training Progress: \tEpoch 61 [7040/8883 (79.14%)]\t\tLoss: 0.93616\n",
      "Training Progress: \tEpoch 61 [7360/8883 (82.73%)]\t\tLoss: 1.07507\n",
      "Training Progress: \tEpoch 61 [7680/8883 (86.33%)]\t\tLoss: 0.87373\n",
      "Training Progress: \tEpoch 61 [8000/8883 (89.93%)]\t\tLoss: 0.82792\n",
      "Training Progress: \tEpoch 61 [8320/8883 (93.53%)]\t\tLoss: 1.04082\n",
      "Training Progress: \tEpoch 61 [8640/8883 (97.12%)]\t\tLoss: 0.91478\n",
      "\tTrain loss: 0.01821, Accuracy: 6656/8883 (74.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1452/1692 (85.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 956/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/8883 (0.00%)]\t\tLoss: 0.95787\n",
      "Training Progress: \tEpoch 62 [320/8883 (3.60%)]\t\tLoss: 0.94023\n",
      "Training Progress: \tEpoch 62 [640/8883 (7.19%)]\t\tLoss: 0.75696\n",
      "Training Progress: \tEpoch 62 [960/8883 (10.79%)]\t\tLoss: 0.85940\n",
      "Training Progress: \tEpoch 62 [1280/8883 (14.39%)]\t\tLoss: 0.86715\n",
      "Training Progress: \tEpoch 62 [1600/8883 (17.99%)]\t\tLoss: 0.83069\n",
      "Training Progress: \tEpoch 62 [1920/8883 (21.58%)]\t\tLoss: 0.82666\n",
      "Training Progress: \tEpoch 62 [2240/8883 (25.18%)]\t\tLoss: 0.76291\n",
      "Training Progress: \tEpoch 62 [2560/8883 (28.78%)]\t\tLoss: 0.70306\n",
      "Training Progress: \tEpoch 62 [2880/8883 (32.37%)]\t\tLoss: 0.98317\n",
      "Training Progress: \tEpoch 62 [3200/8883 (35.97%)]\t\tLoss: 0.97548\n",
      "Training Progress: \tEpoch 62 [3520/8883 (39.57%)]\t\tLoss: 0.82083\n",
      "Training Progress: \tEpoch 62 [3840/8883 (43.17%)]\t\tLoss: 1.01124\n",
      "Training Progress: \tEpoch 62 [4160/8883 (46.76%)]\t\tLoss: 0.85353\n",
      "Training Progress: \tEpoch 62 [4480/8883 (50.36%)]\t\tLoss: 0.87044\n",
      "Training Progress: \tEpoch 62 [4800/8883 (53.96%)]\t\tLoss: 0.90692\n",
      "Training Progress: \tEpoch 62 [5120/8883 (57.55%)]\t\tLoss: 0.78945\n",
      "Training Progress: \tEpoch 62 [5440/8883 (61.15%)]\t\tLoss: 1.00501\n",
      "Training Progress: \tEpoch 62 [5760/8883 (64.75%)]\t\tLoss: 0.90503\n",
      "Training Progress: \tEpoch 62 [6080/8883 (68.35%)]\t\tLoss: 1.06317\n",
      "Training Progress: \tEpoch 62 [6400/8883 (71.94%)]\t\tLoss: 1.16702\n",
      "Training Progress: \tEpoch 62 [6720/8883 (75.54%)]\t\tLoss: 0.92925\n",
      "Training Progress: \tEpoch 62 [7040/8883 (79.14%)]\t\tLoss: 0.99053\n",
      "Training Progress: \tEpoch 62 [7360/8883 (82.73%)]\t\tLoss: 0.94578\n",
      "Training Progress: \tEpoch 62 [7680/8883 (86.33%)]\t\tLoss: 0.92539\n",
      "Training Progress: \tEpoch 62 [8000/8883 (89.93%)]\t\tLoss: 1.02672\n",
      "Training Progress: \tEpoch 62 [8320/8883 (93.53%)]\t\tLoss: 0.78039\n",
      "Training Progress: \tEpoch 62 [8640/8883 (97.12%)]\t\tLoss: 1.32759\n",
      "\tTrain loss: 0.01777, Accuracy: 6712/8883 (75.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 952/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/8883 (0.00%)]\t\tLoss: 0.97692\n",
      "Training Progress: \tEpoch 63 [320/8883 (3.60%)]\t\tLoss: 0.94704\n",
      "Training Progress: \tEpoch 63 [640/8883 (7.19%)]\t\tLoss: 0.84629\n",
      "Training Progress: \tEpoch 63 [960/8883 (10.79%)]\t\tLoss: 0.81855\n",
      "Training Progress: \tEpoch 63 [1280/8883 (14.39%)]\t\tLoss: 0.87088\n",
      "Training Progress: \tEpoch 63 [1600/8883 (17.99%)]\t\tLoss: 0.95675\n",
      "Training Progress: \tEpoch 63 [1920/8883 (21.58%)]\t\tLoss: 0.77846\n",
      "Training Progress: \tEpoch 63 [2240/8883 (25.18%)]\t\tLoss: 0.78439\n",
      "Training Progress: \tEpoch 63 [2560/8883 (28.78%)]\t\tLoss: 0.96749\n",
      "Training Progress: \tEpoch 63 [2880/8883 (32.37%)]\t\tLoss: 0.98665\n",
      "Training Progress: \tEpoch 63 [3200/8883 (35.97%)]\t\tLoss: 1.14320\n",
      "Training Progress: \tEpoch 63 [3520/8883 (39.57%)]\t\tLoss: 0.82337\n",
      "Training Progress: \tEpoch 63 [3840/8883 (43.17%)]\t\tLoss: 1.17740\n",
      "Training Progress: \tEpoch 63 [4160/8883 (46.76%)]\t\tLoss: 1.02386\n",
      "Training Progress: \tEpoch 63 [4480/8883 (50.36%)]\t\tLoss: 0.68954\n",
      "Training Progress: \tEpoch 63 [4800/8883 (53.96%)]\t\tLoss: 0.72773\n",
      "Training Progress: \tEpoch 63 [5120/8883 (57.55%)]\t\tLoss: 0.88690\n",
      "Training Progress: \tEpoch 63 [5440/8883 (61.15%)]\t\tLoss: 1.07030\n",
      "Training Progress: \tEpoch 63 [5760/8883 (64.75%)]\t\tLoss: 1.14101\n",
      "Training Progress: \tEpoch 63 [6080/8883 (68.35%)]\t\tLoss: 0.78727\n",
      "Training Progress: \tEpoch 63 [6400/8883 (71.94%)]\t\tLoss: 1.00372\n",
      "Training Progress: \tEpoch 63 [6720/8883 (75.54%)]\t\tLoss: 1.10257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [7040/8883 (79.14%)]\t\tLoss: 0.87234\n",
      "Training Progress: \tEpoch 63 [7360/8883 (82.73%)]\t\tLoss: 1.05863\n",
      "Training Progress: \tEpoch 63 [7680/8883 (86.33%)]\t\tLoss: 0.79660\n",
      "Training Progress: \tEpoch 63 [8000/8883 (89.93%)]\t\tLoss: 1.03415\n",
      "Training Progress: \tEpoch 63 [8320/8883 (93.53%)]\t\tLoss: 1.04925\n",
      "Training Progress: \tEpoch 63 [8640/8883 (97.12%)]\t\tLoss: 1.13805\n",
      "\tTrain loss: 0.01725, Accuracy: 6796/8883 (76.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1465/1692 (86.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 964/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/8883 (0.00%)]\t\tLoss: 0.77372\n",
      "Training Progress: \tEpoch 64 [320/8883 (3.60%)]\t\tLoss: 1.02096\n",
      "Training Progress: \tEpoch 64 [640/8883 (7.19%)]\t\tLoss: 0.78271\n",
      "Training Progress: \tEpoch 64 [960/8883 (10.79%)]\t\tLoss: 0.91287\n",
      "Training Progress: \tEpoch 64 [1280/8883 (14.39%)]\t\tLoss: 0.77763\n",
      "Training Progress: \tEpoch 64 [1600/8883 (17.99%)]\t\tLoss: 0.86287\n",
      "Training Progress: \tEpoch 64 [1920/8883 (21.58%)]\t\tLoss: 1.00973\n",
      "Training Progress: \tEpoch 64 [2240/8883 (25.18%)]\t\tLoss: 0.79831\n",
      "Training Progress: \tEpoch 64 [2560/8883 (28.78%)]\t\tLoss: 0.90461\n",
      "Training Progress: \tEpoch 64 [2880/8883 (32.37%)]\t\tLoss: 0.94806\n",
      "Training Progress: \tEpoch 64 [3200/8883 (35.97%)]\t\tLoss: 0.91294\n",
      "Training Progress: \tEpoch 64 [3520/8883 (39.57%)]\t\tLoss: 0.93569\n",
      "Training Progress: \tEpoch 64 [3840/8883 (43.17%)]\t\tLoss: 0.99852\n",
      "Training Progress: \tEpoch 64 [4160/8883 (46.76%)]\t\tLoss: 1.01253\n",
      "Training Progress: \tEpoch 64 [4480/8883 (50.36%)]\t\tLoss: 0.85290\n",
      "Training Progress: \tEpoch 64 [4800/8883 (53.96%)]\t\tLoss: 0.70787\n",
      "Training Progress: \tEpoch 64 [5120/8883 (57.55%)]\t\tLoss: 1.07943\n",
      "Training Progress: \tEpoch 64 [5440/8883 (61.15%)]\t\tLoss: 1.28975\n",
      "Training Progress: \tEpoch 64 [5760/8883 (64.75%)]\t\tLoss: 1.17471\n",
      "Training Progress: \tEpoch 64 [6080/8883 (68.35%)]\t\tLoss: 0.82757\n",
      "Training Progress: \tEpoch 64 [6400/8883 (71.94%)]\t\tLoss: 0.97774\n",
      "Training Progress: \tEpoch 64 [6720/8883 (75.54%)]\t\tLoss: 0.88480\n",
      "Training Progress: \tEpoch 64 [7040/8883 (79.14%)]\t\tLoss: 0.78472\n",
      "Training Progress: \tEpoch 64 [7360/8883 (82.73%)]\t\tLoss: 1.06875\n",
      "Training Progress: \tEpoch 64 [7680/8883 (86.33%)]\t\tLoss: 0.81206\n",
      "Training Progress: \tEpoch 64 [8000/8883 (89.93%)]\t\tLoss: 0.95107\n",
      "Training Progress: \tEpoch 64 [8320/8883 (93.53%)]\t\tLoss: 0.83610\n",
      "Training Progress: \tEpoch 64 [8640/8883 (97.12%)]\t\tLoss: 1.18807\n",
      "\tTrain loss: 0.01791, Accuracy: 6665/8883 (75.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1427/1692 (84.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 940/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/8883 (0.00%)]\t\tLoss: 0.70297\n",
      "Training Progress: \tEpoch 65 [320/8883 (3.60%)]\t\tLoss: 1.03188\n",
      "Training Progress: \tEpoch 65 [640/8883 (7.19%)]\t\tLoss: 0.76174\n",
      "Training Progress: \tEpoch 65 [960/8883 (10.79%)]\t\tLoss: 0.89859\n",
      "Training Progress: \tEpoch 65 [1280/8883 (14.39%)]\t\tLoss: 0.89470\n",
      "Training Progress: \tEpoch 65 [1600/8883 (17.99%)]\t\tLoss: 0.86969\n",
      "Training Progress: \tEpoch 65 [1920/8883 (21.58%)]\t\tLoss: 0.91574\n",
      "Training Progress: \tEpoch 65 [2240/8883 (25.18%)]\t\tLoss: 0.98022\n",
      "Training Progress: \tEpoch 65 [2560/8883 (28.78%)]\t\tLoss: 0.69435\n",
      "Training Progress: \tEpoch 65 [2880/8883 (32.37%)]\t\tLoss: 1.17296\n",
      "Training Progress: \tEpoch 65 [3200/8883 (35.97%)]\t\tLoss: 1.09714\n",
      "Training Progress: \tEpoch 65 [3520/8883 (39.57%)]\t\tLoss: 1.04912\n",
      "Training Progress: \tEpoch 65 [3840/8883 (43.17%)]\t\tLoss: 0.99614\n",
      "Training Progress: \tEpoch 65 [4160/8883 (46.76%)]\t\tLoss: 1.01330\n",
      "Training Progress: \tEpoch 65 [4480/8883 (50.36%)]\t\tLoss: 0.91842\n",
      "Training Progress: \tEpoch 65 [4800/8883 (53.96%)]\t\tLoss: 0.67045\n",
      "Training Progress: \tEpoch 65 [5120/8883 (57.55%)]\t\tLoss: 1.05413\n",
      "Training Progress: \tEpoch 65 [5440/8883 (61.15%)]\t\tLoss: 0.97820\n",
      "Training Progress: \tEpoch 65 [5760/8883 (64.75%)]\t\tLoss: 0.82034\n",
      "Training Progress: \tEpoch 65 [6080/8883 (68.35%)]\t\tLoss: 0.76125\n",
      "Training Progress: \tEpoch 65 [6400/8883 (71.94%)]\t\tLoss: 1.09682\n",
      "Training Progress: \tEpoch 65 [6720/8883 (75.54%)]\t\tLoss: 0.92051\n",
      "Training Progress: \tEpoch 65 [7040/8883 (79.14%)]\t\tLoss: 0.95516\n",
      "Training Progress: \tEpoch 65 [7360/8883 (82.73%)]\t\tLoss: 0.93107\n",
      "Training Progress: \tEpoch 65 [7680/8883 (86.33%)]\t\tLoss: 0.92992\n",
      "Training Progress: \tEpoch 65 [8000/8883 (89.93%)]\t\tLoss: 0.72293\n",
      "Training Progress: \tEpoch 65 [8320/8883 (93.53%)]\t\tLoss: 0.89048\n",
      "Training Progress: \tEpoch 65 [8640/8883 (97.12%)]\t\tLoss: 0.98760\n",
      "\tTrain loss: 0.01697, Accuracy: 6842/8883 (77.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 976/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/8883 (0.00%)]\t\tLoss: 0.94956\n",
      "Training Progress: \tEpoch 66 [320/8883 (3.60%)]\t\tLoss: 0.94257\n",
      "Training Progress: \tEpoch 66 [640/8883 (7.19%)]\t\tLoss: 0.83586\n",
      "Training Progress: \tEpoch 66 [960/8883 (10.79%)]\t\tLoss: 0.75157\n",
      "Training Progress: \tEpoch 66 [1280/8883 (14.39%)]\t\tLoss: 0.96536\n",
      "Training Progress: \tEpoch 66 [1600/8883 (17.99%)]\t\tLoss: 1.06259\n",
      "Training Progress: \tEpoch 66 [1920/8883 (21.58%)]\t\tLoss: 0.86281\n",
      "Training Progress: \tEpoch 66 [2240/8883 (25.18%)]\t\tLoss: 0.95658\n",
      "Training Progress: \tEpoch 66 [2560/8883 (28.78%)]\t\tLoss: 0.68245\n",
      "Training Progress: \tEpoch 66 [2880/8883 (32.37%)]\t\tLoss: 0.87715\n",
      "Training Progress: \tEpoch 66 [3200/8883 (35.97%)]\t\tLoss: 1.16373\n",
      "Training Progress: \tEpoch 66 [3520/8883 (39.57%)]\t\tLoss: 1.04262\n",
      "Training Progress: \tEpoch 66 [3840/8883 (43.17%)]\t\tLoss: 1.26381\n",
      "Training Progress: \tEpoch 66 [4160/8883 (46.76%)]\t\tLoss: 1.16561\n",
      "Training Progress: \tEpoch 66 [4480/8883 (50.36%)]\t\tLoss: 0.89833\n",
      "Training Progress: \tEpoch 66 [4800/8883 (53.96%)]\t\tLoss: 0.77572\n",
      "Training Progress: \tEpoch 66 [5120/8883 (57.55%)]\t\tLoss: 0.94630\n",
      "Training Progress: \tEpoch 66 [5440/8883 (61.15%)]\t\tLoss: 1.06840\n",
      "Training Progress: \tEpoch 66 [5760/8883 (64.75%)]\t\tLoss: 1.07470\n",
      "Training Progress: \tEpoch 66 [6080/8883 (68.35%)]\t\tLoss: 0.78763\n",
      "Training Progress: \tEpoch 66 [6400/8883 (71.94%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 66 [6720/8883 (75.54%)]\t\tLoss: 0.97550\n",
      "Training Progress: \tEpoch 66 [7040/8883 (79.14%)]\t\tLoss: 0.82117\n",
      "Training Progress: \tEpoch 66 [7360/8883 (82.73%)]\t\tLoss: 0.99672\n",
      "Training Progress: \tEpoch 66 [7680/8883 (86.33%)]\t\tLoss: 0.98160\n",
      "Training Progress: \tEpoch 66 [8000/8883 (89.93%)]\t\tLoss: 0.78569\n",
      "Training Progress: \tEpoch 66 [8320/8883 (93.53%)]\t\tLoss: 0.81227\n",
      "Training Progress: \tEpoch 66 [8640/8883 (97.12%)]\t\tLoss: 1.13559\n",
      "\tTrain loss: 0.01673, Accuracy: 6864/8883 (77.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1498/1692 (88.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 977/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/8883 (0.00%)]\t\tLoss: 0.98608\n",
      "Training Progress: \tEpoch 67 [320/8883 (3.60%)]\t\tLoss: 0.98232\n",
      "Training Progress: \tEpoch 67 [640/8883 (7.19%)]\t\tLoss: 0.95592\n",
      "Training Progress: \tEpoch 67 [960/8883 (10.79%)]\t\tLoss: 0.81142\n",
      "Training Progress: \tEpoch 67 [1280/8883 (14.39%)]\t\tLoss: 0.80656\n",
      "Training Progress: \tEpoch 67 [1600/8883 (17.99%)]\t\tLoss: 1.14102\n",
      "Training Progress: \tEpoch 67 [1920/8883 (21.58%)]\t\tLoss: 0.83201\n",
      "Training Progress: \tEpoch 67 [2240/8883 (25.18%)]\t\tLoss: 0.75894\n",
      "Training Progress: \tEpoch 67 [2560/8883 (28.78%)]\t\tLoss: 1.05115\n",
      "Training Progress: \tEpoch 67 [2880/8883 (32.37%)]\t\tLoss: 1.08208\n",
      "Training Progress: \tEpoch 67 [3200/8883 (35.97%)]\t\tLoss: 1.22278\n",
      "Training Progress: \tEpoch 67 [3520/8883 (39.57%)]\t\tLoss: 0.97141\n",
      "Training Progress: \tEpoch 67 [3840/8883 (43.17%)]\t\tLoss: 1.00977\n",
      "Training Progress: \tEpoch 67 [4160/8883 (46.76%)]\t\tLoss: 0.74851\n",
      "Training Progress: \tEpoch 67 [4480/8883 (50.36%)]\t\tLoss: 0.77722\n",
      "Training Progress: \tEpoch 67 [4800/8883 (53.96%)]\t\tLoss: 0.72158\n",
      "Training Progress: \tEpoch 67 [5120/8883 (57.55%)]\t\tLoss: 0.85762\n",
      "Training Progress: \tEpoch 67 [5440/8883 (61.15%)]\t\tLoss: 1.00607\n",
      "Training Progress: \tEpoch 67 [5760/8883 (64.75%)]\t\tLoss: 0.69877\n",
      "Training Progress: \tEpoch 67 [6080/8883 (68.35%)]\t\tLoss: 0.79400\n",
      "Training Progress: \tEpoch 67 [6400/8883 (71.94%)]\t\tLoss: 1.00994\n",
      "Training Progress: \tEpoch 67 [6720/8883 (75.54%)]\t\tLoss: 0.91464\n",
      "Training Progress: \tEpoch 67 [7040/8883 (79.14%)]\t\tLoss: 0.88502\n",
      "Training Progress: \tEpoch 67 [7360/8883 (82.73%)]\t\tLoss: 1.10924\n",
      "Training Progress: \tEpoch 67 [7680/8883 (86.33%)]\t\tLoss: 0.89663\n",
      "Training Progress: \tEpoch 67 [8000/8883 (89.93%)]\t\tLoss: 0.94089\n",
      "Training Progress: \tEpoch 67 [8320/8883 (93.53%)]\t\tLoss: 0.86892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 67 [8640/8883 (97.12%)]\t\tLoss: 0.89564\n",
      "\tTrain loss: 0.01687, Accuracy: 6815/8883 (76.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1487/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1010/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/8883 (0.00%)]\t\tLoss: 0.79975\n",
      "Training Progress: \tEpoch 68 [320/8883 (3.60%)]\t\tLoss: 1.00475\n",
      "Training Progress: \tEpoch 68 [640/8883 (7.19%)]\t\tLoss: 0.77582\n",
      "Training Progress: \tEpoch 68 [960/8883 (10.79%)]\t\tLoss: 0.70524\n",
      "Training Progress: \tEpoch 68 [1280/8883 (14.39%)]\t\tLoss: 0.98280\n",
      "Training Progress: \tEpoch 68 [1600/8883 (17.99%)]\t\tLoss: 1.14975\n",
      "Training Progress: \tEpoch 68 [1920/8883 (21.58%)]\t\tLoss: 0.87103\n",
      "Training Progress: \tEpoch 68 [2240/8883 (25.18%)]\t\tLoss: 0.85872\n",
      "Training Progress: \tEpoch 68 [2560/8883 (28.78%)]\t\tLoss: 0.76071\n",
      "Training Progress: \tEpoch 68 [2880/8883 (32.37%)]\t\tLoss: 0.93772\n",
      "Training Progress: \tEpoch 68 [3200/8883 (35.97%)]\t\tLoss: 1.15790\n",
      "Training Progress: \tEpoch 68 [3520/8883 (39.57%)]\t\tLoss: 0.91892\n",
      "Training Progress: \tEpoch 68 [3840/8883 (43.17%)]\t\tLoss: 1.07768\n",
      "Training Progress: \tEpoch 68 [4160/8883 (46.76%)]\t\tLoss: 0.71981\n",
      "Training Progress: \tEpoch 68 [4480/8883 (50.36%)]\t\tLoss: 1.07385\n",
      "Training Progress: \tEpoch 68 [4800/8883 (53.96%)]\t\tLoss: 0.85171\n",
      "Training Progress: \tEpoch 68 [5120/8883 (57.55%)]\t\tLoss: 1.01611\n",
      "Training Progress: \tEpoch 68 [5440/8883 (61.15%)]\t\tLoss: 1.29964\n",
      "Training Progress: \tEpoch 68 [5760/8883 (64.75%)]\t\tLoss: 0.96658\n",
      "Training Progress: \tEpoch 68 [6080/8883 (68.35%)]\t\tLoss: 0.75733\n",
      "Training Progress: \tEpoch 68 [6400/8883 (71.94%)]\t\tLoss: 1.02765\n",
      "Training Progress: \tEpoch 68 [6720/8883 (75.54%)]\t\tLoss: 0.96798\n",
      "Training Progress: \tEpoch 68 [7040/8883 (79.14%)]\t\tLoss: 0.96067\n",
      "Training Progress: \tEpoch 68 [7360/8883 (82.73%)]\t\tLoss: 0.86149\n",
      "Training Progress: \tEpoch 68 [7680/8883 (86.33%)]\t\tLoss: 0.75450\n",
      "Training Progress: \tEpoch 68 [8000/8883 (89.93%)]\t\tLoss: 0.86087\n",
      "Training Progress: \tEpoch 68 [8320/8883 (93.53%)]\t\tLoss: 0.73071\n",
      "Training Progress: \tEpoch 68 [8640/8883 (97.12%)]\t\tLoss: 0.86706\n",
      "\tTrain loss: 0.01673, Accuracy: 6888/8883 (77.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1511/1692 (89.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1009/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/8883 (0.00%)]\t\tLoss: 0.80996\n",
      "Training Progress: \tEpoch 69 [320/8883 (3.60%)]\t\tLoss: 0.86789\n",
      "Training Progress: \tEpoch 69 [640/8883 (7.19%)]\t\tLoss: 0.60497\n",
      "Training Progress: \tEpoch 69 [960/8883 (10.79%)]\t\tLoss: 0.90449\n",
      "Training Progress: \tEpoch 69 [1280/8883 (14.39%)]\t\tLoss: 1.00010\n",
      "Training Progress: \tEpoch 69 [1600/8883 (17.99%)]\t\tLoss: 1.05790\n",
      "Training Progress: \tEpoch 69 [1920/8883 (21.58%)]\t\tLoss: 1.02551\n",
      "Training Progress: \tEpoch 69 [2240/8883 (25.18%)]\t\tLoss: 0.99789\n",
      "Training Progress: \tEpoch 69 [2560/8883 (28.78%)]\t\tLoss: 0.76881\n",
      "Training Progress: \tEpoch 69 [2880/8883 (32.37%)]\t\tLoss: 0.58801\n",
      "Training Progress: \tEpoch 69 [3200/8883 (35.97%)]\t\tLoss: 0.99638\n",
      "Training Progress: \tEpoch 69 [3520/8883 (39.57%)]\t\tLoss: 0.92133\n",
      "Training Progress: \tEpoch 69 [3840/8883 (43.17%)]\t\tLoss: 1.04788\n",
      "Training Progress: \tEpoch 69 [4160/8883 (46.76%)]\t\tLoss: 0.81974\n",
      "Training Progress: \tEpoch 69 [4480/8883 (50.36%)]\t\tLoss: 0.69932\n",
      "Training Progress: \tEpoch 69 [4800/8883 (53.96%)]\t\tLoss: 0.73268\n",
      "Training Progress: \tEpoch 69 [5120/8883 (57.55%)]\t\tLoss: 1.03321\n",
      "Training Progress: \tEpoch 69 [5440/8883 (61.15%)]\t\tLoss: 1.03672\n",
      "Training Progress: \tEpoch 69 [5760/8883 (64.75%)]\t\tLoss: 0.83974\n",
      "Training Progress: \tEpoch 69 [6080/8883 (68.35%)]\t\tLoss: 0.84906\n",
      "Training Progress: \tEpoch 69 [6400/8883 (71.94%)]\t\tLoss: 0.86715\n",
      "Training Progress: \tEpoch 69 [6720/8883 (75.54%)]\t\tLoss: 0.70103\n",
      "Training Progress: \tEpoch 69 [7040/8883 (79.14%)]\t\tLoss: 0.63329\n",
      "Training Progress: \tEpoch 69 [7360/8883 (82.73%)]\t\tLoss: 1.00231\n",
      "Training Progress: \tEpoch 69 [7680/8883 (86.33%)]\t\tLoss: 0.78460\n",
      "Training Progress: \tEpoch 69 [8000/8883 (89.93%)]\t\tLoss: 0.75264\n",
      "Training Progress: \tEpoch 69 [8320/8883 (93.53%)]\t\tLoss: 0.73700\n",
      "Training Progress: \tEpoch 69 [8640/8883 (97.12%)]\t\tLoss: 1.13747\n",
      "\tTrain loss: 0.01619, Accuracy: 6926/8883 (77.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1517/1692 (89.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1001/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/8883 (0.00%)]\t\tLoss: 0.93837\n",
      "Training Progress: \tEpoch 70 [320/8883 (3.60%)]\t\tLoss: 0.99353\n",
      "Training Progress: \tEpoch 70 [640/8883 (7.19%)]\t\tLoss: 0.58879\n",
      "Training Progress: \tEpoch 70 [960/8883 (10.79%)]\t\tLoss: 0.88355\n",
      "Training Progress: \tEpoch 70 [1280/8883 (14.39%)]\t\tLoss: 0.82717\n",
      "Training Progress: \tEpoch 70 [1600/8883 (17.99%)]\t\tLoss: 1.01497\n",
      "Training Progress: \tEpoch 70 [1920/8883 (21.58%)]\t\tLoss: 1.07998\n",
      "Training Progress: \tEpoch 70 [2240/8883 (25.18%)]\t\tLoss: 0.93924\n",
      "Training Progress: \tEpoch 70 [2560/8883 (28.78%)]\t\tLoss: 0.83577\n",
      "Training Progress: \tEpoch 70 [2880/8883 (32.37%)]\t\tLoss: 1.05014\n",
      "Training Progress: \tEpoch 70 [3200/8883 (35.97%)]\t\tLoss: 1.28235\n",
      "Training Progress: \tEpoch 70 [3520/8883 (39.57%)]\t\tLoss: 1.03698\n",
      "Training Progress: \tEpoch 70 [3840/8883 (43.17%)]\t\tLoss: 0.96524\n",
      "Training Progress: \tEpoch 70 [4160/8883 (46.76%)]\t\tLoss: 0.84283\n",
      "Training Progress: \tEpoch 70 [4480/8883 (50.36%)]\t\tLoss: 0.79228\n",
      "Training Progress: \tEpoch 70 [4800/8883 (53.96%)]\t\tLoss: 0.63142\n",
      "Training Progress: \tEpoch 70 [5120/8883 (57.55%)]\t\tLoss: 0.89426\n",
      "Training Progress: \tEpoch 70 [5440/8883 (61.15%)]\t\tLoss: 0.87710\n",
      "Training Progress: \tEpoch 70 [5760/8883 (64.75%)]\t\tLoss: 1.04904\n",
      "Training Progress: \tEpoch 70 [6080/8883 (68.35%)]\t\tLoss: 0.56081\n",
      "Training Progress: \tEpoch 70 [6400/8883 (71.94%)]\t\tLoss: 0.98654\n",
      "Training Progress: \tEpoch 70 [6720/8883 (75.54%)]\t\tLoss: 0.98059\n",
      "Training Progress: \tEpoch 70 [7040/8883 (79.14%)]\t\tLoss: 0.76962\n",
      "Training Progress: \tEpoch 70 [7360/8883 (82.73%)]\t\tLoss: 1.06310\n",
      "Training Progress: \tEpoch 70 [7680/8883 (86.33%)]\t\tLoss: 0.86982\n",
      "Training Progress: \tEpoch 70 [8000/8883 (89.93%)]\t\tLoss: 0.80684\n",
      "Training Progress: \tEpoch 70 [8320/8883 (93.53%)]\t\tLoss: 0.84644\n",
      "Training Progress: \tEpoch 70 [8640/8883 (97.12%)]\t\tLoss: 0.73500\n",
      "\tTrain loss: 0.01631, Accuracy: 6932/8883 (78.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1511/1692 (89.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 979/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/8883 (0.00%)]\t\tLoss: 0.80301\n",
      "Training Progress: \tEpoch 71 [320/8883 (3.60%)]\t\tLoss: 1.07343\n",
      "Training Progress: \tEpoch 71 [640/8883 (7.19%)]\t\tLoss: 0.77870\n",
      "Training Progress: \tEpoch 71 [960/8883 (10.79%)]\t\tLoss: 0.91221\n",
      "Training Progress: \tEpoch 71 [1280/8883 (14.39%)]\t\tLoss: 0.91082\n",
      "Training Progress: \tEpoch 71 [1600/8883 (17.99%)]\t\tLoss: 1.09336\n",
      "Training Progress: \tEpoch 71 [1920/8883 (21.58%)]\t\tLoss: 0.91932\n",
      "Training Progress: \tEpoch 71 [2240/8883 (25.18%)]\t\tLoss: 0.82216\n",
      "Training Progress: \tEpoch 71 [2560/8883 (28.78%)]\t\tLoss: 0.77330\n",
      "Training Progress: \tEpoch 71 [2880/8883 (32.37%)]\t\tLoss: 0.90064\n",
      "Training Progress: \tEpoch 71 [3200/8883 (35.97%)]\t\tLoss: 1.03840\n",
      "Training Progress: \tEpoch 71 [3520/8883 (39.57%)]\t\tLoss: 0.81184\n",
      "Training Progress: \tEpoch 71 [3840/8883 (43.17%)]\t\tLoss: 1.10260\n",
      "Training Progress: \tEpoch 71 [4160/8883 (46.76%)]\t\tLoss: 0.95630\n",
      "Training Progress: \tEpoch 71 [4480/8883 (50.36%)]\t\tLoss: 0.72421\n",
      "Training Progress: \tEpoch 71 [4800/8883 (53.96%)]\t\tLoss: 0.51341\n",
      "Training Progress: \tEpoch 71 [5120/8883 (57.55%)]\t\tLoss: 1.12846\n",
      "Training Progress: \tEpoch 71 [5440/8883 (61.15%)]\t\tLoss: 1.09454\n",
      "Training Progress: \tEpoch 71 [5760/8883 (64.75%)]\t\tLoss: 1.09722\n",
      "Training Progress: \tEpoch 71 [6080/8883 (68.35%)]\t\tLoss: 0.67568\n",
      "Training Progress: \tEpoch 71 [6400/8883 (71.94%)]\t\tLoss: 0.86158\n",
      "Training Progress: \tEpoch 71 [6720/8883 (75.54%)]\t\tLoss: 0.82108\n",
      "Training Progress: \tEpoch 71 [7040/8883 (79.14%)]\t\tLoss: 1.07462\n",
      "Training Progress: \tEpoch 71 [7360/8883 (82.73%)]\t\tLoss: 0.76193\n",
      "Training Progress: \tEpoch 71 [7680/8883 (86.33%)]\t\tLoss: 0.94831\n",
      "Training Progress: \tEpoch 71 [8000/8883 (89.93%)]\t\tLoss: 0.78626\n",
      "Training Progress: \tEpoch 71 [8320/8883 (93.53%)]\t\tLoss: 0.80967\n",
      "Training Progress: \tEpoch 71 [8640/8883 (97.12%)]\t\tLoss: 1.08824\n",
      "\tTrain loss: 0.01594, Accuracy: 6921/8883 (77.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/8883 (0.00%)]\t\tLoss: 0.89141\n",
      "Training Progress: \tEpoch 72 [320/8883 (3.60%)]\t\tLoss: 0.79205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 72 [640/8883 (7.19%)]\t\tLoss: 0.70072\n",
      "Training Progress: \tEpoch 72 [960/8883 (10.79%)]\t\tLoss: 0.77812\n",
      "Training Progress: \tEpoch 72 [1280/8883 (14.39%)]\t\tLoss: 0.78584\n",
      "Training Progress: \tEpoch 72 [1600/8883 (17.99%)]\t\tLoss: 1.03410\n",
      "Training Progress: \tEpoch 72 [1920/8883 (21.58%)]\t\tLoss: 0.80853\n",
      "Training Progress: \tEpoch 72 [2240/8883 (25.18%)]\t\tLoss: 0.83785\n",
      "Training Progress: \tEpoch 72 [2560/8883 (28.78%)]\t\tLoss: 0.81676\n",
      "Training Progress: \tEpoch 72 [2880/8883 (32.37%)]\t\tLoss: 0.77198\n",
      "Training Progress: \tEpoch 72 [3200/8883 (35.97%)]\t\tLoss: 1.39335\n",
      "Training Progress: \tEpoch 72 [3520/8883 (39.57%)]\t\tLoss: 0.91811\n",
      "Training Progress: \tEpoch 72 [3840/8883 (43.17%)]\t\tLoss: 0.95432\n",
      "Training Progress: \tEpoch 72 [4160/8883 (46.76%)]\t\tLoss: 0.80596\n",
      "Training Progress: \tEpoch 72 [4480/8883 (50.36%)]\t\tLoss: 0.67499\n",
      "Training Progress: \tEpoch 72 [4800/8883 (53.96%)]\t\tLoss: 0.64423\n",
      "Training Progress: \tEpoch 72 [5120/8883 (57.55%)]\t\tLoss: 0.98661\n",
      "Training Progress: \tEpoch 72 [5440/8883 (61.15%)]\t\tLoss: 0.92363\n",
      "Training Progress: \tEpoch 72 [5760/8883 (64.75%)]\t\tLoss: 1.01442\n",
      "Training Progress: \tEpoch 72 [6080/8883 (68.35%)]\t\tLoss: 0.86445\n",
      "Training Progress: \tEpoch 72 [6400/8883 (71.94%)]\t\tLoss: 1.08213\n",
      "Training Progress: \tEpoch 72 [6720/8883 (75.54%)]\t\tLoss: 0.82288\n",
      "Training Progress: \tEpoch 72 [7040/8883 (79.14%)]\t\tLoss: 0.81708\n",
      "Training Progress: \tEpoch 72 [7360/8883 (82.73%)]\t\tLoss: 0.75758\n",
      "Training Progress: \tEpoch 72 [7680/8883 (86.33%)]\t\tLoss: 0.91231\n",
      "Training Progress: \tEpoch 72 [8000/8883 (89.93%)]\t\tLoss: 0.88968\n",
      "Training Progress: \tEpoch 72 [8320/8883 (93.53%)]\t\tLoss: 0.89724\n",
      "Training Progress: \tEpoch 72 [8640/8883 (97.12%)]\t\tLoss: 0.94540\n",
      "\tTrain loss: 0.01581, Accuracy: 6945/8883 (78.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1525/1692 (90.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1027/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/8883 (0.00%)]\t\tLoss: 0.82217\n",
      "Training Progress: \tEpoch 73 [320/8883 (3.60%)]\t\tLoss: 1.10154\n",
      "Training Progress: \tEpoch 73 [640/8883 (7.19%)]\t\tLoss: 0.94958\n",
      "Training Progress: \tEpoch 73 [960/8883 (10.79%)]\t\tLoss: 1.13985\n",
      "Training Progress: \tEpoch 73 [1280/8883 (14.39%)]\t\tLoss: 0.96937\n",
      "Training Progress: \tEpoch 73 [1600/8883 (17.99%)]\t\tLoss: 0.88306\n",
      "Training Progress: \tEpoch 73 [1920/8883 (21.58%)]\t\tLoss: 0.88302\n",
      "Training Progress: \tEpoch 73 [2240/8883 (25.18%)]\t\tLoss: 0.94368\n",
      "Training Progress: \tEpoch 73 [2560/8883 (28.78%)]\t\tLoss: 0.77571\n",
      "Training Progress: \tEpoch 73 [2880/8883 (32.37%)]\t\tLoss: 0.96636\n",
      "Training Progress: \tEpoch 73 [3200/8883 (35.97%)]\t\tLoss: 1.04600\n",
      "Training Progress: \tEpoch 73 [3520/8883 (39.57%)]\t\tLoss: 0.86630\n",
      "Training Progress: \tEpoch 73 [3840/8883 (43.17%)]\t\tLoss: 0.94635\n",
      "Training Progress: \tEpoch 73 [4160/8883 (46.76%)]\t\tLoss: 0.82398\n",
      "Training Progress: \tEpoch 73 [4480/8883 (50.36%)]\t\tLoss: 0.77360\n",
      "Training Progress: \tEpoch 73 [4800/8883 (53.96%)]\t\tLoss: 0.75697\n",
      "Training Progress: \tEpoch 73 [5120/8883 (57.55%)]\t\tLoss: 0.84067\n",
      "Training Progress: \tEpoch 73 [5440/8883 (61.15%)]\t\tLoss: 1.13261\n",
      "Training Progress: \tEpoch 73 [5760/8883 (64.75%)]\t\tLoss: 1.09395\n",
      "Training Progress: \tEpoch 73 [6080/8883 (68.35%)]\t\tLoss: 0.80107\n",
      "Training Progress: \tEpoch 73 [6400/8883 (71.94%)]\t\tLoss: 0.93268\n",
      "Training Progress: \tEpoch 73 [6720/8883 (75.54%)]\t\tLoss: 0.83686\n",
      "Training Progress: \tEpoch 73 [7040/8883 (79.14%)]\t\tLoss: 0.62310\n",
      "Training Progress: \tEpoch 73 [7360/8883 (82.73%)]\t\tLoss: 0.87630\n",
      "Training Progress: \tEpoch 73 [7680/8883 (86.33%)]\t\tLoss: 0.80381\n",
      "Training Progress: \tEpoch 73 [8000/8883 (89.93%)]\t\tLoss: 0.79467\n",
      "Training Progress: \tEpoch 73 [8320/8883 (93.53%)]\t\tLoss: 0.90173\n",
      "Training Progress: \tEpoch 73 [8640/8883 (97.12%)]\t\tLoss: 1.20379\n",
      "\tTrain loss: 0.01666, Accuracy: 6835/8883 (76.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1483/1692 (87.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1001/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/8883 (0.00%)]\t\tLoss: 0.82437\n",
      "Training Progress: \tEpoch 74 [320/8883 (3.60%)]\t\tLoss: 0.95231\n",
      "Training Progress: \tEpoch 74 [640/8883 (7.19%)]\t\tLoss: 0.62175\n",
      "Training Progress: \tEpoch 74 [960/8883 (10.79%)]\t\tLoss: 0.81629\n",
      "Training Progress: \tEpoch 74 [1280/8883 (14.39%)]\t\tLoss: 1.09991\n",
      "Training Progress: \tEpoch 74 [1600/8883 (17.99%)]\t\tLoss: 1.00690\n",
      "Training Progress: \tEpoch 74 [1920/8883 (21.58%)]\t\tLoss: 1.05614\n",
      "Training Progress: \tEpoch 74 [2240/8883 (25.18%)]\t\tLoss: 1.00446\n",
      "Training Progress: \tEpoch 74 [2560/8883 (28.78%)]\t\tLoss: 0.72065\n",
      "Training Progress: \tEpoch 74 [2880/8883 (32.37%)]\t\tLoss: 0.85035\n",
      "Training Progress: \tEpoch 74 [3200/8883 (35.97%)]\t\tLoss: 1.21561\n",
      "Training Progress: \tEpoch 74 [3520/8883 (39.57%)]\t\tLoss: 1.00624\n",
      "Training Progress: \tEpoch 74 [3840/8883 (43.17%)]\t\tLoss: 0.99952\n",
      "Training Progress: \tEpoch 74 [4160/8883 (46.76%)]\t\tLoss: 0.94712\n",
      "Training Progress: \tEpoch 74 [4480/8883 (50.36%)]\t\tLoss: 0.83744\n",
      "Training Progress: \tEpoch 74 [4800/8883 (53.96%)]\t\tLoss: 0.52468\n",
      "Training Progress: \tEpoch 74 [5120/8883 (57.55%)]\t\tLoss: 1.13292\n",
      "Training Progress: \tEpoch 74 [5440/8883 (61.15%)]\t\tLoss: 1.22736\n",
      "Training Progress: \tEpoch 74 [5760/8883 (64.75%)]\t\tLoss: 0.92668\n",
      "Training Progress: \tEpoch 74 [6080/8883 (68.35%)]\t\tLoss: 0.65862\n",
      "Training Progress: \tEpoch 74 [6400/8883 (71.94%)]\t\tLoss: 0.97283\n",
      "Training Progress: \tEpoch 74 [6720/8883 (75.54%)]\t\tLoss: 0.93753\n",
      "Training Progress: \tEpoch 74 [7040/8883 (79.14%)]\t\tLoss: 1.16317\n",
      "Training Progress: \tEpoch 74 [7360/8883 (82.73%)]\t\tLoss: 0.84645\n",
      "Training Progress: \tEpoch 74 [7680/8883 (86.33%)]\t\tLoss: 0.81651\n",
      "Training Progress: \tEpoch 74 [8000/8883 (89.93%)]\t\tLoss: 0.99308\n",
      "Training Progress: \tEpoch 74 [8320/8883 (93.53%)]\t\tLoss: 0.80051\n",
      "Training Progress: \tEpoch 74 [8640/8883 (97.12%)]\t\tLoss: 0.94012\n",
      "\tTrain loss: 0.01618, Accuracy: 6899/8883 (77.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1526/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1003/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/8883 (0.00%)]\t\tLoss: 0.73105\n",
      "Training Progress: \tEpoch 75 [320/8883 (3.60%)]\t\tLoss: 0.97027\n",
      "Training Progress: \tEpoch 75 [640/8883 (7.19%)]\t\tLoss: 0.87311\n",
      "Training Progress: \tEpoch 75 [960/8883 (10.79%)]\t\tLoss: 0.84924\n",
      "Training Progress: \tEpoch 75 [1280/8883 (14.39%)]\t\tLoss: 0.78261\n",
      "Training Progress: \tEpoch 75 [1600/8883 (17.99%)]\t\tLoss: 0.90467\n",
      "Training Progress: \tEpoch 75 [1920/8883 (21.58%)]\t\tLoss: 0.82659\n",
      "Training Progress: \tEpoch 75 [2240/8883 (25.18%)]\t\tLoss: 0.64769\n",
      "Training Progress: \tEpoch 75 [2560/8883 (28.78%)]\t\tLoss: 0.86342\n",
      "Training Progress: \tEpoch 75 [2880/8883 (32.37%)]\t\tLoss: 0.97781\n",
      "Training Progress: \tEpoch 75 [3200/8883 (35.97%)]\t\tLoss: 1.00657\n",
      "Training Progress: \tEpoch 75 [3520/8883 (39.57%)]\t\tLoss: 1.02074\n",
      "Training Progress: \tEpoch 75 [3840/8883 (43.17%)]\t\tLoss: 0.84910\n",
      "Training Progress: \tEpoch 75 [4160/8883 (46.76%)]\t\tLoss: 1.04081\n",
      "Training Progress: \tEpoch 75 [4480/8883 (50.36%)]\t\tLoss: 0.78347\n",
      "Training Progress: \tEpoch 75 [4800/8883 (53.96%)]\t\tLoss: 0.82668\n",
      "Training Progress: \tEpoch 75 [5120/8883 (57.55%)]\t\tLoss: 0.79283\n",
      "Training Progress: \tEpoch 75 [5440/8883 (61.15%)]\t\tLoss: 1.12811\n",
      "Training Progress: \tEpoch 75 [5760/8883 (64.75%)]\t\tLoss: 0.87332\n",
      "Training Progress: \tEpoch 75 [6080/8883 (68.35%)]\t\tLoss: 0.77011\n",
      "Training Progress: \tEpoch 75 [6400/8883 (71.94%)]\t\tLoss: 0.85548\n",
      "Training Progress: \tEpoch 75 [6720/8883 (75.54%)]\t\tLoss: 0.84726\n",
      "Training Progress: \tEpoch 75 [7040/8883 (79.14%)]\t\tLoss: 0.81403\n",
      "Training Progress: \tEpoch 75 [7360/8883 (82.73%)]\t\tLoss: 0.87357\n",
      "Training Progress: \tEpoch 75 [7680/8883 (86.33%)]\t\tLoss: 0.68183\n",
      "Training Progress: \tEpoch 75 [8000/8883 (89.93%)]\t\tLoss: 0.68379\n",
      "Training Progress: \tEpoch 75 [8320/8883 (93.53%)]\t\tLoss: 0.85557\n",
      "Training Progress: \tEpoch 75 [8640/8883 (97.12%)]\t\tLoss: 0.93442\n",
      "\tTrain loss: 0.01558, Accuracy: 6985/8883 (78.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1545/1692 (91.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1021/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/8883 (0.00%)]\t\tLoss: 0.73820\n",
      "Training Progress: \tEpoch 76 [320/8883 (3.60%)]\t\tLoss: 0.88954\n",
      "Training Progress: \tEpoch 76 [640/8883 (7.19%)]\t\tLoss: 0.85353\n",
      "Training Progress: \tEpoch 76 [960/8883 (10.79%)]\t\tLoss: 0.79335\n",
      "Training Progress: \tEpoch 76 [1280/8883 (14.39%)]\t\tLoss: 1.04357\n",
      "Training Progress: \tEpoch 76 [1600/8883 (17.99%)]\t\tLoss: 0.93887\n",
      "Training Progress: \tEpoch 76 [1920/8883 (21.58%)]\t\tLoss: 0.73405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 76 [2240/8883 (25.18%)]\t\tLoss: 0.78655\n",
      "Training Progress: \tEpoch 76 [2560/8883 (28.78%)]\t\tLoss: 0.84197\n",
      "Training Progress: \tEpoch 76 [2880/8883 (32.37%)]\t\tLoss: 0.86450\n",
      "Training Progress: \tEpoch 76 [3200/8883 (35.97%)]\t\tLoss: 1.08407\n",
      "Training Progress: \tEpoch 76 [3520/8883 (39.57%)]\t\tLoss: 0.77100\n",
      "Training Progress: \tEpoch 76 [3840/8883 (43.17%)]\t\tLoss: 0.95394\n",
      "Training Progress: \tEpoch 76 [4160/8883 (46.76%)]\t\tLoss: 0.89176\n",
      "Training Progress: \tEpoch 76 [4480/8883 (50.36%)]\t\tLoss: 0.67118\n",
      "Training Progress: \tEpoch 76 [4800/8883 (53.96%)]\t\tLoss: 0.66095\n",
      "Training Progress: \tEpoch 76 [5120/8883 (57.55%)]\t\tLoss: 1.29364\n",
      "Training Progress: \tEpoch 76 [5440/8883 (61.15%)]\t\tLoss: 0.94069\n",
      "Training Progress: \tEpoch 76 [5760/8883 (64.75%)]\t\tLoss: 1.07421\n",
      "Training Progress: \tEpoch 76 [6080/8883 (68.35%)]\t\tLoss: 0.75276\n",
      "Training Progress: \tEpoch 76 [6400/8883 (71.94%)]\t\tLoss: 1.11206\n",
      "Training Progress: \tEpoch 76 [6720/8883 (75.54%)]\t\tLoss: 0.89308\n",
      "Training Progress: \tEpoch 76 [7040/8883 (79.14%)]\t\tLoss: 0.62112\n",
      "Training Progress: \tEpoch 76 [7360/8883 (82.73%)]\t\tLoss: 0.97946\n",
      "Training Progress: \tEpoch 76 [7680/8883 (86.33%)]\t\tLoss: 0.76110\n",
      "Training Progress: \tEpoch 76 [8000/8883 (89.93%)]\t\tLoss: 0.97401\n",
      "Training Progress: \tEpoch 76 [8320/8883 (93.53%)]\t\tLoss: 0.61493\n",
      "Training Progress: \tEpoch 76 [8640/8883 (97.12%)]\t\tLoss: 0.91215\n",
      "\tTrain loss: 0.01649, Accuracy: 6764/8883 (76.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1473/1692 (87.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/8883 (0.00%)]\t\tLoss: 0.80667\n",
      "Training Progress: \tEpoch 77 [320/8883 (3.60%)]\t\tLoss: 0.85879\n",
      "Training Progress: \tEpoch 77 [640/8883 (7.19%)]\t\tLoss: 0.84519\n",
      "Training Progress: \tEpoch 77 [960/8883 (10.79%)]\t\tLoss: 0.74117\n",
      "Training Progress: \tEpoch 77 [1280/8883 (14.39%)]\t\tLoss: 1.05909\n",
      "Training Progress: \tEpoch 77 [1600/8883 (17.99%)]\t\tLoss: 0.97150\n",
      "Training Progress: \tEpoch 77 [1920/8883 (21.58%)]\t\tLoss: 0.95662\n",
      "Training Progress: \tEpoch 77 [2240/8883 (25.18%)]\t\tLoss: 0.67013\n",
      "Training Progress: \tEpoch 77 [2560/8883 (28.78%)]\t\tLoss: 0.75249\n",
      "Training Progress: \tEpoch 77 [2880/8883 (32.37%)]\t\tLoss: 0.72651\n",
      "Training Progress: \tEpoch 77 [3200/8883 (35.97%)]\t\tLoss: 1.19489\n",
      "Training Progress: \tEpoch 77 [3520/8883 (39.57%)]\t\tLoss: 0.67593\n",
      "Training Progress: \tEpoch 77 [3840/8883 (43.17%)]\t\tLoss: 0.96961\n",
      "Training Progress: \tEpoch 77 [4160/8883 (46.76%)]\t\tLoss: 0.82892\n",
      "Training Progress: \tEpoch 77 [4480/8883 (50.36%)]\t\tLoss: 0.69001\n",
      "Training Progress: \tEpoch 77 [4800/8883 (53.96%)]\t\tLoss: 0.88739\n",
      "Training Progress: \tEpoch 77 [5120/8883 (57.55%)]\t\tLoss: 0.86138\n",
      "Training Progress: \tEpoch 77 [5440/8883 (61.15%)]\t\tLoss: 0.93304\n",
      "Training Progress: \tEpoch 77 [5760/8883 (64.75%)]\t\tLoss: 0.99745\n",
      "Training Progress: \tEpoch 77 [6080/8883 (68.35%)]\t\tLoss: 0.75240\n",
      "Training Progress: \tEpoch 77 [6400/8883 (71.94%)]\t\tLoss: 1.02954\n",
      "Training Progress: \tEpoch 77 [6720/8883 (75.54%)]\t\tLoss: 0.93058\n",
      "Training Progress: \tEpoch 77 [7040/8883 (79.14%)]\t\tLoss: 1.01409\n",
      "Training Progress: \tEpoch 77 [7360/8883 (82.73%)]\t\tLoss: 0.91801\n",
      "Training Progress: \tEpoch 77 [7680/8883 (86.33%)]\t\tLoss: 0.82743\n",
      "Training Progress: \tEpoch 77 [8000/8883 (89.93%)]\t\tLoss: 0.91905\n",
      "Training Progress: \tEpoch 77 [8320/8883 (93.53%)]\t\tLoss: 0.74092\n",
      "Training Progress: \tEpoch 77 [8640/8883 (97.12%)]\t\tLoss: 1.14331\n",
      "\tTrain loss: 0.01576, Accuracy: 6922/8883 (77.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1020/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/8883 (0.00%)]\t\tLoss: 0.77430\n",
      "Training Progress: \tEpoch 78 [320/8883 (3.60%)]\t\tLoss: 0.75205\n",
      "Training Progress: \tEpoch 78 [640/8883 (7.19%)]\t\tLoss: 0.88410\n",
      "Training Progress: \tEpoch 78 [960/8883 (10.79%)]\t\tLoss: 0.82730\n",
      "Training Progress: \tEpoch 78 [1280/8883 (14.39%)]\t\tLoss: 1.07415\n",
      "Training Progress: \tEpoch 78 [1600/8883 (17.99%)]\t\tLoss: 0.74282\n",
      "Training Progress: \tEpoch 78 [1920/8883 (21.58%)]\t\tLoss: 0.96443\n",
      "Training Progress: \tEpoch 78 [2240/8883 (25.18%)]\t\tLoss: 0.79858\n",
      "Training Progress: \tEpoch 78 [2560/8883 (28.78%)]\t\tLoss: 0.88575\n",
      "Training Progress: \tEpoch 78 [2880/8883 (32.37%)]\t\tLoss: 0.83760\n",
      "Training Progress: \tEpoch 78 [3200/8883 (35.97%)]\t\tLoss: 1.16170\n",
      "Training Progress: \tEpoch 78 [3520/8883 (39.57%)]\t\tLoss: 0.88002\n",
      "Training Progress: \tEpoch 78 [3840/8883 (43.17%)]\t\tLoss: 0.72930\n",
      "Training Progress: \tEpoch 78 [4160/8883 (46.76%)]\t\tLoss: 0.86952\n",
      "Training Progress: \tEpoch 78 [4480/8883 (50.36%)]\t\tLoss: 0.56122\n",
      "Training Progress: \tEpoch 78 [4800/8883 (53.96%)]\t\tLoss: 0.72370\n",
      "Training Progress: \tEpoch 78 [5120/8883 (57.55%)]\t\tLoss: 1.01398\n",
      "Training Progress: \tEpoch 78 [5440/8883 (61.15%)]\t\tLoss: 1.13182\n",
      "Training Progress: \tEpoch 78 [5760/8883 (64.75%)]\t\tLoss: 0.96455\n",
      "Training Progress: \tEpoch 78 [6080/8883 (68.35%)]\t\tLoss: 0.80674\n",
      "Training Progress: \tEpoch 78 [6400/8883 (71.94%)]\t\tLoss: 0.77698\n",
      "Training Progress: \tEpoch 78 [6720/8883 (75.54%)]\t\tLoss: 1.00616\n",
      "Training Progress: \tEpoch 78 [7040/8883 (79.14%)]\t\tLoss: 0.74426\n",
      "Training Progress: \tEpoch 78 [7360/8883 (82.73%)]\t\tLoss: 0.87669\n",
      "Training Progress: \tEpoch 78 [7680/8883 (86.33%)]\t\tLoss: 0.77449\n",
      "Training Progress: \tEpoch 78 [8000/8883 (89.93%)]\t\tLoss: 0.61657\n",
      "Training Progress: \tEpoch 78 [8320/8883 (93.53%)]\t\tLoss: 0.83934\n",
      "Training Progress: \tEpoch 78 [8640/8883 (97.12%)]\t\tLoss: 1.05972\n",
      "\tTrain loss: 0.01503, Accuracy: 7032/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1555/1692 (91.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1036/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/8883 (0.00%)]\t\tLoss: 0.94550\n",
      "Training Progress: \tEpoch 79 [320/8883 (3.60%)]\t\tLoss: 0.80472\n",
      "Training Progress: \tEpoch 79 [640/8883 (7.19%)]\t\tLoss: 0.63671\n",
      "Training Progress: \tEpoch 79 [960/8883 (10.79%)]\t\tLoss: 0.81306\n",
      "Training Progress: \tEpoch 79 [1280/8883 (14.39%)]\t\tLoss: 0.81931\n",
      "Training Progress: \tEpoch 79 [1600/8883 (17.99%)]\t\tLoss: 0.87018\n",
      "Training Progress: \tEpoch 79 [1920/8883 (21.58%)]\t\tLoss: 0.78684\n",
      "Training Progress: \tEpoch 79 [2240/8883 (25.18%)]\t\tLoss: 0.81494\n",
      "Training Progress: \tEpoch 79 [2560/8883 (28.78%)]\t\tLoss: 0.93543\n",
      "Training Progress: \tEpoch 79 [2880/8883 (32.37%)]\t\tLoss: 0.93170\n",
      "Training Progress: \tEpoch 79 [3200/8883 (35.97%)]\t\tLoss: 1.06737\n",
      "Training Progress: \tEpoch 79 [3520/8883 (39.57%)]\t\tLoss: 1.06181\n",
      "Training Progress: \tEpoch 79 [3840/8883 (43.17%)]\t\tLoss: 0.73849\n",
      "Training Progress: \tEpoch 79 [4160/8883 (46.76%)]\t\tLoss: 1.08817\n",
      "Training Progress: \tEpoch 79 [4480/8883 (50.36%)]\t\tLoss: 0.66818\n",
      "Training Progress: \tEpoch 79 [4800/8883 (53.96%)]\t\tLoss: 0.80403\n",
      "Training Progress: \tEpoch 79 [5120/8883 (57.55%)]\t\tLoss: 1.01172\n",
      "Training Progress: \tEpoch 79 [5440/8883 (61.15%)]\t\tLoss: 1.03178\n",
      "Training Progress: \tEpoch 79 [5760/8883 (64.75%)]\t\tLoss: 0.92642\n",
      "Training Progress: \tEpoch 79 [6080/8883 (68.35%)]\t\tLoss: 0.89734\n",
      "Training Progress: \tEpoch 79 [6400/8883 (71.94%)]\t\tLoss: 1.31795\n",
      "Training Progress: \tEpoch 79 [6720/8883 (75.54%)]\t\tLoss: 0.93630\n",
      "Training Progress: \tEpoch 79 [7040/8883 (79.14%)]\t\tLoss: 0.80478\n",
      "Training Progress: \tEpoch 79 [7360/8883 (82.73%)]\t\tLoss: 1.02634\n",
      "Training Progress: \tEpoch 79 [7680/8883 (86.33%)]\t\tLoss: 0.90718\n",
      "Training Progress: \tEpoch 79 [8000/8883 (89.93%)]\t\tLoss: 0.79418\n",
      "Training Progress: \tEpoch 79 [8320/8883 (93.53%)]\t\tLoss: 0.83833\n",
      "Training Progress: \tEpoch 79 [8640/8883 (97.12%)]\t\tLoss: 0.82820\n",
      "\tTrain loss: 0.01476, Accuracy: 7009/8883 (78.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1538/1692 (90.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1007/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/8883 (0.00%)]\t\tLoss: 0.81381\n",
      "Training Progress: \tEpoch 80 [320/8883 (3.60%)]\t\tLoss: 0.70755\n",
      "Training Progress: \tEpoch 80 [640/8883 (7.19%)]\t\tLoss: 0.70384\n",
      "Training Progress: \tEpoch 80 [960/8883 (10.79%)]\t\tLoss: 0.80838\n",
      "Training Progress: \tEpoch 80 [1280/8883 (14.39%)]\t\tLoss: 0.79302\n",
      "Training Progress: \tEpoch 80 [1600/8883 (17.99%)]\t\tLoss: 0.97373\n",
      "Training Progress: \tEpoch 80 [1920/8883 (21.58%)]\t\tLoss: 0.87042\n",
      "Training Progress: \tEpoch 80 [2240/8883 (25.18%)]\t\tLoss: 0.83576\n",
      "Training Progress: \tEpoch 80 [2560/8883 (28.78%)]\t\tLoss: 0.91831\n",
      "Training Progress: \tEpoch 80 [2880/8883 (32.37%)]\t\tLoss: 0.89752\n",
      "Training Progress: \tEpoch 80 [3200/8883 (35.97%)]\t\tLoss: 1.12091\n",
      "Training Progress: \tEpoch 80 [3520/8883 (39.57%)]\t\tLoss: 0.94068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 80 [3840/8883 (43.17%)]\t\tLoss: 0.83020\n",
      "Training Progress: \tEpoch 80 [4160/8883 (46.76%)]\t\tLoss: 0.84024\n",
      "Training Progress: \tEpoch 80 [4480/8883 (50.36%)]\t\tLoss: 0.81577\n",
      "Training Progress: \tEpoch 80 [4800/8883 (53.96%)]\t\tLoss: 0.59885\n",
      "Training Progress: \tEpoch 80 [5120/8883 (57.55%)]\t\tLoss: 0.73086\n",
      "Training Progress: \tEpoch 80 [5440/8883 (61.15%)]\t\tLoss: 1.28647\n",
      "Training Progress: \tEpoch 80 [5760/8883 (64.75%)]\t\tLoss: 0.76126\n",
      "Training Progress: \tEpoch 80 [6080/8883 (68.35%)]\t\tLoss: 0.72798\n",
      "Training Progress: \tEpoch 80 [6400/8883 (71.94%)]\t\tLoss: 0.90989\n",
      "Training Progress: \tEpoch 80 [6720/8883 (75.54%)]\t\tLoss: 0.94405\n",
      "Training Progress: \tEpoch 80 [7040/8883 (79.14%)]\t\tLoss: 0.87524\n",
      "Training Progress: \tEpoch 80 [7360/8883 (82.73%)]\t\tLoss: 0.81640\n",
      "Training Progress: \tEpoch 80 [7680/8883 (86.33%)]\t\tLoss: 1.08980\n",
      "Training Progress: \tEpoch 80 [8000/8883 (89.93%)]\t\tLoss: 0.77748\n",
      "Training Progress: \tEpoch 80 [8320/8883 (93.53%)]\t\tLoss: 0.76192\n",
      "Training Progress: \tEpoch 80 [8640/8883 (97.12%)]\t\tLoss: 1.00902\n",
      "\tTrain loss: 0.01504, Accuracy: 6955/8883 (78.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1526/1692 (90.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/8883 (0.00%)]\t\tLoss: 0.60612\n",
      "Training Progress: \tEpoch 81 [320/8883 (3.60%)]\t\tLoss: 0.77095\n",
      "Training Progress: \tEpoch 81 [640/8883 (7.19%)]\t\tLoss: 0.72923\n",
      "Training Progress: \tEpoch 81 [960/8883 (10.79%)]\t\tLoss: 0.91219\n",
      "Training Progress: \tEpoch 81 [1280/8883 (14.39%)]\t\tLoss: 0.97522\n",
      "Training Progress: \tEpoch 81 [1600/8883 (17.99%)]\t\tLoss: 1.21974\n",
      "Training Progress: \tEpoch 81 [1920/8883 (21.58%)]\t\tLoss: 0.91274\n",
      "Training Progress: \tEpoch 81 [2240/8883 (25.18%)]\t\tLoss: 0.71935\n",
      "Training Progress: \tEpoch 81 [2560/8883 (28.78%)]\t\tLoss: 0.73904\n",
      "Training Progress: \tEpoch 81 [2880/8883 (32.37%)]\t\tLoss: 1.00701\n",
      "Training Progress: \tEpoch 81 [3200/8883 (35.97%)]\t\tLoss: 0.93634\n",
      "Training Progress: \tEpoch 81 [3520/8883 (39.57%)]\t\tLoss: 0.82855\n",
      "Training Progress: \tEpoch 81 [3840/8883 (43.17%)]\t\tLoss: 0.79779\n",
      "Training Progress: \tEpoch 81 [4160/8883 (46.76%)]\t\tLoss: 0.98612\n",
      "Training Progress: \tEpoch 81 [4480/8883 (50.36%)]\t\tLoss: 0.81896\n",
      "Training Progress: \tEpoch 81 [4800/8883 (53.96%)]\t\tLoss: 0.85077\n",
      "Training Progress: \tEpoch 81 [5120/8883 (57.55%)]\t\tLoss: 0.95077\n",
      "Training Progress: \tEpoch 81 [5440/8883 (61.15%)]\t\tLoss: 0.91548\n",
      "Training Progress: \tEpoch 81 [5760/8883 (64.75%)]\t\tLoss: 0.92495\n",
      "Training Progress: \tEpoch 81 [6080/8883 (68.35%)]\t\tLoss: 0.77085\n",
      "Training Progress: \tEpoch 81 [6400/8883 (71.94%)]\t\tLoss: 0.96444\n",
      "Training Progress: \tEpoch 81 [6720/8883 (75.54%)]\t\tLoss: 0.91463\n",
      "Training Progress: \tEpoch 81 [7040/8883 (79.14%)]\t\tLoss: 0.74352\n",
      "Training Progress: \tEpoch 81 [7360/8883 (82.73%)]\t\tLoss: 1.05619\n",
      "Training Progress: \tEpoch 81 [7680/8883 (86.33%)]\t\tLoss: 0.66024\n",
      "Training Progress: \tEpoch 81 [8000/8883 (89.93%)]\t\tLoss: 1.20499\n",
      "Training Progress: \tEpoch 81 [8320/8883 (93.53%)]\t\tLoss: 0.86642\n",
      "Training Progress: \tEpoch 81 [8640/8883 (97.12%)]\t\tLoss: 0.99085\n",
      "\tTrain loss: 0.01551, Accuracy: 6986/8883 (78.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1519/1692 (89.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 999/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/8883 (0.00%)]\t\tLoss: 0.67152\n",
      "Training Progress: \tEpoch 82 [320/8883 (3.60%)]\t\tLoss: 0.76066\n",
      "Training Progress: \tEpoch 82 [640/8883 (7.19%)]\t\tLoss: 0.82577\n",
      "Training Progress: \tEpoch 82 [960/8883 (10.79%)]\t\tLoss: 0.93923\n",
      "Training Progress: \tEpoch 82 [1280/8883 (14.39%)]\t\tLoss: 0.93816\n",
      "Training Progress: \tEpoch 82 [1600/8883 (17.99%)]\t\tLoss: 1.21112\n",
      "Training Progress: \tEpoch 82 [1920/8883 (21.58%)]\t\tLoss: 0.78945\n",
      "Training Progress: \tEpoch 82 [2240/8883 (25.18%)]\t\tLoss: 1.03147\n",
      "Training Progress: \tEpoch 82 [2560/8883 (28.78%)]\t\tLoss: 0.73174\n",
      "Training Progress: \tEpoch 82 [2880/8883 (32.37%)]\t\tLoss: 0.90464\n",
      "Training Progress: \tEpoch 82 [3200/8883 (35.97%)]\t\tLoss: 1.16295\n",
      "Training Progress: \tEpoch 82 [3520/8883 (39.57%)]\t\tLoss: 0.95874\n",
      "Training Progress: \tEpoch 82 [3840/8883 (43.17%)]\t\tLoss: 0.89714\n",
      "Training Progress: \tEpoch 82 [4160/8883 (46.76%)]\t\tLoss: 0.87635\n",
      "Training Progress: \tEpoch 82 [4480/8883 (50.36%)]\t\tLoss: 0.74177\n",
      "Training Progress: \tEpoch 82 [4800/8883 (53.96%)]\t\tLoss: 0.73652\n",
      "Training Progress: \tEpoch 82 [5120/8883 (57.55%)]\t\tLoss: 1.14034\n",
      "Training Progress: \tEpoch 82 [5440/8883 (61.15%)]\t\tLoss: 0.80000\n",
      "Training Progress: \tEpoch 82 [5760/8883 (64.75%)]\t\tLoss: 0.77026\n",
      "Training Progress: \tEpoch 82 [6080/8883 (68.35%)]\t\tLoss: 0.74393\n",
      "Training Progress: \tEpoch 82 [6400/8883 (71.94%)]\t\tLoss: 1.03956\n",
      "Training Progress: \tEpoch 82 [6720/8883 (75.54%)]\t\tLoss: 0.72112\n",
      "Training Progress: \tEpoch 82 [7040/8883 (79.14%)]\t\tLoss: 0.76778\n",
      "Training Progress: \tEpoch 82 [7360/8883 (82.73%)]\t\tLoss: 0.84570\n",
      "Training Progress: \tEpoch 82 [7680/8883 (86.33%)]\t\tLoss: 0.81062\n",
      "Training Progress: \tEpoch 82 [8000/8883 (89.93%)]\t\tLoss: 1.03530\n",
      "Training Progress: \tEpoch 82 [8320/8883 (93.53%)]\t\tLoss: 0.71798\n",
      "Training Progress: \tEpoch 82 [8640/8883 (97.12%)]\t\tLoss: 0.73285\n",
      "\tTrain loss: 0.01484, Accuracy: 7069/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1550/1692 (91.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1038/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/8883 (0.00%)]\t\tLoss: 0.75047\n",
      "Training Progress: \tEpoch 83 [320/8883 (3.60%)]\t\tLoss: 1.01619\n",
      "Training Progress: \tEpoch 83 [640/8883 (7.19%)]\t\tLoss: 0.68321\n",
      "Training Progress: \tEpoch 83 [960/8883 (10.79%)]\t\tLoss: 0.70223\n",
      "Training Progress: \tEpoch 83 [1280/8883 (14.39%)]\t\tLoss: 0.73482\n",
      "Training Progress: \tEpoch 83 [1600/8883 (17.99%)]\t\tLoss: 0.85454\n",
      "Training Progress: \tEpoch 83 [1920/8883 (21.58%)]\t\tLoss: 0.73012\n",
      "Training Progress: \tEpoch 83 [2240/8883 (25.18%)]\t\tLoss: 0.66804\n",
      "Training Progress: \tEpoch 83 [2560/8883 (28.78%)]\t\tLoss: 0.72138\n",
      "Training Progress: \tEpoch 83 [2880/8883 (32.37%)]\t\tLoss: 0.92488\n",
      "Training Progress: \tEpoch 83 [3200/8883 (35.97%)]\t\tLoss: 0.92357\n",
      "Training Progress: \tEpoch 83 [3520/8883 (39.57%)]\t\tLoss: 0.86280\n",
      "Training Progress: \tEpoch 83 [3840/8883 (43.17%)]\t\tLoss: 0.72318\n",
      "Training Progress: \tEpoch 83 [4160/8883 (46.76%)]\t\tLoss: 0.89668\n",
      "Training Progress: \tEpoch 83 [4480/8883 (50.36%)]\t\tLoss: 0.73307\n",
      "Training Progress: \tEpoch 83 [4800/8883 (53.96%)]\t\tLoss: 0.52989\n",
      "Training Progress: \tEpoch 83 [5120/8883 (57.55%)]\t\tLoss: 0.87979\n",
      "Training Progress: \tEpoch 83 [5440/8883 (61.15%)]\t\tLoss: 0.91274\n",
      "Training Progress: \tEpoch 83 [5760/8883 (64.75%)]\t\tLoss: 0.82735\n",
      "Training Progress: \tEpoch 83 [6080/8883 (68.35%)]\t\tLoss: 0.69362\n",
      "Training Progress: \tEpoch 83 [6400/8883 (71.94%)]\t\tLoss: 1.10068\n",
      "Training Progress: \tEpoch 83 [6720/8883 (75.54%)]\t\tLoss: 0.76571\n",
      "Training Progress: \tEpoch 83 [7040/8883 (79.14%)]\t\tLoss: 0.83430\n",
      "Training Progress: \tEpoch 83 [7360/8883 (82.73%)]\t\tLoss: 1.01825\n",
      "Training Progress: \tEpoch 83 [7680/8883 (86.33%)]\t\tLoss: 0.99196\n",
      "Training Progress: \tEpoch 83 [8000/8883 (89.93%)]\t\tLoss: 0.80862\n",
      "Training Progress: \tEpoch 83 [8320/8883 (93.53%)]\t\tLoss: 0.69035\n",
      "Training Progress: \tEpoch 83 [8640/8883 (97.12%)]\t\tLoss: 1.11952\n",
      "\tTrain loss: 0.01500, Accuracy: 7077/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1547/1692 (91.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1047/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/8883 (0.00%)]\t\tLoss: 0.72628\n",
      "Training Progress: \tEpoch 84 [320/8883 (3.60%)]\t\tLoss: 0.87482\n",
      "Training Progress: \tEpoch 84 [640/8883 (7.19%)]\t\tLoss: 0.93046\n",
      "Training Progress: \tEpoch 84 [960/8883 (10.79%)]\t\tLoss: 0.72565\n",
      "Training Progress: \tEpoch 84 [1280/8883 (14.39%)]\t\tLoss: 1.07407\n",
      "Training Progress: \tEpoch 84 [1600/8883 (17.99%)]\t\tLoss: 0.87185\n",
      "Training Progress: \tEpoch 84 [1920/8883 (21.58%)]\t\tLoss: 0.95480\n",
      "Training Progress: \tEpoch 84 [2240/8883 (25.18%)]\t\tLoss: 0.56798\n",
      "Training Progress: \tEpoch 84 [2560/8883 (28.78%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 84 [2880/8883 (32.37%)]\t\tLoss: 0.80323\n",
      "Training Progress: \tEpoch 84 [3200/8883 (35.97%)]\t\tLoss: 1.00800\n",
      "Training Progress: \tEpoch 84 [3520/8883 (39.57%)]\t\tLoss: 0.80469\n",
      "Training Progress: \tEpoch 84 [3840/8883 (43.17%)]\t\tLoss: 0.80362\n",
      "Training Progress: \tEpoch 84 [4160/8883 (46.76%)]\t\tLoss: 0.93896\n",
      "Training Progress: \tEpoch 84 [4480/8883 (50.36%)]\t\tLoss: 0.70318\n",
      "Training Progress: \tEpoch 84 [4800/8883 (53.96%)]\t\tLoss: 0.75663\n",
      "Training Progress: \tEpoch 84 [5120/8883 (57.55%)]\t\tLoss: 0.94685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [5440/8883 (61.15%)]\t\tLoss: 0.87666\n",
      "Training Progress: \tEpoch 84 [5760/8883 (64.75%)]\t\tLoss: 0.92815\n",
      "Training Progress: \tEpoch 84 [6080/8883 (68.35%)]\t\tLoss: 0.72188\n",
      "Training Progress: \tEpoch 84 [6400/8883 (71.94%)]\t\tLoss: 0.72426\n",
      "Training Progress: \tEpoch 84 [6720/8883 (75.54%)]\t\tLoss: 0.86197\n",
      "Training Progress: \tEpoch 84 [7040/8883 (79.14%)]\t\tLoss: 0.79825\n",
      "Training Progress: \tEpoch 84 [7360/8883 (82.73%)]\t\tLoss: 0.78750\n",
      "Training Progress: \tEpoch 84 [7680/8883 (86.33%)]\t\tLoss: 0.80098\n",
      "Training Progress: \tEpoch 84 [8000/8883 (89.93%)]\t\tLoss: 0.60477\n",
      "Training Progress: \tEpoch 84 [8320/8883 (93.53%)]\t\tLoss: 0.75087\n",
      "Training Progress: \tEpoch 84 [8640/8883 (97.12%)]\t\tLoss: 0.91986\n",
      "\tTrain loss: 0.01591, Accuracy: 6938/8883 (78.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1506/1692 (89.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1024/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/8883 (0.00%)]\t\tLoss: 1.04741\n",
      "Training Progress: \tEpoch 85 [320/8883 (3.60%)]\t\tLoss: 1.03362\n",
      "Training Progress: \tEpoch 85 [640/8883 (7.19%)]\t\tLoss: 0.80873\n",
      "Training Progress: \tEpoch 85 [960/8883 (10.79%)]\t\tLoss: 0.74375\n",
      "Training Progress: \tEpoch 85 [1280/8883 (14.39%)]\t\tLoss: 0.81437\n",
      "Training Progress: \tEpoch 85 [1600/8883 (17.99%)]\t\tLoss: 0.79285\n",
      "Training Progress: \tEpoch 85 [1920/8883 (21.58%)]\t\tLoss: 0.91073\n",
      "Training Progress: \tEpoch 85 [2240/8883 (25.18%)]\t\tLoss: 0.69994\n",
      "Training Progress: \tEpoch 85 [2560/8883 (28.78%)]\t\tLoss: 0.60642\n",
      "Training Progress: \tEpoch 85 [2880/8883 (32.37%)]\t\tLoss: 0.62901\n",
      "Training Progress: \tEpoch 85 [3200/8883 (35.97%)]\t\tLoss: 1.01468\n",
      "Training Progress: \tEpoch 85 [3520/8883 (39.57%)]\t\tLoss: 1.08444\n",
      "Training Progress: \tEpoch 85 [3840/8883 (43.17%)]\t\tLoss: 1.02922\n",
      "Training Progress: \tEpoch 85 [4160/8883 (46.76%)]\t\tLoss: 0.96599\n",
      "Training Progress: \tEpoch 85 [4480/8883 (50.36%)]\t\tLoss: 0.79772\n",
      "Training Progress: \tEpoch 85 [4800/8883 (53.96%)]\t\tLoss: 0.66087\n",
      "Training Progress: \tEpoch 85 [5120/8883 (57.55%)]\t\tLoss: 1.02334\n",
      "Training Progress: \tEpoch 85 [5440/8883 (61.15%)]\t\tLoss: 0.78351\n",
      "Training Progress: \tEpoch 85 [5760/8883 (64.75%)]\t\tLoss: 1.00639\n",
      "Training Progress: \tEpoch 85 [6080/8883 (68.35%)]\t\tLoss: 0.71685\n",
      "Training Progress: \tEpoch 85 [6400/8883 (71.94%)]\t\tLoss: 1.01962\n",
      "Training Progress: \tEpoch 85 [6720/8883 (75.54%)]\t\tLoss: 0.80185\n",
      "Training Progress: \tEpoch 85 [7040/8883 (79.14%)]\t\tLoss: 0.57835\n",
      "Training Progress: \tEpoch 85 [7360/8883 (82.73%)]\t\tLoss: 0.91877\n",
      "Training Progress: \tEpoch 85 [7680/8883 (86.33%)]\t\tLoss: 0.85920\n",
      "Training Progress: \tEpoch 85 [8000/8883 (89.93%)]\t\tLoss: 0.63758\n",
      "Training Progress: \tEpoch 85 [8320/8883 (93.53%)]\t\tLoss: 0.86721\n",
      "Training Progress: \tEpoch 85 [8640/8883 (97.12%)]\t\tLoss: 0.98326\n",
      "\tTrain loss: 0.01486, Accuracy: 7061/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1557/1692 (92.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1039/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/8883 (0.00%)]\t\tLoss: 0.75156\n",
      "Training Progress: \tEpoch 86 [320/8883 (3.60%)]\t\tLoss: 0.73396\n",
      "Training Progress: \tEpoch 86 [640/8883 (7.19%)]\t\tLoss: 0.84438\n",
      "Training Progress: \tEpoch 86 [960/8883 (10.79%)]\t\tLoss: 0.75960\n",
      "Training Progress: \tEpoch 86 [1280/8883 (14.39%)]\t\tLoss: 0.71643\n",
      "Training Progress: \tEpoch 86 [1600/8883 (17.99%)]\t\tLoss: 0.98373\n",
      "Training Progress: \tEpoch 86 [1920/8883 (21.58%)]\t\tLoss: 0.68968\n",
      "Training Progress: \tEpoch 86 [2240/8883 (25.18%)]\t\tLoss: 0.77928\n",
      "Training Progress: \tEpoch 86 [2560/8883 (28.78%)]\t\tLoss: 0.88127\n",
      "Training Progress: \tEpoch 86 [2880/8883 (32.37%)]\t\tLoss: 0.76301\n",
      "Training Progress: \tEpoch 86 [3200/8883 (35.97%)]\t\tLoss: 1.00732\n",
      "Training Progress: \tEpoch 86 [3520/8883 (39.57%)]\t\tLoss: 0.84675\n",
      "Training Progress: \tEpoch 86 [3840/8883 (43.17%)]\t\tLoss: 0.90603\n",
      "Training Progress: \tEpoch 86 [4160/8883 (46.76%)]\t\tLoss: 0.84958\n",
      "Training Progress: \tEpoch 86 [4480/8883 (50.36%)]\t\tLoss: 0.65809\n",
      "Training Progress: \tEpoch 86 [4800/8883 (53.96%)]\t\tLoss: 0.67169\n",
      "Training Progress: \tEpoch 86 [5120/8883 (57.55%)]\t\tLoss: 1.21888\n",
      "Training Progress: \tEpoch 86 [5440/8883 (61.15%)]\t\tLoss: 0.98035\n",
      "Training Progress: \tEpoch 86 [5760/8883 (64.75%)]\t\tLoss: 0.75458\n",
      "Training Progress: \tEpoch 86 [6080/8883 (68.35%)]\t\tLoss: 0.92710\n",
      "Training Progress: \tEpoch 86 [6400/8883 (71.94%)]\t\tLoss: 0.98599\n",
      "Training Progress: \tEpoch 86 [6720/8883 (75.54%)]\t\tLoss: 0.80981\n",
      "Training Progress: \tEpoch 86 [7040/8883 (79.14%)]\t\tLoss: 0.83208\n",
      "Training Progress: \tEpoch 86 [7360/8883 (82.73%)]\t\tLoss: 0.86314\n",
      "Training Progress: \tEpoch 86 [7680/8883 (86.33%)]\t\tLoss: 0.75241\n",
      "Training Progress: \tEpoch 86 [8000/8883 (89.93%)]\t\tLoss: 0.82373\n",
      "Training Progress: \tEpoch 86 [8320/8883 (93.53%)]\t\tLoss: 0.81687\n",
      "Training Progress: \tEpoch 86 [8640/8883 (97.12%)]\t\tLoss: 1.05076\n",
      "\tTrain loss: 0.01459, Accuracy: 7063/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1556/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1011/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/8883 (0.00%)]\t\tLoss: 0.52364\n",
      "Training Progress: \tEpoch 87 [320/8883 (3.60%)]\t\tLoss: 0.95817\n",
      "Training Progress: \tEpoch 87 [640/8883 (7.19%)]\t\tLoss: 0.69051\n",
      "Training Progress: \tEpoch 87 [960/8883 (10.79%)]\t\tLoss: 0.61440\n",
      "Training Progress: \tEpoch 87 [1280/8883 (14.39%)]\t\tLoss: 0.94156\n",
      "Training Progress: \tEpoch 87 [1600/8883 (17.99%)]\t\tLoss: 0.87488\n",
      "Training Progress: \tEpoch 87 [1920/8883 (21.58%)]\t\tLoss: 0.84281\n",
      "Training Progress: \tEpoch 87 [2240/8883 (25.18%)]\t\tLoss: 0.74656\n",
      "Training Progress: \tEpoch 87 [2560/8883 (28.78%)]\t\tLoss: 0.74522\n",
      "Training Progress: \tEpoch 87 [2880/8883 (32.37%)]\t\tLoss: 0.85139\n",
      "Training Progress: \tEpoch 87 [3200/8883 (35.97%)]\t\tLoss: 0.85785\n",
      "Training Progress: \tEpoch 87 [3520/8883 (39.57%)]\t\tLoss: 1.01063\n",
      "Training Progress: \tEpoch 87 [3840/8883 (43.17%)]\t\tLoss: 0.84004\n",
      "Training Progress: \tEpoch 87 [4160/8883 (46.76%)]\t\tLoss: 0.94518\n",
      "Training Progress: \tEpoch 87 [4480/8883 (50.36%)]\t\tLoss: 0.59487\n",
      "Training Progress: \tEpoch 87 [4800/8883 (53.96%)]\t\tLoss: 0.72093\n",
      "Training Progress: \tEpoch 87 [5120/8883 (57.55%)]\t\tLoss: 1.19909\n",
      "Training Progress: \tEpoch 87 [5440/8883 (61.15%)]\t\tLoss: 0.99311\n",
      "Training Progress: \tEpoch 87 [5760/8883 (64.75%)]\t\tLoss: 0.97178\n",
      "Training Progress: \tEpoch 87 [6080/8883 (68.35%)]\t\tLoss: 0.88761\n",
      "Training Progress: \tEpoch 87 [6400/8883 (71.94%)]\t\tLoss: 0.84706\n",
      "Training Progress: \tEpoch 87 [6720/8883 (75.54%)]\t\tLoss: 1.26862\n",
      "Training Progress: \tEpoch 87 [7040/8883 (79.14%)]\t\tLoss: 1.03753\n",
      "Training Progress: \tEpoch 87 [7360/8883 (82.73%)]\t\tLoss: 0.78718\n",
      "Training Progress: \tEpoch 87 [7680/8883 (86.33%)]\t\tLoss: 0.84918\n",
      "Training Progress: \tEpoch 87 [8000/8883 (89.93%)]\t\tLoss: 0.85080\n",
      "Training Progress: \tEpoch 87 [8320/8883 (93.53%)]\t\tLoss: 0.61351\n",
      "Training Progress: \tEpoch 87 [8640/8883 (97.12%)]\t\tLoss: 0.82290\n",
      "\tTrain loss: 0.01401, Accuracy: 7090/8883 (79.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1570/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1028/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/8883 (0.00%)]\t\tLoss: 0.85407\n",
      "Training Progress: \tEpoch 88 [320/8883 (3.60%)]\t\tLoss: 0.85116\n",
      "Training Progress: \tEpoch 88 [640/8883 (7.19%)]\t\tLoss: 0.74706\n",
      "Training Progress: \tEpoch 88 [960/8883 (10.79%)]\t\tLoss: 0.68976\n",
      "Training Progress: \tEpoch 88 [1280/8883 (14.39%)]\t\tLoss: 0.87922\n",
      "Training Progress: \tEpoch 88 [1600/8883 (17.99%)]\t\tLoss: 1.01722\n",
      "Training Progress: \tEpoch 88 [1920/8883 (21.58%)]\t\tLoss: 0.66278\n",
      "Training Progress: \tEpoch 88 [2240/8883 (25.18%)]\t\tLoss: 0.72939\n",
      "Training Progress: \tEpoch 88 [2560/8883 (28.78%)]\t\tLoss: 0.83658\n",
      "Training Progress: \tEpoch 88 [2880/8883 (32.37%)]\t\tLoss: 0.80974\n",
      "Training Progress: \tEpoch 88 [3200/8883 (35.97%)]\t\tLoss: 1.19517\n",
      "Training Progress: \tEpoch 88 [3520/8883 (39.57%)]\t\tLoss: 0.95070\n",
      "Training Progress: \tEpoch 88 [3840/8883 (43.17%)]\t\tLoss: 0.86421\n",
      "Training Progress: \tEpoch 88 [4160/8883 (46.76%)]\t\tLoss: 0.83762\n",
      "Training Progress: \tEpoch 88 [4480/8883 (50.36%)]\t\tLoss: 0.61103\n",
      "Training Progress: \tEpoch 88 [4800/8883 (53.96%)]\t\tLoss: 0.66356\n",
      "Training Progress: \tEpoch 88 [5120/8883 (57.55%)]\t\tLoss: 1.16549\n",
      "Training Progress: \tEpoch 88 [5440/8883 (61.15%)]\t\tLoss: 1.20502\n",
      "Training Progress: \tEpoch 88 [5760/8883 (64.75%)]\t\tLoss: 0.87289\n",
      "Training Progress: \tEpoch 88 [6080/8883 (68.35%)]\t\tLoss: 0.63956\n",
      "Training Progress: \tEpoch 88 [6400/8883 (71.94%)]\t\tLoss: 0.93757\n",
      "Training Progress: \tEpoch 88 [6720/8883 (75.54%)]\t\tLoss: 0.69059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 88 [7040/8883 (79.14%)]\t\tLoss: 0.80425\n",
      "Training Progress: \tEpoch 88 [7360/8883 (82.73%)]\t\tLoss: 0.81523\n",
      "Training Progress: \tEpoch 88 [7680/8883 (86.33%)]\t\tLoss: 0.85705\n",
      "Training Progress: \tEpoch 88 [8000/8883 (89.93%)]\t\tLoss: 0.58767\n",
      "Training Progress: \tEpoch 88 [8320/8883 (93.53%)]\t\tLoss: 0.59712\n",
      "Training Progress: \tEpoch 88 [8640/8883 (97.12%)]\t\tLoss: 0.89445\n",
      "\tTrain loss: 0.01423, Accuracy: 7088/8883 (79.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1571/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/8883 (0.00%)]\t\tLoss: 0.85166\n",
      "Training Progress: \tEpoch 89 [320/8883 (3.60%)]\t\tLoss: 0.91783\n",
      "Training Progress: \tEpoch 89 [640/8883 (7.19%)]\t\tLoss: 0.70085\n",
      "Training Progress: \tEpoch 89 [960/8883 (10.79%)]\t\tLoss: 0.62703\n",
      "Training Progress: \tEpoch 89 [1280/8883 (14.39%)]\t\tLoss: 0.74125\n",
      "Training Progress: \tEpoch 89 [1600/8883 (17.99%)]\t\tLoss: 0.78518\n",
      "Training Progress: \tEpoch 89 [1920/8883 (21.58%)]\t\tLoss: 0.85041\n",
      "Training Progress: \tEpoch 89 [2240/8883 (25.18%)]\t\tLoss: 0.91635\n",
      "Training Progress: \tEpoch 89 [2560/8883 (28.78%)]\t\tLoss: 1.03264\n",
      "Training Progress: \tEpoch 89 [2880/8883 (32.37%)]\t\tLoss: 0.84391\n",
      "Training Progress: \tEpoch 89 [3200/8883 (35.97%)]\t\tLoss: 0.92878\n",
      "Training Progress: \tEpoch 89 [3520/8883 (39.57%)]\t\tLoss: 0.84382\n",
      "Training Progress: \tEpoch 89 [3840/8883 (43.17%)]\t\tLoss: 0.84944\n",
      "Training Progress: \tEpoch 89 [4160/8883 (46.76%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 89 [4480/8883 (50.36%)]\t\tLoss: 0.95808\n",
      "Training Progress: \tEpoch 89 [4800/8883 (53.96%)]\t\tLoss: 0.77426\n",
      "Training Progress: \tEpoch 89 [5120/8883 (57.55%)]\t\tLoss: 0.97861\n",
      "Training Progress: \tEpoch 89 [5440/8883 (61.15%)]\t\tLoss: 0.98883\n",
      "Training Progress: \tEpoch 89 [5760/8883 (64.75%)]\t\tLoss: 0.80177\n",
      "Training Progress: \tEpoch 89 [6080/8883 (68.35%)]\t\tLoss: 0.80343\n",
      "Training Progress: \tEpoch 89 [6400/8883 (71.94%)]\t\tLoss: 0.78345\n",
      "Training Progress: \tEpoch 89 [6720/8883 (75.54%)]\t\tLoss: 0.92427\n",
      "Training Progress: \tEpoch 89 [7040/8883 (79.14%)]\t\tLoss: 0.61124\n",
      "Training Progress: \tEpoch 89 [7360/8883 (82.73%)]\t\tLoss: 0.89213\n",
      "Training Progress: \tEpoch 89 [7680/8883 (86.33%)]\t\tLoss: 0.77687\n",
      "Training Progress: \tEpoch 89 [8000/8883 (89.93%)]\t\tLoss: 0.98274\n",
      "Training Progress: \tEpoch 89 [8320/8883 (93.53%)]\t\tLoss: 0.74959\n",
      "Training Progress: \tEpoch 89 [8640/8883 (97.12%)]\t\tLoss: 1.01116\n",
      "\tTrain loss: 0.01439, Accuracy: 7076/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1554/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/8883 (0.00%)]\t\tLoss: 0.72254\n",
      "Training Progress: \tEpoch 90 [320/8883 (3.60%)]\t\tLoss: 0.93935\n",
      "Training Progress: \tEpoch 90 [640/8883 (7.19%)]\t\tLoss: 0.76025\n",
      "Training Progress: \tEpoch 90 [960/8883 (10.79%)]\t\tLoss: 1.11839\n",
      "Training Progress: \tEpoch 90 [1280/8883 (14.39%)]\t\tLoss: 0.96264\n",
      "Training Progress: \tEpoch 90 [1600/8883 (17.99%)]\t\tLoss: 0.84206\n",
      "Training Progress: \tEpoch 90 [1920/8883 (21.58%)]\t\tLoss: 0.80921\n",
      "Training Progress: \tEpoch 90 [2240/8883 (25.18%)]\t\tLoss: 1.13377\n",
      "Training Progress: \tEpoch 90 [2560/8883 (28.78%)]\t\tLoss: 0.84287\n",
      "Training Progress: \tEpoch 90 [2880/8883 (32.37%)]\t\tLoss: 0.57544\n",
      "Training Progress: \tEpoch 90 [3200/8883 (35.97%)]\t\tLoss: 1.15781\n",
      "Training Progress: \tEpoch 90 [3520/8883 (39.57%)]\t\tLoss: 0.88717\n",
      "Training Progress: \tEpoch 90 [3840/8883 (43.17%)]\t\tLoss: 0.73750\n",
      "Training Progress: \tEpoch 90 [4160/8883 (46.76%)]\t\tLoss: 0.92982\n",
      "Training Progress: \tEpoch 90 [4480/8883 (50.36%)]\t\tLoss: 0.56842\n",
      "Training Progress: \tEpoch 90 [4800/8883 (53.96%)]\t\tLoss: 0.84459\n",
      "Training Progress: \tEpoch 90 [5120/8883 (57.55%)]\t\tLoss: 1.07975\n",
      "Training Progress: \tEpoch 90 [5440/8883 (61.15%)]\t\tLoss: 0.88099\n",
      "Training Progress: \tEpoch 90 [5760/8883 (64.75%)]\t\tLoss: 0.91848\n",
      "Training Progress: \tEpoch 90 [6080/8883 (68.35%)]\t\tLoss: 0.73241\n",
      "Training Progress: \tEpoch 90 [6400/8883 (71.94%)]\t\tLoss: 0.87216\n",
      "Training Progress: \tEpoch 90 [6720/8883 (75.54%)]\t\tLoss: 0.86166\n",
      "Training Progress: \tEpoch 90 [7040/8883 (79.14%)]\t\tLoss: 0.76026\n",
      "Training Progress: \tEpoch 90 [7360/8883 (82.73%)]\t\tLoss: 0.64507\n",
      "Training Progress: \tEpoch 90 [7680/8883 (86.33%)]\t\tLoss: 1.15364\n",
      "Training Progress: \tEpoch 90 [8000/8883 (89.93%)]\t\tLoss: 0.69220\n",
      "Training Progress: \tEpoch 90 [8320/8883 (93.53%)]\t\tLoss: 0.64603\n",
      "Training Progress: \tEpoch 90 [8640/8883 (97.12%)]\t\tLoss: 0.98898\n",
      "\tTrain loss: 0.01466, Accuracy: 7081/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1565/1692 (92.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1054/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/8883 (0.00%)]\t\tLoss: 0.74595\n",
      "Training Progress: \tEpoch 91 [320/8883 (3.60%)]\t\tLoss: 0.79625\n",
      "Training Progress: \tEpoch 91 [640/8883 (7.19%)]\t\tLoss: 0.79530\n",
      "Training Progress: \tEpoch 91 [960/8883 (10.79%)]\t\tLoss: 0.78751\n",
      "Training Progress: \tEpoch 91 [1280/8883 (14.39%)]\t\tLoss: 0.79518\n",
      "Training Progress: \tEpoch 91 [1600/8883 (17.99%)]\t\tLoss: 0.88346\n",
      "Training Progress: \tEpoch 91 [1920/8883 (21.58%)]\t\tLoss: 0.74829\n",
      "Training Progress: \tEpoch 91 [2240/8883 (25.18%)]\t\tLoss: 0.76372\n",
      "Training Progress: \tEpoch 91 [2560/8883 (28.78%)]\t\tLoss: 0.71140\n",
      "Training Progress: \tEpoch 91 [2880/8883 (32.37%)]\t\tLoss: 1.00904\n",
      "Training Progress: \tEpoch 91 [3200/8883 (35.97%)]\t\tLoss: 1.16428\n",
      "Training Progress: \tEpoch 91 [3520/8883 (39.57%)]\t\tLoss: 0.87303\n",
      "Training Progress: \tEpoch 91 [3840/8883 (43.17%)]\t\tLoss: 0.93312\n",
      "Training Progress: \tEpoch 91 [4160/8883 (46.76%)]\t\tLoss: 0.86275\n",
      "Training Progress: \tEpoch 91 [4480/8883 (50.36%)]\t\tLoss: 0.65211\n",
      "Training Progress: \tEpoch 91 [4800/8883 (53.96%)]\t\tLoss: 0.75606\n",
      "Training Progress: \tEpoch 91 [5120/8883 (57.55%)]\t\tLoss: 0.91260\n",
      "Training Progress: \tEpoch 91 [5440/8883 (61.15%)]\t\tLoss: 0.97105\n",
      "Training Progress: \tEpoch 91 [5760/8883 (64.75%)]\t\tLoss: 0.77005\n",
      "Training Progress: \tEpoch 91 [6080/8883 (68.35%)]\t\tLoss: 0.74437\n",
      "Training Progress: \tEpoch 91 [6400/8883 (71.94%)]\t\tLoss: 0.94377\n",
      "Training Progress: \tEpoch 91 [6720/8883 (75.54%)]\t\tLoss: 1.06624\n",
      "Training Progress: \tEpoch 91 [7040/8883 (79.14%)]\t\tLoss: 0.79438\n",
      "Training Progress: \tEpoch 91 [7360/8883 (82.73%)]\t\tLoss: 1.11626\n",
      "Training Progress: \tEpoch 91 [7680/8883 (86.33%)]\t\tLoss: 1.03846\n",
      "Training Progress: \tEpoch 91 [8000/8883 (89.93%)]\t\tLoss: 0.90551\n",
      "Training Progress: \tEpoch 91 [8320/8883 (93.53%)]\t\tLoss: 0.70320\n",
      "Training Progress: \tEpoch 91 [8640/8883 (97.12%)]\t\tLoss: 0.70079\n",
      "\tTrain loss: 0.01392, Accuracy: 7146/8883 (80.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1589/1692 (93.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/8883 (0.00%)]\t\tLoss: 0.97304\n",
      "Training Progress: \tEpoch 92 [320/8883 (3.60%)]\t\tLoss: 0.76097\n",
      "Training Progress: \tEpoch 92 [640/8883 (7.19%)]\t\tLoss: 0.91017\n",
      "Training Progress: \tEpoch 92 [960/8883 (10.79%)]\t\tLoss: 0.72276\n",
      "Training Progress: \tEpoch 92 [1280/8883 (14.39%)]\t\tLoss: 0.99605\n",
      "Training Progress: \tEpoch 92 [1600/8883 (17.99%)]\t\tLoss: 0.82704\n",
      "Training Progress: \tEpoch 92 [1920/8883 (21.58%)]\t\tLoss: 0.90253\n",
      "Training Progress: \tEpoch 92 [2240/8883 (25.18%)]\t\tLoss: 0.87921\n",
      "Training Progress: \tEpoch 92 [2560/8883 (28.78%)]\t\tLoss: 0.71348\n",
      "Training Progress: \tEpoch 92 [2880/8883 (32.37%)]\t\tLoss: 0.88544\n",
      "Training Progress: \tEpoch 92 [3200/8883 (35.97%)]\t\tLoss: 0.96839\n",
      "Training Progress: \tEpoch 92 [3520/8883 (39.57%)]\t\tLoss: 0.92034\n",
      "Training Progress: \tEpoch 92 [3840/8883 (43.17%)]\t\tLoss: 0.76028\n",
      "Training Progress: \tEpoch 92 [4160/8883 (46.76%)]\t\tLoss: 0.88592\n",
      "Training Progress: \tEpoch 92 [4480/8883 (50.36%)]\t\tLoss: 1.02569\n",
      "Training Progress: \tEpoch 92 [4800/8883 (53.96%)]\t\tLoss: 0.69836\n",
      "Training Progress: \tEpoch 92 [5120/8883 (57.55%)]\t\tLoss: 0.99549\n",
      "Training Progress: \tEpoch 92 [5440/8883 (61.15%)]\t\tLoss: 1.11654\n",
      "Training Progress: \tEpoch 92 [5760/8883 (64.75%)]\t\tLoss: 0.84491\n",
      "Training Progress: \tEpoch 92 [6080/8883 (68.35%)]\t\tLoss: 0.64409\n",
      "Training Progress: \tEpoch 92 [6400/8883 (71.94%)]\t\tLoss: 0.77327\n",
      "Training Progress: \tEpoch 92 [6720/8883 (75.54%)]\t\tLoss: 0.81029\n",
      "Training Progress: \tEpoch 92 [7040/8883 (79.14%)]\t\tLoss: 0.75054\n",
      "Training Progress: \tEpoch 92 [7360/8883 (82.73%)]\t\tLoss: 1.00778\n",
      "Training Progress: \tEpoch 92 [7680/8883 (86.33%)]\t\tLoss: 0.76183\n",
      "Training Progress: \tEpoch 92 [8000/8883 (89.93%)]\t\tLoss: 0.57776\n",
      "Training Progress: \tEpoch 92 [8320/8883 (93.53%)]\t\tLoss: 0.47277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 92 [8640/8883 (97.12%)]\t\tLoss: 0.78968\n",
      "\tTrain loss: 0.01390, Accuracy: 7123/8883 (80.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1580/1692 (93.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1070/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/8883 (0.00%)]\t\tLoss: 0.82766\n",
      "Training Progress: \tEpoch 93 [320/8883 (3.60%)]\t\tLoss: 0.81375\n",
      "Training Progress: \tEpoch 93 [640/8883 (7.19%)]\t\tLoss: 0.72526\n",
      "Training Progress: \tEpoch 93 [960/8883 (10.79%)]\t\tLoss: 0.83284\n",
      "Training Progress: \tEpoch 93 [1280/8883 (14.39%)]\t\tLoss: 0.79038\n",
      "Training Progress: \tEpoch 93 [1600/8883 (17.99%)]\t\tLoss: 0.89879\n",
      "Training Progress: \tEpoch 93 [1920/8883 (21.58%)]\t\tLoss: 0.73472\n",
      "Training Progress: \tEpoch 93 [2240/8883 (25.18%)]\t\tLoss: 0.81073\n",
      "Training Progress: \tEpoch 93 [2560/8883 (28.78%)]\t\tLoss: 0.64651\n",
      "Training Progress: \tEpoch 93 [2880/8883 (32.37%)]\t\tLoss: 0.80559\n",
      "Training Progress: \tEpoch 93 [3200/8883 (35.97%)]\t\tLoss: 1.01950\n",
      "Training Progress: \tEpoch 93 [3520/8883 (39.57%)]\t\tLoss: 0.92247\n",
      "Training Progress: \tEpoch 93 [3840/8883 (43.17%)]\t\tLoss: 1.00522\n",
      "Training Progress: \tEpoch 93 [4160/8883 (46.76%)]\t\tLoss: 0.94296\n",
      "Training Progress: \tEpoch 93 [4480/8883 (50.36%)]\t\tLoss: 0.72968\n",
      "Training Progress: \tEpoch 93 [4800/8883 (53.96%)]\t\tLoss: 0.60634\n",
      "Training Progress: \tEpoch 93 [5120/8883 (57.55%)]\t\tLoss: 0.89061\n",
      "Training Progress: \tEpoch 93 [5440/8883 (61.15%)]\t\tLoss: 0.86990\n",
      "Training Progress: \tEpoch 93 [5760/8883 (64.75%)]\t\tLoss: 0.87922\n",
      "Training Progress: \tEpoch 93 [6080/8883 (68.35%)]\t\tLoss: 0.77496\n",
      "Training Progress: \tEpoch 93 [6400/8883 (71.94%)]\t\tLoss: 0.91089\n",
      "Training Progress: \tEpoch 93 [6720/8883 (75.54%)]\t\tLoss: 0.77050\n",
      "Training Progress: \tEpoch 93 [7040/8883 (79.14%)]\t\tLoss: 0.89849\n",
      "Training Progress: \tEpoch 93 [7360/8883 (82.73%)]\t\tLoss: 0.83302\n",
      "Training Progress: \tEpoch 93 [7680/8883 (86.33%)]\t\tLoss: 0.65695\n",
      "Training Progress: \tEpoch 93 [8000/8883 (89.93%)]\t\tLoss: 0.69632\n",
      "Training Progress: \tEpoch 93 [8320/8883 (93.53%)]\t\tLoss: 0.74416\n",
      "Training Progress: \tEpoch 93 [8640/8883 (97.12%)]\t\tLoss: 0.69212\n",
      "\tTrain loss: 0.01371, Accuracy: 7133/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1592/1692 (94.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1056/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/8883 (0.00%)]\t\tLoss: 0.72416\n",
      "Training Progress: \tEpoch 94 [320/8883 (3.60%)]\t\tLoss: 0.79201\n",
      "Training Progress: \tEpoch 94 [640/8883 (7.19%)]\t\tLoss: 0.55134\n",
      "Training Progress: \tEpoch 94 [960/8883 (10.79%)]\t\tLoss: 0.74532\n",
      "Training Progress: \tEpoch 94 [1280/8883 (14.39%)]\t\tLoss: 0.88424\n",
      "Training Progress: \tEpoch 94 [1600/8883 (17.99%)]\t\tLoss: 0.98513\n",
      "Training Progress: \tEpoch 94 [1920/8883 (21.58%)]\t\tLoss: 0.62882\n",
      "Training Progress: \tEpoch 94 [2240/8883 (25.18%)]\t\tLoss: 0.64679\n",
      "Training Progress: \tEpoch 94 [2560/8883 (28.78%)]\t\tLoss: 0.58427\n",
      "Training Progress: \tEpoch 94 [2880/8883 (32.37%)]\t\tLoss: 1.10077\n",
      "Training Progress: \tEpoch 94 [3200/8883 (35.97%)]\t\tLoss: 0.88410\n",
      "Training Progress: \tEpoch 94 [3520/8883 (39.57%)]\t\tLoss: 0.88014\n",
      "Training Progress: \tEpoch 94 [3840/8883 (43.17%)]\t\tLoss: 0.83930\n",
      "Training Progress: \tEpoch 94 [4160/8883 (46.76%)]\t\tLoss: 0.84216\n",
      "Training Progress: \tEpoch 94 [4480/8883 (50.36%)]\t\tLoss: 0.52378\n",
      "Training Progress: \tEpoch 94 [4800/8883 (53.96%)]\t\tLoss: 0.58216\n",
      "Training Progress: \tEpoch 94 [5120/8883 (57.55%)]\t\tLoss: 0.78418\n",
      "Training Progress: \tEpoch 94 [5440/8883 (61.15%)]\t\tLoss: 1.08267\n",
      "Training Progress: \tEpoch 94 [5760/8883 (64.75%)]\t\tLoss: 1.15801\n",
      "Training Progress: \tEpoch 94 [6080/8883 (68.35%)]\t\tLoss: 0.79221\n",
      "Training Progress: \tEpoch 94 [6400/8883 (71.94%)]\t\tLoss: 0.98922\n",
      "Training Progress: \tEpoch 94 [6720/8883 (75.54%)]\t\tLoss: 0.98145\n",
      "Training Progress: \tEpoch 94 [7040/8883 (79.14%)]\t\tLoss: 0.78050\n",
      "Training Progress: \tEpoch 94 [7360/8883 (82.73%)]\t\tLoss: 0.73977\n",
      "Training Progress: \tEpoch 94 [7680/8883 (86.33%)]\t\tLoss: 0.84928\n",
      "Training Progress: \tEpoch 94 [8000/8883 (89.93%)]\t\tLoss: 0.76323\n",
      "Training Progress: \tEpoch 94 [8320/8883 (93.53%)]\t\tLoss: 0.81662\n",
      "Training Progress: \tEpoch 94 [8640/8883 (97.12%)]\t\tLoss: 0.72412\n",
      "\tTrain loss: 0.01421, Accuracy: 7100/8883 (79.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1581/1692 (93.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1047/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/8883 (0.00%)]\t\tLoss: 0.90594\n",
      "Training Progress: \tEpoch 95 [320/8883 (3.60%)]\t\tLoss: 0.83792\n",
      "Training Progress: \tEpoch 95 [640/8883 (7.19%)]\t\tLoss: 0.84283\n",
      "Training Progress: \tEpoch 95 [960/8883 (10.79%)]\t\tLoss: 0.72223\n",
      "Training Progress: \tEpoch 95 [1280/8883 (14.39%)]\t\tLoss: 0.92946\n",
      "Training Progress: \tEpoch 95 [1600/8883 (17.99%)]\t\tLoss: 1.05413\n",
      "Training Progress: \tEpoch 95 [1920/8883 (21.58%)]\t\tLoss: 0.57150\n",
      "Training Progress: \tEpoch 95 [2240/8883 (25.18%)]\t\tLoss: 0.68926\n",
      "Training Progress: \tEpoch 95 [2560/8883 (28.78%)]\t\tLoss: 0.82416\n",
      "Training Progress: \tEpoch 95 [2880/8883 (32.37%)]\t\tLoss: 0.89426\n",
      "Training Progress: \tEpoch 95 [3200/8883 (35.97%)]\t\tLoss: 1.04163\n",
      "Training Progress: \tEpoch 95 [3520/8883 (39.57%)]\t\tLoss: 0.91295\n",
      "Training Progress: \tEpoch 95 [3840/8883 (43.17%)]\t\tLoss: 1.00423\n",
      "Training Progress: \tEpoch 95 [4160/8883 (46.76%)]\t\tLoss: 0.96018\n",
      "Training Progress: \tEpoch 95 [4480/8883 (50.36%)]\t\tLoss: 0.76767\n",
      "Training Progress: \tEpoch 95 [4800/8883 (53.96%)]\t\tLoss: 0.82453\n",
      "Training Progress: \tEpoch 95 [5120/8883 (57.55%)]\t\tLoss: 0.64733\n",
      "Training Progress: \tEpoch 95 [5440/8883 (61.15%)]\t\tLoss: 0.86970\n",
      "Training Progress: \tEpoch 95 [5760/8883 (64.75%)]\t\tLoss: 0.68222\n",
      "Training Progress: \tEpoch 95 [6080/8883 (68.35%)]\t\tLoss: 0.61721\n",
      "Training Progress: \tEpoch 95 [6400/8883 (71.94%)]\t\tLoss: 1.04022\n",
      "Training Progress: \tEpoch 95 [6720/8883 (75.54%)]\t\tLoss: 0.90919\n",
      "Training Progress: \tEpoch 95 [7040/8883 (79.14%)]\t\tLoss: 0.70778\n",
      "Training Progress: \tEpoch 95 [7360/8883 (82.73%)]\t\tLoss: 0.73373\n",
      "Training Progress: \tEpoch 95 [7680/8883 (86.33%)]\t\tLoss: 1.08672\n",
      "Training Progress: \tEpoch 95 [8000/8883 (89.93%)]\t\tLoss: 0.93840\n",
      "Training Progress: \tEpoch 95 [8320/8883 (93.53%)]\t\tLoss: 0.87038\n",
      "Training Progress: \tEpoch 95 [8640/8883 (97.12%)]\t\tLoss: 0.82611\n",
      "\tTrain loss: 0.01379, Accuracy: 7125/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1578/1692 (93.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/8883 (0.00%)]\t\tLoss: 0.80230\n",
      "Training Progress: \tEpoch 96 [320/8883 (3.60%)]\t\tLoss: 0.91166\n",
      "Training Progress: \tEpoch 96 [640/8883 (7.19%)]\t\tLoss: 0.91525\n",
      "Training Progress: \tEpoch 96 [960/8883 (10.79%)]\t\tLoss: 0.91490\n",
      "Training Progress: \tEpoch 96 [1280/8883 (14.39%)]\t\tLoss: 0.67368\n",
      "Training Progress: \tEpoch 96 [1600/8883 (17.99%)]\t\tLoss: 0.78307\n",
      "Training Progress: \tEpoch 96 [1920/8883 (21.58%)]\t\tLoss: 0.68016\n",
      "Training Progress: \tEpoch 96 [2240/8883 (25.18%)]\t\tLoss: 0.71235\n",
      "Training Progress: \tEpoch 96 [2560/8883 (28.78%)]\t\tLoss: 0.64843\n",
      "Training Progress: \tEpoch 96 [2880/8883 (32.37%)]\t\tLoss: 0.83997\n",
      "Training Progress: \tEpoch 96 [3200/8883 (35.97%)]\t\tLoss: 0.91356\n",
      "Training Progress: \tEpoch 96 [3520/8883 (39.57%)]\t\tLoss: 0.92340\n",
      "Training Progress: \tEpoch 96 [3840/8883 (43.17%)]\t\tLoss: 0.85364\n",
      "Training Progress: \tEpoch 96 [4160/8883 (46.76%)]\t\tLoss: 0.92383\n",
      "Training Progress: \tEpoch 96 [4480/8883 (50.36%)]\t\tLoss: 0.68787\n",
      "Training Progress: \tEpoch 96 [4800/8883 (53.96%)]\t\tLoss: 0.75177\n",
      "Training Progress: \tEpoch 96 [5120/8883 (57.55%)]\t\tLoss: 1.03435\n",
      "Training Progress: \tEpoch 96 [5440/8883 (61.15%)]\t\tLoss: 0.89114\n",
      "Training Progress: \tEpoch 96 [5760/8883 (64.75%)]\t\tLoss: 0.92001\n",
      "Training Progress: \tEpoch 96 [6080/8883 (68.35%)]\t\tLoss: 0.73261\n",
      "Training Progress: \tEpoch 96 [6400/8883 (71.94%)]\t\tLoss: 0.95541\n",
      "Training Progress: \tEpoch 96 [6720/8883 (75.54%)]\t\tLoss: 0.82219\n",
      "Training Progress: \tEpoch 96 [7040/8883 (79.14%)]\t\tLoss: 0.70868\n",
      "Training Progress: \tEpoch 96 [7360/8883 (82.73%)]\t\tLoss: 0.80012\n",
      "Training Progress: \tEpoch 96 [7680/8883 (86.33%)]\t\tLoss: 0.63615\n",
      "Training Progress: \tEpoch 96 [8000/8883 (89.93%)]\t\tLoss: 0.65914\n",
      "Training Progress: \tEpoch 96 [8320/8883 (93.53%)]\t\tLoss: 0.53180\n",
      "Training Progress: \tEpoch 96 [8640/8883 (97.12%)]\t\tLoss: 1.00361\n",
      "\tTrain loss: 0.01331, Accuracy: 7134/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1591/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1084/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/8883 (0.00%)]\t\tLoss: 0.68962\n",
      "Training Progress: \tEpoch 97 [320/8883 (3.60%)]\t\tLoss: 0.84715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 97 [640/8883 (7.19%)]\t\tLoss: 0.69996\n",
      "Training Progress: \tEpoch 97 [960/8883 (10.79%)]\t\tLoss: 0.69641\n",
      "Training Progress: \tEpoch 97 [1280/8883 (14.39%)]\t\tLoss: 0.80865\n",
      "Training Progress: \tEpoch 97 [1600/8883 (17.99%)]\t\tLoss: 0.93388\n",
      "Training Progress: \tEpoch 97 [1920/8883 (21.58%)]\t\tLoss: 0.56348\n",
      "Training Progress: \tEpoch 97 [2240/8883 (25.18%)]\t\tLoss: 0.83571\n",
      "Training Progress: \tEpoch 97 [2560/8883 (28.78%)]\t\tLoss: 0.65222\n",
      "Training Progress: \tEpoch 97 [2880/8883 (32.37%)]\t\tLoss: 0.76524\n",
      "Training Progress: \tEpoch 97 [3200/8883 (35.97%)]\t\tLoss: 0.81280\n",
      "Training Progress: \tEpoch 97 [3520/8883 (39.57%)]\t\tLoss: 0.86026\n",
      "Training Progress: \tEpoch 97 [3840/8883 (43.17%)]\t\tLoss: 0.90222\n",
      "Training Progress: \tEpoch 97 [4160/8883 (46.76%)]\t\tLoss: 0.91015\n",
      "Training Progress: \tEpoch 97 [4480/8883 (50.36%)]\t\tLoss: 0.59560\n",
      "Training Progress: \tEpoch 97 [4800/8883 (53.96%)]\t\tLoss: 0.81395\n",
      "Training Progress: \tEpoch 97 [5120/8883 (57.55%)]\t\tLoss: 0.98115\n",
      "Training Progress: \tEpoch 97 [5440/8883 (61.15%)]\t\tLoss: 0.89273\n",
      "Training Progress: \tEpoch 97 [5760/8883 (64.75%)]\t\tLoss: 0.94320\n",
      "Training Progress: \tEpoch 97 [6080/8883 (68.35%)]\t\tLoss: 0.83107\n",
      "Training Progress: \tEpoch 97 [6400/8883 (71.94%)]\t\tLoss: 0.88355\n",
      "Training Progress: \tEpoch 97 [6720/8883 (75.54%)]\t\tLoss: 0.81007\n",
      "Training Progress: \tEpoch 97 [7040/8883 (79.14%)]\t\tLoss: 0.72606\n",
      "Training Progress: \tEpoch 97 [7360/8883 (82.73%)]\t\tLoss: 1.18174\n",
      "Training Progress: \tEpoch 97 [7680/8883 (86.33%)]\t\tLoss: 0.90980\n",
      "Training Progress: \tEpoch 97 [8000/8883 (89.93%)]\t\tLoss: 0.63160\n",
      "Training Progress: \tEpoch 97 [8320/8883 (93.53%)]\t\tLoss: 0.67021\n",
      "Training Progress: \tEpoch 97 [8640/8883 (97.12%)]\t\tLoss: 0.89813\n",
      "\tTrain loss: 0.01321, Accuracy: 7141/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1593/1692 (94.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1013/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/8883 (0.00%)]\t\tLoss: 0.73237\n",
      "Training Progress: \tEpoch 98 [320/8883 (3.60%)]\t\tLoss: 0.87666\n",
      "Training Progress: \tEpoch 98 [640/8883 (7.19%)]\t\tLoss: 0.94521\n",
      "Training Progress: \tEpoch 98 [960/8883 (10.79%)]\t\tLoss: 0.76091\n",
      "Training Progress: \tEpoch 98 [1280/8883 (14.39%)]\t\tLoss: 0.74228\n",
      "Training Progress: \tEpoch 98 [1600/8883 (17.99%)]\t\tLoss: 0.81814\n",
      "Training Progress: \tEpoch 98 [1920/8883 (21.58%)]\t\tLoss: 0.66006\n",
      "Training Progress: \tEpoch 98 [2240/8883 (25.18%)]\t\tLoss: 0.85191\n",
      "Training Progress: \tEpoch 98 [2560/8883 (28.78%)]\t\tLoss: 0.75927\n",
      "Training Progress: \tEpoch 98 [2880/8883 (32.37%)]\t\tLoss: 0.81427\n",
      "Training Progress: \tEpoch 98 [3200/8883 (35.97%)]\t\tLoss: 0.90738\n",
      "Training Progress: \tEpoch 98 [3520/8883 (39.57%)]\t\tLoss: 0.94177\n",
      "Training Progress: \tEpoch 98 [3840/8883 (43.17%)]\t\tLoss: 0.81074\n",
      "Training Progress: \tEpoch 98 [4160/8883 (46.76%)]\t\tLoss: 0.84297\n",
      "Training Progress: \tEpoch 98 [4480/8883 (50.36%)]\t\tLoss: 0.81379\n",
      "Training Progress: \tEpoch 98 [4800/8883 (53.96%)]\t\tLoss: 0.54911\n",
      "Training Progress: \tEpoch 98 [5120/8883 (57.55%)]\t\tLoss: 0.94750\n",
      "Training Progress: \tEpoch 98 [5440/8883 (61.15%)]\t\tLoss: 0.89159\n",
      "Training Progress: \tEpoch 98 [5760/8883 (64.75%)]\t\tLoss: 0.78842\n",
      "Training Progress: \tEpoch 98 [6080/8883 (68.35%)]\t\tLoss: 0.70272\n",
      "Training Progress: \tEpoch 98 [6400/8883 (71.94%)]\t\tLoss: 0.91803\n",
      "Training Progress: \tEpoch 98 [6720/8883 (75.54%)]\t\tLoss: 0.67706\n",
      "Training Progress: \tEpoch 98 [7040/8883 (79.14%)]\t\tLoss: 0.86759\n",
      "Training Progress: \tEpoch 98 [7360/8883 (82.73%)]\t\tLoss: 0.60842\n",
      "Training Progress: \tEpoch 98 [7680/8883 (86.33%)]\t\tLoss: 0.88827\n",
      "Training Progress: \tEpoch 98 [8000/8883 (89.93%)]\t\tLoss: 0.94576\n",
      "Training Progress: \tEpoch 98 [8320/8883 (93.53%)]\t\tLoss: 0.54980\n",
      "Training Progress: \tEpoch 98 [8640/8883 (97.12%)]\t\tLoss: 0.88148\n",
      "\tTrain loss: 0.01373, Accuracy: 7100/8883 (79.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1577/1692 (93.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1006/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/8883 (0.00%)]\t\tLoss: 0.63520\n",
      "Training Progress: \tEpoch 99 [320/8883 (3.60%)]\t\tLoss: 0.66537\n",
      "Training Progress: \tEpoch 99 [640/8883 (7.19%)]\t\tLoss: 0.71157\n",
      "Training Progress: \tEpoch 99 [960/8883 (10.79%)]\t\tLoss: 0.73486\n",
      "Training Progress: \tEpoch 99 [1280/8883 (14.39%)]\t\tLoss: 1.04995\n",
      "Training Progress: \tEpoch 99 [1600/8883 (17.99%)]\t\tLoss: 0.81983\n",
      "Training Progress: \tEpoch 99 [1920/8883 (21.58%)]\t\tLoss: 0.61636\n",
      "Training Progress: \tEpoch 99 [2240/8883 (25.18%)]\t\tLoss: 0.70001\n",
      "Training Progress: \tEpoch 99 [2560/8883 (28.78%)]\t\tLoss: 0.69341\n",
      "Training Progress: \tEpoch 99 [2880/8883 (32.37%)]\t\tLoss: 0.62036\n",
      "Training Progress: \tEpoch 99 [3200/8883 (35.97%)]\t\tLoss: 1.23284\n",
      "Training Progress: \tEpoch 99 [3520/8883 (39.57%)]\t\tLoss: 0.77698\n",
      "Training Progress: \tEpoch 99 [3840/8883 (43.17%)]\t\tLoss: 1.05636\n",
      "Training Progress: \tEpoch 99 [4160/8883 (46.76%)]\t\tLoss: 0.73556\n",
      "Training Progress: \tEpoch 99 [4480/8883 (50.36%)]\t\tLoss: 0.70666\n",
      "Training Progress: \tEpoch 99 [4800/8883 (53.96%)]\t\tLoss: 0.60504\n",
      "Training Progress: \tEpoch 99 [5120/8883 (57.55%)]\t\tLoss: 0.68446\n",
      "Training Progress: \tEpoch 99 [5440/8883 (61.15%)]\t\tLoss: 1.43208\n",
      "Training Progress: \tEpoch 99 [5760/8883 (64.75%)]\t\tLoss: 0.90136\n",
      "Training Progress: \tEpoch 99 [6080/8883 (68.35%)]\t\tLoss: 0.77839\n",
      "Training Progress: \tEpoch 99 [6400/8883 (71.94%)]\t\tLoss: 0.94529\n",
      "Training Progress: \tEpoch 99 [6720/8883 (75.54%)]\t\tLoss: 0.96205\n",
      "Training Progress: \tEpoch 99 [7040/8883 (79.14%)]\t\tLoss: 0.76127\n",
      "Training Progress: \tEpoch 99 [7360/8883 (82.73%)]\t\tLoss: 0.80731\n",
      "Training Progress: \tEpoch 99 [7680/8883 (86.33%)]\t\tLoss: 0.74483\n",
      "Training Progress: \tEpoch 99 [8000/8883 (89.93%)]\t\tLoss: 0.93131\n",
      "Training Progress: \tEpoch 99 [8320/8883 (93.53%)]\t\tLoss: 0.81810\n",
      "Training Progress: \tEpoch 99 [8640/8883 (97.12%)]\t\tLoss: 0.83685\n",
      "\tTrain loss: 0.01354, Accuracy: 7145/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1589/1692 (93.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1019/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/8883 (0.00%)]\t\tLoss: 0.77609\n",
      "Training Progress: \tEpoch 100 [320/8883 (3.60%)]\t\tLoss: 0.68480\n",
      "Training Progress: \tEpoch 100 [640/8883 (7.19%)]\t\tLoss: 0.69316\n",
      "Training Progress: \tEpoch 100 [960/8883 (10.79%)]\t\tLoss: 0.66618\n",
      "Training Progress: \tEpoch 100 [1280/8883 (14.39%)]\t\tLoss: 0.83809\n",
      "Training Progress: \tEpoch 100 [1600/8883 (17.99%)]\t\tLoss: 0.81602\n",
      "Training Progress: \tEpoch 100 [1920/8883 (21.58%)]\t\tLoss: 0.73004\n",
      "Training Progress: \tEpoch 100 [2240/8883 (25.18%)]\t\tLoss: 0.76838\n",
      "Training Progress: \tEpoch 100 [2560/8883 (28.78%)]\t\tLoss: 0.64440\n",
      "Training Progress: \tEpoch 100 [2880/8883 (32.37%)]\t\tLoss: 1.00294\n",
      "Training Progress: \tEpoch 100 [3200/8883 (35.97%)]\t\tLoss: 1.09831\n",
      "Training Progress: \tEpoch 100 [3520/8883 (39.57%)]\t\tLoss: 1.14802\n",
      "Training Progress: \tEpoch 100 [3840/8883 (43.17%)]\t\tLoss: 0.75059\n",
      "Training Progress: \tEpoch 100 [4160/8883 (46.76%)]\t\tLoss: 0.94516\n",
      "Training Progress: \tEpoch 100 [4480/8883 (50.36%)]\t\tLoss: 0.59524\n",
      "Training Progress: \tEpoch 100 [4800/8883 (53.96%)]\t\tLoss: 0.69757\n",
      "Training Progress: \tEpoch 100 [5120/8883 (57.55%)]\t\tLoss: 0.88405\n",
      "Training Progress: \tEpoch 100 [5440/8883 (61.15%)]\t\tLoss: 0.82191\n",
      "Training Progress: \tEpoch 100 [5760/8883 (64.75%)]\t\tLoss: 0.82575\n",
      "Training Progress: \tEpoch 100 [6080/8883 (68.35%)]\t\tLoss: 0.76575\n",
      "Training Progress: \tEpoch 100 [6400/8883 (71.94%)]\t\tLoss: 1.02573\n",
      "Training Progress: \tEpoch 100 [6720/8883 (75.54%)]\t\tLoss: 0.83767\n",
      "Training Progress: \tEpoch 100 [7040/8883 (79.14%)]\t\tLoss: 0.76941\n",
      "Training Progress: \tEpoch 100 [7360/8883 (82.73%)]\t\tLoss: 0.81297\n",
      "Training Progress: \tEpoch 100 [7680/8883 (86.33%)]\t\tLoss: 0.76142\n",
      "Training Progress: \tEpoch 100 [8000/8883 (89.93%)]\t\tLoss: 0.83773\n",
      "Training Progress: \tEpoch 100 [8320/8883 (93.53%)]\t\tLoss: 0.79041\n",
      "Training Progress: \tEpoch 100 [8640/8883 (97.12%)]\t\tLoss: 0.78948\n",
      "\tTrain loss: 0.01345, Accuracy: 7144/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1601/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1024/1772 (57.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9462174940898345\n",
      "Best test accuracy:\n",
      "0.6258465011286681\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_lstm, cnn_lstm_optimizer, data_loaders_wgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d4020",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbfcf7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/8883 (0.00%)]\t\tLoss: 1.49631\n",
      "Training Progress: \tEpoch 1 [320/8883 (3.60%)]\t\tLoss: 1.34690\n",
      "Training Progress: \tEpoch 1 [640/8883 (7.19%)]\t\tLoss: 1.40287\n",
      "Training Progress: \tEpoch 1 [960/8883 (10.79%)]\t\tLoss: 1.48454\n",
      "Training Progress: \tEpoch 1 [1280/8883 (14.39%)]\t\tLoss: 1.41889\n",
      "Training Progress: \tEpoch 1 [1600/8883 (17.99%)]\t\tLoss: 1.45936\n",
      "Training Progress: \tEpoch 1 [1920/8883 (21.58%)]\t\tLoss: 1.42860\n",
      "Training Progress: \tEpoch 1 [2240/8883 (25.18%)]\t\tLoss: 1.42542\n",
      "Training Progress: \tEpoch 1 [2560/8883 (28.78%)]\t\tLoss: 1.39264\n",
      "Training Progress: \tEpoch 1 [2880/8883 (32.37%)]\t\tLoss: 1.40555\n",
      "Training Progress: \tEpoch 1 [3200/8883 (35.97%)]\t\tLoss: 1.41193\n",
      "Training Progress: \tEpoch 1 [3520/8883 (39.57%)]\t\tLoss: 1.40208\n",
      "Training Progress: \tEpoch 1 [3840/8883 (43.17%)]\t\tLoss: 1.36866\n",
      "Training Progress: \tEpoch 1 [4160/8883 (46.76%)]\t\tLoss: 1.35985\n",
      "Training Progress: \tEpoch 1 [4480/8883 (50.36%)]\t\tLoss: 1.39293\n",
      "Training Progress: \tEpoch 1 [4800/8883 (53.96%)]\t\tLoss: 1.39763\n",
      "Training Progress: \tEpoch 1 [5120/8883 (57.55%)]\t\tLoss: 1.43188\n",
      "Training Progress: \tEpoch 1 [5440/8883 (61.15%)]\t\tLoss: 1.37043\n",
      "Training Progress: \tEpoch 1 [5760/8883 (64.75%)]\t\tLoss: 1.32548\n",
      "Training Progress: \tEpoch 1 [6080/8883 (68.35%)]\t\tLoss: 1.33432\n",
      "Training Progress: \tEpoch 1 [6400/8883 (71.94%)]\t\tLoss: 1.35431\n",
      "Training Progress: \tEpoch 1 [6720/8883 (75.54%)]\t\tLoss: 1.36289\n",
      "Training Progress: \tEpoch 1 [7040/8883 (79.14%)]\t\tLoss: 1.40656\n",
      "Training Progress: \tEpoch 1 [7360/8883 (82.73%)]\t\tLoss: 1.41254\n",
      "Training Progress: \tEpoch 1 [7680/8883 (86.33%)]\t\tLoss: 1.33473\n",
      "Training Progress: \tEpoch 1 [8000/8883 (89.93%)]\t\tLoss: 1.41386\n",
      "Training Progress: \tEpoch 1 [8320/8883 (93.53%)]\t\tLoss: 1.34551\n",
      "Training Progress: \tEpoch 1 [8640/8883 (97.12%)]\t\tLoss: 1.31609\n",
      "\tTrain loss: 0.04146, Accuracy: 3054/8883 (34.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 594/1692 (35.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 640/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/8883 (0.00%)]\t\tLoss: 1.32806\n",
      "Training Progress: \tEpoch 2 [320/8883 (3.60%)]\t\tLoss: 1.32503\n",
      "Training Progress: \tEpoch 2 [640/8883 (7.19%)]\t\tLoss: 1.35158\n",
      "Training Progress: \tEpoch 2 [960/8883 (10.79%)]\t\tLoss: 1.34246\n",
      "Training Progress: \tEpoch 2 [1280/8883 (14.39%)]\t\tLoss: 1.33827\n",
      "Training Progress: \tEpoch 2 [1600/8883 (17.99%)]\t\tLoss: 1.42256\n",
      "Training Progress: \tEpoch 2 [1920/8883 (21.58%)]\t\tLoss: 1.49407\n",
      "Training Progress: \tEpoch 2 [2240/8883 (25.18%)]\t\tLoss: 1.42280\n",
      "Training Progress: \tEpoch 2 [2560/8883 (28.78%)]\t\tLoss: 1.45803\n",
      "Training Progress: \tEpoch 2 [2880/8883 (32.37%)]\t\tLoss: 1.40141\n",
      "Training Progress: \tEpoch 2 [3200/8883 (35.97%)]\t\tLoss: 1.30992\n",
      "Training Progress: \tEpoch 2 [3520/8883 (39.57%)]\t\tLoss: 1.39089\n",
      "Training Progress: \tEpoch 2 [3840/8883 (43.17%)]\t\tLoss: 1.40538\n",
      "Training Progress: \tEpoch 2 [4160/8883 (46.76%)]\t\tLoss: 1.32705\n",
      "Training Progress: \tEpoch 2 [4480/8883 (50.36%)]\t\tLoss: 1.28600\n",
      "Training Progress: \tEpoch 2 [4800/8883 (53.96%)]\t\tLoss: 1.27569\n",
      "Training Progress: \tEpoch 2 [5120/8883 (57.55%)]\t\tLoss: 1.32077\n",
      "Training Progress: \tEpoch 2 [5440/8883 (61.15%)]\t\tLoss: 1.39753\n",
      "Training Progress: \tEpoch 2 [5760/8883 (64.75%)]\t\tLoss: 1.31377\n",
      "Training Progress: \tEpoch 2 [6080/8883 (68.35%)]\t\tLoss: 1.20202\n",
      "Training Progress: \tEpoch 2 [6400/8883 (71.94%)]\t\tLoss: 1.22179\n",
      "Training Progress: \tEpoch 2 [6720/8883 (75.54%)]\t\tLoss: 1.28120\n",
      "Training Progress: \tEpoch 2 [7040/8883 (79.14%)]\t\tLoss: 1.39728\n",
      "Training Progress: \tEpoch 2 [7360/8883 (82.73%)]\t\tLoss: 1.41691\n",
      "Training Progress: \tEpoch 2 [7680/8883 (86.33%)]\t\tLoss: 1.25737\n",
      "Training Progress: \tEpoch 2 [8000/8883 (89.93%)]\t\tLoss: 1.27129\n",
      "Training Progress: \tEpoch 2 [8320/8883 (93.53%)]\t\tLoss: 1.22589\n",
      "Training Progress: \tEpoch 2 [8640/8883 (97.12%)]\t\tLoss: 1.21459\n",
      "\tTrain loss: 0.03961, Accuracy: 3486/8883 (39.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 676/1692 (39.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/8883 (0.00%)]\t\tLoss: 1.31730\n",
      "Training Progress: \tEpoch 3 [320/8883 (3.60%)]\t\tLoss: 1.22416\n",
      "Training Progress: \tEpoch 3 [640/8883 (7.19%)]\t\tLoss: 1.33229\n",
      "Training Progress: \tEpoch 3 [960/8883 (10.79%)]\t\tLoss: 1.29845\n",
      "Training Progress: \tEpoch 3 [1280/8883 (14.39%)]\t\tLoss: 1.31495\n",
      "Training Progress: \tEpoch 3 [1600/8883 (17.99%)]\t\tLoss: 1.38661\n",
      "Training Progress: \tEpoch 3 [1920/8883 (21.58%)]\t\tLoss: 1.33959\n",
      "Training Progress: \tEpoch 3 [2240/8883 (25.18%)]\t\tLoss: 1.25819\n",
      "Training Progress: \tEpoch 3 [2560/8883 (28.78%)]\t\tLoss: 1.33100\n",
      "Training Progress: \tEpoch 3 [2880/8883 (32.37%)]\t\tLoss: 1.28073\n",
      "Training Progress: \tEpoch 3 [3200/8883 (35.97%)]\t\tLoss: 1.25418\n",
      "Training Progress: \tEpoch 3 [3520/8883 (39.57%)]\t\tLoss: 1.39742\n",
      "Training Progress: \tEpoch 3 [3840/8883 (43.17%)]\t\tLoss: 1.36698\n",
      "Training Progress: \tEpoch 3 [4160/8883 (46.76%)]\t\tLoss: 1.31967\n",
      "Training Progress: \tEpoch 3 [4480/8883 (50.36%)]\t\tLoss: 1.37392\n",
      "Training Progress: \tEpoch 3 [4800/8883 (53.96%)]\t\tLoss: 1.24103\n",
      "Training Progress: \tEpoch 3 [5120/8883 (57.55%)]\t\tLoss: 1.30964\n",
      "Training Progress: \tEpoch 3 [5440/8883 (61.15%)]\t\tLoss: 1.34744\n",
      "Training Progress: \tEpoch 3 [5760/8883 (64.75%)]\t\tLoss: 1.32857\n",
      "Training Progress: \tEpoch 3 [6080/8883 (68.35%)]\t\tLoss: 1.21231\n",
      "Training Progress: \tEpoch 3 [6400/8883 (71.94%)]\t\tLoss: 1.28477\n",
      "Training Progress: \tEpoch 3 [6720/8883 (75.54%)]\t\tLoss: 1.31631\n",
      "Training Progress: \tEpoch 3 [7040/8883 (79.14%)]\t\tLoss: 1.39319\n",
      "Training Progress: \tEpoch 3 [7360/8883 (82.73%)]\t\tLoss: 1.31497\n",
      "Training Progress: \tEpoch 3 [7680/8883 (86.33%)]\t\tLoss: 1.27043\n",
      "Training Progress: \tEpoch 3 [8000/8883 (89.93%)]\t\tLoss: 1.32553\n",
      "Training Progress: \tEpoch 3 [8320/8883 (93.53%)]\t\tLoss: 1.14798\n",
      "Training Progress: \tEpoch 3 [8640/8883 (97.12%)]\t\tLoss: 1.25847\n",
      "\tTrain loss: 0.03863, Accuracy: 3654/8883 (41.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 771/1692 (45.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 728/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/8883 (0.00%)]\t\tLoss: 1.17176\n",
      "Training Progress: \tEpoch 4 [320/8883 (3.60%)]\t\tLoss: 1.18787\n",
      "Training Progress: \tEpoch 4 [640/8883 (7.19%)]\t\tLoss: 1.19960\n",
      "Training Progress: \tEpoch 4 [960/8883 (10.79%)]\t\tLoss: 1.29443\n",
      "Training Progress: \tEpoch 4 [1280/8883 (14.39%)]\t\tLoss: 1.37647\n",
      "Training Progress: \tEpoch 4 [1600/8883 (17.99%)]\t\tLoss: 1.46469\n",
      "Training Progress: \tEpoch 4 [1920/8883 (21.58%)]\t\tLoss: 1.31074\n",
      "Training Progress: \tEpoch 4 [2240/8883 (25.18%)]\t\tLoss: 1.27433\n",
      "Training Progress: \tEpoch 4 [2560/8883 (28.78%)]\t\tLoss: 1.43698\n",
      "Training Progress: \tEpoch 4 [2880/8883 (32.37%)]\t\tLoss: 1.26526\n",
      "Training Progress: \tEpoch 4 [3200/8883 (35.97%)]\t\tLoss: 1.26486\n",
      "Training Progress: \tEpoch 4 [3520/8883 (39.57%)]\t\tLoss: 1.39806\n",
      "Training Progress: \tEpoch 4 [3840/8883 (43.17%)]\t\tLoss: 1.36482\n",
      "Training Progress: \tEpoch 4 [4160/8883 (46.76%)]\t\tLoss: 1.27147\n",
      "Training Progress: \tEpoch 4 [4480/8883 (50.36%)]\t\tLoss: 1.29914\n",
      "Training Progress: \tEpoch 4 [4800/8883 (53.96%)]\t\tLoss: 1.29069\n",
      "Training Progress: \tEpoch 4 [5120/8883 (57.55%)]\t\tLoss: 1.22809\n",
      "Training Progress: \tEpoch 4 [5440/8883 (61.15%)]\t\tLoss: 1.34959\n",
      "Training Progress: \tEpoch 4 [5760/8883 (64.75%)]\t\tLoss: 1.26465\n",
      "Training Progress: \tEpoch 4 [6080/8883 (68.35%)]\t\tLoss: 1.21235\n",
      "Training Progress: \tEpoch 4 [6400/8883 (71.94%)]\t\tLoss: 1.22211\n",
      "Training Progress: \tEpoch 4 [6720/8883 (75.54%)]\t\tLoss: 1.26375\n",
      "Training Progress: \tEpoch 4 [7040/8883 (79.14%)]\t\tLoss: 1.33627\n",
      "Training Progress: \tEpoch 4 [7360/8883 (82.73%)]\t\tLoss: 1.16693\n",
      "Training Progress: \tEpoch 4 [7680/8883 (86.33%)]\t\tLoss: 1.28046\n",
      "Training Progress: \tEpoch 4 [8000/8883 (89.93%)]\t\tLoss: 1.31407\n",
      "Training Progress: \tEpoch 4 [8320/8883 (93.53%)]\t\tLoss: 1.15548\n",
      "Training Progress: \tEpoch 4 [8640/8883 (97.12%)]\t\tLoss: 1.29267\n",
      "\tTrain loss: 0.03824, Accuracy: 3629/8883 (40.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 737/1692 (43.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 702/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/8883 (0.00%)]\t\tLoss: 1.22326\n",
      "Training Progress: \tEpoch 5 [320/8883 (3.60%)]\t\tLoss: 1.15264\n",
      "Training Progress: \tEpoch 5 [640/8883 (7.19%)]\t\tLoss: 1.19842\n",
      "Training Progress: \tEpoch 5 [960/8883 (10.79%)]\t\tLoss: 1.27803\n",
      "Training Progress: \tEpoch 5 [1280/8883 (14.39%)]\t\tLoss: 1.15426\n",
      "Training Progress: \tEpoch 5 [1600/8883 (17.99%)]\t\tLoss: 1.43559\n",
      "Training Progress: \tEpoch 5 [1920/8883 (21.58%)]\t\tLoss: 1.30966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 5 [2240/8883 (25.18%)]\t\tLoss: 1.20684\n",
      "Training Progress: \tEpoch 5 [2560/8883 (28.78%)]\t\tLoss: 1.32167\n",
      "Training Progress: \tEpoch 5 [2880/8883 (32.37%)]\t\tLoss: 1.29425\n",
      "Training Progress: \tEpoch 5 [3200/8883 (35.97%)]\t\tLoss: 1.21470\n",
      "Training Progress: \tEpoch 5 [3520/8883 (39.57%)]\t\tLoss: 1.30678\n",
      "Training Progress: \tEpoch 5 [3840/8883 (43.17%)]\t\tLoss: 1.35776\n",
      "Training Progress: \tEpoch 5 [4160/8883 (46.76%)]\t\tLoss: 1.32851\n",
      "Training Progress: \tEpoch 5 [4480/8883 (50.36%)]\t\tLoss: 1.20350\n",
      "Training Progress: \tEpoch 5 [4800/8883 (53.96%)]\t\tLoss: 1.21200\n",
      "Training Progress: \tEpoch 5 [5120/8883 (57.55%)]\t\tLoss: 1.21690\n",
      "Training Progress: \tEpoch 5 [5440/8883 (61.15%)]\t\tLoss: 1.30900\n",
      "Training Progress: \tEpoch 5 [5760/8883 (64.75%)]\t\tLoss: 1.29076\n",
      "Training Progress: \tEpoch 5 [6080/8883 (68.35%)]\t\tLoss: 1.15065\n",
      "Training Progress: \tEpoch 5 [6400/8883 (71.94%)]\t\tLoss: 1.20438\n",
      "Training Progress: \tEpoch 5 [6720/8883 (75.54%)]\t\tLoss: 1.23684\n",
      "Training Progress: \tEpoch 5 [7040/8883 (79.14%)]\t\tLoss: 1.17557\n",
      "Training Progress: \tEpoch 5 [7360/8883 (82.73%)]\t\tLoss: 1.29924\n",
      "Training Progress: \tEpoch 5 [7680/8883 (86.33%)]\t\tLoss: 1.19341\n",
      "Training Progress: \tEpoch 5 [8000/8883 (89.93%)]\t\tLoss: 1.31497\n",
      "Training Progress: \tEpoch 5 [8320/8883 (93.53%)]\t\tLoss: 1.23761\n",
      "Training Progress: \tEpoch 5 [8640/8883 (97.12%)]\t\tLoss: 1.23052\n",
      "\tTrain loss: 0.03728, Accuracy: 3875/8883 (43.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 806/1692 (47.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 800/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/8883 (0.00%)]\t\tLoss: 1.11864\n",
      "Training Progress: \tEpoch 6 [320/8883 (3.60%)]\t\tLoss: 1.24109\n",
      "Training Progress: \tEpoch 6 [640/8883 (7.19%)]\t\tLoss: 1.29012\n",
      "Training Progress: \tEpoch 6 [960/8883 (10.79%)]\t\tLoss: 1.36682\n",
      "Training Progress: \tEpoch 6 [1280/8883 (14.39%)]\t\tLoss: 1.27280\n",
      "Training Progress: \tEpoch 6 [1600/8883 (17.99%)]\t\tLoss: 1.44009\n",
      "Training Progress: \tEpoch 6 [1920/8883 (21.58%)]\t\tLoss: 1.23548\n",
      "Training Progress: \tEpoch 6 [2240/8883 (25.18%)]\t\tLoss: 1.33840\n",
      "Training Progress: \tEpoch 6 [2560/8883 (28.78%)]\t\tLoss: 1.28849\n",
      "Training Progress: \tEpoch 6 [2880/8883 (32.37%)]\t\tLoss: 1.27220\n",
      "Training Progress: \tEpoch 6 [3200/8883 (35.97%)]\t\tLoss: 1.38090\n",
      "Training Progress: \tEpoch 6 [3520/8883 (39.57%)]\t\tLoss: 1.48017\n",
      "Training Progress: \tEpoch 6 [3840/8883 (43.17%)]\t\tLoss: 1.47297\n",
      "Training Progress: \tEpoch 6 [4160/8883 (46.76%)]\t\tLoss: 1.20065\n",
      "Training Progress: \tEpoch 6 [4480/8883 (50.36%)]\t\tLoss: 1.16999\n",
      "Training Progress: \tEpoch 6 [4800/8883 (53.96%)]\t\tLoss: 1.18114\n",
      "Training Progress: \tEpoch 6 [5120/8883 (57.55%)]\t\tLoss: 1.23822\n",
      "Training Progress: \tEpoch 6 [5440/8883 (61.15%)]\t\tLoss: 1.34962\n",
      "Training Progress: \tEpoch 6 [5760/8883 (64.75%)]\t\tLoss: 1.26347\n",
      "Training Progress: \tEpoch 6 [6080/8883 (68.35%)]\t\tLoss: 1.13298\n",
      "Training Progress: \tEpoch 6 [6400/8883 (71.94%)]\t\tLoss: 1.16728\n",
      "Training Progress: \tEpoch 6 [6720/8883 (75.54%)]\t\tLoss: 1.17629\n",
      "Training Progress: \tEpoch 6 [7040/8883 (79.14%)]\t\tLoss: 1.33980\n",
      "Training Progress: \tEpoch 6 [7360/8883 (82.73%)]\t\tLoss: 1.29965\n",
      "Training Progress: \tEpoch 6 [7680/8883 (86.33%)]\t\tLoss: 1.18036\n",
      "Training Progress: \tEpoch 6 [8000/8883 (89.93%)]\t\tLoss: 1.25098\n",
      "Training Progress: \tEpoch 6 [8320/8883 (93.53%)]\t\tLoss: 1.09821\n",
      "Training Progress: \tEpoch 6 [8640/8883 (97.12%)]\t\tLoss: 1.21779\n",
      "\tTrain loss: 0.03693, Accuracy: 3924/8883 (44.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 838/1692 (49.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 820/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/8883 (0.00%)]\t\tLoss: 1.16348\n",
      "Training Progress: \tEpoch 7 [320/8883 (3.60%)]\t\tLoss: 1.19900\n",
      "Training Progress: \tEpoch 7 [640/8883 (7.19%)]\t\tLoss: 1.17106\n",
      "Training Progress: \tEpoch 7 [960/8883 (10.79%)]\t\tLoss: 1.16547\n",
      "Training Progress: \tEpoch 7 [1280/8883 (14.39%)]\t\tLoss: 1.31590\n",
      "Training Progress: \tEpoch 7 [1600/8883 (17.99%)]\t\tLoss: 1.38959\n",
      "Training Progress: \tEpoch 7 [1920/8883 (21.58%)]\t\tLoss: 1.20776\n",
      "Training Progress: \tEpoch 7 [2240/8883 (25.18%)]\t\tLoss: 1.18383\n",
      "Training Progress: \tEpoch 7 [2560/8883 (28.78%)]\t\tLoss: 1.26412\n",
      "Training Progress: \tEpoch 7 [2880/8883 (32.37%)]\t\tLoss: 1.16683\n",
      "Training Progress: \tEpoch 7 [3200/8883 (35.97%)]\t\tLoss: 1.24498\n",
      "Training Progress: \tEpoch 7 [3520/8883 (39.57%)]\t\tLoss: 1.44287\n",
      "Training Progress: \tEpoch 7 [3840/8883 (43.17%)]\t\tLoss: 1.41466\n",
      "Training Progress: \tEpoch 7 [4160/8883 (46.76%)]\t\tLoss: 1.18337\n",
      "Training Progress: \tEpoch 7 [4480/8883 (50.36%)]\t\tLoss: 1.31332\n",
      "Training Progress: \tEpoch 7 [4800/8883 (53.96%)]\t\tLoss: 1.12635\n",
      "Training Progress: \tEpoch 7 [5120/8883 (57.55%)]\t\tLoss: 1.11972\n",
      "Training Progress: \tEpoch 7 [5440/8883 (61.15%)]\t\tLoss: 1.44636\n",
      "Training Progress: \tEpoch 7 [5760/8883 (64.75%)]\t\tLoss: 1.30125\n",
      "Training Progress: \tEpoch 7 [6080/8883 (68.35%)]\t\tLoss: 1.17458\n",
      "Training Progress: \tEpoch 7 [6400/8883 (71.94%)]\t\tLoss: 1.14168\n",
      "Training Progress: \tEpoch 7 [6720/8883 (75.54%)]\t\tLoss: 1.30554\n",
      "Training Progress: \tEpoch 7 [7040/8883 (79.14%)]\t\tLoss: 1.36891\n",
      "Training Progress: \tEpoch 7 [7360/8883 (82.73%)]\t\tLoss: 1.21093\n",
      "Training Progress: \tEpoch 7 [7680/8883 (86.33%)]\t\tLoss: 1.16085\n",
      "Training Progress: \tEpoch 7 [8000/8883 (89.93%)]\t\tLoss: 1.24406\n",
      "Training Progress: \tEpoch 7 [8320/8883 (93.53%)]\t\tLoss: 1.14822\n",
      "Training Progress: \tEpoch 7 [8640/8883 (97.12%)]\t\tLoss: 1.26660\n",
      "\tTrain loss: 0.03618, Accuracy: 4101/8883 (46.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 840/1692 (49.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/8883 (0.00%)]\t\tLoss: 1.06806\n",
      "Training Progress: \tEpoch 8 [320/8883 (3.60%)]\t\tLoss: 1.13762\n",
      "Training Progress: \tEpoch 8 [640/8883 (7.19%)]\t\tLoss: 1.34796\n",
      "Training Progress: \tEpoch 8 [960/8883 (10.79%)]\t\tLoss: 1.15794\n",
      "Training Progress: \tEpoch 8 [1280/8883 (14.39%)]\t\tLoss: 1.27281\n",
      "Training Progress: \tEpoch 8 [1600/8883 (17.99%)]\t\tLoss: 1.18107\n",
      "Training Progress: \tEpoch 8 [1920/8883 (21.58%)]\t\tLoss: 1.25758\n",
      "Training Progress: \tEpoch 8 [2240/8883 (25.18%)]\t\tLoss: 1.34182\n",
      "Training Progress: \tEpoch 8 [2560/8883 (28.78%)]\t\tLoss: 1.15736\n",
      "Training Progress: \tEpoch 8 [2880/8883 (32.37%)]\t\tLoss: 1.21813\n",
      "Training Progress: \tEpoch 8 [3200/8883 (35.97%)]\t\tLoss: 1.38226\n",
      "Training Progress: \tEpoch 8 [3520/8883 (39.57%)]\t\tLoss: 1.44977\n",
      "Training Progress: \tEpoch 8 [3840/8883 (43.17%)]\t\tLoss: 1.30699\n",
      "Training Progress: \tEpoch 8 [4160/8883 (46.76%)]\t\tLoss: 1.15813\n",
      "Training Progress: \tEpoch 8 [4480/8883 (50.36%)]\t\tLoss: 1.17679\n",
      "Training Progress: \tEpoch 8 [4800/8883 (53.96%)]\t\tLoss: 1.13684\n",
      "Training Progress: \tEpoch 8 [5120/8883 (57.55%)]\t\tLoss: 1.15903\n",
      "Training Progress: \tEpoch 8 [5440/8883 (61.15%)]\t\tLoss: 1.38227\n",
      "Training Progress: \tEpoch 8 [5760/8883 (64.75%)]\t\tLoss: 1.19210\n",
      "Training Progress: \tEpoch 8 [6080/8883 (68.35%)]\t\tLoss: 1.19448\n",
      "Training Progress: \tEpoch 8 [6400/8883 (71.94%)]\t\tLoss: 1.22480\n",
      "Training Progress: \tEpoch 8 [6720/8883 (75.54%)]\t\tLoss: 1.22318\n",
      "Training Progress: \tEpoch 8 [7040/8883 (79.14%)]\t\tLoss: 1.19009\n",
      "Training Progress: \tEpoch 8 [7360/8883 (82.73%)]\t\tLoss: 1.25778\n",
      "Training Progress: \tEpoch 8 [7680/8883 (86.33%)]\t\tLoss: 1.16977\n",
      "Training Progress: \tEpoch 8 [8000/8883 (89.93%)]\t\tLoss: 1.25000\n",
      "Training Progress: \tEpoch 8 [8320/8883 (93.53%)]\t\tLoss: 1.12829\n",
      "Training Progress: \tEpoch 8 [8640/8883 (97.12%)]\t\tLoss: 1.23955\n",
      "\tTrain loss: 0.03561, Accuracy: 4102/8883 (46.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 840/1692 (49.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/8883 (0.00%)]\t\tLoss: 1.16159\n",
      "Training Progress: \tEpoch 9 [320/8883 (3.60%)]\t\tLoss: 1.20949\n",
      "Training Progress: \tEpoch 9 [640/8883 (7.19%)]\t\tLoss: 1.24409\n",
      "Training Progress: \tEpoch 9 [960/8883 (10.79%)]\t\tLoss: 1.16966\n",
      "Training Progress: \tEpoch 9 [1280/8883 (14.39%)]\t\tLoss: 1.28827\n",
      "Training Progress: \tEpoch 9 [1600/8883 (17.99%)]\t\tLoss: 1.31134\n",
      "Training Progress: \tEpoch 9 [1920/8883 (21.58%)]\t\tLoss: 1.22042\n",
      "Training Progress: \tEpoch 9 [2240/8883 (25.18%)]\t\tLoss: 1.18637\n",
      "Training Progress: \tEpoch 9 [2560/8883 (28.78%)]\t\tLoss: 1.17360\n",
      "Training Progress: \tEpoch 9 [2880/8883 (32.37%)]\t\tLoss: 1.17100\n",
      "Training Progress: \tEpoch 9 [3200/8883 (35.97%)]\t\tLoss: 1.51201\n",
      "Training Progress: \tEpoch 9 [3520/8883 (39.57%)]\t\tLoss: 1.30855\n",
      "Training Progress: \tEpoch 9 [3840/8883 (43.17%)]\t\tLoss: 1.27268\n",
      "Training Progress: \tEpoch 9 [4160/8883 (46.76%)]\t\tLoss: 1.14831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 9 [4480/8883 (50.36%)]\t\tLoss: 1.09185\n",
      "Training Progress: \tEpoch 9 [4800/8883 (53.96%)]\t\tLoss: 1.11330\n",
      "Training Progress: \tEpoch 9 [5120/8883 (57.55%)]\t\tLoss: 1.25256\n",
      "Training Progress: \tEpoch 9 [5440/8883 (61.15%)]\t\tLoss: 1.25540\n",
      "Training Progress: \tEpoch 9 [5760/8883 (64.75%)]\t\tLoss: 1.20709\n",
      "Training Progress: \tEpoch 9 [6080/8883 (68.35%)]\t\tLoss: 1.07337\n",
      "Training Progress: \tEpoch 9 [6400/8883 (71.94%)]\t\tLoss: 1.22966\n",
      "Training Progress: \tEpoch 9 [6720/8883 (75.54%)]\t\tLoss: 1.24703\n",
      "Training Progress: \tEpoch 9 [7040/8883 (79.14%)]\t\tLoss: 1.20143\n",
      "Training Progress: \tEpoch 9 [7360/8883 (82.73%)]\t\tLoss: 1.36128\n",
      "Training Progress: \tEpoch 9 [7680/8883 (86.33%)]\t\tLoss: 1.14060\n",
      "Training Progress: \tEpoch 9 [8000/8883 (89.93%)]\t\tLoss: 1.32709\n",
      "Training Progress: \tEpoch 9 [8320/8883 (93.53%)]\t\tLoss: 1.07288\n",
      "Training Progress: \tEpoch 9 [8640/8883 (97.12%)]\t\tLoss: 1.26018\n",
      "\tTrain loss: 0.03488, Accuracy: 4229/8883 (47.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 912/1692 (53.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/8883 (0.00%)]\t\tLoss: 1.06233\n",
      "Training Progress: \tEpoch 10 [320/8883 (3.60%)]\t\tLoss: 1.12533\n",
      "Training Progress: \tEpoch 10 [640/8883 (7.19%)]\t\tLoss: 1.26730\n",
      "Training Progress: \tEpoch 10 [960/8883 (10.79%)]\t\tLoss: 1.21109\n",
      "Training Progress: \tEpoch 10 [1280/8883 (14.39%)]\t\tLoss: 1.27100\n",
      "Training Progress: \tEpoch 10 [1600/8883 (17.99%)]\t\tLoss: 1.27468\n",
      "Training Progress: \tEpoch 10 [1920/8883 (21.58%)]\t\tLoss: 1.25626\n",
      "Training Progress: \tEpoch 10 [2240/8883 (25.18%)]\t\tLoss: 1.21518\n",
      "Training Progress: \tEpoch 10 [2560/8883 (28.78%)]\t\tLoss: 1.23356\n",
      "Training Progress: \tEpoch 10 [2880/8883 (32.37%)]\t\tLoss: 1.24920\n",
      "Training Progress: \tEpoch 10 [3200/8883 (35.97%)]\t\tLoss: 1.41492\n",
      "Training Progress: \tEpoch 10 [3520/8883 (39.57%)]\t\tLoss: 1.32403\n",
      "Training Progress: \tEpoch 10 [3840/8883 (43.17%)]\t\tLoss: 1.33223\n",
      "Training Progress: \tEpoch 10 [4160/8883 (46.76%)]\t\tLoss: 1.26968\n",
      "Training Progress: \tEpoch 10 [4480/8883 (50.36%)]\t\tLoss: 1.17138\n",
      "Training Progress: \tEpoch 10 [4800/8883 (53.96%)]\t\tLoss: 1.14233\n",
      "Training Progress: \tEpoch 10 [5120/8883 (57.55%)]\t\tLoss: 1.18169\n",
      "Training Progress: \tEpoch 10 [5440/8883 (61.15%)]\t\tLoss: 1.28735\n",
      "Training Progress: \tEpoch 10 [5760/8883 (64.75%)]\t\tLoss: 1.25706\n",
      "Training Progress: \tEpoch 10 [6080/8883 (68.35%)]\t\tLoss: 1.13570\n",
      "Training Progress: \tEpoch 10 [6400/8883 (71.94%)]\t\tLoss: 1.14296\n",
      "Training Progress: \tEpoch 10 [6720/8883 (75.54%)]\t\tLoss: 1.11985\n",
      "Training Progress: \tEpoch 10 [7040/8883 (79.14%)]\t\tLoss: 1.21194\n",
      "Training Progress: \tEpoch 10 [7360/8883 (82.73%)]\t\tLoss: 1.33172\n",
      "Training Progress: \tEpoch 10 [7680/8883 (86.33%)]\t\tLoss: 1.13931\n",
      "Training Progress: \tEpoch 10 [8000/8883 (89.93%)]\t\tLoss: 1.25857\n",
      "Training Progress: \tEpoch 10 [8320/8883 (93.53%)]\t\tLoss: 1.24816\n",
      "Training Progress: \tEpoch 10 [8640/8883 (97.12%)]\t\tLoss: 1.28912\n",
      "\tTrain loss: 0.03425, Accuracy: 4327/8883 (48.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 887/1692 (52.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 859/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/8883 (0.00%)]\t\tLoss: 1.09135\n",
      "Training Progress: \tEpoch 11 [320/8883 (3.60%)]\t\tLoss: 1.09104\n",
      "Training Progress: \tEpoch 11 [640/8883 (7.19%)]\t\tLoss: 1.30404\n",
      "Training Progress: \tEpoch 11 [960/8883 (10.79%)]\t\tLoss: 1.18523\n",
      "Training Progress: \tEpoch 11 [1280/8883 (14.39%)]\t\tLoss: 1.27508\n",
      "Training Progress: \tEpoch 11 [1600/8883 (17.99%)]\t\tLoss: 1.21339\n",
      "Training Progress: \tEpoch 11 [1920/8883 (21.58%)]\t\tLoss: 1.23460\n",
      "Training Progress: \tEpoch 11 [2240/8883 (25.18%)]\t\tLoss: 1.16496\n",
      "Training Progress: \tEpoch 11 [2560/8883 (28.78%)]\t\tLoss: 1.28498\n",
      "Training Progress: \tEpoch 11 [2880/8883 (32.37%)]\t\tLoss: 1.23522\n",
      "Training Progress: \tEpoch 11 [3200/8883 (35.97%)]\t\tLoss: 1.16227\n",
      "Training Progress: \tEpoch 11 [3520/8883 (39.57%)]\t\tLoss: 1.30430\n",
      "Training Progress: \tEpoch 11 [3840/8883 (43.17%)]\t\tLoss: 1.39471\n",
      "Training Progress: \tEpoch 11 [4160/8883 (46.76%)]\t\tLoss: 1.19606\n",
      "Training Progress: \tEpoch 11 [4480/8883 (50.36%)]\t\tLoss: 1.06512\n",
      "Training Progress: \tEpoch 11 [4800/8883 (53.96%)]\t\tLoss: 1.05542\n",
      "Training Progress: \tEpoch 11 [5120/8883 (57.55%)]\t\tLoss: 1.23948\n",
      "Training Progress: \tEpoch 11 [5440/8883 (61.15%)]\t\tLoss: 1.21449\n",
      "Training Progress: \tEpoch 11 [5760/8883 (64.75%)]\t\tLoss: 1.32661\n",
      "Training Progress: \tEpoch 11 [6080/8883 (68.35%)]\t\tLoss: 1.19898\n",
      "Training Progress: \tEpoch 11 [6400/8883 (71.94%)]\t\tLoss: 1.22446\n",
      "Training Progress: \tEpoch 11 [6720/8883 (75.54%)]\t\tLoss: 1.17990\n",
      "Training Progress: \tEpoch 11 [7040/8883 (79.14%)]\t\tLoss: 1.18674\n",
      "Training Progress: \tEpoch 11 [7360/8883 (82.73%)]\t\tLoss: 1.28419\n",
      "Training Progress: \tEpoch 11 [7680/8883 (86.33%)]\t\tLoss: 1.11785\n",
      "Training Progress: \tEpoch 11 [8000/8883 (89.93%)]\t\tLoss: 1.22237\n",
      "Training Progress: \tEpoch 11 [8320/8883 (93.53%)]\t\tLoss: 1.10392\n",
      "Training Progress: \tEpoch 11 [8640/8883 (97.12%)]\t\tLoss: 1.13185\n",
      "\tTrain loss: 0.03450, Accuracy: 4280/8883 (48.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 876/1692 (51.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 841/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/8883 (0.00%)]\t\tLoss: 1.04819\n",
      "Training Progress: \tEpoch 12 [320/8883 (3.60%)]\t\tLoss: 1.09919\n",
      "Training Progress: \tEpoch 12 [640/8883 (7.19%)]\t\tLoss: 1.12206\n",
      "Training Progress: \tEpoch 12 [960/8883 (10.79%)]\t\tLoss: 1.15019\n",
      "Training Progress: \tEpoch 12 [1280/8883 (14.39%)]\t\tLoss: 1.38070\n",
      "Training Progress: \tEpoch 12 [1600/8883 (17.99%)]\t\tLoss: 1.30233\n",
      "Training Progress: \tEpoch 12 [1920/8883 (21.58%)]\t\tLoss: 1.23289\n",
      "Training Progress: \tEpoch 12 [2240/8883 (25.18%)]\t\tLoss: 1.09753\n",
      "Training Progress: \tEpoch 12 [2560/8883 (28.78%)]\t\tLoss: 1.26358\n",
      "Training Progress: \tEpoch 12 [2880/8883 (32.37%)]\t\tLoss: 1.27849\n",
      "Training Progress: \tEpoch 12 [3200/8883 (35.97%)]\t\tLoss: 1.35665\n",
      "Training Progress: \tEpoch 12 [3520/8883 (39.57%)]\t\tLoss: 1.36206\n",
      "Training Progress: \tEpoch 12 [3840/8883 (43.17%)]\t\tLoss: 1.51145\n",
      "Training Progress: \tEpoch 12 [4160/8883 (46.76%)]\t\tLoss: 1.22174\n",
      "Training Progress: \tEpoch 12 [4480/8883 (50.36%)]\t\tLoss: 1.07177\n",
      "Training Progress: \tEpoch 12 [4800/8883 (53.96%)]\t\tLoss: 1.04126\n",
      "Training Progress: \tEpoch 12 [5120/8883 (57.55%)]\t\tLoss: 1.23586\n",
      "Training Progress: \tEpoch 12 [5440/8883 (61.15%)]\t\tLoss: 1.35547\n",
      "Training Progress: \tEpoch 12 [5760/8883 (64.75%)]\t\tLoss: 1.17833\n",
      "Training Progress: \tEpoch 12 [6080/8883 (68.35%)]\t\tLoss: 1.08346\n",
      "Training Progress: \tEpoch 12 [6400/8883 (71.94%)]\t\tLoss: 1.09712\n",
      "Training Progress: \tEpoch 12 [6720/8883 (75.54%)]\t\tLoss: 1.19469\n",
      "Training Progress: \tEpoch 12 [7040/8883 (79.14%)]\t\tLoss: 1.12240\n",
      "Training Progress: \tEpoch 12 [7360/8883 (82.73%)]\t\tLoss: 1.26915\n",
      "Training Progress: \tEpoch 12 [7680/8883 (86.33%)]\t\tLoss: 1.07863\n",
      "Training Progress: \tEpoch 12 [8000/8883 (89.93%)]\t\tLoss: 1.15575\n",
      "Training Progress: \tEpoch 12 [8320/8883 (93.53%)]\t\tLoss: 1.09196\n",
      "Training Progress: \tEpoch 12 [8640/8883 (97.12%)]\t\tLoss: 1.14326\n",
      "\tTrain loss: 0.03359, Accuracy: 4369/8883 (49.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 908/1692 (53.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/8883 (0.00%)]\t\tLoss: 1.08784\n",
      "Training Progress: \tEpoch 13 [320/8883 (3.60%)]\t\tLoss: 1.11536\n",
      "Training Progress: \tEpoch 13 [640/8883 (7.19%)]\t\tLoss: 1.02696\n",
      "Training Progress: \tEpoch 13 [960/8883 (10.79%)]\t\tLoss: 1.12271\n",
      "Training Progress: \tEpoch 13 [1280/8883 (14.39%)]\t\tLoss: 1.13794\n",
      "Training Progress: \tEpoch 13 [1600/8883 (17.99%)]\t\tLoss: 1.31689\n",
      "Training Progress: \tEpoch 13 [1920/8883 (21.58%)]\t\tLoss: 1.23509\n",
      "Training Progress: \tEpoch 13 [2240/8883 (25.18%)]\t\tLoss: 1.08519\n",
      "Training Progress: \tEpoch 13 [2560/8883 (28.78%)]\t\tLoss: 1.05956\n",
      "Training Progress: \tEpoch 13 [2880/8883 (32.37%)]\t\tLoss: 1.18578\n",
      "Training Progress: \tEpoch 13 [3200/8883 (35.97%)]\t\tLoss: 1.35157\n",
      "Training Progress: \tEpoch 13 [3520/8883 (39.57%)]\t\tLoss: 1.25838\n",
      "Training Progress: \tEpoch 13 [3840/8883 (43.17%)]\t\tLoss: 1.18895\n",
      "Training Progress: \tEpoch 13 [4160/8883 (46.76%)]\t\tLoss: 1.11160\n",
      "Training Progress: \tEpoch 13 [4480/8883 (50.36%)]\t\tLoss: 1.00864\n",
      "Training Progress: \tEpoch 13 [4800/8883 (53.96%)]\t\tLoss: 1.12456\n",
      "Training Progress: \tEpoch 13 [5120/8883 (57.55%)]\t\tLoss: 1.07520\n",
      "Training Progress: \tEpoch 13 [5440/8883 (61.15%)]\t\tLoss: 1.36454\n",
      "Training Progress: \tEpoch 13 [5760/8883 (64.75%)]\t\tLoss: 1.22316\n",
      "Training Progress: \tEpoch 13 [6080/8883 (68.35%)]\t\tLoss: 1.00079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 13 [6400/8883 (71.94%)]\t\tLoss: 1.18333\n",
      "Training Progress: \tEpoch 13 [6720/8883 (75.54%)]\t\tLoss: 1.11199\n",
      "Training Progress: \tEpoch 13 [7040/8883 (79.14%)]\t\tLoss: 1.09567\n",
      "Training Progress: \tEpoch 13 [7360/8883 (82.73%)]\t\tLoss: 1.09741\n",
      "Training Progress: \tEpoch 13 [7680/8883 (86.33%)]\t\tLoss: 1.10560\n",
      "Training Progress: \tEpoch 13 [8000/8883 (89.93%)]\t\tLoss: 1.18118\n",
      "Training Progress: \tEpoch 13 [8320/8883 (93.53%)]\t\tLoss: 1.02012\n",
      "Training Progress: \tEpoch 13 [8640/8883 (97.12%)]\t\tLoss: 1.16027\n",
      "\tTrain loss: 0.03276, Accuracy: 4550/8883 (51.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 925/1692 (54.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 831/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/8883 (0.00%)]\t\tLoss: 1.05813\n",
      "Training Progress: \tEpoch 14 [320/8883 (3.60%)]\t\tLoss: 1.10718\n",
      "Training Progress: \tEpoch 14 [640/8883 (7.19%)]\t\tLoss: 1.12838\n",
      "Training Progress: \tEpoch 14 [960/8883 (10.79%)]\t\tLoss: 1.15083\n",
      "Training Progress: \tEpoch 14 [1280/8883 (14.39%)]\t\tLoss: 1.15961\n",
      "Training Progress: \tEpoch 14 [1600/8883 (17.99%)]\t\tLoss: 1.24745\n",
      "Training Progress: \tEpoch 14 [1920/8883 (21.58%)]\t\tLoss: 1.15750\n",
      "Training Progress: \tEpoch 14 [2240/8883 (25.18%)]\t\tLoss: 1.08758\n",
      "Training Progress: \tEpoch 14 [2560/8883 (28.78%)]\t\tLoss: 1.26824\n",
      "Training Progress: \tEpoch 14 [2880/8883 (32.37%)]\t\tLoss: 1.26249\n",
      "Training Progress: \tEpoch 14 [3200/8883 (35.97%)]\t\tLoss: 1.23046\n",
      "Training Progress: \tEpoch 14 [3520/8883 (39.57%)]\t\tLoss: 1.32760\n",
      "Training Progress: \tEpoch 14 [3840/8883 (43.17%)]\t\tLoss: 1.42438\n",
      "Training Progress: \tEpoch 14 [4160/8883 (46.76%)]\t\tLoss: 1.11330\n",
      "Training Progress: \tEpoch 14 [4480/8883 (50.36%)]\t\tLoss: 1.08577\n",
      "Training Progress: \tEpoch 14 [4800/8883 (53.96%)]\t\tLoss: 1.12796\n",
      "Training Progress: \tEpoch 14 [5120/8883 (57.55%)]\t\tLoss: 1.22775\n",
      "Training Progress: \tEpoch 14 [5440/8883 (61.15%)]\t\tLoss: 1.36465\n",
      "Training Progress: \tEpoch 14 [5760/8883 (64.75%)]\t\tLoss: 1.21149\n",
      "Training Progress: \tEpoch 14 [6080/8883 (68.35%)]\t\tLoss: 1.03144\n",
      "Training Progress: \tEpoch 14 [6400/8883 (71.94%)]\t\tLoss: 1.16820\n",
      "Training Progress: \tEpoch 14 [6720/8883 (75.54%)]\t\tLoss: 1.11363\n",
      "Training Progress: \tEpoch 14 [7040/8883 (79.14%)]\t\tLoss: 1.17589\n",
      "Training Progress: \tEpoch 14 [7360/8883 (82.73%)]\t\tLoss: 1.28849\n",
      "Training Progress: \tEpoch 14 [7680/8883 (86.33%)]\t\tLoss: 1.20365\n",
      "Training Progress: \tEpoch 14 [8000/8883 (89.93%)]\t\tLoss: 1.31518\n",
      "Training Progress: \tEpoch 14 [8320/8883 (93.53%)]\t\tLoss: 1.13485\n",
      "Training Progress: \tEpoch 14 [8640/8883 (97.12%)]\t\tLoss: 1.24795\n",
      "\tTrain loss: 0.03275, Accuracy: 4542/8883 (51.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 923/1692 (54.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 851/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/8883 (0.00%)]\t\tLoss: 1.11001\n",
      "Training Progress: \tEpoch 15 [320/8883 (3.60%)]\t\tLoss: 1.09766\n",
      "Training Progress: \tEpoch 15 [640/8883 (7.19%)]\t\tLoss: 1.17887\n",
      "Training Progress: \tEpoch 15 [960/8883 (10.79%)]\t\tLoss: 1.12598\n",
      "Training Progress: \tEpoch 15 [1280/8883 (14.39%)]\t\tLoss: 1.26637\n",
      "Training Progress: \tEpoch 15 [1600/8883 (17.99%)]\t\tLoss: 1.30861\n",
      "Training Progress: \tEpoch 15 [1920/8883 (21.58%)]\t\tLoss: 1.13722\n",
      "Training Progress: \tEpoch 15 [2240/8883 (25.18%)]\t\tLoss: 1.05247\n",
      "Training Progress: \tEpoch 15 [2560/8883 (28.78%)]\t\tLoss: 1.09529\n",
      "Training Progress: \tEpoch 15 [2880/8883 (32.37%)]\t\tLoss: 1.11350\n",
      "Training Progress: \tEpoch 15 [3200/8883 (35.97%)]\t\tLoss: 1.13129\n",
      "Training Progress: \tEpoch 15 [3520/8883 (39.57%)]\t\tLoss: 1.39374\n",
      "Training Progress: \tEpoch 15 [3840/8883 (43.17%)]\t\tLoss: 1.33378\n",
      "Training Progress: \tEpoch 15 [4160/8883 (46.76%)]\t\tLoss: 1.17319\n",
      "Training Progress: \tEpoch 15 [4480/8883 (50.36%)]\t\tLoss: 0.96841\n",
      "Training Progress: \tEpoch 15 [4800/8883 (53.96%)]\t\tLoss: 1.03455\n",
      "Training Progress: \tEpoch 15 [5120/8883 (57.55%)]\t\tLoss: 1.27307\n",
      "Training Progress: \tEpoch 15 [5440/8883 (61.15%)]\t\tLoss: 1.22014\n",
      "Training Progress: \tEpoch 15 [5760/8883 (64.75%)]\t\tLoss: 1.21772\n",
      "Training Progress: \tEpoch 15 [6080/8883 (68.35%)]\t\tLoss: 1.01631\n",
      "Training Progress: \tEpoch 15 [6400/8883 (71.94%)]\t\tLoss: 1.26576\n",
      "Training Progress: \tEpoch 15 [6720/8883 (75.54%)]\t\tLoss: 1.16759\n",
      "Training Progress: \tEpoch 15 [7040/8883 (79.14%)]\t\tLoss: 1.22626\n",
      "Training Progress: \tEpoch 15 [7360/8883 (82.73%)]\t\tLoss: 1.11225\n",
      "Training Progress: \tEpoch 15 [7680/8883 (86.33%)]\t\tLoss: 1.01835\n",
      "Training Progress: \tEpoch 15 [8000/8883 (89.93%)]\t\tLoss: 1.18239\n",
      "Training Progress: \tEpoch 15 [8320/8883 (93.53%)]\t\tLoss: 1.10326\n",
      "Training Progress: \tEpoch 15 [8640/8883 (97.12%)]\t\tLoss: 1.17088\n",
      "\tTrain loss: 0.03190, Accuracy: 4687/8883 (52.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 969/1692 (57.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 851/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/8883 (0.00%)]\t\tLoss: 0.99005\n",
      "Training Progress: \tEpoch 16 [320/8883 (3.60%)]\t\tLoss: 1.28845\n",
      "Training Progress: \tEpoch 16 [640/8883 (7.19%)]\t\tLoss: 1.15359\n",
      "Training Progress: \tEpoch 16 [960/8883 (10.79%)]\t\tLoss: 1.03046\n",
      "Training Progress: \tEpoch 16 [1280/8883 (14.39%)]\t\tLoss: 1.20290\n",
      "Training Progress: \tEpoch 16 [1600/8883 (17.99%)]\t\tLoss: 1.20328\n",
      "Training Progress: \tEpoch 16 [1920/8883 (21.58%)]\t\tLoss: 1.26168\n",
      "Training Progress: \tEpoch 16 [2240/8883 (25.18%)]\t\tLoss: 1.05262\n",
      "Training Progress: \tEpoch 16 [2560/8883 (28.78%)]\t\tLoss: 1.03160\n",
      "Training Progress: \tEpoch 16 [2880/8883 (32.37%)]\t\tLoss: 1.38565\n",
      "Training Progress: \tEpoch 16 [3200/8883 (35.97%)]\t\tLoss: 1.28246\n",
      "Training Progress: \tEpoch 16 [3520/8883 (39.57%)]\t\tLoss: 1.14244\n",
      "Training Progress: \tEpoch 16 [3840/8883 (43.17%)]\t\tLoss: 1.44977\n",
      "Training Progress: \tEpoch 16 [4160/8883 (46.76%)]\t\tLoss: 1.20206\n",
      "Training Progress: \tEpoch 16 [4480/8883 (50.36%)]\t\tLoss: 1.01627\n",
      "Training Progress: \tEpoch 16 [4800/8883 (53.96%)]\t\tLoss: 1.11670\n",
      "Training Progress: \tEpoch 16 [5120/8883 (57.55%)]\t\tLoss: 1.13110\n",
      "Training Progress: \tEpoch 16 [5440/8883 (61.15%)]\t\tLoss: 1.45394\n",
      "Training Progress: \tEpoch 16 [5760/8883 (64.75%)]\t\tLoss: 1.16556\n",
      "Training Progress: \tEpoch 16 [6080/8883 (68.35%)]\t\tLoss: 1.12971\n",
      "Training Progress: \tEpoch 16 [6400/8883 (71.94%)]\t\tLoss: 1.07380\n",
      "Training Progress: \tEpoch 16 [6720/8883 (75.54%)]\t\tLoss: 1.26924\n",
      "Training Progress: \tEpoch 16 [7040/8883 (79.14%)]\t\tLoss: 1.29871\n",
      "Training Progress: \tEpoch 16 [7360/8883 (82.73%)]\t\tLoss: 1.23863\n",
      "Training Progress: \tEpoch 16 [7680/8883 (86.33%)]\t\tLoss: 1.13416\n",
      "Training Progress: \tEpoch 16 [8000/8883 (89.93%)]\t\tLoss: 1.14682\n",
      "Training Progress: \tEpoch 16 [8320/8883 (93.53%)]\t\tLoss: 1.05715\n",
      "Training Progress: \tEpoch 16 [8640/8883 (97.12%)]\t\tLoss: 1.19464\n",
      "\tTrain loss: 0.03139, Accuracy: 4796/8883 (53.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 985/1692 (58.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/8883 (0.00%)]\t\tLoss: 1.05687\n",
      "Training Progress: \tEpoch 17 [320/8883 (3.60%)]\t\tLoss: 0.96205\n",
      "Training Progress: \tEpoch 17 [640/8883 (7.19%)]\t\tLoss: 1.33211\n",
      "Training Progress: \tEpoch 17 [960/8883 (10.79%)]\t\tLoss: 1.14835\n",
      "Training Progress: \tEpoch 17 [1280/8883 (14.39%)]\t\tLoss: 1.15138\n",
      "Training Progress: \tEpoch 17 [1600/8883 (17.99%)]\t\tLoss: 1.31895\n",
      "Training Progress: \tEpoch 17 [1920/8883 (21.58%)]\t\tLoss: 1.23077\n",
      "Training Progress: \tEpoch 17 [2240/8883 (25.18%)]\t\tLoss: 1.20593\n",
      "Training Progress: \tEpoch 17 [2560/8883 (28.78%)]\t\tLoss: 1.19944\n",
      "Training Progress: \tEpoch 17 [2880/8883 (32.37%)]\t\tLoss: 1.02233\n",
      "Training Progress: \tEpoch 17 [3200/8883 (35.97%)]\t\tLoss: 1.34889\n",
      "Training Progress: \tEpoch 17 [3520/8883 (39.57%)]\t\tLoss: 1.24748\n",
      "Training Progress: \tEpoch 17 [3840/8883 (43.17%)]\t\tLoss: 1.37263\n",
      "Training Progress: \tEpoch 17 [4160/8883 (46.76%)]\t\tLoss: 1.07534\n",
      "Training Progress: \tEpoch 17 [4480/8883 (50.36%)]\t\tLoss: 1.22764\n",
      "Training Progress: \tEpoch 17 [4800/8883 (53.96%)]\t\tLoss: 1.02647\n",
      "Training Progress: \tEpoch 17 [5120/8883 (57.55%)]\t\tLoss: 1.21293\n",
      "Training Progress: \tEpoch 17 [5440/8883 (61.15%)]\t\tLoss: 1.30236\n",
      "Training Progress: \tEpoch 17 [5760/8883 (64.75%)]\t\tLoss: 1.18009\n",
      "Training Progress: \tEpoch 17 [6080/8883 (68.35%)]\t\tLoss: 1.05813\n",
      "Training Progress: \tEpoch 17 [6400/8883 (71.94%)]\t\tLoss: 1.08177\n",
      "Training Progress: \tEpoch 17 [6720/8883 (75.54%)]\t\tLoss: 1.13208\n",
      "Training Progress: \tEpoch 17 [7040/8883 (79.14%)]\t\tLoss: 1.19923\n",
      "Training Progress: \tEpoch 17 [7360/8883 (82.73%)]\t\tLoss: 1.27596\n",
      "Training Progress: \tEpoch 17 [7680/8883 (86.33%)]\t\tLoss: 1.08751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 17 [8000/8883 (89.93%)]\t\tLoss: 1.07988\n",
      "Training Progress: \tEpoch 17 [8320/8883 (93.53%)]\t\tLoss: 1.03793\n",
      "Training Progress: \tEpoch 17 [8640/8883 (97.12%)]\t\tLoss: 1.15060\n",
      "\tTrain loss: 0.03140, Accuracy: 4754/8883 (53.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 971/1692 (57.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 869/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/8883 (0.00%)]\t\tLoss: 1.08364\n",
      "Training Progress: \tEpoch 18 [320/8883 (3.60%)]\t\tLoss: 1.16916\n",
      "Training Progress: \tEpoch 18 [640/8883 (7.19%)]\t\tLoss: 1.14200\n",
      "Training Progress: \tEpoch 18 [960/8883 (10.79%)]\t\tLoss: 1.07958\n",
      "Training Progress: \tEpoch 18 [1280/8883 (14.39%)]\t\tLoss: 1.18894\n",
      "Training Progress: \tEpoch 18 [1600/8883 (17.99%)]\t\tLoss: 1.32153\n",
      "Training Progress: \tEpoch 18 [1920/8883 (21.58%)]\t\tLoss: 1.25581\n",
      "Training Progress: \tEpoch 18 [2240/8883 (25.18%)]\t\tLoss: 1.18357\n",
      "Training Progress: \tEpoch 18 [2560/8883 (28.78%)]\t\tLoss: 1.18251\n",
      "Training Progress: \tEpoch 18 [2880/8883 (32.37%)]\t\tLoss: 1.19455\n",
      "Training Progress: \tEpoch 18 [3200/8883 (35.97%)]\t\tLoss: 1.19934\n",
      "Training Progress: \tEpoch 18 [3520/8883 (39.57%)]\t\tLoss: 1.40145\n",
      "Training Progress: \tEpoch 18 [3840/8883 (43.17%)]\t\tLoss: 1.23271\n",
      "Training Progress: \tEpoch 18 [4160/8883 (46.76%)]\t\tLoss: 1.09365\n",
      "Training Progress: \tEpoch 18 [4480/8883 (50.36%)]\t\tLoss: 1.08822\n",
      "Training Progress: \tEpoch 18 [4800/8883 (53.96%)]\t\tLoss: 1.10079\n",
      "Training Progress: \tEpoch 18 [5120/8883 (57.55%)]\t\tLoss: 1.33257\n",
      "Training Progress: \tEpoch 18 [5440/8883 (61.15%)]\t\tLoss: 1.46779\n",
      "Training Progress: \tEpoch 18 [5760/8883 (64.75%)]\t\tLoss: 1.10214\n",
      "Training Progress: \tEpoch 18 [6080/8883 (68.35%)]\t\tLoss: 1.13451\n",
      "Training Progress: \tEpoch 18 [6400/8883 (71.94%)]\t\tLoss: 1.13436\n",
      "Training Progress: \tEpoch 18 [6720/8883 (75.54%)]\t\tLoss: 1.20555\n",
      "Training Progress: \tEpoch 18 [7040/8883 (79.14%)]\t\tLoss: 1.09297\n",
      "Training Progress: \tEpoch 18 [7360/8883 (82.73%)]\t\tLoss: 1.16518\n",
      "Training Progress: \tEpoch 18 [7680/8883 (86.33%)]\t\tLoss: 1.13537\n",
      "Training Progress: \tEpoch 18 [8000/8883 (89.93%)]\t\tLoss: 1.21915\n",
      "Training Progress: \tEpoch 18 [8320/8883 (93.53%)]\t\tLoss: 0.97806\n",
      "Training Progress: \tEpoch 18 [8640/8883 (97.12%)]\t\tLoss: 1.02187\n",
      "\tTrain loss: 0.03013, Accuracy: 4841/8883 (54.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1019/1692 (60.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/8883 (0.00%)]\t\tLoss: 1.01561\n",
      "Training Progress: \tEpoch 19 [320/8883 (3.60%)]\t\tLoss: 1.14582\n",
      "Training Progress: \tEpoch 19 [640/8883 (7.19%)]\t\tLoss: 1.14195\n",
      "Training Progress: \tEpoch 19 [960/8883 (10.79%)]\t\tLoss: 1.07176\n",
      "Training Progress: \tEpoch 19 [1280/8883 (14.39%)]\t\tLoss: 1.30435\n",
      "Training Progress: \tEpoch 19 [1600/8883 (17.99%)]\t\tLoss: 1.30629\n",
      "Training Progress: \tEpoch 19 [1920/8883 (21.58%)]\t\tLoss: 1.11120\n",
      "Training Progress: \tEpoch 19 [2240/8883 (25.18%)]\t\tLoss: 1.06769\n",
      "Training Progress: \tEpoch 19 [2560/8883 (28.78%)]\t\tLoss: 1.21845\n",
      "Training Progress: \tEpoch 19 [2880/8883 (32.37%)]\t\tLoss: 1.12420\n",
      "Training Progress: \tEpoch 19 [3200/8883 (35.97%)]\t\tLoss: 1.11937\n",
      "Training Progress: \tEpoch 19 [3520/8883 (39.57%)]\t\tLoss: 1.12117\n",
      "Training Progress: \tEpoch 19 [3840/8883 (43.17%)]\t\tLoss: 1.46472\n",
      "Training Progress: \tEpoch 19 [4160/8883 (46.76%)]\t\tLoss: 1.14204\n",
      "Training Progress: \tEpoch 19 [4480/8883 (50.36%)]\t\tLoss: 1.19027\n",
      "Training Progress: \tEpoch 19 [4800/8883 (53.96%)]\t\tLoss: 1.13784\n",
      "Training Progress: \tEpoch 19 [5120/8883 (57.55%)]\t\tLoss: 1.20427\n",
      "Training Progress: \tEpoch 19 [5440/8883 (61.15%)]\t\tLoss: 1.27836\n",
      "Training Progress: \tEpoch 19 [5760/8883 (64.75%)]\t\tLoss: 1.18473\n",
      "Training Progress: \tEpoch 19 [6080/8883 (68.35%)]\t\tLoss: 1.19065\n",
      "Training Progress: \tEpoch 19 [6400/8883 (71.94%)]\t\tLoss: 1.19044\n",
      "Training Progress: \tEpoch 19 [6720/8883 (75.54%)]\t\tLoss: 1.08617\n",
      "Training Progress: \tEpoch 19 [7040/8883 (79.14%)]\t\tLoss: 1.36420\n",
      "Training Progress: \tEpoch 19 [7360/8883 (82.73%)]\t\tLoss: 1.14156\n",
      "Training Progress: \tEpoch 19 [7680/8883 (86.33%)]\t\tLoss: 1.17174\n",
      "Training Progress: \tEpoch 19 [8000/8883 (89.93%)]\t\tLoss: 1.17222\n",
      "Training Progress: \tEpoch 19 [8320/8883 (93.53%)]\t\tLoss: 1.00390\n",
      "Training Progress: \tEpoch 19 [8640/8883 (97.12%)]\t\tLoss: 1.10537\n",
      "\tTrain loss: 0.03075, Accuracy: 4803/8883 (54.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1008/1692 (59.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 888/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/8883 (0.00%)]\t\tLoss: 1.10963\n",
      "Training Progress: \tEpoch 20 [320/8883 (3.60%)]\t\tLoss: 0.96902\n",
      "Training Progress: \tEpoch 20 [640/8883 (7.19%)]\t\tLoss: 1.12290\n",
      "Training Progress: \tEpoch 20 [960/8883 (10.79%)]\t\tLoss: 1.16728\n",
      "Training Progress: \tEpoch 20 [1280/8883 (14.39%)]\t\tLoss: 1.20819\n",
      "Training Progress: \tEpoch 20 [1600/8883 (17.99%)]\t\tLoss: 1.28324\n",
      "Training Progress: \tEpoch 20 [1920/8883 (21.58%)]\t\tLoss: 0.99976\n",
      "Training Progress: \tEpoch 20 [2240/8883 (25.18%)]\t\tLoss: 1.11321\n",
      "Training Progress: \tEpoch 20 [2560/8883 (28.78%)]\t\tLoss: 1.09983\n",
      "Training Progress: \tEpoch 20 [2880/8883 (32.37%)]\t\tLoss: 1.22171\n",
      "Training Progress: \tEpoch 20 [3200/8883 (35.97%)]\t\tLoss: 1.17788\n",
      "Training Progress: \tEpoch 20 [3520/8883 (39.57%)]\t\tLoss: 1.22975\n",
      "Training Progress: \tEpoch 20 [3840/8883 (43.17%)]\t\tLoss: 1.27852\n",
      "Training Progress: \tEpoch 20 [4160/8883 (46.76%)]\t\tLoss: 1.06219\n",
      "Training Progress: \tEpoch 20 [4480/8883 (50.36%)]\t\tLoss: 1.06676\n",
      "Training Progress: \tEpoch 20 [4800/8883 (53.96%)]\t\tLoss: 1.04218\n",
      "Training Progress: \tEpoch 20 [5120/8883 (57.55%)]\t\tLoss: 1.16735\n",
      "Training Progress: \tEpoch 20 [5440/8883 (61.15%)]\t\tLoss: 1.26018\n",
      "Training Progress: \tEpoch 20 [5760/8883 (64.75%)]\t\tLoss: 1.15760\n",
      "Training Progress: \tEpoch 20 [6080/8883 (68.35%)]\t\tLoss: 1.08730\n",
      "Training Progress: \tEpoch 20 [6400/8883 (71.94%)]\t\tLoss: 0.92480\n",
      "Training Progress: \tEpoch 20 [6720/8883 (75.54%)]\t\tLoss: 0.98553\n",
      "Training Progress: \tEpoch 20 [7040/8883 (79.14%)]\t\tLoss: 1.35066\n",
      "Training Progress: \tEpoch 20 [7360/8883 (82.73%)]\t\tLoss: 1.18868\n",
      "Training Progress: \tEpoch 20 [7680/8883 (86.33%)]\t\tLoss: 1.10648\n",
      "Training Progress: \tEpoch 20 [8000/8883 (89.93%)]\t\tLoss: 1.07135\n",
      "Training Progress: \tEpoch 20 [8320/8883 (93.53%)]\t\tLoss: 1.04351\n",
      "Training Progress: \tEpoch 20 [8640/8883 (97.12%)]\t\tLoss: 1.19889\n",
      "\tTrain loss: 0.02994, Accuracy: 5006/8883 (56.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1038/1692 (61.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 895/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/8883 (0.00%)]\t\tLoss: 1.05336\n",
      "Training Progress: \tEpoch 21 [320/8883 (3.60%)]\t\tLoss: 1.11590\n",
      "Training Progress: \tEpoch 21 [640/8883 (7.19%)]\t\tLoss: 1.24668\n",
      "Training Progress: \tEpoch 21 [960/8883 (10.79%)]\t\tLoss: 1.05643\n",
      "Training Progress: \tEpoch 21 [1280/8883 (14.39%)]\t\tLoss: 1.15297\n",
      "Training Progress: \tEpoch 21 [1600/8883 (17.99%)]\t\tLoss: 1.20381\n",
      "Training Progress: \tEpoch 21 [1920/8883 (21.58%)]\t\tLoss: 1.06292\n",
      "Training Progress: \tEpoch 21 [2240/8883 (25.18%)]\t\tLoss: 1.20111\n",
      "Training Progress: \tEpoch 21 [2560/8883 (28.78%)]\t\tLoss: 1.08162\n",
      "Training Progress: \tEpoch 21 [2880/8883 (32.37%)]\t\tLoss: 1.07934\n",
      "Training Progress: \tEpoch 21 [3200/8883 (35.97%)]\t\tLoss: 1.14572\n",
      "Training Progress: \tEpoch 21 [3520/8883 (39.57%)]\t\tLoss: 1.19678\n",
      "Training Progress: \tEpoch 21 [3840/8883 (43.17%)]\t\tLoss: 1.28572\n",
      "Training Progress: \tEpoch 21 [4160/8883 (46.76%)]\t\tLoss: 1.08687\n",
      "Training Progress: \tEpoch 21 [4480/8883 (50.36%)]\t\tLoss: 1.08471\n",
      "Training Progress: \tEpoch 21 [4800/8883 (53.96%)]\t\tLoss: 1.00876\n",
      "Training Progress: \tEpoch 21 [5120/8883 (57.55%)]\t\tLoss: 1.23497\n",
      "Training Progress: \tEpoch 21 [5440/8883 (61.15%)]\t\tLoss: 1.39701\n",
      "Training Progress: \tEpoch 21 [5760/8883 (64.75%)]\t\tLoss: 1.10207\n",
      "Training Progress: \tEpoch 21 [6080/8883 (68.35%)]\t\tLoss: 1.01761\n",
      "Training Progress: \tEpoch 21 [6400/8883 (71.94%)]\t\tLoss: 0.97956\n",
      "Training Progress: \tEpoch 21 [6720/8883 (75.54%)]\t\tLoss: 1.12494\n",
      "Training Progress: \tEpoch 21 [7040/8883 (79.14%)]\t\tLoss: 1.01961\n",
      "Training Progress: \tEpoch 21 [7360/8883 (82.73%)]\t\tLoss: 1.13837\n",
      "Training Progress: \tEpoch 21 [7680/8883 (86.33%)]\t\tLoss: 1.11025\n",
      "Training Progress: \tEpoch 21 [8000/8883 (89.93%)]\t\tLoss: 1.16153\n",
      "Training Progress: \tEpoch 21 [8320/8883 (93.53%)]\t\tLoss: 1.00836\n",
      "Training Progress: \tEpoch 21 [8640/8883 (97.12%)]\t\tLoss: 1.02440\n",
      "\tTrain loss: 0.02951, Accuracy: 4999/8883 (56.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1046/1692 (61.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 894/1772 (50.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 22 [0/8883 (0.00%)]\t\tLoss: 1.01448\n",
      "Training Progress: \tEpoch 22 [320/8883 (3.60%)]\t\tLoss: 1.03078\n",
      "Training Progress: \tEpoch 22 [640/8883 (7.19%)]\t\tLoss: 1.17182\n",
      "Training Progress: \tEpoch 22 [960/8883 (10.79%)]\t\tLoss: 1.18845\n",
      "Training Progress: \tEpoch 22 [1280/8883 (14.39%)]\t\tLoss: 1.10238\n",
      "Training Progress: \tEpoch 22 [1600/8883 (17.99%)]\t\tLoss: 1.16753\n",
      "Training Progress: \tEpoch 22 [1920/8883 (21.58%)]\t\tLoss: 1.27822\n",
      "Training Progress: \tEpoch 22 [2240/8883 (25.18%)]\t\tLoss: 1.14512\n",
      "Training Progress: \tEpoch 22 [2560/8883 (28.78%)]\t\tLoss: 1.11883\n",
      "Training Progress: \tEpoch 22 [2880/8883 (32.37%)]\t\tLoss: 1.21889\n",
      "Training Progress: \tEpoch 22 [3200/8883 (35.97%)]\t\tLoss: 1.33263\n",
      "Training Progress: \tEpoch 22 [3520/8883 (39.57%)]\t\tLoss: 1.21630\n",
      "Training Progress: \tEpoch 22 [3840/8883 (43.17%)]\t\tLoss: 1.42037\n",
      "Training Progress: \tEpoch 22 [4160/8883 (46.76%)]\t\tLoss: 1.18895\n",
      "Training Progress: \tEpoch 22 [4480/8883 (50.36%)]\t\tLoss: 0.98221\n",
      "Training Progress: \tEpoch 22 [4800/8883 (53.96%)]\t\tLoss: 0.91979\n",
      "Training Progress: \tEpoch 22 [5120/8883 (57.55%)]\t\tLoss: 1.12681\n",
      "Training Progress: \tEpoch 22 [5440/8883 (61.15%)]\t\tLoss: 1.17197\n",
      "Training Progress: \tEpoch 22 [5760/8883 (64.75%)]\t\tLoss: 1.15420\n",
      "Training Progress: \tEpoch 22 [6080/8883 (68.35%)]\t\tLoss: 0.94675\n",
      "Training Progress: \tEpoch 22 [6400/8883 (71.94%)]\t\tLoss: 0.99184\n",
      "Training Progress: \tEpoch 22 [6720/8883 (75.54%)]\t\tLoss: 1.13487\n",
      "Training Progress: \tEpoch 22 [7040/8883 (79.14%)]\t\tLoss: 1.13380\n",
      "Training Progress: \tEpoch 22 [7360/8883 (82.73%)]\t\tLoss: 1.18532\n",
      "Training Progress: \tEpoch 22 [7680/8883 (86.33%)]\t\tLoss: 1.17186\n",
      "Training Progress: \tEpoch 22 [8000/8883 (89.93%)]\t\tLoss: 1.06920\n",
      "Training Progress: \tEpoch 22 [8320/8883 (93.53%)]\t\tLoss: 0.88700\n",
      "Training Progress: \tEpoch 22 [8640/8883 (97.12%)]\t\tLoss: 1.16634\n",
      "\tTrain loss: 0.02937, Accuracy: 5028/8883 (56.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1046/1692 (61.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/8883 (0.00%)]\t\tLoss: 1.09797\n",
      "Training Progress: \tEpoch 23 [320/8883 (3.60%)]\t\tLoss: 1.00838\n",
      "Training Progress: \tEpoch 23 [640/8883 (7.19%)]\t\tLoss: 1.05836\n",
      "Training Progress: \tEpoch 23 [960/8883 (10.79%)]\t\tLoss: 1.05990\n",
      "Training Progress: \tEpoch 23 [1280/8883 (14.39%)]\t\tLoss: 1.25023\n",
      "Training Progress: \tEpoch 23 [1600/8883 (17.99%)]\t\tLoss: 1.24611\n",
      "Training Progress: \tEpoch 23 [1920/8883 (21.58%)]\t\tLoss: 1.06812\n",
      "Training Progress: \tEpoch 23 [2240/8883 (25.18%)]\t\tLoss: 1.06798\n",
      "Training Progress: \tEpoch 23 [2560/8883 (28.78%)]\t\tLoss: 1.00999\n",
      "Training Progress: \tEpoch 23 [2880/8883 (32.37%)]\t\tLoss: 1.16372\n",
      "Training Progress: \tEpoch 23 [3200/8883 (35.97%)]\t\tLoss: 1.11685\n",
      "Training Progress: \tEpoch 23 [3520/8883 (39.57%)]\t\tLoss: 1.11853\n",
      "Training Progress: \tEpoch 23 [3840/8883 (43.17%)]\t\tLoss: 1.43527\n",
      "Training Progress: \tEpoch 23 [4160/8883 (46.76%)]\t\tLoss: 0.97200\n",
      "Training Progress: \tEpoch 23 [4480/8883 (50.36%)]\t\tLoss: 1.06730\n",
      "Training Progress: \tEpoch 23 [4800/8883 (53.96%)]\t\tLoss: 0.94416\n",
      "Training Progress: \tEpoch 23 [5120/8883 (57.55%)]\t\tLoss: 1.08392\n",
      "Training Progress: \tEpoch 23 [5440/8883 (61.15%)]\t\tLoss: 1.39407\n",
      "Training Progress: \tEpoch 23 [5760/8883 (64.75%)]\t\tLoss: 1.13919\n",
      "Training Progress: \tEpoch 23 [6080/8883 (68.35%)]\t\tLoss: 0.99034\n",
      "Training Progress: \tEpoch 23 [6400/8883 (71.94%)]\t\tLoss: 1.09444\n",
      "Training Progress: \tEpoch 23 [6720/8883 (75.54%)]\t\tLoss: 1.05122\n",
      "Training Progress: \tEpoch 23 [7040/8883 (79.14%)]\t\tLoss: 1.20725\n",
      "Training Progress: \tEpoch 23 [7360/8883 (82.73%)]\t\tLoss: 1.08829\n",
      "Training Progress: \tEpoch 23 [7680/8883 (86.33%)]\t\tLoss: 1.10377\n",
      "Training Progress: \tEpoch 23 [8000/8883 (89.93%)]\t\tLoss: 1.25651\n",
      "Training Progress: \tEpoch 23 [8320/8883 (93.53%)]\t\tLoss: 0.93056\n",
      "Training Progress: \tEpoch 23 [8640/8883 (97.12%)]\t\tLoss: 1.09159\n",
      "\tTrain loss: 0.02944, Accuracy: 5142/8883 (57.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1064/1692 (62.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 925/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/8883 (0.00%)]\t\tLoss: 1.00066\n",
      "Training Progress: \tEpoch 24 [320/8883 (3.60%)]\t\tLoss: 0.99401\n",
      "Training Progress: \tEpoch 24 [640/8883 (7.19%)]\t\tLoss: 1.07369\n",
      "Training Progress: \tEpoch 24 [960/8883 (10.79%)]\t\tLoss: 1.18838\n",
      "Training Progress: \tEpoch 24 [1280/8883 (14.39%)]\t\tLoss: 1.24890\n",
      "Training Progress: \tEpoch 24 [1600/8883 (17.99%)]\t\tLoss: 1.25774\n",
      "Training Progress: \tEpoch 24 [1920/8883 (21.58%)]\t\tLoss: 1.20114\n",
      "Training Progress: \tEpoch 24 [2240/8883 (25.18%)]\t\tLoss: 1.18219\n",
      "Training Progress: \tEpoch 24 [2560/8883 (28.78%)]\t\tLoss: 1.07277\n",
      "Training Progress: \tEpoch 24 [2880/8883 (32.37%)]\t\tLoss: 1.01226\n",
      "Training Progress: \tEpoch 24 [3200/8883 (35.97%)]\t\tLoss: 1.21115\n",
      "Training Progress: \tEpoch 24 [3520/8883 (39.57%)]\t\tLoss: 1.22151\n",
      "Training Progress: \tEpoch 24 [3840/8883 (43.17%)]\t\tLoss: 1.21873\n",
      "Training Progress: \tEpoch 24 [4160/8883 (46.76%)]\t\tLoss: 1.13632\n",
      "Training Progress: \tEpoch 24 [4480/8883 (50.36%)]\t\tLoss: 0.98000\n",
      "Training Progress: \tEpoch 24 [4800/8883 (53.96%)]\t\tLoss: 0.98576\n",
      "Training Progress: \tEpoch 24 [5120/8883 (57.55%)]\t\tLoss: 1.11193\n",
      "Training Progress: \tEpoch 24 [5440/8883 (61.15%)]\t\tLoss: 1.16657\n",
      "Training Progress: \tEpoch 24 [5760/8883 (64.75%)]\t\tLoss: 1.15852\n",
      "Training Progress: \tEpoch 24 [6080/8883 (68.35%)]\t\tLoss: 1.02699\n",
      "Training Progress: \tEpoch 24 [6400/8883 (71.94%)]\t\tLoss: 1.16326\n",
      "Training Progress: \tEpoch 24 [6720/8883 (75.54%)]\t\tLoss: 1.05740\n",
      "Training Progress: \tEpoch 24 [7040/8883 (79.14%)]\t\tLoss: 1.18982\n",
      "Training Progress: \tEpoch 24 [7360/8883 (82.73%)]\t\tLoss: 0.99471\n",
      "Training Progress: \tEpoch 24 [7680/8883 (86.33%)]\t\tLoss: 1.12648\n",
      "Training Progress: \tEpoch 24 [8000/8883 (89.93%)]\t\tLoss: 0.97580\n",
      "Training Progress: \tEpoch 24 [8320/8883 (93.53%)]\t\tLoss: 1.10512\n",
      "Training Progress: \tEpoch 24 [8640/8883 (97.12%)]\t\tLoss: 1.15601\n",
      "\tTrain loss: 0.02867, Accuracy: 5201/8883 (58.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1078/1692 (63.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 922/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/8883 (0.00%)]\t\tLoss: 1.08113\n",
      "Training Progress: \tEpoch 25 [320/8883 (3.60%)]\t\tLoss: 0.96542\n",
      "Training Progress: \tEpoch 25 [640/8883 (7.19%)]\t\tLoss: 1.19177\n",
      "Training Progress: \tEpoch 25 [960/8883 (10.79%)]\t\tLoss: 1.01647\n",
      "Training Progress: \tEpoch 25 [1280/8883 (14.39%)]\t\tLoss: 1.11687\n",
      "Training Progress: \tEpoch 25 [1600/8883 (17.99%)]\t\tLoss: 1.24648\n",
      "Training Progress: \tEpoch 25 [1920/8883 (21.58%)]\t\tLoss: 1.11594\n",
      "Training Progress: \tEpoch 25 [2240/8883 (25.18%)]\t\tLoss: 0.97670\n",
      "Training Progress: \tEpoch 25 [2560/8883 (28.78%)]\t\tLoss: 0.99041\n",
      "Training Progress: \tEpoch 25 [2880/8883 (32.37%)]\t\tLoss: 1.26301\n",
      "Training Progress: \tEpoch 25 [3200/8883 (35.97%)]\t\tLoss: 1.21527\n",
      "Training Progress: \tEpoch 25 [3520/8883 (39.57%)]\t\tLoss: 1.17338\n",
      "Training Progress: \tEpoch 25 [3840/8883 (43.17%)]\t\tLoss: 1.17671\n",
      "Training Progress: \tEpoch 25 [4160/8883 (46.76%)]\t\tLoss: 0.96149\n",
      "Training Progress: \tEpoch 25 [4480/8883 (50.36%)]\t\tLoss: 0.99608\n",
      "Training Progress: \tEpoch 25 [4800/8883 (53.96%)]\t\tLoss: 0.99197\n",
      "Training Progress: \tEpoch 25 [5120/8883 (57.55%)]\t\tLoss: 1.08836\n",
      "Training Progress: \tEpoch 25 [5440/8883 (61.15%)]\t\tLoss: 1.10843\n",
      "Training Progress: \tEpoch 25 [5760/8883 (64.75%)]\t\tLoss: 1.23183\n",
      "Training Progress: \tEpoch 25 [6080/8883 (68.35%)]\t\tLoss: 1.04859\n",
      "Training Progress: \tEpoch 25 [6400/8883 (71.94%)]\t\tLoss: 1.10736\n",
      "Training Progress: \tEpoch 25 [6720/8883 (75.54%)]\t\tLoss: 1.03275\n",
      "Training Progress: \tEpoch 25 [7040/8883 (79.14%)]\t\tLoss: 1.06146\n",
      "Training Progress: \tEpoch 25 [7360/8883 (82.73%)]\t\tLoss: 1.12599\n",
      "Training Progress: \tEpoch 25 [7680/8883 (86.33%)]\t\tLoss: 1.09960\n",
      "Training Progress: \tEpoch 25 [8000/8883 (89.93%)]\t\tLoss: 1.00042\n",
      "Training Progress: \tEpoch 25 [8320/8883 (93.53%)]\t\tLoss: 0.91094\n",
      "Training Progress: \tEpoch 25 [8640/8883 (97.12%)]\t\tLoss: 1.04917\n",
      "\tTrain loss: 0.02835, Accuracy: 5238/8883 (58.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1092/1692 (64.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 907/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/8883 (0.00%)]\t\tLoss: 0.93721\n",
      "Training Progress: \tEpoch 26 [320/8883 (3.60%)]\t\tLoss: 1.04273\n",
      "Training Progress: \tEpoch 26 [640/8883 (7.19%)]\t\tLoss: 1.14047\n",
      "Training Progress: \tEpoch 26 [960/8883 (10.79%)]\t\tLoss: 0.99470\n",
      "Training Progress: \tEpoch 26 [1280/8883 (14.39%)]\t\tLoss: 1.21354\n",
      "Training Progress: \tEpoch 26 [1600/8883 (17.99%)]\t\tLoss: 1.25329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 26 [1920/8883 (21.58%)]\t\tLoss: 1.05252\n",
      "Training Progress: \tEpoch 26 [2240/8883 (25.18%)]\t\tLoss: 1.04434\n",
      "Training Progress: \tEpoch 26 [2560/8883 (28.78%)]\t\tLoss: 1.10916\n",
      "Training Progress: \tEpoch 26 [2880/8883 (32.37%)]\t\tLoss: 1.15240\n",
      "Training Progress: \tEpoch 26 [3200/8883 (35.97%)]\t\tLoss: 1.10714\n",
      "Training Progress: \tEpoch 26 [3520/8883 (39.57%)]\t\tLoss: 1.31672\n",
      "Training Progress: \tEpoch 26 [3840/8883 (43.17%)]\t\tLoss: 1.35186\n",
      "Training Progress: \tEpoch 26 [4160/8883 (46.76%)]\t\tLoss: 1.18811\n",
      "Training Progress: \tEpoch 26 [4480/8883 (50.36%)]\t\tLoss: 0.93316\n",
      "Training Progress: \tEpoch 26 [4800/8883 (53.96%)]\t\tLoss: 0.93742\n",
      "Training Progress: \tEpoch 26 [5120/8883 (57.55%)]\t\tLoss: 1.23757\n",
      "Training Progress: \tEpoch 26 [5440/8883 (61.15%)]\t\tLoss: 1.34409\n",
      "Training Progress: \tEpoch 26 [5760/8883 (64.75%)]\t\tLoss: 1.11951\n",
      "Training Progress: \tEpoch 26 [6080/8883 (68.35%)]\t\tLoss: 0.97633\n",
      "Training Progress: \tEpoch 26 [6400/8883 (71.94%)]\t\tLoss: 1.02385\n",
      "Training Progress: \tEpoch 26 [6720/8883 (75.54%)]\t\tLoss: 1.02756\n",
      "Training Progress: \tEpoch 26 [7040/8883 (79.14%)]\t\tLoss: 0.90563\n",
      "Training Progress: \tEpoch 26 [7360/8883 (82.73%)]\t\tLoss: 1.12439\n",
      "Training Progress: \tEpoch 26 [7680/8883 (86.33%)]\t\tLoss: 0.94621\n",
      "Training Progress: \tEpoch 26 [8000/8883 (89.93%)]\t\tLoss: 1.06774\n",
      "Training Progress: \tEpoch 26 [8320/8883 (93.53%)]\t\tLoss: 1.05565\n",
      "Training Progress: \tEpoch 26 [8640/8883 (97.12%)]\t\tLoss: 0.96608\n",
      "\tTrain loss: 0.02757, Accuracy: 5271/8883 (59.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1101/1692 (65.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 907/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/8883 (0.00%)]\t\tLoss: 1.09686\n",
      "Training Progress: \tEpoch 27 [320/8883 (3.60%)]\t\tLoss: 0.93398\n",
      "Training Progress: \tEpoch 27 [640/8883 (7.19%)]\t\tLoss: 1.07097\n",
      "Training Progress: \tEpoch 27 [960/8883 (10.79%)]\t\tLoss: 1.14504\n",
      "Training Progress: \tEpoch 27 [1280/8883 (14.39%)]\t\tLoss: 1.14038\n",
      "Training Progress: \tEpoch 27 [1600/8883 (17.99%)]\t\tLoss: 1.32262\n",
      "Training Progress: \tEpoch 27 [1920/8883 (21.58%)]\t\tLoss: 1.24879\n",
      "Training Progress: \tEpoch 27 [2240/8883 (25.18%)]\t\tLoss: 1.11812\n",
      "Training Progress: \tEpoch 27 [2560/8883 (28.78%)]\t\tLoss: 1.01960\n",
      "Training Progress: \tEpoch 27 [2880/8883 (32.37%)]\t\tLoss: 1.01501\n",
      "Training Progress: \tEpoch 27 [3200/8883 (35.97%)]\t\tLoss: 1.13193\n",
      "Training Progress: \tEpoch 27 [3520/8883 (39.57%)]\t\tLoss: 1.13448\n",
      "Training Progress: \tEpoch 27 [3840/8883 (43.17%)]\t\tLoss: 1.35780\n",
      "Training Progress: \tEpoch 27 [4160/8883 (46.76%)]\t\tLoss: 1.15386\n",
      "Training Progress: \tEpoch 27 [4480/8883 (50.36%)]\t\tLoss: 0.89852\n",
      "Training Progress: \tEpoch 27 [4800/8883 (53.96%)]\t\tLoss: 0.83859\n",
      "Training Progress: \tEpoch 27 [5120/8883 (57.55%)]\t\tLoss: 1.09411\n",
      "Training Progress: \tEpoch 27 [5440/8883 (61.15%)]\t\tLoss: 1.25174\n",
      "Training Progress: \tEpoch 27 [5760/8883 (64.75%)]\t\tLoss: 1.10679\n",
      "Training Progress: \tEpoch 27 [6080/8883 (68.35%)]\t\tLoss: 0.90339\n",
      "Training Progress: \tEpoch 27 [6400/8883 (71.94%)]\t\tLoss: 1.10412\n",
      "Training Progress: \tEpoch 27 [6720/8883 (75.54%)]\t\tLoss: 0.99335\n",
      "Training Progress: \tEpoch 27 [7040/8883 (79.14%)]\t\tLoss: 1.06863\n",
      "Training Progress: \tEpoch 27 [7360/8883 (82.73%)]\t\tLoss: 1.20601\n",
      "Training Progress: \tEpoch 27 [7680/8883 (86.33%)]\t\tLoss: 1.15354\n",
      "Training Progress: \tEpoch 27 [8000/8883 (89.93%)]\t\tLoss: 1.18975\n",
      "Training Progress: \tEpoch 27 [8320/8883 (93.53%)]\t\tLoss: 0.81254\n",
      "Training Progress: \tEpoch 27 [8640/8883 (97.12%)]\t\tLoss: 0.98913\n",
      "\tTrain loss: 0.02744, Accuracy: 5362/8883 (60.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1134/1692 (67.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 936/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/8883 (0.00%)]\t\tLoss: 0.97532\n",
      "Training Progress: \tEpoch 28 [320/8883 (3.60%)]\t\tLoss: 1.06737\n",
      "Training Progress: \tEpoch 28 [640/8883 (7.19%)]\t\tLoss: 1.12684\n",
      "Training Progress: \tEpoch 28 [960/8883 (10.79%)]\t\tLoss: 0.98160\n",
      "Training Progress: \tEpoch 28 [1280/8883 (14.39%)]\t\tLoss: 1.10563\n",
      "Training Progress: \tEpoch 28 [1600/8883 (17.99%)]\t\tLoss: 1.22540\n",
      "Training Progress: \tEpoch 28 [1920/8883 (21.58%)]\t\tLoss: 1.00056\n",
      "Training Progress: \tEpoch 28 [2240/8883 (25.18%)]\t\tLoss: 1.05961\n",
      "Training Progress: \tEpoch 28 [2560/8883 (28.78%)]\t\tLoss: 1.03475\n",
      "Training Progress: \tEpoch 28 [2880/8883 (32.37%)]\t\tLoss: 1.15760\n",
      "Training Progress: \tEpoch 28 [3200/8883 (35.97%)]\t\tLoss: 1.30766\n",
      "Training Progress: \tEpoch 28 [3520/8883 (39.57%)]\t\tLoss: 1.14123\n",
      "Training Progress: \tEpoch 28 [3840/8883 (43.17%)]\t\tLoss: 1.33302\n",
      "Training Progress: \tEpoch 28 [4160/8883 (46.76%)]\t\tLoss: 0.94792\n",
      "Training Progress: \tEpoch 28 [4480/8883 (50.36%)]\t\tLoss: 1.03820\n",
      "Training Progress: \tEpoch 28 [4800/8883 (53.96%)]\t\tLoss: 0.99361\n",
      "Training Progress: \tEpoch 28 [5120/8883 (57.55%)]\t\tLoss: 1.05043\n",
      "Training Progress: \tEpoch 28 [5440/8883 (61.15%)]\t\tLoss: 1.19703\n",
      "Training Progress: \tEpoch 28 [5760/8883 (64.75%)]\t\tLoss: 1.13253\n",
      "Training Progress: \tEpoch 28 [6080/8883 (68.35%)]\t\tLoss: 1.00025\n",
      "Training Progress: \tEpoch 28 [6400/8883 (71.94%)]\t\tLoss: 1.04994\n",
      "Training Progress: \tEpoch 28 [6720/8883 (75.54%)]\t\tLoss: 1.00103\n",
      "Training Progress: \tEpoch 28 [7040/8883 (79.14%)]\t\tLoss: 1.08543\n",
      "Training Progress: \tEpoch 28 [7360/8883 (82.73%)]\t\tLoss: 1.06588\n",
      "Training Progress: \tEpoch 28 [7680/8883 (86.33%)]\t\tLoss: 1.02059\n",
      "Training Progress: \tEpoch 28 [8000/8883 (89.93%)]\t\tLoss: 1.14259\n",
      "Training Progress: \tEpoch 28 [8320/8883 (93.53%)]\t\tLoss: 0.98868\n",
      "Training Progress: \tEpoch 28 [8640/8883 (97.12%)]\t\tLoss: 1.00497\n",
      "\tTrain loss: 0.02707, Accuracy: 5406/8883 (60.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1153/1692 (68.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 932/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/8883 (0.00%)]\t\tLoss: 1.03078\n",
      "Training Progress: \tEpoch 29 [320/8883 (3.60%)]\t\tLoss: 0.90848\n",
      "Training Progress: \tEpoch 29 [640/8883 (7.19%)]\t\tLoss: 1.07550\n",
      "Training Progress: \tEpoch 29 [960/8883 (10.79%)]\t\tLoss: 1.02698\n",
      "Training Progress: \tEpoch 29 [1280/8883 (14.39%)]\t\tLoss: 1.16987\n",
      "Training Progress: \tEpoch 29 [1600/8883 (17.99%)]\t\tLoss: 1.12067\n",
      "Training Progress: \tEpoch 29 [1920/8883 (21.58%)]\t\tLoss: 1.25071\n",
      "Training Progress: \tEpoch 29 [2240/8883 (25.18%)]\t\tLoss: 1.02915\n",
      "Training Progress: \tEpoch 29 [2560/8883 (28.78%)]\t\tLoss: 1.07203\n",
      "Training Progress: \tEpoch 29 [2880/8883 (32.37%)]\t\tLoss: 1.21143\n",
      "Training Progress: \tEpoch 29 [3200/8883 (35.97%)]\t\tLoss: 1.09403\n",
      "Training Progress: \tEpoch 29 [3520/8883 (39.57%)]\t\tLoss: 1.13904\n",
      "Training Progress: \tEpoch 29 [3840/8883 (43.17%)]\t\tLoss: 1.20172\n",
      "Training Progress: \tEpoch 29 [4160/8883 (46.76%)]\t\tLoss: 1.02765\n",
      "Training Progress: \tEpoch 29 [4480/8883 (50.36%)]\t\tLoss: 0.93907\n",
      "Training Progress: \tEpoch 29 [4800/8883 (53.96%)]\t\tLoss: 0.84277\n",
      "Training Progress: \tEpoch 29 [5120/8883 (57.55%)]\t\tLoss: 1.16327\n",
      "Training Progress: \tEpoch 29 [5440/8883 (61.15%)]\t\tLoss: 1.40419\n",
      "Training Progress: \tEpoch 29 [5760/8883 (64.75%)]\t\tLoss: 1.23463\n",
      "Training Progress: \tEpoch 29 [6080/8883 (68.35%)]\t\tLoss: 0.95747\n",
      "Training Progress: \tEpoch 29 [6400/8883 (71.94%)]\t\tLoss: 0.95493\n",
      "Training Progress: \tEpoch 29 [6720/8883 (75.54%)]\t\tLoss: 0.93966\n",
      "Training Progress: \tEpoch 29 [7040/8883 (79.14%)]\t\tLoss: 0.96633\n",
      "Training Progress: \tEpoch 29 [7360/8883 (82.73%)]\t\tLoss: 1.06817\n",
      "Training Progress: \tEpoch 29 [7680/8883 (86.33%)]\t\tLoss: 1.14045\n",
      "Training Progress: \tEpoch 29 [8000/8883 (89.93%)]\t\tLoss: 1.11491\n",
      "Training Progress: \tEpoch 29 [8320/8883 (93.53%)]\t\tLoss: 0.95848\n",
      "Training Progress: \tEpoch 29 [8640/8883 (97.12%)]\t\tLoss: 1.11872\n",
      "\tTrain loss: 0.02664, Accuracy: 5414/8883 (60.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1121/1692 (66.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 920/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/8883 (0.00%)]\t\tLoss: 0.96417\n",
      "Training Progress: \tEpoch 30 [320/8883 (3.60%)]\t\tLoss: 0.97693\n",
      "Training Progress: \tEpoch 30 [640/8883 (7.19%)]\t\tLoss: 1.11068\n",
      "Training Progress: \tEpoch 30 [960/8883 (10.79%)]\t\tLoss: 1.00916\n",
      "Training Progress: \tEpoch 30 [1280/8883 (14.39%)]\t\tLoss: 0.99023\n",
      "Training Progress: \tEpoch 30 [1600/8883 (17.99%)]\t\tLoss: 1.21665\n",
      "Training Progress: \tEpoch 30 [1920/8883 (21.58%)]\t\tLoss: 1.07177\n",
      "Training Progress: \tEpoch 30 [2240/8883 (25.18%)]\t\tLoss: 0.95695\n",
      "Training Progress: \tEpoch 30 [2560/8883 (28.78%)]\t\tLoss: 0.85288\n",
      "Training Progress: \tEpoch 30 [2880/8883 (32.37%)]\t\tLoss: 1.03396\n",
      "Training Progress: \tEpoch 30 [3200/8883 (35.97%)]\t\tLoss: 1.08903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 30 [3520/8883 (39.57%)]\t\tLoss: 1.27413\n",
      "Training Progress: \tEpoch 30 [3840/8883 (43.17%)]\t\tLoss: 1.23129\n",
      "Training Progress: \tEpoch 30 [4160/8883 (46.76%)]\t\tLoss: 0.95111\n",
      "Training Progress: \tEpoch 30 [4480/8883 (50.36%)]\t\tLoss: 1.00138\n",
      "Training Progress: \tEpoch 30 [4800/8883 (53.96%)]\t\tLoss: 0.96210\n",
      "Training Progress: \tEpoch 30 [5120/8883 (57.55%)]\t\tLoss: 1.12679\n",
      "Training Progress: \tEpoch 30 [5440/8883 (61.15%)]\t\tLoss: 1.22687\n",
      "Training Progress: \tEpoch 30 [5760/8883 (64.75%)]\t\tLoss: 1.01158\n",
      "Training Progress: \tEpoch 30 [6080/8883 (68.35%)]\t\tLoss: 0.90554\n",
      "Training Progress: \tEpoch 30 [6400/8883 (71.94%)]\t\tLoss: 0.99538\n",
      "Training Progress: \tEpoch 30 [6720/8883 (75.54%)]\t\tLoss: 1.05735\n",
      "Training Progress: \tEpoch 30 [7040/8883 (79.14%)]\t\tLoss: 0.84190\n",
      "Training Progress: \tEpoch 30 [7360/8883 (82.73%)]\t\tLoss: 1.10103\n",
      "Training Progress: \tEpoch 30 [7680/8883 (86.33%)]\t\tLoss: 1.13484\n",
      "Training Progress: \tEpoch 30 [8000/8883 (89.93%)]\t\tLoss: 1.09571\n",
      "Training Progress: \tEpoch 30 [8320/8883 (93.53%)]\t\tLoss: 1.00254\n",
      "Training Progress: \tEpoch 30 [8640/8883 (97.12%)]\t\tLoss: 1.21192\n",
      "\tTrain loss: 0.02653, Accuracy: 5472/8883 (61.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1143/1692 (67.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 900/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/8883 (0.00%)]\t\tLoss: 0.93233\n",
      "Training Progress: \tEpoch 31 [320/8883 (3.60%)]\t\tLoss: 0.84999\n",
      "Training Progress: \tEpoch 31 [640/8883 (7.19%)]\t\tLoss: 0.96678\n",
      "Training Progress: \tEpoch 31 [960/8883 (10.79%)]\t\tLoss: 1.11998\n",
      "Training Progress: \tEpoch 31 [1280/8883 (14.39%)]\t\tLoss: 1.02160\n",
      "Training Progress: \tEpoch 31 [1600/8883 (17.99%)]\t\tLoss: 1.01822\n",
      "Training Progress: \tEpoch 31 [1920/8883 (21.58%)]\t\tLoss: 1.06366\n",
      "Training Progress: \tEpoch 31 [2240/8883 (25.18%)]\t\tLoss: 1.07848\n",
      "Training Progress: \tEpoch 31 [2560/8883 (28.78%)]\t\tLoss: 0.95806\n",
      "Training Progress: \tEpoch 31 [2880/8883 (32.37%)]\t\tLoss: 1.05570\n",
      "Training Progress: \tEpoch 31 [3200/8883 (35.97%)]\t\tLoss: 1.18297\n",
      "Training Progress: \tEpoch 31 [3520/8883 (39.57%)]\t\tLoss: 1.10035\n",
      "Training Progress: \tEpoch 31 [3840/8883 (43.17%)]\t\tLoss: 1.14666\n",
      "Training Progress: \tEpoch 31 [4160/8883 (46.76%)]\t\tLoss: 1.02438\n",
      "Training Progress: \tEpoch 31 [4480/8883 (50.36%)]\t\tLoss: 0.92848\n",
      "Training Progress: \tEpoch 31 [4800/8883 (53.96%)]\t\tLoss: 0.91018\n",
      "Training Progress: \tEpoch 31 [5120/8883 (57.55%)]\t\tLoss: 1.18815\n",
      "Training Progress: \tEpoch 31 [5440/8883 (61.15%)]\t\tLoss: 1.16135\n",
      "Training Progress: \tEpoch 31 [5760/8883 (64.75%)]\t\tLoss: 1.08085\n",
      "Training Progress: \tEpoch 31 [6080/8883 (68.35%)]\t\tLoss: 0.97492\n",
      "Training Progress: \tEpoch 31 [6400/8883 (71.94%)]\t\tLoss: 1.11160\n",
      "Training Progress: \tEpoch 31 [6720/8883 (75.54%)]\t\tLoss: 1.05543\n",
      "Training Progress: \tEpoch 31 [7040/8883 (79.14%)]\t\tLoss: 1.21765\n",
      "Training Progress: \tEpoch 31 [7360/8883 (82.73%)]\t\tLoss: 0.90050\n",
      "Training Progress: \tEpoch 31 [7680/8883 (86.33%)]\t\tLoss: 1.23109\n",
      "Training Progress: \tEpoch 31 [8000/8883 (89.93%)]\t\tLoss: 1.23655\n",
      "Training Progress: \tEpoch 31 [8320/8883 (93.53%)]\t\tLoss: 0.90014\n",
      "Training Progress: \tEpoch 31 [8640/8883 (97.12%)]\t\tLoss: 1.06518\n",
      "\tTrain loss: 0.02592, Accuracy: 5595/8883 (62.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1179/1692 (69.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 938/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/8883 (0.00%)]\t\tLoss: 1.03490\n",
      "Training Progress: \tEpoch 32 [320/8883 (3.60%)]\t\tLoss: 0.96306\n",
      "Training Progress: \tEpoch 32 [640/8883 (7.19%)]\t\tLoss: 0.92615\n",
      "Training Progress: \tEpoch 32 [960/8883 (10.79%)]\t\tLoss: 1.04380\n",
      "Training Progress: \tEpoch 32 [1280/8883 (14.39%)]\t\tLoss: 1.05101\n",
      "Training Progress: \tEpoch 32 [1600/8883 (17.99%)]\t\tLoss: 1.22637\n",
      "Training Progress: \tEpoch 32 [1920/8883 (21.58%)]\t\tLoss: 1.01926\n",
      "Training Progress: \tEpoch 32 [2240/8883 (25.18%)]\t\tLoss: 0.97565\n",
      "Training Progress: \tEpoch 32 [2560/8883 (28.78%)]\t\tLoss: 0.95763\n",
      "Training Progress: \tEpoch 32 [2880/8883 (32.37%)]\t\tLoss: 1.12985\n",
      "Training Progress: \tEpoch 32 [3200/8883 (35.97%)]\t\tLoss: 1.04731\n",
      "Training Progress: \tEpoch 32 [3520/8883 (39.57%)]\t\tLoss: 1.37232\n",
      "Training Progress: \tEpoch 32 [3840/8883 (43.17%)]\t\tLoss: 1.26773\n",
      "Training Progress: \tEpoch 32 [4160/8883 (46.76%)]\t\tLoss: 1.14664\n",
      "Training Progress: \tEpoch 32 [4480/8883 (50.36%)]\t\tLoss: 0.92234\n",
      "Training Progress: \tEpoch 32 [4800/8883 (53.96%)]\t\tLoss: 0.89020\n",
      "Training Progress: \tEpoch 32 [5120/8883 (57.55%)]\t\tLoss: 1.25029\n",
      "Training Progress: \tEpoch 32 [5440/8883 (61.15%)]\t\tLoss: 1.19918\n",
      "Training Progress: \tEpoch 32 [5760/8883 (64.75%)]\t\tLoss: 1.03155\n",
      "Training Progress: \tEpoch 32 [6080/8883 (68.35%)]\t\tLoss: 0.87235\n",
      "Training Progress: \tEpoch 32 [6400/8883 (71.94%)]\t\tLoss: 0.96738\n",
      "Training Progress: \tEpoch 32 [6720/8883 (75.54%)]\t\tLoss: 1.03799\n",
      "Training Progress: \tEpoch 32 [7040/8883 (79.14%)]\t\tLoss: 1.09779\n",
      "Training Progress: \tEpoch 32 [7360/8883 (82.73%)]\t\tLoss: 1.04681\n",
      "Training Progress: \tEpoch 32 [7680/8883 (86.33%)]\t\tLoss: 0.89315\n",
      "Training Progress: \tEpoch 32 [8000/8883 (89.93%)]\t\tLoss: 1.09485\n",
      "Training Progress: \tEpoch 32 [8320/8883 (93.53%)]\t\tLoss: 0.94732\n",
      "Training Progress: \tEpoch 32 [8640/8883 (97.12%)]\t\tLoss: 1.04198\n",
      "\tTrain loss: 0.02554, Accuracy: 5633/8883 (63.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1175/1692 (69.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 934/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/8883 (0.00%)]\t\tLoss: 0.87601\n",
      "Training Progress: \tEpoch 33 [320/8883 (3.60%)]\t\tLoss: 0.97479\n",
      "Training Progress: \tEpoch 33 [640/8883 (7.19%)]\t\tLoss: 1.08583\n",
      "Training Progress: \tEpoch 33 [960/8883 (10.79%)]\t\tLoss: 0.89810\n",
      "Training Progress: \tEpoch 33 [1280/8883 (14.39%)]\t\tLoss: 0.89994\n",
      "Training Progress: \tEpoch 33 [1600/8883 (17.99%)]\t\tLoss: 1.19482\n",
      "Training Progress: \tEpoch 33 [1920/8883 (21.58%)]\t\tLoss: 1.29845\n",
      "Training Progress: \tEpoch 33 [2240/8883 (25.18%)]\t\tLoss: 0.97819\n",
      "Training Progress: \tEpoch 33 [2560/8883 (28.78%)]\t\tLoss: 1.01933\n",
      "Training Progress: \tEpoch 33 [2880/8883 (32.37%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 33 [3200/8883 (35.97%)]\t\tLoss: 1.12125\n",
      "Training Progress: \tEpoch 33 [3520/8883 (39.57%)]\t\tLoss: 1.11527\n",
      "Training Progress: \tEpoch 33 [3840/8883 (43.17%)]\t\tLoss: 1.01241\n",
      "Training Progress: \tEpoch 33 [4160/8883 (46.76%)]\t\tLoss: 1.03185\n",
      "Training Progress: \tEpoch 33 [4480/8883 (50.36%)]\t\tLoss: 0.92711\n",
      "Training Progress: \tEpoch 33 [4800/8883 (53.96%)]\t\tLoss: 0.93828\n",
      "Training Progress: \tEpoch 33 [5120/8883 (57.55%)]\t\tLoss: 0.97036\n",
      "Training Progress: \tEpoch 33 [5440/8883 (61.15%)]\t\tLoss: 1.34945\n",
      "Training Progress: \tEpoch 33 [5760/8883 (64.75%)]\t\tLoss: 1.12645\n",
      "Training Progress: \tEpoch 33 [6080/8883 (68.35%)]\t\tLoss: 0.82261\n",
      "Training Progress: \tEpoch 33 [6400/8883 (71.94%)]\t\tLoss: 1.21794\n",
      "Training Progress: \tEpoch 33 [6720/8883 (75.54%)]\t\tLoss: 1.17391\n",
      "Training Progress: \tEpoch 33 [7040/8883 (79.14%)]\t\tLoss: 0.87767\n",
      "Training Progress: \tEpoch 33 [7360/8883 (82.73%)]\t\tLoss: 1.08859\n",
      "Training Progress: \tEpoch 33 [7680/8883 (86.33%)]\t\tLoss: 0.94404\n",
      "Training Progress: \tEpoch 33 [8000/8883 (89.93%)]\t\tLoss: 1.17229\n",
      "Training Progress: \tEpoch 33 [8320/8883 (93.53%)]\t\tLoss: 1.02625\n",
      "Training Progress: \tEpoch 33 [8640/8883 (97.12%)]\t\tLoss: 1.00541\n",
      "\tTrain loss: 0.02578, Accuracy: 5680/8883 (63.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 950/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/8883 (0.00%)]\t\tLoss: 1.11873\n",
      "Training Progress: \tEpoch 34 [320/8883 (3.60%)]\t\tLoss: 0.83845\n",
      "Training Progress: \tEpoch 34 [640/8883 (7.19%)]\t\tLoss: 1.04083\n",
      "Training Progress: \tEpoch 34 [960/8883 (10.79%)]\t\tLoss: 0.95417\n",
      "Training Progress: \tEpoch 34 [1280/8883 (14.39%)]\t\tLoss: 0.94333\n",
      "Training Progress: \tEpoch 34 [1600/8883 (17.99%)]\t\tLoss: 0.98583\n",
      "Training Progress: \tEpoch 34 [1920/8883 (21.58%)]\t\tLoss: 1.12171\n",
      "Training Progress: \tEpoch 34 [2240/8883 (25.18%)]\t\tLoss: 1.16146\n",
      "Training Progress: \tEpoch 34 [2560/8883 (28.78%)]\t\tLoss: 1.01275\n",
      "Training Progress: \tEpoch 34 [2880/8883 (32.37%)]\t\tLoss: 1.00852\n",
      "Training Progress: \tEpoch 34 [3200/8883 (35.97%)]\t\tLoss: 1.08741\n",
      "Training Progress: \tEpoch 34 [3520/8883 (39.57%)]\t\tLoss: 1.18475\n",
      "Training Progress: \tEpoch 34 [3840/8883 (43.17%)]\t\tLoss: 1.11137\n",
      "Training Progress: \tEpoch 34 [4160/8883 (46.76%)]\t\tLoss: 1.05785\n",
      "Training Progress: \tEpoch 34 [4480/8883 (50.36%)]\t\tLoss: 0.93129\n",
      "Training Progress: \tEpoch 34 [4800/8883 (53.96%)]\t\tLoss: 0.96416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 34 [5120/8883 (57.55%)]\t\tLoss: 1.08786\n",
      "Training Progress: \tEpoch 34 [5440/8883 (61.15%)]\t\tLoss: 1.21033\n",
      "Training Progress: \tEpoch 34 [5760/8883 (64.75%)]\t\tLoss: 1.21139\n",
      "Training Progress: \tEpoch 34 [6080/8883 (68.35%)]\t\tLoss: 0.96498\n",
      "Training Progress: \tEpoch 34 [6400/8883 (71.94%)]\t\tLoss: 0.89670\n",
      "Training Progress: \tEpoch 34 [6720/8883 (75.54%)]\t\tLoss: 1.19122\n",
      "Training Progress: \tEpoch 34 [7040/8883 (79.14%)]\t\tLoss: 1.03602\n",
      "Training Progress: \tEpoch 34 [7360/8883 (82.73%)]\t\tLoss: 1.26906\n",
      "Training Progress: \tEpoch 34 [7680/8883 (86.33%)]\t\tLoss: 0.94713\n",
      "Training Progress: \tEpoch 34 [8000/8883 (89.93%)]\t\tLoss: 0.97452\n",
      "Training Progress: \tEpoch 34 [8320/8883 (93.53%)]\t\tLoss: 1.06536\n",
      "Training Progress: \tEpoch 34 [8640/8883 (97.12%)]\t\tLoss: 1.14797\n",
      "\tTrain loss: 0.02535, Accuracy: 5682/8883 (63.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1192/1692 (70.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 941/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/8883 (0.00%)]\t\tLoss: 0.95358\n",
      "Training Progress: \tEpoch 35 [320/8883 (3.60%)]\t\tLoss: 0.82011\n",
      "Training Progress: \tEpoch 35 [640/8883 (7.19%)]\t\tLoss: 0.92576\n",
      "Training Progress: \tEpoch 35 [960/8883 (10.79%)]\t\tLoss: 1.04918\n",
      "Training Progress: \tEpoch 35 [1280/8883 (14.39%)]\t\tLoss: 1.30114\n",
      "Training Progress: \tEpoch 35 [1600/8883 (17.99%)]\t\tLoss: 1.16396\n",
      "Training Progress: \tEpoch 35 [1920/8883 (21.58%)]\t\tLoss: 1.20428\n",
      "Training Progress: \tEpoch 35 [2240/8883 (25.18%)]\t\tLoss: 1.05669\n",
      "Training Progress: \tEpoch 35 [2560/8883 (28.78%)]\t\tLoss: 1.12994\n",
      "Training Progress: \tEpoch 35 [2880/8883 (32.37%)]\t\tLoss: 1.21920\n",
      "Training Progress: \tEpoch 35 [3200/8883 (35.97%)]\t\tLoss: 1.01646\n",
      "Training Progress: \tEpoch 35 [3520/8883 (39.57%)]\t\tLoss: 1.14441\n",
      "Training Progress: \tEpoch 35 [3840/8883 (43.17%)]\t\tLoss: 1.07980\n",
      "Training Progress: \tEpoch 35 [4160/8883 (46.76%)]\t\tLoss: 1.05786\n",
      "Training Progress: \tEpoch 35 [4480/8883 (50.36%)]\t\tLoss: 0.89931\n",
      "Training Progress: \tEpoch 35 [4800/8883 (53.96%)]\t\tLoss: 0.99925\n",
      "Training Progress: \tEpoch 35 [5120/8883 (57.55%)]\t\tLoss: 1.19697\n",
      "Training Progress: \tEpoch 35 [5440/8883 (61.15%)]\t\tLoss: 1.37817\n",
      "Training Progress: \tEpoch 35 [5760/8883 (64.75%)]\t\tLoss: 1.16332\n",
      "Training Progress: \tEpoch 35 [6080/8883 (68.35%)]\t\tLoss: 0.91085\n",
      "Training Progress: \tEpoch 35 [6400/8883 (71.94%)]\t\tLoss: 0.95078\n",
      "Training Progress: \tEpoch 35 [6720/8883 (75.54%)]\t\tLoss: 1.22285\n",
      "Training Progress: \tEpoch 35 [7040/8883 (79.14%)]\t\tLoss: 0.81151\n",
      "Training Progress: \tEpoch 35 [7360/8883 (82.73%)]\t\tLoss: 0.99290\n",
      "Training Progress: \tEpoch 35 [7680/8883 (86.33%)]\t\tLoss: 1.07458\n",
      "Training Progress: \tEpoch 35 [8000/8883 (89.93%)]\t\tLoss: 1.15369\n",
      "Training Progress: \tEpoch 35 [8320/8883 (93.53%)]\t\tLoss: 0.99036\n",
      "Training Progress: \tEpoch 35 [8640/8883 (97.12%)]\t\tLoss: 1.16654\n",
      "\tTrain loss: 0.02531, Accuracy: 5752/8883 (64.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 936/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/8883 (0.00%)]\t\tLoss: 1.03612\n",
      "Training Progress: \tEpoch 36 [320/8883 (3.60%)]\t\tLoss: 1.10687\n",
      "Training Progress: \tEpoch 36 [640/8883 (7.19%)]\t\tLoss: 0.86257\n",
      "Training Progress: \tEpoch 36 [960/8883 (10.79%)]\t\tLoss: 0.91236\n",
      "Training Progress: \tEpoch 36 [1280/8883 (14.39%)]\t\tLoss: 1.15135\n",
      "Training Progress: \tEpoch 36 [1600/8883 (17.99%)]\t\tLoss: 1.12939\n",
      "Training Progress: \tEpoch 36 [1920/8883 (21.58%)]\t\tLoss: 0.94509\n",
      "Training Progress: \tEpoch 36 [2240/8883 (25.18%)]\t\tLoss: 1.02374\n",
      "Training Progress: \tEpoch 36 [2560/8883 (28.78%)]\t\tLoss: 1.01679\n",
      "Training Progress: \tEpoch 36 [2880/8883 (32.37%)]\t\tLoss: 1.07082\n",
      "Training Progress: \tEpoch 36 [3200/8883 (35.97%)]\t\tLoss: 1.03908\n",
      "Training Progress: \tEpoch 36 [3520/8883 (39.57%)]\t\tLoss: 1.10071\n",
      "Training Progress: \tEpoch 36 [3840/8883 (43.17%)]\t\tLoss: 1.28414\n",
      "Training Progress: \tEpoch 36 [4160/8883 (46.76%)]\t\tLoss: 1.04426\n",
      "Training Progress: \tEpoch 36 [4480/8883 (50.36%)]\t\tLoss: 0.86951\n",
      "Training Progress: \tEpoch 36 [4800/8883 (53.96%)]\t\tLoss: 0.89884\n",
      "Training Progress: \tEpoch 36 [5120/8883 (57.55%)]\t\tLoss: 1.10778\n",
      "Training Progress: \tEpoch 36 [5440/8883 (61.15%)]\t\tLoss: 1.03948\n",
      "Training Progress: \tEpoch 36 [5760/8883 (64.75%)]\t\tLoss: 1.09817\n",
      "Training Progress: \tEpoch 36 [6080/8883 (68.35%)]\t\tLoss: 0.92000\n",
      "Training Progress: \tEpoch 36 [6400/8883 (71.94%)]\t\tLoss: 1.00776\n",
      "Training Progress: \tEpoch 36 [6720/8883 (75.54%)]\t\tLoss: 1.45001\n",
      "Training Progress: \tEpoch 36 [7040/8883 (79.14%)]\t\tLoss: 0.87146\n",
      "Training Progress: \tEpoch 36 [7360/8883 (82.73%)]\t\tLoss: 1.12884\n",
      "Training Progress: \tEpoch 36 [7680/8883 (86.33%)]\t\tLoss: 0.94854\n",
      "Training Progress: \tEpoch 36 [8000/8883 (89.93%)]\t\tLoss: 1.21697\n",
      "Training Progress: \tEpoch 36 [8320/8883 (93.53%)]\t\tLoss: 0.83200\n",
      "Training Progress: \tEpoch 36 [8640/8883 (97.12%)]\t\tLoss: 1.16021\n",
      "\tTrain loss: 0.02452, Accuracy: 5849/8883 (65.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1216/1692 (71.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 948/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/8883 (0.00%)]\t\tLoss: 1.01022\n",
      "Training Progress: \tEpoch 37 [320/8883 (3.60%)]\t\tLoss: 0.94147\n",
      "Training Progress: \tEpoch 37 [640/8883 (7.19%)]\t\tLoss: 1.03630\n",
      "Training Progress: \tEpoch 37 [960/8883 (10.79%)]\t\tLoss: 0.92276\n",
      "Training Progress: \tEpoch 37 [1280/8883 (14.39%)]\t\tLoss: 1.11999\n",
      "Training Progress: \tEpoch 37 [1600/8883 (17.99%)]\t\tLoss: 1.23791\n",
      "Training Progress: \tEpoch 37 [1920/8883 (21.58%)]\t\tLoss: 1.04023\n",
      "Training Progress: \tEpoch 37 [2240/8883 (25.18%)]\t\tLoss: 1.00595\n",
      "Training Progress: \tEpoch 37 [2560/8883 (28.78%)]\t\tLoss: 0.97405\n",
      "Training Progress: \tEpoch 37 [2880/8883 (32.37%)]\t\tLoss: 0.99539\n",
      "Training Progress: \tEpoch 37 [3200/8883 (35.97%)]\t\tLoss: 0.96741\n",
      "Training Progress: \tEpoch 37 [3520/8883 (39.57%)]\t\tLoss: 1.07459\n",
      "Training Progress: \tEpoch 37 [3840/8883 (43.17%)]\t\tLoss: 0.98431\n",
      "Training Progress: \tEpoch 37 [4160/8883 (46.76%)]\t\tLoss: 1.03804\n",
      "Training Progress: \tEpoch 37 [4480/8883 (50.36%)]\t\tLoss: 0.96488\n",
      "Training Progress: \tEpoch 37 [4800/8883 (53.96%)]\t\tLoss: 0.86193\n",
      "Training Progress: \tEpoch 37 [5120/8883 (57.55%)]\t\tLoss: 1.00959\n",
      "Training Progress: \tEpoch 37 [5440/8883 (61.15%)]\t\tLoss: 1.12603\n",
      "Training Progress: \tEpoch 37 [5760/8883 (64.75%)]\t\tLoss: 1.01964\n",
      "Training Progress: \tEpoch 37 [6080/8883 (68.35%)]\t\tLoss: 1.01894\n",
      "Training Progress: \tEpoch 37 [6400/8883 (71.94%)]\t\tLoss: 1.10446\n",
      "Training Progress: \tEpoch 37 [6720/8883 (75.54%)]\t\tLoss: 1.13540\n",
      "Training Progress: \tEpoch 37 [7040/8883 (79.14%)]\t\tLoss: 1.10681\n",
      "Training Progress: \tEpoch 37 [7360/8883 (82.73%)]\t\tLoss: 0.95135\n",
      "Training Progress: \tEpoch 37 [7680/8883 (86.33%)]\t\tLoss: 0.96731\n",
      "Training Progress: \tEpoch 37 [8000/8883 (89.93%)]\t\tLoss: 1.07614\n",
      "Training Progress: \tEpoch 37 [8320/8883 (93.53%)]\t\tLoss: 0.92125\n",
      "Training Progress: \tEpoch 37 [8640/8883 (97.12%)]\t\tLoss: 1.00579\n",
      "\tTrain loss: 0.02452, Accuracy: 5811/8883 (65.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1209/1692 (71.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 962/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/8883 (0.00%)]\t\tLoss: 0.94767\n",
      "Training Progress: \tEpoch 38 [320/8883 (3.60%)]\t\tLoss: 0.93086\n",
      "Training Progress: \tEpoch 38 [640/8883 (7.19%)]\t\tLoss: 0.82977\n",
      "Training Progress: \tEpoch 38 [960/8883 (10.79%)]\t\tLoss: 0.98769\n",
      "Training Progress: \tEpoch 38 [1280/8883 (14.39%)]\t\tLoss: 0.98427\n",
      "Training Progress: \tEpoch 38 [1600/8883 (17.99%)]\t\tLoss: 1.08105\n",
      "Training Progress: \tEpoch 38 [1920/8883 (21.58%)]\t\tLoss: 0.98732\n",
      "Training Progress: \tEpoch 38 [2240/8883 (25.18%)]\t\tLoss: 1.04134\n",
      "Training Progress: \tEpoch 38 [2560/8883 (28.78%)]\t\tLoss: 1.19949\n",
      "Training Progress: \tEpoch 38 [2880/8883 (32.37%)]\t\tLoss: 1.01731\n",
      "Training Progress: \tEpoch 38 [3200/8883 (35.97%)]\t\tLoss: 1.16118\n",
      "Training Progress: \tEpoch 38 [3520/8883 (39.57%)]\t\tLoss: 0.97338\n",
      "Training Progress: \tEpoch 38 [3840/8883 (43.17%)]\t\tLoss: 1.17257\n",
      "Training Progress: \tEpoch 38 [4160/8883 (46.76%)]\t\tLoss: 0.86929\n",
      "Training Progress: \tEpoch 38 [4480/8883 (50.36%)]\t\tLoss: 1.02673\n",
      "Training Progress: \tEpoch 38 [4800/8883 (53.96%)]\t\tLoss: 0.81857\n",
      "Training Progress: \tEpoch 38 [5120/8883 (57.55%)]\t\tLoss: 1.16009\n",
      "Training Progress: \tEpoch 38 [5440/8883 (61.15%)]\t\tLoss: 1.10610\n",
      "Training Progress: \tEpoch 38 [5760/8883 (64.75%)]\t\tLoss: 1.13495\n",
      "Training Progress: \tEpoch 38 [6080/8883 (68.35%)]\t\tLoss: 1.10747\n",
      "Training Progress: \tEpoch 38 [6400/8883 (71.94%)]\t\tLoss: 1.07085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 38 [6720/8883 (75.54%)]\t\tLoss: 0.94026\n",
      "Training Progress: \tEpoch 38 [7040/8883 (79.14%)]\t\tLoss: 0.95242\n",
      "Training Progress: \tEpoch 38 [7360/8883 (82.73%)]\t\tLoss: 0.95746\n",
      "Training Progress: \tEpoch 38 [7680/8883 (86.33%)]\t\tLoss: 0.97402\n",
      "Training Progress: \tEpoch 38 [8000/8883 (89.93%)]\t\tLoss: 1.24543\n",
      "Training Progress: \tEpoch 38 [8320/8883 (93.53%)]\t\tLoss: 0.88527\n",
      "Training Progress: \tEpoch 38 [8640/8883 (97.12%)]\t\tLoss: 1.04975\n",
      "\tTrain loss: 0.02460, Accuracy: 5917/8883 (66.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 932/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/8883 (0.00%)]\t\tLoss: 1.01395\n",
      "Training Progress: \tEpoch 39 [320/8883 (3.60%)]\t\tLoss: 0.78659\n",
      "Training Progress: \tEpoch 39 [640/8883 (7.19%)]\t\tLoss: 0.97772\n",
      "Training Progress: \tEpoch 39 [960/8883 (10.79%)]\t\tLoss: 0.96769\n",
      "Training Progress: \tEpoch 39 [1280/8883 (14.39%)]\t\tLoss: 1.02498\n",
      "Training Progress: \tEpoch 39 [1600/8883 (17.99%)]\t\tLoss: 1.07448\n",
      "Training Progress: \tEpoch 39 [1920/8883 (21.58%)]\t\tLoss: 1.08889\n",
      "Training Progress: \tEpoch 39 [2240/8883 (25.18%)]\t\tLoss: 1.11289\n",
      "Training Progress: \tEpoch 39 [2560/8883 (28.78%)]\t\tLoss: 0.86837\n",
      "Training Progress: \tEpoch 39 [2880/8883 (32.37%)]\t\tLoss: 1.06857\n",
      "Training Progress: \tEpoch 39 [3200/8883 (35.97%)]\t\tLoss: 1.04240\n",
      "Training Progress: \tEpoch 39 [3520/8883 (39.57%)]\t\tLoss: 1.12650\n",
      "Training Progress: \tEpoch 39 [3840/8883 (43.17%)]\t\tLoss: 1.14890\n",
      "Training Progress: \tEpoch 39 [4160/8883 (46.76%)]\t\tLoss: 1.01460\n",
      "Training Progress: \tEpoch 39 [4480/8883 (50.36%)]\t\tLoss: 0.88380\n",
      "Training Progress: \tEpoch 39 [4800/8883 (53.96%)]\t\tLoss: 0.89992\n",
      "Training Progress: \tEpoch 39 [5120/8883 (57.55%)]\t\tLoss: 1.00113\n",
      "Training Progress: \tEpoch 39 [5440/8883 (61.15%)]\t\tLoss: 1.20764\n",
      "Training Progress: \tEpoch 39 [5760/8883 (64.75%)]\t\tLoss: 1.07343\n",
      "Training Progress: \tEpoch 39 [6080/8883 (68.35%)]\t\tLoss: 0.92198\n",
      "Training Progress: \tEpoch 39 [6400/8883 (71.94%)]\t\tLoss: 1.20153\n",
      "Training Progress: \tEpoch 39 [6720/8883 (75.54%)]\t\tLoss: 0.98311\n",
      "Training Progress: \tEpoch 39 [7040/8883 (79.14%)]\t\tLoss: 0.85488\n",
      "Training Progress: \tEpoch 39 [7360/8883 (82.73%)]\t\tLoss: 0.93737\n",
      "Training Progress: \tEpoch 39 [7680/8883 (86.33%)]\t\tLoss: 0.99741\n",
      "Training Progress: \tEpoch 39 [8000/8883 (89.93%)]\t\tLoss: 0.86451\n",
      "Training Progress: \tEpoch 39 [8320/8883 (93.53%)]\t\tLoss: 0.93216\n",
      "Training Progress: \tEpoch 39 [8640/8883 (97.12%)]\t\tLoss: 1.05080\n",
      "\tTrain loss: 0.02358, Accuracy: 6049/8883 (68.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1265/1692 (74.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 942/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/8883 (0.00%)]\t\tLoss: 0.84688\n",
      "Training Progress: \tEpoch 40 [320/8883 (3.60%)]\t\tLoss: 0.74491\n",
      "Training Progress: \tEpoch 40 [640/8883 (7.19%)]\t\tLoss: 0.90101\n",
      "Training Progress: \tEpoch 40 [960/8883 (10.79%)]\t\tLoss: 0.91468\n",
      "Training Progress: \tEpoch 40 [1280/8883 (14.39%)]\t\tLoss: 0.92490\n",
      "Training Progress: \tEpoch 40 [1600/8883 (17.99%)]\t\tLoss: 1.21367\n",
      "Training Progress: \tEpoch 40 [1920/8883 (21.58%)]\t\tLoss: 0.87294\n",
      "Training Progress: \tEpoch 40 [2240/8883 (25.18%)]\t\tLoss: 0.94952\n",
      "Training Progress: \tEpoch 40 [2560/8883 (28.78%)]\t\tLoss: 0.98989\n",
      "Training Progress: \tEpoch 40 [2880/8883 (32.37%)]\t\tLoss: 0.94296\n",
      "Training Progress: \tEpoch 40 [3200/8883 (35.97%)]\t\tLoss: 1.14231\n",
      "Training Progress: \tEpoch 40 [3520/8883 (39.57%)]\t\tLoss: 1.05628\n",
      "Training Progress: \tEpoch 40 [3840/8883 (43.17%)]\t\tLoss: 0.99255\n",
      "Training Progress: \tEpoch 40 [4160/8883 (46.76%)]\t\tLoss: 0.92448\n",
      "Training Progress: \tEpoch 40 [4480/8883 (50.36%)]\t\tLoss: 0.94916\n",
      "Training Progress: \tEpoch 40 [4800/8883 (53.96%)]\t\tLoss: 0.83286\n",
      "Training Progress: \tEpoch 40 [5120/8883 (57.55%)]\t\tLoss: 1.31223\n",
      "Training Progress: \tEpoch 40 [5440/8883 (61.15%)]\t\tLoss: 1.05631\n",
      "Training Progress: \tEpoch 40 [5760/8883 (64.75%)]\t\tLoss: 1.06200\n",
      "Training Progress: \tEpoch 40 [6080/8883 (68.35%)]\t\tLoss: 0.77678\n",
      "Training Progress: \tEpoch 40 [6400/8883 (71.94%)]\t\tLoss: 1.02000\n",
      "Training Progress: \tEpoch 40 [6720/8883 (75.54%)]\t\tLoss: 0.98701\n",
      "Training Progress: \tEpoch 40 [7040/8883 (79.14%)]\t\tLoss: 0.86845\n",
      "Training Progress: \tEpoch 40 [7360/8883 (82.73%)]\t\tLoss: 1.00386\n",
      "Training Progress: \tEpoch 40 [7680/8883 (86.33%)]\t\tLoss: 0.90832\n",
      "Training Progress: \tEpoch 40 [8000/8883 (89.93%)]\t\tLoss: 0.91559\n",
      "Training Progress: \tEpoch 40 [8320/8883 (93.53%)]\t\tLoss: 1.15762\n",
      "Training Progress: \tEpoch 40 [8640/8883 (97.12%)]\t\tLoss: 0.93797\n",
      "\tTrain loss: 0.02427, Accuracy: 5920/8883 (66.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1241/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 939/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/8883 (0.00%)]\t\tLoss: 0.86656\n",
      "Training Progress: \tEpoch 41 [320/8883 (3.60%)]\t\tLoss: 0.90582\n",
      "Training Progress: \tEpoch 41 [640/8883 (7.19%)]\t\tLoss: 0.84635\n",
      "Training Progress: \tEpoch 41 [960/8883 (10.79%)]\t\tLoss: 0.95061\n",
      "Training Progress: \tEpoch 41 [1280/8883 (14.39%)]\t\tLoss: 1.08849\n",
      "Training Progress: \tEpoch 41 [1600/8883 (17.99%)]\t\tLoss: 1.23370\n",
      "Training Progress: \tEpoch 41 [1920/8883 (21.58%)]\t\tLoss: 0.97644\n",
      "Training Progress: \tEpoch 41 [2240/8883 (25.18%)]\t\tLoss: 0.93554\n",
      "Training Progress: \tEpoch 41 [2560/8883 (28.78%)]\t\tLoss: 0.90527\n",
      "Training Progress: \tEpoch 41 [2880/8883 (32.37%)]\t\tLoss: 1.27249\n",
      "Training Progress: \tEpoch 41 [3200/8883 (35.97%)]\t\tLoss: 1.05702\n",
      "Training Progress: \tEpoch 41 [3520/8883 (39.57%)]\t\tLoss: 1.15243\n",
      "Training Progress: \tEpoch 41 [3840/8883 (43.17%)]\t\tLoss: 1.20740\n",
      "Training Progress: \tEpoch 41 [4160/8883 (46.76%)]\t\tLoss: 1.03134\n",
      "Training Progress: \tEpoch 41 [4480/8883 (50.36%)]\t\tLoss: 1.10314\n",
      "Training Progress: \tEpoch 41 [4800/8883 (53.96%)]\t\tLoss: 0.95933\n",
      "Training Progress: \tEpoch 41 [5120/8883 (57.55%)]\t\tLoss: 1.13510\n",
      "Training Progress: \tEpoch 41 [5440/8883 (61.15%)]\t\tLoss: 1.15741\n",
      "Training Progress: \tEpoch 41 [5760/8883 (64.75%)]\t\tLoss: 1.06376\n",
      "Training Progress: \tEpoch 41 [6080/8883 (68.35%)]\t\tLoss: 0.91028\n",
      "Training Progress: \tEpoch 41 [6400/8883 (71.94%)]\t\tLoss: 0.83482\n",
      "Training Progress: \tEpoch 41 [6720/8883 (75.54%)]\t\tLoss: 1.04993\n",
      "Training Progress: \tEpoch 41 [7040/8883 (79.14%)]\t\tLoss: 0.97245\n",
      "Training Progress: \tEpoch 41 [7360/8883 (82.73%)]\t\tLoss: 1.01822\n",
      "Training Progress: \tEpoch 41 [7680/8883 (86.33%)]\t\tLoss: 0.96116\n",
      "Training Progress: \tEpoch 41 [8000/8883 (89.93%)]\t\tLoss: 0.95675\n",
      "Training Progress: \tEpoch 41 [8320/8883 (93.53%)]\t\tLoss: 0.94214\n",
      "Training Progress: \tEpoch 41 [8640/8883 (97.12%)]\t\tLoss: 1.18324\n",
      "\tTrain loss: 0.02383, Accuracy: 5974/8883 (67.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 934/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/8883 (0.00%)]\t\tLoss: 0.83771\n",
      "Training Progress: \tEpoch 42 [320/8883 (3.60%)]\t\tLoss: 0.99500\n",
      "Training Progress: \tEpoch 42 [640/8883 (7.19%)]\t\tLoss: 1.02783\n",
      "Training Progress: \tEpoch 42 [960/8883 (10.79%)]\t\tLoss: 1.05093\n",
      "Training Progress: \tEpoch 42 [1280/8883 (14.39%)]\t\tLoss: 1.04182\n",
      "Training Progress: \tEpoch 42 [1600/8883 (17.99%)]\t\tLoss: 1.17117\n",
      "Training Progress: \tEpoch 42 [1920/8883 (21.58%)]\t\tLoss: 1.23821\n",
      "Training Progress: \tEpoch 42 [2240/8883 (25.18%)]\t\tLoss: 0.99019\n",
      "Training Progress: \tEpoch 42 [2560/8883 (28.78%)]\t\tLoss: 0.87449\n",
      "Training Progress: \tEpoch 42 [2880/8883 (32.37%)]\t\tLoss: 1.03603\n",
      "Training Progress: \tEpoch 42 [3200/8883 (35.97%)]\t\tLoss: 1.22339\n",
      "Training Progress: \tEpoch 42 [3520/8883 (39.57%)]\t\tLoss: 1.00673\n",
      "Training Progress: \tEpoch 42 [3840/8883 (43.17%)]\t\tLoss: 1.15610\n",
      "Training Progress: \tEpoch 42 [4160/8883 (46.76%)]\t\tLoss: 0.97499\n",
      "Training Progress: \tEpoch 42 [4480/8883 (50.36%)]\t\tLoss: 0.85803\n",
      "Training Progress: \tEpoch 42 [4800/8883 (53.96%)]\t\tLoss: 0.94051\n",
      "Training Progress: \tEpoch 42 [5120/8883 (57.55%)]\t\tLoss: 0.99298\n",
      "Training Progress: \tEpoch 42 [5440/8883 (61.15%)]\t\tLoss: 1.06501\n",
      "Training Progress: \tEpoch 42 [5760/8883 (64.75%)]\t\tLoss: 1.00010\n",
      "Training Progress: \tEpoch 42 [6080/8883 (68.35%)]\t\tLoss: 0.93303\n",
      "Training Progress: \tEpoch 42 [6400/8883 (71.94%)]\t\tLoss: 1.04380\n",
      "Training Progress: \tEpoch 42 [6720/8883 (75.54%)]\t\tLoss: 0.96018\n",
      "Training Progress: \tEpoch 42 [7040/8883 (79.14%)]\t\tLoss: 0.98950\n",
      "Training Progress: \tEpoch 42 [7360/8883 (82.73%)]\t\tLoss: 1.01653\n",
      "Training Progress: \tEpoch 42 [7680/8883 (86.33%)]\t\tLoss: 0.95499\n",
      "Training Progress: \tEpoch 42 [8000/8883 (89.93%)]\t\tLoss: 1.03322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [8320/8883 (93.53%)]\t\tLoss: 0.97016\n",
      "Training Progress: \tEpoch 42 [8640/8883 (97.12%)]\t\tLoss: 1.02488\n",
      "\tTrain loss: 0.02258, Accuracy: 6032/8883 (67.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1265/1692 (74.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 949/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/8883 (0.00%)]\t\tLoss: 1.00908\n",
      "Training Progress: \tEpoch 43 [320/8883 (3.60%)]\t\tLoss: 0.95330\n",
      "Training Progress: \tEpoch 43 [640/8883 (7.19%)]\t\tLoss: 0.96285\n",
      "Training Progress: \tEpoch 43 [960/8883 (10.79%)]\t\tLoss: 0.93575\n",
      "Training Progress: \tEpoch 43 [1280/8883 (14.39%)]\t\tLoss: 0.93523\n",
      "Training Progress: \tEpoch 43 [1600/8883 (17.99%)]\t\tLoss: 1.06046\n",
      "Training Progress: \tEpoch 43 [1920/8883 (21.58%)]\t\tLoss: 1.11792\n",
      "Training Progress: \tEpoch 43 [2240/8883 (25.18%)]\t\tLoss: 0.94800\n",
      "Training Progress: \tEpoch 43 [2560/8883 (28.78%)]\t\tLoss: 0.90375\n",
      "Training Progress: \tEpoch 43 [2880/8883 (32.37%)]\t\tLoss: 0.90369\n",
      "Training Progress: \tEpoch 43 [3200/8883 (35.97%)]\t\tLoss: 1.12282\n",
      "Training Progress: \tEpoch 43 [3520/8883 (39.57%)]\t\tLoss: 0.93520\n",
      "Training Progress: \tEpoch 43 [3840/8883 (43.17%)]\t\tLoss: 1.24655\n",
      "Training Progress: \tEpoch 43 [4160/8883 (46.76%)]\t\tLoss: 1.06085\n",
      "Training Progress: \tEpoch 43 [4480/8883 (50.36%)]\t\tLoss: 0.89282\n",
      "Training Progress: \tEpoch 43 [4800/8883 (53.96%)]\t\tLoss: 0.80997\n",
      "Training Progress: \tEpoch 43 [5120/8883 (57.55%)]\t\tLoss: 0.93583\n",
      "Training Progress: \tEpoch 43 [5440/8883 (61.15%)]\t\tLoss: 1.13072\n",
      "Training Progress: \tEpoch 43 [5760/8883 (64.75%)]\t\tLoss: 0.94950\n",
      "Training Progress: \tEpoch 43 [6080/8883 (68.35%)]\t\tLoss: 0.80428\n",
      "Training Progress: \tEpoch 43 [6400/8883 (71.94%)]\t\tLoss: 0.89300\n",
      "Training Progress: \tEpoch 43 [6720/8883 (75.54%)]\t\tLoss: 1.06316\n",
      "Training Progress: \tEpoch 43 [7040/8883 (79.14%)]\t\tLoss: 0.97380\n",
      "Training Progress: \tEpoch 43 [7360/8883 (82.73%)]\t\tLoss: 0.76222\n",
      "Training Progress: \tEpoch 43 [7680/8883 (86.33%)]\t\tLoss: 1.01081\n",
      "Training Progress: \tEpoch 43 [8000/8883 (89.93%)]\t\tLoss: 0.83067\n",
      "Training Progress: \tEpoch 43 [8320/8883 (93.53%)]\t\tLoss: 0.89050\n",
      "Training Progress: \tEpoch 43 [8640/8883 (97.12%)]\t\tLoss: 0.91411\n",
      "\tTrain loss: 0.02254, Accuracy: 6112/8883 (68.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1283/1692 (75.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 919/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/8883 (0.00%)]\t\tLoss: 0.96418\n",
      "Training Progress: \tEpoch 44 [320/8883 (3.60%)]\t\tLoss: 0.77765\n",
      "Training Progress: \tEpoch 44 [640/8883 (7.19%)]\t\tLoss: 1.15411\n",
      "Training Progress: \tEpoch 44 [960/8883 (10.79%)]\t\tLoss: 0.86812\n",
      "Training Progress: \tEpoch 44 [1280/8883 (14.39%)]\t\tLoss: 0.97697\n",
      "Training Progress: \tEpoch 44 [1600/8883 (17.99%)]\t\tLoss: 1.09554\n",
      "Training Progress: \tEpoch 44 [1920/8883 (21.58%)]\t\tLoss: 1.06069\n",
      "Training Progress: \tEpoch 44 [2240/8883 (25.18%)]\t\tLoss: 1.26303\n",
      "Training Progress: \tEpoch 44 [2560/8883 (28.78%)]\t\tLoss: 1.06651\n",
      "Training Progress: \tEpoch 44 [2880/8883 (32.37%)]\t\tLoss: 1.03998\n",
      "Training Progress: \tEpoch 44 [3200/8883 (35.97%)]\t\tLoss: 1.00522\n",
      "Training Progress: \tEpoch 44 [3520/8883 (39.57%)]\t\tLoss: 1.04051\n",
      "Training Progress: \tEpoch 44 [3840/8883 (43.17%)]\t\tLoss: 1.09159\n",
      "Training Progress: \tEpoch 44 [4160/8883 (46.76%)]\t\tLoss: 0.92198\n",
      "Training Progress: \tEpoch 44 [4480/8883 (50.36%)]\t\tLoss: 0.81754\n",
      "Training Progress: \tEpoch 44 [4800/8883 (53.96%)]\t\tLoss: 0.82514\n",
      "Training Progress: \tEpoch 44 [5120/8883 (57.55%)]\t\tLoss: 0.97535\n",
      "Training Progress: \tEpoch 44 [5440/8883 (61.15%)]\t\tLoss: 1.08668\n",
      "Training Progress: \tEpoch 44 [5760/8883 (64.75%)]\t\tLoss: 0.99645\n",
      "Training Progress: \tEpoch 44 [6080/8883 (68.35%)]\t\tLoss: 0.90378\n",
      "Training Progress: \tEpoch 44 [6400/8883 (71.94%)]\t\tLoss: 1.10760\n",
      "Training Progress: \tEpoch 44 [6720/8883 (75.54%)]\t\tLoss: 1.27319\n",
      "Training Progress: \tEpoch 44 [7040/8883 (79.14%)]\t\tLoss: 0.82005\n",
      "Training Progress: \tEpoch 44 [7360/8883 (82.73%)]\t\tLoss: 0.83095\n",
      "Training Progress: \tEpoch 44 [7680/8883 (86.33%)]\t\tLoss: 0.98580\n",
      "Training Progress: \tEpoch 44 [8000/8883 (89.93%)]\t\tLoss: 1.06805\n",
      "Training Progress: \tEpoch 44 [8320/8883 (93.53%)]\t\tLoss: 0.85110\n",
      "Training Progress: \tEpoch 44 [8640/8883 (97.12%)]\t\tLoss: 0.97258\n",
      "\tTrain loss: 0.02224, Accuracy: 6179/8883 (69.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1292/1692 (76.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 948/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/8883 (0.00%)]\t\tLoss: 0.92896\n",
      "Training Progress: \tEpoch 45 [320/8883 (3.60%)]\t\tLoss: 1.01132\n",
      "Training Progress: \tEpoch 45 [640/8883 (7.19%)]\t\tLoss: 0.96215\n",
      "Training Progress: \tEpoch 45 [960/8883 (10.79%)]\t\tLoss: 0.98900\n",
      "Training Progress: \tEpoch 45 [1280/8883 (14.39%)]\t\tLoss: 1.15230\n",
      "Training Progress: \tEpoch 45 [1600/8883 (17.99%)]\t\tLoss: 1.15255\n",
      "Training Progress: \tEpoch 45 [1920/8883 (21.58%)]\t\tLoss: 1.03953\n",
      "Training Progress: \tEpoch 45 [2240/8883 (25.18%)]\t\tLoss: 1.12582\n",
      "Training Progress: \tEpoch 45 [2560/8883 (28.78%)]\t\tLoss: 0.91660\n",
      "Training Progress: \tEpoch 45 [2880/8883 (32.37%)]\t\tLoss: 1.06299\n",
      "Training Progress: \tEpoch 45 [3200/8883 (35.97%)]\t\tLoss: 1.07706\n",
      "Training Progress: \tEpoch 45 [3520/8883 (39.57%)]\t\tLoss: 1.15252\n",
      "Training Progress: \tEpoch 45 [3840/8883 (43.17%)]\t\tLoss: 1.21270\n",
      "Training Progress: \tEpoch 45 [4160/8883 (46.76%)]\t\tLoss: 1.07279\n",
      "Training Progress: \tEpoch 45 [4480/8883 (50.36%)]\t\tLoss: 0.90400\n",
      "Training Progress: \tEpoch 45 [4800/8883 (53.96%)]\t\tLoss: 0.77633\n",
      "Training Progress: \tEpoch 45 [5120/8883 (57.55%)]\t\tLoss: 0.99633\n",
      "Training Progress: \tEpoch 45 [5440/8883 (61.15%)]\t\tLoss: 1.11045\n",
      "Training Progress: \tEpoch 45 [5760/8883 (64.75%)]\t\tLoss: 1.00139\n",
      "Training Progress: \tEpoch 45 [6080/8883 (68.35%)]\t\tLoss: 0.97256\n",
      "Training Progress: \tEpoch 45 [6400/8883 (71.94%)]\t\tLoss: 1.00852\n",
      "Training Progress: \tEpoch 45 [6720/8883 (75.54%)]\t\tLoss: 0.93170\n",
      "Training Progress: \tEpoch 45 [7040/8883 (79.14%)]\t\tLoss: 1.00613\n",
      "Training Progress: \tEpoch 45 [7360/8883 (82.73%)]\t\tLoss: 1.07649\n",
      "Training Progress: \tEpoch 45 [7680/8883 (86.33%)]\t\tLoss: 0.90561\n",
      "Training Progress: \tEpoch 45 [8000/8883 (89.93%)]\t\tLoss: 1.20072\n",
      "Training Progress: \tEpoch 45 [8320/8883 (93.53%)]\t\tLoss: 0.75825\n",
      "Training Progress: \tEpoch 45 [8640/8883 (97.12%)]\t\tLoss: 0.98669\n",
      "\tTrain loss: 0.02221, Accuracy: 6250/8883 (70.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1286/1692 (76.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 943/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/8883 (0.00%)]\t\tLoss: 1.04876\n",
      "Training Progress: \tEpoch 46 [320/8883 (3.60%)]\t\tLoss: 0.80593\n",
      "Training Progress: \tEpoch 46 [640/8883 (7.19%)]\t\tLoss: 0.95499\n",
      "Training Progress: \tEpoch 46 [960/8883 (10.79%)]\t\tLoss: 0.89718\n",
      "Training Progress: \tEpoch 46 [1280/8883 (14.39%)]\t\tLoss: 1.16576\n",
      "Training Progress: \tEpoch 46 [1600/8883 (17.99%)]\t\tLoss: 1.13418\n",
      "Training Progress: \tEpoch 46 [1920/8883 (21.58%)]\t\tLoss: 0.99182\n",
      "Training Progress: \tEpoch 46 [2240/8883 (25.18%)]\t\tLoss: 1.06708\n",
      "Training Progress: \tEpoch 46 [2560/8883 (28.78%)]\t\tLoss: 0.98941\n",
      "Training Progress: \tEpoch 46 [2880/8883 (32.37%)]\t\tLoss: 0.99504\n",
      "Training Progress: \tEpoch 46 [3200/8883 (35.97%)]\t\tLoss: 1.15003\n",
      "Training Progress: \tEpoch 46 [3520/8883 (39.57%)]\t\tLoss: 0.89699\n",
      "Training Progress: \tEpoch 46 [3840/8883 (43.17%)]\t\tLoss: 1.09498\n",
      "Training Progress: \tEpoch 46 [4160/8883 (46.76%)]\t\tLoss: 0.91007\n",
      "Training Progress: \tEpoch 46 [4480/8883 (50.36%)]\t\tLoss: 0.77076\n",
      "Training Progress: \tEpoch 46 [4800/8883 (53.96%)]\t\tLoss: 0.73889\n",
      "Training Progress: \tEpoch 46 [5120/8883 (57.55%)]\t\tLoss: 1.03986\n",
      "Training Progress: \tEpoch 46 [5440/8883 (61.15%)]\t\tLoss: 1.12130\n",
      "Training Progress: \tEpoch 46 [5760/8883 (64.75%)]\t\tLoss: 0.99305\n",
      "Training Progress: \tEpoch 46 [6080/8883 (68.35%)]\t\tLoss: 0.85949\n",
      "Training Progress: \tEpoch 46 [6400/8883 (71.94%)]\t\tLoss: 0.99516\n",
      "Training Progress: \tEpoch 46 [6720/8883 (75.54%)]\t\tLoss: 1.02623\n",
      "Training Progress: \tEpoch 46 [7040/8883 (79.14%)]\t\tLoss: 0.80915\n",
      "Training Progress: \tEpoch 46 [7360/8883 (82.73%)]\t\tLoss: 0.91409\n",
      "Training Progress: \tEpoch 46 [7680/8883 (86.33%)]\t\tLoss: 0.84285\n",
      "Training Progress: \tEpoch 46 [8000/8883 (89.93%)]\t\tLoss: 0.95397\n",
      "Training Progress: \tEpoch 46 [8320/8883 (93.53%)]\t\tLoss: 0.92217\n",
      "Training Progress: \tEpoch 46 [8640/8883 (97.12%)]\t\tLoss: 0.87552\n",
      "\tTrain loss: 0.02170, Accuracy: 6166/8883 (69.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1290/1692 (76.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/8883 (0.00%)]\t\tLoss: 1.09123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 47 [320/8883 (3.60%)]\t\tLoss: 0.82745\n",
      "Training Progress: \tEpoch 47 [640/8883 (7.19%)]\t\tLoss: 0.88669\n",
      "Training Progress: \tEpoch 47 [960/8883 (10.79%)]\t\tLoss: 1.01806\n",
      "Training Progress: \tEpoch 47 [1280/8883 (14.39%)]\t\tLoss: 1.00869\n",
      "Training Progress: \tEpoch 47 [1600/8883 (17.99%)]\t\tLoss: 1.22028\n",
      "Training Progress: \tEpoch 47 [1920/8883 (21.58%)]\t\tLoss: 1.07038\n",
      "Training Progress: \tEpoch 47 [2240/8883 (25.18%)]\t\tLoss: 0.88244\n",
      "Training Progress: \tEpoch 47 [2560/8883 (28.78%)]\t\tLoss: 0.93245\n",
      "Training Progress: \tEpoch 47 [2880/8883 (32.37%)]\t\tLoss: 1.02621\n",
      "Training Progress: \tEpoch 47 [3200/8883 (35.97%)]\t\tLoss: 0.97188\n",
      "Training Progress: \tEpoch 47 [3520/8883 (39.57%)]\t\tLoss: 1.08135\n",
      "Training Progress: \tEpoch 47 [3840/8883 (43.17%)]\t\tLoss: 1.04814\n",
      "Training Progress: \tEpoch 47 [4160/8883 (46.76%)]\t\tLoss: 1.07326\n",
      "Training Progress: \tEpoch 47 [4480/8883 (50.36%)]\t\tLoss: 0.76413\n",
      "Training Progress: \tEpoch 47 [4800/8883 (53.96%)]\t\tLoss: 0.80440\n",
      "Training Progress: \tEpoch 47 [5120/8883 (57.55%)]\t\tLoss: 1.04731\n",
      "Training Progress: \tEpoch 47 [5440/8883 (61.15%)]\t\tLoss: 1.18385\n",
      "Training Progress: \tEpoch 47 [5760/8883 (64.75%)]\t\tLoss: 0.92046\n",
      "Training Progress: \tEpoch 47 [6080/8883 (68.35%)]\t\tLoss: 0.75295\n",
      "Training Progress: \tEpoch 47 [6400/8883 (71.94%)]\t\tLoss: 1.10668\n",
      "Training Progress: \tEpoch 47 [6720/8883 (75.54%)]\t\tLoss: 0.91245\n",
      "Training Progress: \tEpoch 47 [7040/8883 (79.14%)]\t\tLoss: 1.03862\n",
      "Training Progress: \tEpoch 47 [7360/8883 (82.73%)]\t\tLoss: 0.81561\n",
      "Training Progress: \tEpoch 47 [7680/8883 (86.33%)]\t\tLoss: 0.93010\n",
      "Training Progress: \tEpoch 47 [8000/8883 (89.93%)]\t\tLoss: 0.93912\n",
      "Training Progress: \tEpoch 47 [8320/8883 (93.53%)]\t\tLoss: 0.92192\n",
      "Training Progress: \tEpoch 47 [8640/8883 (97.12%)]\t\tLoss: 0.99818\n",
      "\tTrain loss: 0.02080, Accuracy: 6345/8883 (71.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1322/1692 (78.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 975/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/8883 (0.00%)]\t\tLoss: 0.96718\n",
      "Training Progress: \tEpoch 48 [320/8883 (3.60%)]\t\tLoss: 0.90623\n",
      "Training Progress: \tEpoch 48 [640/8883 (7.19%)]\t\tLoss: 0.81302\n",
      "Training Progress: \tEpoch 48 [960/8883 (10.79%)]\t\tLoss: 0.91365\n",
      "Training Progress: \tEpoch 48 [1280/8883 (14.39%)]\t\tLoss: 1.03842\n",
      "Training Progress: \tEpoch 48 [1600/8883 (17.99%)]\t\tLoss: 1.09542\n",
      "Training Progress: \tEpoch 48 [1920/8883 (21.58%)]\t\tLoss: 0.91707\n",
      "Training Progress: \tEpoch 48 [2240/8883 (25.18%)]\t\tLoss: 0.98900\n",
      "Training Progress: \tEpoch 48 [2560/8883 (28.78%)]\t\tLoss: 0.90713\n",
      "Training Progress: \tEpoch 48 [2880/8883 (32.37%)]\t\tLoss: 0.93432\n",
      "Training Progress: \tEpoch 48 [3200/8883 (35.97%)]\t\tLoss: 1.06180\n",
      "Training Progress: \tEpoch 48 [3520/8883 (39.57%)]\t\tLoss: 0.97511\n",
      "Training Progress: \tEpoch 48 [3840/8883 (43.17%)]\t\tLoss: 1.02439\n",
      "Training Progress: \tEpoch 48 [4160/8883 (46.76%)]\t\tLoss: 0.88546\n",
      "Training Progress: \tEpoch 48 [4480/8883 (50.36%)]\t\tLoss: 0.78968\n",
      "Training Progress: \tEpoch 48 [4800/8883 (53.96%)]\t\tLoss: 0.76884\n",
      "Training Progress: \tEpoch 48 [5120/8883 (57.55%)]\t\tLoss: 1.20922\n",
      "Training Progress: \tEpoch 48 [5440/8883 (61.15%)]\t\tLoss: 1.00429\n",
      "Training Progress: \tEpoch 48 [5760/8883 (64.75%)]\t\tLoss: 1.04396\n",
      "Training Progress: \tEpoch 48 [6080/8883 (68.35%)]\t\tLoss: 0.89226\n",
      "Training Progress: \tEpoch 48 [6400/8883 (71.94%)]\t\tLoss: 0.92516\n",
      "Training Progress: \tEpoch 48 [6720/8883 (75.54%)]\t\tLoss: 1.06883\n",
      "Training Progress: \tEpoch 48 [7040/8883 (79.14%)]\t\tLoss: 1.09767\n",
      "Training Progress: \tEpoch 48 [7360/8883 (82.73%)]\t\tLoss: 1.00714\n",
      "Training Progress: \tEpoch 48 [7680/8883 (86.33%)]\t\tLoss: 0.88659\n",
      "Training Progress: \tEpoch 48 [8000/8883 (89.93%)]\t\tLoss: 1.20238\n",
      "Training Progress: \tEpoch 48 [8320/8883 (93.53%)]\t\tLoss: 0.99614\n",
      "Training Progress: \tEpoch 48 [8640/8883 (97.12%)]\t\tLoss: 1.03844\n",
      "\tTrain loss: 0.02094, Accuracy: 6452/8883 (72.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1361/1692 (80.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 989/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/8883 (0.00%)]\t\tLoss: 1.07316\n",
      "Training Progress: \tEpoch 49 [320/8883 (3.60%)]\t\tLoss: 0.74456\n",
      "Training Progress: \tEpoch 49 [640/8883 (7.19%)]\t\tLoss: 0.81300\n",
      "Training Progress: \tEpoch 49 [960/8883 (10.79%)]\t\tLoss: 0.93841\n",
      "Training Progress: \tEpoch 49 [1280/8883 (14.39%)]\t\tLoss: 0.92138\n",
      "Training Progress: \tEpoch 49 [1600/8883 (17.99%)]\t\tLoss: 1.04807\n",
      "Training Progress: \tEpoch 49 [1920/8883 (21.58%)]\t\tLoss: 0.96531\n",
      "Training Progress: \tEpoch 49 [2240/8883 (25.18%)]\t\tLoss: 0.85943\n",
      "Training Progress: \tEpoch 49 [2560/8883 (28.78%)]\t\tLoss: 0.74201\n",
      "Training Progress: \tEpoch 49 [2880/8883 (32.37%)]\t\tLoss: 1.21740\n",
      "Training Progress: \tEpoch 49 [3200/8883 (35.97%)]\t\tLoss: 1.24880\n",
      "Training Progress: \tEpoch 49 [3520/8883 (39.57%)]\t\tLoss: 1.04937\n",
      "Training Progress: \tEpoch 49 [3840/8883 (43.17%)]\t\tLoss: 1.19182\n",
      "Training Progress: \tEpoch 49 [4160/8883 (46.76%)]\t\tLoss: 0.97998\n",
      "Training Progress: \tEpoch 49 [4480/8883 (50.36%)]\t\tLoss: 0.90484\n",
      "Training Progress: \tEpoch 49 [4800/8883 (53.96%)]\t\tLoss: 0.73310\n",
      "Training Progress: \tEpoch 49 [5120/8883 (57.55%)]\t\tLoss: 1.11371\n",
      "Training Progress: \tEpoch 49 [5440/8883 (61.15%)]\t\tLoss: 0.87520\n",
      "Training Progress: \tEpoch 49 [5760/8883 (64.75%)]\t\tLoss: 0.99259\n",
      "Training Progress: \tEpoch 49 [6080/8883 (68.35%)]\t\tLoss: 0.98733\n",
      "Training Progress: \tEpoch 49 [6400/8883 (71.94%)]\t\tLoss: 0.89248\n",
      "Training Progress: \tEpoch 49 [6720/8883 (75.54%)]\t\tLoss: 1.02675\n",
      "Training Progress: \tEpoch 49 [7040/8883 (79.14%)]\t\tLoss: 0.71122\n",
      "Training Progress: \tEpoch 49 [7360/8883 (82.73%)]\t\tLoss: 1.03831\n",
      "Training Progress: \tEpoch 49 [7680/8883 (86.33%)]\t\tLoss: 0.98970\n",
      "Training Progress: \tEpoch 49 [8000/8883 (89.93%)]\t\tLoss: 0.89229\n",
      "Training Progress: \tEpoch 49 [8320/8883 (93.53%)]\t\tLoss: 0.90034\n",
      "Training Progress: \tEpoch 49 [8640/8883 (97.12%)]\t\tLoss: 0.97301\n",
      "\tTrain loss: 0.02080, Accuracy: 6407/8883 (72.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1343/1692 (79.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 990/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/8883 (0.00%)]\t\tLoss: 0.99581\n",
      "Training Progress: \tEpoch 50 [320/8883 (3.60%)]\t\tLoss: 0.89188\n",
      "Training Progress: \tEpoch 50 [640/8883 (7.19%)]\t\tLoss: 0.97411\n",
      "Training Progress: \tEpoch 50 [960/8883 (10.79%)]\t\tLoss: 0.93748\n",
      "Training Progress: \tEpoch 50 [1280/8883 (14.39%)]\t\tLoss: 0.90051\n",
      "Training Progress: \tEpoch 50 [1600/8883 (17.99%)]\t\tLoss: 1.04811\n",
      "Training Progress: \tEpoch 50 [1920/8883 (21.58%)]\t\tLoss: 1.14608\n",
      "Training Progress: \tEpoch 50 [2240/8883 (25.18%)]\t\tLoss: 1.03098\n",
      "Training Progress: \tEpoch 50 [2560/8883 (28.78%)]\t\tLoss: 0.82204\n",
      "Training Progress: \tEpoch 50 [2880/8883 (32.37%)]\t\tLoss: 1.01562\n",
      "Training Progress: \tEpoch 50 [3200/8883 (35.97%)]\t\tLoss: 1.11121\n",
      "Training Progress: \tEpoch 50 [3520/8883 (39.57%)]\t\tLoss: 1.06652\n",
      "Training Progress: \tEpoch 50 [3840/8883 (43.17%)]\t\tLoss: 1.08183\n",
      "Training Progress: \tEpoch 50 [4160/8883 (46.76%)]\t\tLoss: 1.09188\n",
      "Training Progress: \tEpoch 50 [4480/8883 (50.36%)]\t\tLoss: 0.77726\n",
      "Training Progress: \tEpoch 50 [4800/8883 (53.96%)]\t\tLoss: 0.75818\n",
      "Training Progress: \tEpoch 50 [5120/8883 (57.55%)]\t\tLoss: 1.11601\n",
      "Training Progress: \tEpoch 50 [5440/8883 (61.15%)]\t\tLoss: 0.97415\n",
      "Training Progress: \tEpoch 50 [5760/8883 (64.75%)]\t\tLoss: 1.16927\n",
      "Training Progress: \tEpoch 50 [6080/8883 (68.35%)]\t\tLoss: 0.84390\n",
      "Training Progress: \tEpoch 50 [6400/8883 (71.94%)]\t\tLoss: 0.89101\n",
      "Training Progress: \tEpoch 50 [6720/8883 (75.54%)]\t\tLoss: 0.86560\n",
      "Training Progress: \tEpoch 50 [7040/8883 (79.14%)]\t\tLoss: 0.92843\n",
      "Training Progress: \tEpoch 50 [7360/8883 (82.73%)]\t\tLoss: 0.89004\n",
      "Training Progress: \tEpoch 50 [7680/8883 (86.33%)]\t\tLoss: 0.95858\n",
      "Training Progress: \tEpoch 50 [8000/8883 (89.93%)]\t\tLoss: 0.91979\n",
      "Training Progress: \tEpoch 50 [8320/8883 (93.53%)]\t\tLoss: 0.87609\n",
      "Training Progress: \tEpoch 50 [8640/8883 (97.12%)]\t\tLoss: 1.01236\n",
      "\tTrain loss: 0.02043, Accuracy: 6417/8883 (72.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1343/1692 (79.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/8883 (0.00%)]\t\tLoss: 0.82829\n",
      "Training Progress: \tEpoch 51 [320/8883 (3.60%)]\t\tLoss: 0.90949\n",
      "Training Progress: \tEpoch 51 [640/8883 (7.19%)]\t\tLoss: 0.98381\n",
      "Training Progress: \tEpoch 51 [960/8883 (10.79%)]\t\tLoss: 0.91176\n",
      "Training Progress: \tEpoch 51 [1280/8883 (14.39%)]\t\tLoss: 1.03352\n",
      "Training Progress: \tEpoch 51 [1600/8883 (17.99%)]\t\tLoss: 1.12458\n",
      "Training Progress: \tEpoch 51 [1920/8883 (21.58%)]\t\tLoss: 0.83770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 51 [2240/8883 (25.18%)]\t\tLoss: 0.85345\n",
      "Training Progress: \tEpoch 51 [2560/8883 (28.78%)]\t\tLoss: 0.82787\n",
      "Training Progress: \tEpoch 51 [2880/8883 (32.37%)]\t\tLoss: 0.96051\n",
      "Training Progress: \tEpoch 51 [3200/8883 (35.97%)]\t\tLoss: 1.06063\n",
      "Training Progress: \tEpoch 51 [3520/8883 (39.57%)]\t\tLoss: 1.00789\n",
      "Training Progress: \tEpoch 51 [3840/8883 (43.17%)]\t\tLoss: 1.08655\n",
      "Training Progress: \tEpoch 51 [4160/8883 (46.76%)]\t\tLoss: 0.84672\n",
      "Training Progress: \tEpoch 51 [4480/8883 (50.36%)]\t\tLoss: 0.91440\n",
      "Training Progress: \tEpoch 51 [4800/8883 (53.96%)]\t\tLoss: 0.62120\n",
      "Training Progress: \tEpoch 51 [5120/8883 (57.55%)]\t\tLoss: 1.11930\n",
      "Training Progress: \tEpoch 51 [5440/8883 (61.15%)]\t\tLoss: 1.12230\n",
      "Training Progress: \tEpoch 51 [5760/8883 (64.75%)]\t\tLoss: 1.08040\n",
      "Training Progress: \tEpoch 51 [6080/8883 (68.35%)]\t\tLoss: 0.88837\n",
      "Training Progress: \tEpoch 51 [6400/8883 (71.94%)]\t\tLoss: 0.97283\n",
      "Training Progress: \tEpoch 51 [6720/8883 (75.54%)]\t\tLoss: 0.92779\n",
      "Training Progress: \tEpoch 51 [7040/8883 (79.14%)]\t\tLoss: 1.06715\n",
      "Training Progress: \tEpoch 51 [7360/8883 (82.73%)]\t\tLoss: 1.06717\n",
      "Training Progress: \tEpoch 51 [7680/8883 (86.33%)]\t\tLoss: 0.82677\n",
      "Training Progress: \tEpoch 51 [8000/8883 (89.93%)]\t\tLoss: 0.89162\n",
      "Training Progress: \tEpoch 51 [8320/8883 (93.53%)]\t\tLoss: 0.91659\n",
      "Training Progress: \tEpoch 51 [8640/8883 (97.12%)]\t\tLoss: 1.05662\n",
      "\tTrain loss: 0.01942, Accuracy: 6594/8883 (74.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1379/1692 (81.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 998/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/8883 (0.00%)]\t\tLoss: 0.94885\n",
      "Training Progress: \tEpoch 52 [320/8883 (3.60%)]\t\tLoss: 0.80278\n",
      "Training Progress: \tEpoch 52 [640/8883 (7.19%)]\t\tLoss: 0.78660\n",
      "Training Progress: \tEpoch 52 [960/8883 (10.79%)]\t\tLoss: 0.90478\n",
      "Training Progress: \tEpoch 52 [1280/8883 (14.39%)]\t\tLoss: 1.08873\n",
      "Training Progress: \tEpoch 52 [1600/8883 (17.99%)]\t\tLoss: 1.03402\n",
      "Training Progress: \tEpoch 52 [1920/8883 (21.58%)]\t\tLoss: 0.73995\n",
      "Training Progress: \tEpoch 52 [2240/8883 (25.18%)]\t\tLoss: 0.77101\n",
      "Training Progress: \tEpoch 52 [2560/8883 (28.78%)]\t\tLoss: 0.89217\n",
      "Training Progress: \tEpoch 52 [2880/8883 (32.37%)]\t\tLoss: 0.91572\n",
      "Training Progress: \tEpoch 52 [3200/8883 (35.97%)]\t\tLoss: 0.91720\n",
      "Training Progress: \tEpoch 52 [3520/8883 (39.57%)]\t\tLoss: 0.84720\n",
      "Training Progress: \tEpoch 52 [3840/8883 (43.17%)]\t\tLoss: 1.04730\n",
      "Training Progress: \tEpoch 52 [4160/8883 (46.76%)]\t\tLoss: 0.81180\n",
      "Training Progress: \tEpoch 52 [4480/8883 (50.36%)]\t\tLoss: 0.90545\n",
      "Training Progress: \tEpoch 52 [4800/8883 (53.96%)]\t\tLoss: 0.73030\n",
      "Training Progress: \tEpoch 52 [5120/8883 (57.55%)]\t\tLoss: 1.04433\n",
      "Training Progress: \tEpoch 52 [5440/8883 (61.15%)]\t\tLoss: 1.03468\n",
      "Training Progress: \tEpoch 52 [5760/8883 (64.75%)]\t\tLoss: 0.92766\n",
      "Training Progress: \tEpoch 52 [6080/8883 (68.35%)]\t\tLoss: 0.87342\n",
      "Training Progress: \tEpoch 52 [6400/8883 (71.94%)]\t\tLoss: 1.06061\n",
      "Training Progress: \tEpoch 52 [6720/8883 (75.54%)]\t\tLoss: 0.84672\n",
      "Training Progress: \tEpoch 52 [7040/8883 (79.14%)]\t\tLoss: 0.71260\n",
      "Training Progress: \tEpoch 52 [7360/8883 (82.73%)]\t\tLoss: 0.86492\n",
      "Training Progress: \tEpoch 52 [7680/8883 (86.33%)]\t\tLoss: 0.73964\n",
      "Training Progress: \tEpoch 52 [8000/8883 (89.93%)]\t\tLoss: 0.86376\n",
      "Training Progress: \tEpoch 52 [8320/8883 (93.53%)]\t\tLoss: 0.78500\n",
      "Training Progress: \tEpoch 52 [8640/8883 (97.12%)]\t\tLoss: 1.10459\n",
      "\tTrain loss: 0.01938, Accuracy: 6550/8883 (73.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1386/1692 (81.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 988/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/8883 (0.00%)]\t\tLoss: 0.84919\n",
      "Training Progress: \tEpoch 53 [320/8883 (3.60%)]\t\tLoss: 0.91372\n",
      "Training Progress: \tEpoch 53 [640/8883 (7.19%)]\t\tLoss: 0.72559\n",
      "Training Progress: \tEpoch 53 [960/8883 (10.79%)]\t\tLoss: 0.87181\n",
      "Training Progress: \tEpoch 53 [1280/8883 (14.39%)]\t\tLoss: 1.05188\n",
      "Training Progress: \tEpoch 53 [1600/8883 (17.99%)]\t\tLoss: 1.03128\n",
      "Training Progress: \tEpoch 53 [1920/8883 (21.58%)]\t\tLoss: 1.11132\n",
      "Training Progress: \tEpoch 53 [2240/8883 (25.18%)]\t\tLoss: 1.00035\n",
      "Training Progress: \tEpoch 53 [2560/8883 (28.78%)]\t\tLoss: 0.79928\n",
      "Training Progress: \tEpoch 53 [2880/8883 (32.37%)]\t\tLoss: 1.03735\n",
      "Training Progress: \tEpoch 53 [3200/8883 (35.97%)]\t\tLoss: 1.04772\n",
      "Training Progress: \tEpoch 53 [3520/8883 (39.57%)]\t\tLoss: 0.92004\n",
      "Training Progress: \tEpoch 53 [3840/8883 (43.17%)]\t\tLoss: 0.90559\n",
      "Training Progress: \tEpoch 53 [4160/8883 (46.76%)]\t\tLoss: 0.88589\n",
      "Training Progress: \tEpoch 53 [4480/8883 (50.36%)]\t\tLoss: 0.73601\n",
      "Training Progress: \tEpoch 53 [4800/8883 (53.96%)]\t\tLoss: 0.86683\n",
      "Training Progress: \tEpoch 53 [5120/8883 (57.55%)]\t\tLoss: 1.27462\n",
      "Training Progress: \tEpoch 53 [5440/8883 (61.15%)]\t\tLoss: 1.01314\n",
      "Training Progress: \tEpoch 53 [5760/8883 (64.75%)]\t\tLoss: 1.13746\n",
      "Training Progress: \tEpoch 53 [6080/8883 (68.35%)]\t\tLoss: 0.87255\n",
      "Training Progress: \tEpoch 53 [6400/8883 (71.94%)]\t\tLoss: 1.03619\n",
      "Training Progress: \tEpoch 53 [6720/8883 (75.54%)]\t\tLoss: 1.22611\n",
      "Training Progress: \tEpoch 53 [7040/8883 (79.14%)]\t\tLoss: 0.80096\n",
      "Training Progress: \tEpoch 53 [7360/8883 (82.73%)]\t\tLoss: 1.00558\n",
      "Training Progress: \tEpoch 53 [7680/8883 (86.33%)]\t\tLoss: 0.69187\n",
      "Training Progress: \tEpoch 53 [8000/8883 (89.93%)]\t\tLoss: 0.84673\n",
      "Training Progress: \tEpoch 53 [8320/8883 (93.53%)]\t\tLoss: 0.89704\n",
      "Training Progress: \tEpoch 53 [8640/8883 (97.12%)]\t\tLoss: 1.02771\n",
      "\tTrain loss: 0.02134, Accuracy: 6101/8883 (68.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1270/1692 (75.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 918/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/8883 (0.00%)]\t\tLoss: 1.04539\n",
      "Training Progress: \tEpoch 54 [320/8883 (3.60%)]\t\tLoss: 0.97250\n",
      "Training Progress: \tEpoch 54 [640/8883 (7.19%)]\t\tLoss: 0.73176\n",
      "Training Progress: \tEpoch 54 [960/8883 (10.79%)]\t\tLoss: 0.75805\n",
      "Training Progress: \tEpoch 54 [1280/8883 (14.39%)]\t\tLoss: 0.96521\n",
      "Training Progress: \tEpoch 54 [1600/8883 (17.99%)]\t\tLoss: 1.06173\n",
      "Training Progress: \tEpoch 54 [1920/8883 (21.58%)]\t\tLoss: 0.93780\n",
      "Training Progress: \tEpoch 54 [2240/8883 (25.18%)]\t\tLoss: 0.90071\n",
      "Training Progress: \tEpoch 54 [2560/8883 (28.78%)]\t\tLoss: 0.90342\n",
      "Training Progress: \tEpoch 54 [2880/8883 (32.37%)]\t\tLoss: 0.92549\n",
      "Training Progress: \tEpoch 54 [3200/8883 (35.97%)]\t\tLoss: 0.96060\n",
      "Training Progress: \tEpoch 54 [3520/8883 (39.57%)]\t\tLoss: 1.10547\n",
      "Training Progress: \tEpoch 54 [3840/8883 (43.17%)]\t\tLoss: 0.86050\n",
      "Training Progress: \tEpoch 54 [4160/8883 (46.76%)]\t\tLoss: 0.81743\n",
      "Training Progress: \tEpoch 54 [4480/8883 (50.36%)]\t\tLoss: 0.77325\n",
      "Training Progress: \tEpoch 54 [4800/8883 (53.96%)]\t\tLoss: 0.61905\n",
      "Training Progress: \tEpoch 54 [5120/8883 (57.55%)]\t\tLoss: 0.88535\n",
      "Training Progress: \tEpoch 54 [5440/8883 (61.15%)]\t\tLoss: 1.02999\n",
      "Training Progress: \tEpoch 54 [5760/8883 (64.75%)]\t\tLoss: 1.08142\n",
      "Training Progress: \tEpoch 54 [6080/8883 (68.35%)]\t\tLoss: 0.98487\n",
      "Training Progress: \tEpoch 54 [6400/8883 (71.94%)]\t\tLoss: 0.98936\n",
      "Training Progress: \tEpoch 54 [6720/8883 (75.54%)]\t\tLoss: 1.19216\n",
      "Training Progress: \tEpoch 54 [7040/8883 (79.14%)]\t\tLoss: 0.86691\n",
      "Training Progress: \tEpoch 54 [7360/8883 (82.73%)]\t\tLoss: 0.95872\n",
      "Training Progress: \tEpoch 54 [7680/8883 (86.33%)]\t\tLoss: 1.06572\n",
      "Training Progress: \tEpoch 54 [8000/8883 (89.93%)]\t\tLoss: 1.08697\n",
      "Training Progress: \tEpoch 54 [8320/8883 (93.53%)]\t\tLoss: 0.77710\n",
      "Training Progress: \tEpoch 54 [8640/8883 (97.12%)]\t\tLoss: 0.92170\n",
      "\tTrain loss: 0.01929, Accuracy: 6654/8883 (74.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1397/1692 (82.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/8883 (0.00%)]\t\tLoss: 0.81122\n",
      "Training Progress: \tEpoch 55 [320/8883 (3.60%)]\t\tLoss: 0.83307\n",
      "Training Progress: \tEpoch 55 [640/8883 (7.19%)]\t\tLoss: 0.92263\n",
      "Training Progress: \tEpoch 55 [960/8883 (10.79%)]\t\tLoss: 0.89881\n",
      "Training Progress: \tEpoch 55 [1280/8883 (14.39%)]\t\tLoss: 1.13823\n",
      "Training Progress: \tEpoch 55 [1600/8883 (17.99%)]\t\tLoss: 1.07639\n",
      "Training Progress: \tEpoch 55 [1920/8883 (21.58%)]\t\tLoss: 1.07792\n",
      "Training Progress: \tEpoch 55 [2240/8883 (25.18%)]\t\tLoss: 0.74253\n",
      "Training Progress: \tEpoch 55 [2560/8883 (28.78%)]\t\tLoss: 0.79632\n",
      "Training Progress: \tEpoch 55 [2880/8883 (32.37%)]\t\tLoss: 0.89008\n",
      "Training Progress: \tEpoch 55 [3200/8883 (35.97%)]\t\tLoss: 1.19786\n",
      "Training Progress: \tEpoch 55 [3520/8883 (39.57%)]\t\tLoss: 0.96111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 55 [3840/8883 (43.17%)]\t\tLoss: 1.02241\n",
      "Training Progress: \tEpoch 55 [4160/8883 (46.76%)]\t\tLoss: 0.77046\n",
      "Training Progress: \tEpoch 55 [4480/8883 (50.36%)]\t\tLoss: 0.67872\n",
      "Training Progress: \tEpoch 55 [4800/8883 (53.96%)]\t\tLoss: 0.86668\n",
      "Training Progress: \tEpoch 55 [5120/8883 (57.55%)]\t\tLoss: 1.21015\n",
      "Training Progress: \tEpoch 55 [5440/8883 (61.15%)]\t\tLoss: 1.07626\n",
      "Training Progress: \tEpoch 55 [5760/8883 (64.75%)]\t\tLoss: 1.22101\n",
      "Training Progress: \tEpoch 55 [6080/8883 (68.35%)]\t\tLoss: 0.80437\n",
      "Training Progress: \tEpoch 55 [6400/8883 (71.94%)]\t\tLoss: 1.29157\n",
      "Training Progress: \tEpoch 55 [6720/8883 (75.54%)]\t\tLoss: 0.85395\n",
      "Training Progress: \tEpoch 55 [7040/8883 (79.14%)]\t\tLoss: 0.79808\n",
      "Training Progress: \tEpoch 55 [7360/8883 (82.73%)]\t\tLoss: 0.81391\n",
      "Training Progress: \tEpoch 55 [7680/8883 (86.33%)]\t\tLoss: 0.81799\n",
      "Training Progress: \tEpoch 55 [8000/8883 (89.93%)]\t\tLoss: 1.05156\n",
      "Training Progress: \tEpoch 55 [8320/8883 (93.53%)]\t\tLoss: 0.82716\n",
      "Training Progress: \tEpoch 55 [8640/8883 (97.12%)]\t\tLoss: 1.03041\n",
      "\tTrain loss: 0.01920, Accuracy: 6688/8883 (75.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1405/1692 (83.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 986/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/8883 (0.00%)]\t\tLoss: 0.87327\n",
      "Training Progress: \tEpoch 56 [320/8883 (3.60%)]\t\tLoss: 0.83823\n",
      "Training Progress: \tEpoch 56 [640/8883 (7.19%)]\t\tLoss: 0.71122\n",
      "Training Progress: \tEpoch 56 [960/8883 (10.79%)]\t\tLoss: 0.86571\n",
      "Training Progress: \tEpoch 56 [1280/8883 (14.39%)]\t\tLoss: 0.93089\n",
      "Training Progress: \tEpoch 56 [1600/8883 (17.99%)]\t\tLoss: 1.01986\n",
      "Training Progress: \tEpoch 56 [1920/8883 (21.58%)]\t\tLoss: 0.95598\n",
      "Training Progress: \tEpoch 56 [2240/8883 (25.18%)]\t\tLoss: 0.87706\n",
      "Training Progress: \tEpoch 56 [2560/8883 (28.78%)]\t\tLoss: 0.82845\n",
      "Training Progress: \tEpoch 56 [2880/8883 (32.37%)]\t\tLoss: 1.07806\n",
      "Training Progress: \tEpoch 56 [3200/8883 (35.97%)]\t\tLoss: 1.11277\n",
      "Training Progress: \tEpoch 56 [3520/8883 (39.57%)]\t\tLoss: 0.92925\n",
      "Training Progress: \tEpoch 56 [3840/8883 (43.17%)]\t\tLoss: 1.13259\n",
      "Training Progress: \tEpoch 56 [4160/8883 (46.76%)]\t\tLoss: 1.15005\n",
      "Training Progress: \tEpoch 56 [4480/8883 (50.36%)]\t\tLoss: 0.75713\n",
      "Training Progress: \tEpoch 56 [4800/8883 (53.96%)]\t\tLoss: 0.68765\n",
      "Training Progress: \tEpoch 56 [5120/8883 (57.55%)]\t\tLoss: 0.89463\n",
      "Training Progress: \tEpoch 56 [5440/8883 (61.15%)]\t\tLoss: 1.30065\n",
      "Training Progress: \tEpoch 56 [5760/8883 (64.75%)]\t\tLoss: 1.12856\n",
      "Training Progress: \tEpoch 56 [6080/8883 (68.35%)]\t\tLoss: 0.88294\n",
      "Training Progress: \tEpoch 56 [6400/8883 (71.94%)]\t\tLoss: 0.93664\n",
      "Training Progress: \tEpoch 56 [6720/8883 (75.54%)]\t\tLoss: 0.98621\n",
      "Training Progress: \tEpoch 56 [7040/8883 (79.14%)]\t\tLoss: 0.85882\n",
      "Training Progress: \tEpoch 56 [7360/8883 (82.73%)]\t\tLoss: 0.72150\n",
      "Training Progress: \tEpoch 56 [7680/8883 (86.33%)]\t\tLoss: 1.08162\n",
      "Training Progress: \tEpoch 56 [8000/8883 (89.93%)]\t\tLoss: 1.03573\n",
      "Training Progress: \tEpoch 56 [8320/8883 (93.53%)]\t\tLoss: 0.94958\n",
      "Training Progress: \tEpoch 56 [8640/8883 (97.12%)]\t\tLoss: 0.97296\n",
      "\tTrain loss: 0.01929, Accuracy: 6557/8883 (73.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1375/1692 (81.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 972/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/8883 (0.00%)]\t\tLoss: 0.91220\n",
      "Training Progress: \tEpoch 57 [320/8883 (3.60%)]\t\tLoss: 0.77411\n",
      "Training Progress: \tEpoch 57 [640/8883 (7.19%)]\t\tLoss: 0.67775\n",
      "Training Progress: \tEpoch 57 [960/8883 (10.79%)]\t\tLoss: 0.98660\n",
      "Training Progress: \tEpoch 57 [1280/8883 (14.39%)]\t\tLoss: 0.99739\n",
      "Training Progress: \tEpoch 57 [1600/8883 (17.99%)]\t\tLoss: 0.88375\n",
      "Training Progress: \tEpoch 57 [1920/8883 (21.58%)]\t\tLoss: 0.91385\n",
      "Training Progress: \tEpoch 57 [2240/8883 (25.18%)]\t\tLoss: 0.93861\n",
      "Training Progress: \tEpoch 57 [2560/8883 (28.78%)]\t\tLoss: 0.72377\n",
      "Training Progress: \tEpoch 57 [2880/8883 (32.37%)]\t\tLoss: 0.87341\n",
      "Training Progress: \tEpoch 57 [3200/8883 (35.97%)]\t\tLoss: 1.06457\n",
      "Training Progress: \tEpoch 57 [3520/8883 (39.57%)]\t\tLoss: 1.10251\n",
      "Training Progress: \tEpoch 57 [3840/8883 (43.17%)]\t\tLoss: 1.22677\n",
      "Training Progress: \tEpoch 57 [4160/8883 (46.76%)]\t\tLoss: 0.85404\n",
      "Training Progress: \tEpoch 57 [4480/8883 (50.36%)]\t\tLoss: 0.72697\n",
      "Training Progress: \tEpoch 57 [4800/8883 (53.96%)]\t\tLoss: 0.69757\n",
      "Training Progress: \tEpoch 57 [5120/8883 (57.55%)]\t\tLoss: 1.10383\n",
      "Training Progress: \tEpoch 57 [5440/8883 (61.15%)]\t\tLoss: 1.02075\n",
      "Training Progress: \tEpoch 57 [5760/8883 (64.75%)]\t\tLoss: 1.12651\n",
      "Training Progress: \tEpoch 57 [6080/8883 (68.35%)]\t\tLoss: 0.74916\n",
      "Training Progress: \tEpoch 57 [6400/8883 (71.94%)]\t\tLoss: 0.97104\n",
      "Training Progress: \tEpoch 57 [6720/8883 (75.54%)]\t\tLoss: 0.93562\n",
      "Training Progress: \tEpoch 57 [7040/8883 (79.14%)]\t\tLoss: 0.92046\n",
      "Training Progress: \tEpoch 57 [7360/8883 (82.73%)]\t\tLoss: 1.04129\n",
      "Training Progress: \tEpoch 57 [7680/8883 (86.33%)]\t\tLoss: 0.72608\n",
      "Training Progress: \tEpoch 57 [8000/8883 (89.93%)]\t\tLoss: 0.94485\n",
      "Training Progress: \tEpoch 57 [8320/8883 (93.53%)]\t\tLoss: 0.88501\n",
      "Training Progress: \tEpoch 57 [8640/8883 (97.12%)]\t\tLoss: 1.06817\n",
      "\tTrain loss: 0.01794, Accuracy: 6695/8883 (75.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1433/1692 (84.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1023/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/8883 (0.00%)]\t\tLoss: 0.95084\n",
      "Training Progress: \tEpoch 58 [320/8883 (3.60%)]\t\tLoss: 0.83819\n",
      "Training Progress: \tEpoch 58 [640/8883 (7.19%)]\t\tLoss: 0.70174\n",
      "Training Progress: \tEpoch 58 [960/8883 (10.79%)]\t\tLoss: 0.76922\n",
      "Training Progress: \tEpoch 58 [1280/8883 (14.39%)]\t\tLoss: 0.79123\n",
      "Training Progress: \tEpoch 58 [1600/8883 (17.99%)]\t\tLoss: 0.84337\n",
      "Training Progress: \tEpoch 58 [1920/8883 (21.58%)]\t\tLoss: 0.89572\n",
      "Training Progress: \tEpoch 58 [2240/8883 (25.18%)]\t\tLoss: 1.04372\n",
      "Training Progress: \tEpoch 58 [2560/8883 (28.78%)]\t\tLoss: 1.01134\n",
      "Training Progress: \tEpoch 58 [2880/8883 (32.37%)]\t\tLoss: 1.06084\n",
      "Training Progress: \tEpoch 58 [3200/8883 (35.97%)]\t\tLoss: 0.90537\n",
      "Training Progress: \tEpoch 58 [3520/8883 (39.57%)]\t\tLoss: 0.93169\n",
      "Training Progress: \tEpoch 58 [3840/8883 (43.17%)]\t\tLoss: 1.06893\n",
      "Training Progress: \tEpoch 58 [4160/8883 (46.76%)]\t\tLoss: 0.91928\n",
      "Training Progress: \tEpoch 58 [4480/8883 (50.36%)]\t\tLoss: 0.71760\n",
      "Training Progress: \tEpoch 58 [4800/8883 (53.96%)]\t\tLoss: 0.76197\n",
      "Training Progress: \tEpoch 58 [5120/8883 (57.55%)]\t\tLoss: 1.03516\n",
      "Training Progress: \tEpoch 58 [5440/8883 (61.15%)]\t\tLoss: 1.07779\n",
      "Training Progress: \tEpoch 58 [5760/8883 (64.75%)]\t\tLoss: 1.10077\n",
      "Training Progress: \tEpoch 58 [6080/8883 (68.35%)]\t\tLoss: 0.87622\n",
      "Training Progress: \tEpoch 58 [6400/8883 (71.94%)]\t\tLoss: 1.18791\n",
      "Training Progress: \tEpoch 58 [6720/8883 (75.54%)]\t\tLoss: 0.98416\n",
      "Training Progress: \tEpoch 58 [7040/8883 (79.14%)]\t\tLoss: 0.76664\n",
      "Training Progress: \tEpoch 58 [7360/8883 (82.73%)]\t\tLoss: 0.84376\n",
      "Training Progress: \tEpoch 58 [7680/8883 (86.33%)]\t\tLoss: 0.77392\n",
      "Training Progress: \tEpoch 58 [8000/8883 (89.93%)]\t\tLoss: 0.83955\n",
      "Training Progress: \tEpoch 58 [8320/8883 (93.53%)]\t\tLoss: 0.81036\n",
      "Training Progress: \tEpoch 58 [8640/8883 (97.12%)]\t\tLoss: 1.17063\n",
      "\tTrain loss: 0.01789, Accuracy: 6781/8883 (76.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1456/1692 (86.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1038/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/8883 (0.00%)]\t\tLoss: 0.81194\n",
      "Training Progress: \tEpoch 59 [320/8883 (3.60%)]\t\tLoss: 0.83725\n",
      "Training Progress: \tEpoch 59 [640/8883 (7.19%)]\t\tLoss: 1.02209\n",
      "Training Progress: \tEpoch 59 [960/8883 (10.79%)]\t\tLoss: 0.88160\n",
      "Training Progress: \tEpoch 59 [1280/8883 (14.39%)]\t\tLoss: 1.20996\n",
      "Training Progress: \tEpoch 59 [1600/8883 (17.99%)]\t\tLoss: 1.23261\n",
      "Training Progress: \tEpoch 59 [1920/8883 (21.58%)]\t\tLoss: 0.88712\n",
      "Training Progress: \tEpoch 59 [2240/8883 (25.18%)]\t\tLoss: 0.97956\n",
      "Training Progress: \tEpoch 59 [2560/8883 (28.78%)]\t\tLoss: 0.91287\n",
      "Training Progress: \tEpoch 59 [2880/8883 (32.37%)]\t\tLoss: 0.84058\n",
      "Training Progress: \tEpoch 59 [3200/8883 (35.97%)]\t\tLoss: 0.93595\n",
      "Training Progress: \tEpoch 59 [3520/8883 (39.57%)]\t\tLoss: 1.01108\n",
      "Training Progress: \tEpoch 59 [3840/8883 (43.17%)]\t\tLoss: 0.95096\n",
      "Training Progress: \tEpoch 59 [4160/8883 (46.76%)]\t\tLoss: 0.93480\n",
      "Training Progress: \tEpoch 59 [4480/8883 (50.36%)]\t\tLoss: 0.91317\n",
      "Training Progress: \tEpoch 59 [4800/8883 (53.96%)]\t\tLoss: 0.87754\n",
      "Training Progress: \tEpoch 59 [5120/8883 (57.55%)]\t\tLoss: 0.94482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 59 [5440/8883 (61.15%)]\t\tLoss: 0.98332\n",
      "Training Progress: \tEpoch 59 [5760/8883 (64.75%)]\t\tLoss: 1.13442\n",
      "Training Progress: \tEpoch 59 [6080/8883 (68.35%)]\t\tLoss: 0.85860\n",
      "Training Progress: \tEpoch 59 [6400/8883 (71.94%)]\t\tLoss: 1.05407\n",
      "Training Progress: \tEpoch 59 [6720/8883 (75.54%)]\t\tLoss: 1.13486\n",
      "Training Progress: \tEpoch 59 [7040/8883 (79.14%)]\t\tLoss: 0.78483\n",
      "Training Progress: \tEpoch 59 [7360/8883 (82.73%)]\t\tLoss: 0.82849\n",
      "Training Progress: \tEpoch 59 [7680/8883 (86.33%)]\t\tLoss: 1.01606\n",
      "Training Progress: \tEpoch 59 [8000/8883 (89.93%)]\t\tLoss: 0.73366\n",
      "Training Progress: \tEpoch 59 [8320/8883 (93.53%)]\t\tLoss: 0.77295\n",
      "Training Progress: \tEpoch 59 [8640/8883 (97.12%)]\t\tLoss: 0.86002\n",
      "\tTrain loss: 0.01859, Accuracy: 6662/8883 (74.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1415/1692 (83.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 980/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/8883 (0.00%)]\t\tLoss: 0.97119\n",
      "Training Progress: \tEpoch 60 [320/8883 (3.60%)]\t\tLoss: 0.92645\n",
      "Training Progress: \tEpoch 60 [640/8883 (7.19%)]\t\tLoss: 0.67226\n",
      "Training Progress: \tEpoch 60 [960/8883 (10.79%)]\t\tLoss: 0.76235\n",
      "Training Progress: \tEpoch 60 [1280/8883 (14.39%)]\t\tLoss: 0.92130\n",
      "Training Progress: \tEpoch 60 [1600/8883 (17.99%)]\t\tLoss: 0.88579\n",
      "Training Progress: \tEpoch 60 [1920/8883 (21.58%)]\t\tLoss: 1.07911\n",
      "Training Progress: \tEpoch 60 [2240/8883 (25.18%)]\t\tLoss: 0.86138\n",
      "Training Progress: \tEpoch 60 [2560/8883 (28.78%)]\t\tLoss: 0.72559\n",
      "Training Progress: \tEpoch 60 [2880/8883 (32.37%)]\t\tLoss: 0.93463\n",
      "Training Progress: \tEpoch 60 [3200/8883 (35.97%)]\t\tLoss: 0.87664\n",
      "Training Progress: \tEpoch 60 [3520/8883 (39.57%)]\t\tLoss: 0.93379\n",
      "Training Progress: \tEpoch 60 [3840/8883 (43.17%)]\t\tLoss: 1.28325\n",
      "Training Progress: \tEpoch 60 [4160/8883 (46.76%)]\t\tLoss: 0.88633\n",
      "Training Progress: \tEpoch 60 [4480/8883 (50.36%)]\t\tLoss: 0.70852\n",
      "Training Progress: \tEpoch 60 [4800/8883 (53.96%)]\t\tLoss: 0.81911\n",
      "Training Progress: \tEpoch 60 [5120/8883 (57.55%)]\t\tLoss: 0.88146\n",
      "Training Progress: \tEpoch 60 [5440/8883 (61.15%)]\t\tLoss: 1.10571\n",
      "Training Progress: \tEpoch 60 [5760/8883 (64.75%)]\t\tLoss: 1.27731\n",
      "Training Progress: \tEpoch 60 [6080/8883 (68.35%)]\t\tLoss: 0.90466\n",
      "Training Progress: \tEpoch 60 [6400/8883 (71.94%)]\t\tLoss: 1.13072\n",
      "Training Progress: \tEpoch 60 [6720/8883 (75.54%)]\t\tLoss: 0.93989\n",
      "Training Progress: \tEpoch 60 [7040/8883 (79.14%)]\t\tLoss: 0.92719\n",
      "Training Progress: \tEpoch 60 [7360/8883 (82.73%)]\t\tLoss: 0.88613\n",
      "Training Progress: \tEpoch 60 [7680/8883 (86.33%)]\t\tLoss: 0.82842\n",
      "Training Progress: \tEpoch 60 [8000/8883 (89.93%)]\t\tLoss: 0.65278\n",
      "Training Progress: \tEpoch 60 [8320/8883 (93.53%)]\t\tLoss: 0.81369\n",
      "Training Progress: \tEpoch 60 [8640/8883 (97.12%)]\t\tLoss: 1.04453\n",
      "\tTrain loss: 0.01817, Accuracy: 6773/8883 (76.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1456/1692 (86.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 976/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/8883 (0.00%)]\t\tLoss: 0.94605\n",
      "Training Progress: \tEpoch 61 [320/8883 (3.60%)]\t\tLoss: 0.81183\n",
      "Training Progress: \tEpoch 61 [640/8883 (7.19%)]\t\tLoss: 0.92683\n",
      "Training Progress: \tEpoch 61 [960/8883 (10.79%)]\t\tLoss: 0.80452\n",
      "Training Progress: \tEpoch 61 [1280/8883 (14.39%)]\t\tLoss: 0.82417\n",
      "Training Progress: \tEpoch 61 [1600/8883 (17.99%)]\t\tLoss: 1.33355\n",
      "Training Progress: \tEpoch 61 [1920/8883 (21.58%)]\t\tLoss: 1.04128\n",
      "Training Progress: \tEpoch 61 [2240/8883 (25.18%)]\t\tLoss: 0.76824\n",
      "Training Progress: \tEpoch 61 [2560/8883 (28.78%)]\t\tLoss: 0.99907\n",
      "Training Progress: \tEpoch 61 [2880/8883 (32.37%)]\t\tLoss: 0.92463\n",
      "Training Progress: \tEpoch 61 [3200/8883 (35.97%)]\t\tLoss: 0.94183\n",
      "Training Progress: \tEpoch 61 [3520/8883 (39.57%)]\t\tLoss: 1.06043\n",
      "Training Progress: \tEpoch 61 [3840/8883 (43.17%)]\t\tLoss: 0.96760\n",
      "Training Progress: \tEpoch 61 [4160/8883 (46.76%)]\t\tLoss: 0.96710\n",
      "Training Progress: \tEpoch 61 [4480/8883 (50.36%)]\t\tLoss: 0.94305\n",
      "Training Progress: \tEpoch 61 [4800/8883 (53.96%)]\t\tLoss: 0.86792\n",
      "Training Progress: \tEpoch 61 [5120/8883 (57.55%)]\t\tLoss: 0.98546\n",
      "Training Progress: \tEpoch 61 [5440/8883 (61.15%)]\t\tLoss: 0.91111\n",
      "Training Progress: \tEpoch 61 [5760/8883 (64.75%)]\t\tLoss: 0.97270\n",
      "Training Progress: \tEpoch 61 [6080/8883 (68.35%)]\t\tLoss: 0.82133\n",
      "Training Progress: \tEpoch 61 [6400/8883 (71.94%)]\t\tLoss: 1.18900\n",
      "Training Progress: \tEpoch 61 [6720/8883 (75.54%)]\t\tLoss: 1.06244\n",
      "Training Progress: \tEpoch 61 [7040/8883 (79.14%)]\t\tLoss: 0.91679\n",
      "Training Progress: \tEpoch 61 [7360/8883 (82.73%)]\t\tLoss: 0.75358\n",
      "Training Progress: \tEpoch 61 [7680/8883 (86.33%)]\t\tLoss: 0.89354\n",
      "Training Progress: \tEpoch 61 [8000/8883 (89.93%)]\t\tLoss: 0.92536\n",
      "Training Progress: \tEpoch 61 [8320/8883 (93.53%)]\t\tLoss: 1.04469\n",
      "Training Progress: \tEpoch 61 [8640/8883 (97.12%)]\t\tLoss: 0.75895\n",
      "\tTrain loss: 0.01736, Accuracy: 6811/8883 (76.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1470/1692 (86.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1039/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/8883 (0.00%)]\t\tLoss: 0.91660\n",
      "Training Progress: \tEpoch 62 [320/8883 (3.60%)]\t\tLoss: 0.95544\n",
      "Training Progress: \tEpoch 62 [640/8883 (7.19%)]\t\tLoss: 0.80567\n",
      "Training Progress: \tEpoch 62 [960/8883 (10.79%)]\t\tLoss: 0.82564\n",
      "Training Progress: \tEpoch 62 [1280/8883 (14.39%)]\t\tLoss: 0.92920\n",
      "Training Progress: \tEpoch 62 [1600/8883 (17.99%)]\t\tLoss: 0.95953\n",
      "Training Progress: \tEpoch 62 [1920/8883 (21.58%)]\t\tLoss: 0.87747\n",
      "Training Progress: \tEpoch 62 [2240/8883 (25.18%)]\t\tLoss: 0.88834\n",
      "Training Progress: \tEpoch 62 [2560/8883 (28.78%)]\t\tLoss: 0.80519\n",
      "Training Progress: \tEpoch 62 [2880/8883 (32.37%)]\t\tLoss: 0.74406\n",
      "Training Progress: \tEpoch 62 [3200/8883 (35.97%)]\t\tLoss: 1.14820\n",
      "Training Progress: \tEpoch 62 [3520/8883 (39.57%)]\t\tLoss: 1.09045\n",
      "Training Progress: \tEpoch 62 [3840/8883 (43.17%)]\t\tLoss: 0.92227\n",
      "Training Progress: \tEpoch 62 [4160/8883 (46.76%)]\t\tLoss: 0.84315\n",
      "Training Progress: \tEpoch 62 [4480/8883 (50.36%)]\t\tLoss: 0.77363\n",
      "Training Progress: \tEpoch 62 [4800/8883 (53.96%)]\t\tLoss: 0.87733\n",
      "Training Progress: \tEpoch 62 [5120/8883 (57.55%)]\t\tLoss: 0.95082\n",
      "Training Progress: \tEpoch 62 [5440/8883 (61.15%)]\t\tLoss: 1.06265\n",
      "Training Progress: \tEpoch 62 [5760/8883 (64.75%)]\t\tLoss: 0.85461\n",
      "Training Progress: \tEpoch 62 [6080/8883 (68.35%)]\t\tLoss: 0.98461\n",
      "Training Progress: \tEpoch 62 [6400/8883 (71.94%)]\t\tLoss: 0.91811\n",
      "Training Progress: \tEpoch 62 [6720/8883 (75.54%)]\t\tLoss: 0.77738\n",
      "Training Progress: \tEpoch 62 [7040/8883 (79.14%)]\t\tLoss: 0.98668\n",
      "Training Progress: \tEpoch 62 [7360/8883 (82.73%)]\t\tLoss: 0.85800\n",
      "Training Progress: \tEpoch 62 [7680/8883 (86.33%)]\t\tLoss: 0.89387\n",
      "Training Progress: \tEpoch 62 [8000/8883 (89.93%)]\t\tLoss: 0.70941\n",
      "Training Progress: \tEpoch 62 [8320/8883 (93.53%)]\t\tLoss: 0.77187\n",
      "Training Progress: \tEpoch 62 [8640/8883 (97.12%)]\t\tLoss: 1.12132\n",
      "\tTrain loss: 0.01718, Accuracy: 6916/8883 (77.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1489/1692 (88.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1016/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/8883 (0.00%)]\t\tLoss: 0.88685\n",
      "Training Progress: \tEpoch 63 [320/8883 (3.60%)]\t\tLoss: 0.76543\n",
      "Training Progress: \tEpoch 63 [640/8883 (7.19%)]\t\tLoss: 0.81797\n",
      "Training Progress: \tEpoch 63 [960/8883 (10.79%)]\t\tLoss: 0.98630\n",
      "Training Progress: \tEpoch 63 [1280/8883 (14.39%)]\t\tLoss: 0.88519\n",
      "Training Progress: \tEpoch 63 [1600/8883 (17.99%)]\t\tLoss: 1.10645\n",
      "Training Progress: \tEpoch 63 [1920/8883 (21.58%)]\t\tLoss: 0.73273\n",
      "Training Progress: \tEpoch 63 [2240/8883 (25.18%)]\t\tLoss: 0.66837\n",
      "Training Progress: \tEpoch 63 [2560/8883 (28.78%)]\t\tLoss: 0.76321\n",
      "Training Progress: \tEpoch 63 [2880/8883 (32.37%)]\t\tLoss: 0.91703\n",
      "Training Progress: \tEpoch 63 [3200/8883 (35.97%)]\t\tLoss: 1.25629\n",
      "Training Progress: \tEpoch 63 [3520/8883 (39.57%)]\t\tLoss: 1.06185\n",
      "Training Progress: \tEpoch 63 [3840/8883 (43.17%)]\t\tLoss: 1.21396\n",
      "Training Progress: \tEpoch 63 [4160/8883 (46.76%)]\t\tLoss: 0.78735\n",
      "Training Progress: \tEpoch 63 [4480/8883 (50.36%)]\t\tLoss: 0.92799\n",
      "Training Progress: \tEpoch 63 [4800/8883 (53.96%)]\t\tLoss: 0.79139\n",
      "Training Progress: \tEpoch 63 [5120/8883 (57.55%)]\t\tLoss: 1.01216\n",
      "Training Progress: \tEpoch 63 [5440/8883 (61.15%)]\t\tLoss: 0.91811\n",
      "Training Progress: \tEpoch 63 [5760/8883 (64.75%)]\t\tLoss: 1.14576\n",
      "Training Progress: \tEpoch 63 [6080/8883 (68.35%)]\t\tLoss: 0.88472\n",
      "Training Progress: \tEpoch 63 [6400/8883 (71.94%)]\t\tLoss: 0.78862\n",
      "Training Progress: \tEpoch 63 [6720/8883 (75.54%)]\t\tLoss: 1.05753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [7040/8883 (79.14%)]\t\tLoss: 0.87197\n",
      "Training Progress: \tEpoch 63 [7360/8883 (82.73%)]\t\tLoss: 0.88379\n",
      "Training Progress: \tEpoch 63 [7680/8883 (86.33%)]\t\tLoss: 0.71587\n",
      "Training Progress: \tEpoch 63 [8000/8883 (89.93%)]\t\tLoss: 0.79922\n",
      "Training Progress: \tEpoch 63 [8320/8883 (93.53%)]\t\tLoss: 0.80996\n",
      "Training Progress: \tEpoch 63 [8640/8883 (97.12%)]\t\tLoss: 0.87031\n",
      "\tTrain loss: 0.01693, Accuracy: 6900/8883 (77.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1502/1692 (88.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1019/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/8883 (0.00%)]\t\tLoss: 0.78187\n",
      "Training Progress: \tEpoch 64 [320/8883 (3.60%)]\t\tLoss: 0.70585\n",
      "Training Progress: \tEpoch 64 [640/8883 (7.19%)]\t\tLoss: 0.80094\n",
      "Training Progress: \tEpoch 64 [960/8883 (10.79%)]\t\tLoss: 0.93133\n",
      "Training Progress: \tEpoch 64 [1280/8883 (14.39%)]\t\tLoss: 0.92699\n",
      "Training Progress: \tEpoch 64 [1600/8883 (17.99%)]\t\tLoss: 0.91399\n",
      "Training Progress: \tEpoch 64 [1920/8883 (21.58%)]\t\tLoss: 0.96051\n",
      "Training Progress: \tEpoch 64 [2240/8883 (25.18%)]\t\tLoss: 0.74690\n",
      "Training Progress: \tEpoch 64 [2560/8883 (28.78%)]\t\tLoss: 0.79017\n",
      "Training Progress: \tEpoch 64 [2880/8883 (32.37%)]\t\tLoss: 1.07673\n",
      "Training Progress: \tEpoch 64 [3200/8883 (35.97%)]\t\tLoss: 1.18781\n",
      "Training Progress: \tEpoch 64 [3520/8883 (39.57%)]\t\tLoss: 1.06593\n",
      "Training Progress: \tEpoch 64 [3840/8883 (43.17%)]\t\tLoss: 1.04181\n",
      "Training Progress: \tEpoch 64 [4160/8883 (46.76%)]\t\tLoss: 0.80736\n",
      "Training Progress: \tEpoch 64 [4480/8883 (50.36%)]\t\tLoss: 0.76247\n",
      "Training Progress: \tEpoch 64 [4800/8883 (53.96%)]\t\tLoss: 0.73558\n",
      "Training Progress: \tEpoch 64 [5120/8883 (57.55%)]\t\tLoss: 0.83921\n",
      "Training Progress: \tEpoch 64 [5440/8883 (61.15%)]\t\tLoss: 1.16508\n",
      "Training Progress: \tEpoch 64 [5760/8883 (64.75%)]\t\tLoss: 1.06693\n",
      "Training Progress: \tEpoch 64 [6080/8883 (68.35%)]\t\tLoss: 0.95678\n",
      "Training Progress: \tEpoch 64 [6400/8883 (71.94%)]\t\tLoss: 1.29306\n",
      "Training Progress: \tEpoch 64 [6720/8883 (75.54%)]\t\tLoss: 1.02743\n",
      "Training Progress: \tEpoch 64 [7040/8883 (79.14%)]\t\tLoss: 0.73745\n",
      "Training Progress: \tEpoch 64 [7360/8883 (82.73%)]\t\tLoss: 0.81847\n",
      "Training Progress: \tEpoch 64 [7680/8883 (86.33%)]\t\tLoss: 1.03771\n",
      "Training Progress: \tEpoch 64 [8000/8883 (89.93%)]\t\tLoss: 0.84627\n",
      "Training Progress: \tEpoch 64 [8320/8883 (93.53%)]\t\tLoss: 0.90838\n",
      "Training Progress: \tEpoch 64 [8640/8883 (97.12%)]\t\tLoss: 1.21215\n",
      "\tTrain loss: 0.01617, Accuracy: 6968/8883 (78.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1517/1692 (89.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1030/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/8883 (0.00%)]\t\tLoss: 0.88429\n",
      "Training Progress: \tEpoch 65 [320/8883 (3.60%)]\t\tLoss: 0.71574\n",
      "Training Progress: \tEpoch 65 [640/8883 (7.19%)]\t\tLoss: 0.64322\n",
      "Training Progress: \tEpoch 65 [960/8883 (10.79%)]\t\tLoss: 0.94333\n",
      "Training Progress: \tEpoch 65 [1280/8883 (14.39%)]\t\tLoss: 0.91820\n",
      "Training Progress: \tEpoch 65 [1600/8883 (17.99%)]\t\tLoss: 1.44907\n",
      "Training Progress: \tEpoch 65 [1920/8883 (21.58%)]\t\tLoss: 0.96790\n",
      "Training Progress: \tEpoch 65 [2240/8883 (25.18%)]\t\tLoss: 0.79024\n",
      "Training Progress: \tEpoch 65 [2560/8883 (28.78%)]\t\tLoss: 0.83801\n",
      "Training Progress: \tEpoch 65 [2880/8883 (32.37%)]\t\tLoss: 0.77169\n",
      "Training Progress: \tEpoch 65 [3200/8883 (35.97%)]\t\tLoss: 0.99086\n",
      "Training Progress: \tEpoch 65 [3520/8883 (39.57%)]\t\tLoss: 1.14844\n",
      "Training Progress: \tEpoch 65 [3840/8883 (43.17%)]\t\tLoss: 0.95992\n",
      "Training Progress: \tEpoch 65 [4160/8883 (46.76%)]\t\tLoss: 0.95843\n",
      "Training Progress: \tEpoch 65 [4480/8883 (50.36%)]\t\tLoss: 0.77307\n",
      "Training Progress: \tEpoch 65 [4800/8883 (53.96%)]\t\tLoss: 0.65683\n",
      "Training Progress: \tEpoch 65 [5120/8883 (57.55%)]\t\tLoss: 1.16287\n",
      "Training Progress: \tEpoch 65 [5440/8883 (61.15%)]\t\tLoss: 1.13357\n",
      "Training Progress: \tEpoch 65 [5760/8883 (64.75%)]\t\tLoss: 0.85268\n",
      "Training Progress: \tEpoch 65 [6080/8883 (68.35%)]\t\tLoss: 0.66430\n",
      "Training Progress: \tEpoch 65 [6400/8883 (71.94%)]\t\tLoss: 0.81078\n",
      "Training Progress: \tEpoch 65 [6720/8883 (75.54%)]\t\tLoss: 1.07814\n",
      "Training Progress: \tEpoch 65 [7040/8883 (79.14%)]\t\tLoss: 0.70306\n",
      "Training Progress: \tEpoch 65 [7360/8883 (82.73%)]\t\tLoss: 0.78818\n",
      "Training Progress: \tEpoch 65 [7680/8883 (86.33%)]\t\tLoss: 0.78827\n",
      "Training Progress: \tEpoch 65 [8000/8883 (89.93%)]\t\tLoss: 0.97890\n",
      "Training Progress: \tEpoch 65 [8320/8883 (93.53%)]\t\tLoss: 0.66334\n",
      "Training Progress: \tEpoch 65 [8640/8883 (97.12%)]\t\tLoss: 0.71315\n",
      "\tTrain loss: 0.01598, Accuracy: 6949/8883 (78.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1003/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/8883 (0.00%)]\t\tLoss: 0.80736\n",
      "Training Progress: \tEpoch 66 [320/8883 (3.60%)]\t\tLoss: 0.76874\n",
      "Training Progress: \tEpoch 66 [640/8883 (7.19%)]\t\tLoss: 0.73757\n",
      "Training Progress: \tEpoch 66 [960/8883 (10.79%)]\t\tLoss: 0.73848\n",
      "Training Progress: \tEpoch 66 [1280/8883 (14.39%)]\t\tLoss: 0.79524\n",
      "Training Progress: \tEpoch 66 [1600/8883 (17.99%)]\t\tLoss: 0.94140\n",
      "Training Progress: \tEpoch 66 [1920/8883 (21.58%)]\t\tLoss: 0.69034\n",
      "Training Progress: \tEpoch 66 [2240/8883 (25.18%)]\t\tLoss: 0.55186\n",
      "Training Progress: \tEpoch 66 [2560/8883 (28.78%)]\t\tLoss: 0.87917\n",
      "Training Progress: \tEpoch 66 [2880/8883 (32.37%)]\t\tLoss: 0.70500\n",
      "Training Progress: \tEpoch 66 [3200/8883 (35.97%)]\t\tLoss: 1.24998\n",
      "Training Progress: \tEpoch 66 [3520/8883 (39.57%)]\t\tLoss: 1.01357\n",
      "Training Progress: \tEpoch 66 [3840/8883 (43.17%)]\t\tLoss: 1.03785\n",
      "Training Progress: \tEpoch 66 [4160/8883 (46.76%)]\t\tLoss: 1.02711\n",
      "Training Progress: \tEpoch 66 [4480/8883 (50.36%)]\t\tLoss: 0.92520\n",
      "Training Progress: \tEpoch 66 [4800/8883 (53.96%)]\t\tLoss: 0.70741\n",
      "Training Progress: \tEpoch 66 [5120/8883 (57.55%)]\t\tLoss: 0.84731\n",
      "Training Progress: \tEpoch 66 [5440/8883 (61.15%)]\t\tLoss: 0.83829\n",
      "Training Progress: \tEpoch 66 [5760/8883 (64.75%)]\t\tLoss: 1.05003\n",
      "Training Progress: \tEpoch 66 [6080/8883 (68.35%)]\t\tLoss: 0.82744\n",
      "Training Progress: \tEpoch 66 [6400/8883 (71.94%)]\t\tLoss: 0.88487\n",
      "Training Progress: \tEpoch 66 [6720/8883 (75.54%)]\t\tLoss: 0.96720\n",
      "Training Progress: \tEpoch 66 [7040/8883 (79.14%)]\t\tLoss: 0.90627\n",
      "Training Progress: \tEpoch 66 [7360/8883 (82.73%)]\t\tLoss: 0.71042\n",
      "Training Progress: \tEpoch 66 [7680/8883 (86.33%)]\t\tLoss: 0.74080\n",
      "Training Progress: \tEpoch 66 [8000/8883 (89.93%)]\t\tLoss: 0.72362\n",
      "Training Progress: \tEpoch 66 [8320/8883 (93.53%)]\t\tLoss: 0.99583\n",
      "Training Progress: \tEpoch 66 [8640/8883 (97.12%)]\t\tLoss: 0.91376\n",
      "\tTrain loss: 0.01618, Accuracy: 6930/8883 (78.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1522/1692 (89.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/8883 (0.00%)]\t\tLoss: 0.96564\n",
      "Training Progress: \tEpoch 67 [320/8883 (3.60%)]\t\tLoss: 0.93032\n",
      "Training Progress: \tEpoch 67 [640/8883 (7.19%)]\t\tLoss: 0.86759\n",
      "Training Progress: \tEpoch 67 [960/8883 (10.79%)]\t\tLoss: 0.73484\n",
      "Training Progress: \tEpoch 67 [1280/8883 (14.39%)]\t\tLoss: 0.87765\n",
      "Training Progress: \tEpoch 67 [1600/8883 (17.99%)]\t\tLoss: 0.91815\n",
      "Training Progress: \tEpoch 67 [1920/8883 (21.58%)]\t\tLoss: 0.93630\n",
      "Training Progress: \tEpoch 67 [2240/8883 (25.18%)]\t\tLoss: 0.62099\n",
      "Training Progress: \tEpoch 67 [2560/8883 (28.78%)]\t\tLoss: 0.85357\n",
      "Training Progress: \tEpoch 67 [2880/8883 (32.37%)]\t\tLoss: 0.78588\n",
      "Training Progress: \tEpoch 67 [3200/8883 (35.97%)]\t\tLoss: 0.77949\n",
      "Training Progress: \tEpoch 67 [3520/8883 (39.57%)]\t\tLoss: 1.06206\n",
      "Training Progress: \tEpoch 67 [3840/8883 (43.17%)]\t\tLoss: 1.12217\n",
      "Training Progress: \tEpoch 67 [4160/8883 (46.76%)]\t\tLoss: 0.89204\n",
      "Training Progress: \tEpoch 67 [4480/8883 (50.36%)]\t\tLoss: 0.80384\n",
      "Training Progress: \tEpoch 67 [4800/8883 (53.96%)]\t\tLoss: 0.78886\n",
      "Training Progress: \tEpoch 67 [5120/8883 (57.55%)]\t\tLoss: 1.08633\n",
      "Training Progress: \tEpoch 67 [5440/8883 (61.15%)]\t\tLoss: 1.04207\n",
      "Training Progress: \tEpoch 67 [5760/8883 (64.75%)]\t\tLoss: 0.88021\n",
      "Training Progress: \tEpoch 67 [6080/8883 (68.35%)]\t\tLoss: 0.70214\n",
      "Training Progress: \tEpoch 67 [6400/8883 (71.94%)]\t\tLoss: 1.15330\n",
      "Training Progress: \tEpoch 67 [6720/8883 (75.54%)]\t\tLoss: 1.02293\n",
      "Training Progress: \tEpoch 67 [7040/8883 (79.14%)]\t\tLoss: 0.88829\n",
      "Training Progress: \tEpoch 67 [7360/8883 (82.73%)]\t\tLoss: 0.80887\n",
      "Training Progress: \tEpoch 67 [7680/8883 (86.33%)]\t\tLoss: 0.66981\n",
      "Training Progress: \tEpoch 67 [8000/8883 (89.93%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 67 [8320/8883 (93.53%)]\t\tLoss: 0.66697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 67 [8640/8883 (97.12%)]\t\tLoss: 0.81955\n",
      "\tTrain loss: 0.01566, Accuracy: 6945/8883 (78.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1522/1692 (89.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1020/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/8883 (0.00%)]\t\tLoss: 0.74914\n",
      "Training Progress: \tEpoch 68 [320/8883 (3.60%)]\t\tLoss: 0.86427\n",
      "Training Progress: \tEpoch 68 [640/8883 (7.19%)]\t\tLoss: 0.85085\n",
      "Training Progress: \tEpoch 68 [960/8883 (10.79%)]\t\tLoss: 0.89854\n",
      "Training Progress: \tEpoch 68 [1280/8883 (14.39%)]\t\tLoss: 0.86510\n",
      "Training Progress: \tEpoch 68 [1600/8883 (17.99%)]\t\tLoss: 1.20165\n",
      "Training Progress: \tEpoch 68 [1920/8883 (21.58%)]\t\tLoss: 1.13673\n",
      "Training Progress: \tEpoch 68 [2240/8883 (25.18%)]\t\tLoss: 0.86735\n",
      "Training Progress: \tEpoch 68 [2560/8883 (28.78%)]\t\tLoss: 0.71859\n",
      "Training Progress: \tEpoch 68 [2880/8883 (32.37%)]\t\tLoss: 0.78503\n",
      "Training Progress: \tEpoch 68 [3200/8883 (35.97%)]\t\tLoss: 0.79852\n",
      "Training Progress: \tEpoch 68 [3520/8883 (39.57%)]\t\tLoss: 1.02127\n",
      "Training Progress: \tEpoch 68 [3840/8883 (43.17%)]\t\tLoss: 0.92579\n",
      "Training Progress: \tEpoch 68 [4160/8883 (46.76%)]\t\tLoss: 0.76051\n",
      "Training Progress: \tEpoch 68 [4480/8883 (50.36%)]\t\tLoss: 0.80606\n",
      "Training Progress: \tEpoch 68 [4800/8883 (53.96%)]\t\tLoss: 0.72341\n",
      "Training Progress: \tEpoch 68 [5120/8883 (57.55%)]\t\tLoss: 1.00095\n",
      "Training Progress: \tEpoch 68 [5440/8883 (61.15%)]\t\tLoss: 0.94508\n",
      "Training Progress: \tEpoch 68 [5760/8883 (64.75%)]\t\tLoss: 0.94931\n",
      "Training Progress: \tEpoch 68 [6080/8883 (68.35%)]\t\tLoss: 0.79296\n",
      "Training Progress: \tEpoch 68 [6400/8883 (71.94%)]\t\tLoss: 0.95861\n",
      "Training Progress: \tEpoch 68 [6720/8883 (75.54%)]\t\tLoss: 1.11983\n",
      "Training Progress: \tEpoch 68 [7040/8883 (79.14%)]\t\tLoss: 0.87771\n",
      "Training Progress: \tEpoch 68 [7360/8883 (82.73%)]\t\tLoss: 0.91094\n",
      "Training Progress: \tEpoch 68 [7680/8883 (86.33%)]\t\tLoss: 0.87525\n",
      "Training Progress: \tEpoch 68 [8000/8883 (89.93%)]\t\tLoss: 1.00950\n",
      "Training Progress: \tEpoch 68 [8320/8883 (93.53%)]\t\tLoss: 0.84511\n",
      "Training Progress: \tEpoch 68 [8640/8883 (97.12%)]\t\tLoss: 0.80411\n",
      "\tTrain loss: 0.01628, Accuracy: 6951/8883 (78.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1510/1692 (89.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1058/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/8883 (0.00%)]\t\tLoss: 1.05514\n",
      "Training Progress: \tEpoch 69 [320/8883 (3.60%)]\t\tLoss: 0.74309\n",
      "Training Progress: \tEpoch 69 [640/8883 (7.19%)]\t\tLoss: 0.75608\n",
      "Training Progress: \tEpoch 69 [960/8883 (10.79%)]\t\tLoss: 0.85700\n",
      "Training Progress: \tEpoch 69 [1280/8883 (14.39%)]\t\tLoss: 0.92293\n",
      "Training Progress: \tEpoch 69 [1600/8883 (17.99%)]\t\tLoss: 1.15265\n",
      "Training Progress: \tEpoch 69 [1920/8883 (21.58%)]\t\tLoss: 0.80558\n",
      "Training Progress: \tEpoch 69 [2240/8883 (25.18%)]\t\tLoss: 0.86256\n",
      "Training Progress: \tEpoch 69 [2560/8883 (28.78%)]\t\tLoss: 0.70553\n",
      "Training Progress: \tEpoch 69 [2880/8883 (32.37%)]\t\tLoss: 0.76456\n",
      "Training Progress: \tEpoch 69 [3200/8883 (35.97%)]\t\tLoss: 1.00062\n",
      "Training Progress: \tEpoch 69 [3520/8883 (39.57%)]\t\tLoss: 1.13345\n",
      "Training Progress: \tEpoch 69 [3840/8883 (43.17%)]\t\tLoss: 1.07509\n",
      "Training Progress: \tEpoch 69 [4160/8883 (46.76%)]\t\tLoss: 0.98774\n",
      "Training Progress: \tEpoch 69 [4480/8883 (50.36%)]\t\tLoss: 0.55457\n",
      "Training Progress: \tEpoch 69 [4800/8883 (53.96%)]\t\tLoss: 0.66314\n",
      "Training Progress: \tEpoch 69 [5120/8883 (57.55%)]\t\tLoss: 0.87639\n",
      "Training Progress: \tEpoch 69 [5440/8883 (61.15%)]\t\tLoss: 0.96662\n",
      "Training Progress: \tEpoch 69 [5760/8883 (64.75%)]\t\tLoss: 0.93385\n",
      "Training Progress: \tEpoch 69 [6080/8883 (68.35%)]\t\tLoss: 0.75170\n",
      "Training Progress: \tEpoch 69 [6400/8883 (71.94%)]\t\tLoss: 0.88490\n",
      "Training Progress: \tEpoch 69 [6720/8883 (75.54%)]\t\tLoss: 1.03408\n",
      "Training Progress: \tEpoch 69 [7040/8883 (79.14%)]\t\tLoss: 0.64419\n",
      "Training Progress: \tEpoch 69 [7360/8883 (82.73%)]\t\tLoss: 0.78316\n",
      "Training Progress: \tEpoch 69 [7680/8883 (86.33%)]\t\tLoss: 0.91344\n",
      "Training Progress: \tEpoch 69 [8000/8883 (89.93%)]\t\tLoss: 0.90715\n",
      "Training Progress: \tEpoch 69 [8320/8883 (93.53%)]\t\tLoss: 0.80483\n",
      "Training Progress: \tEpoch 69 [8640/8883 (97.12%)]\t\tLoss: 1.01495\n",
      "\tTrain loss: 0.01621, Accuracy: 6959/8883 (78.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1520/1692 (89.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 994/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/8883 (0.00%)]\t\tLoss: 0.90758\n",
      "Training Progress: \tEpoch 70 [320/8883 (3.60%)]\t\tLoss: 0.77447\n",
      "Training Progress: \tEpoch 70 [640/8883 (7.19%)]\t\tLoss: 0.65875\n",
      "Training Progress: \tEpoch 70 [960/8883 (10.79%)]\t\tLoss: 0.76644\n",
      "Training Progress: \tEpoch 70 [1280/8883 (14.39%)]\t\tLoss: 1.00769\n",
      "Training Progress: \tEpoch 70 [1600/8883 (17.99%)]\t\tLoss: 0.83537\n",
      "Training Progress: \tEpoch 70 [1920/8883 (21.58%)]\t\tLoss: 0.78515\n",
      "Training Progress: \tEpoch 70 [2240/8883 (25.18%)]\t\tLoss: 0.97923\n",
      "Training Progress: \tEpoch 70 [2560/8883 (28.78%)]\t\tLoss: 0.66060\n",
      "Training Progress: \tEpoch 70 [2880/8883 (32.37%)]\t\tLoss: 0.89440\n",
      "Training Progress: \tEpoch 70 [3200/8883 (35.97%)]\t\tLoss: 0.99439\n",
      "Training Progress: \tEpoch 70 [3520/8883 (39.57%)]\t\tLoss: 0.95028\n",
      "Training Progress: \tEpoch 70 [3840/8883 (43.17%)]\t\tLoss: 1.00725\n",
      "Training Progress: \tEpoch 70 [4160/8883 (46.76%)]\t\tLoss: 0.82130\n",
      "Training Progress: \tEpoch 70 [4480/8883 (50.36%)]\t\tLoss: 0.86348\n",
      "Training Progress: \tEpoch 70 [4800/8883 (53.96%)]\t\tLoss: 0.66155\n",
      "Training Progress: \tEpoch 70 [5120/8883 (57.55%)]\t\tLoss: 0.94078\n",
      "Training Progress: \tEpoch 70 [5440/8883 (61.15%)]\t\tLoss: 0.88597\n",
      "Training Progress: \tEpoch 70 [5760/8883 (64.75%)]\t\tLoss: 0.75573\n",
      "Training Progress: \tEpoch 70 [6080/8883 (68.35%)]\t\tLoss: 0.66117\n",
      "Training Progress: \tEpoch 70 [6400/8883 (71.94%)]\t\tLoss: 0.82319\n",
      "Training Progress: \tEpoch 70 [6720/8883 (75.54%)]\t\tLoss: 0.90221\n",
      "Training Progress: \tEpoch 70 [7040/8883 (79.14%)]\t\tLoss: 0.75029\n",
      "Training Progress: \tEpoch 70 [7360/8883 (82.73%)]\t\tLoss: 0.74515\n",
      "Training Progress: \tEpoch 70 [7680/8883 (86.33%)]\t\tLoss: 0.62342\n",
      "Training Progress: \tEpoch 70 [8000/8883 (89.93%)]\t\tLoss: 0.73577\n",
      "Training Progress: \tEpoch 70 [8320/8883 (93.53%)]\t\tLoss: 0.86573\n",
      "Training Progress: \tEpoch 70 [8640/8883 (97.12%)]\t\tLoss: 0.80646\n",
      "\tTrain loss: 0.01498, Accuracy: 7045/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/8883 (0.00%)]\t\tLoss: 0.64403\n",
      "Training Progress: \tEpoch 71 [320/8883 (3.60%)]\t\tLoss: 0.90433\n",
      "Training Progress: \tEpoch 71 [640/8883 (7.19%)]\t\tLoss: 0.97132\n",
      "Training Progress: \tEpoch 71 [960/8883 (10.79%)]\t\tLoss: 0.89667\n",
      "Training Progress: \tEpoch 71 [1280/8883 (14.39%)]\t\tLoss: 0.76404\n",
      "Training Progress: \tEpoch 71 [1600/8883 (17.99%)]\t\tLoss: 1.05113\n",
      "Training Progress: \tEpoch 71 [1920/8883 (21.58%)]\t\tLoss: 0.76540\n",
      "Training Progress: \tEpoch 71 [2240/8883 (25.18%)]\t\tLoss: 0.63525\n",
      "Training Progress: \tEpoch 71 [2560/8883 (28.78%)]\t\tLoss: 1.15391\n",
      "Training Progress: \tEpoch 71 [2880/8883 (32.37%)]\t\tLoss: 1.14705\n",
      "Training Progress: \tEpoch 71 [3200/8883 (35.97%)]\t\tLoss: 1.15712\n",
      "Training Progress: \tEpoch 71 [3520/8883 (39.57%)]\t\tLoss: 0.96184\n",
      "Training Progress: \tEpoch 71 [3840/8883 (43.17%)]\t\tLoss: 1.03712\n",
      "Training Progress: \tEpoch 71 [4160/8883 (46.76%)]\t\tLoss: 0.91846\n",
      "Training Progress: \tEpoch 71 [4480/8883 (50.36%)]\t\tLoss: 0.78201\n",
      "Training Progress: \tEpoch 71 [4800/8883 (53.96%)]\t\tLoss: 0.68378\n",
      "Training Progress: \tEpoch 71 [5120/8883 (57.55%)]\t\tLoss: 0.74451\n",
      "Training Progress: \tEpoch 71 [5440/8883 (61.15%)]\t\tLoss: 0.85941\n",
      "Training Progress: \tEpoch 71 [5760/8883 (64.75%)]\t\tLoss: 0.83052\n",
      "Training Progress: \tEpoch 71 [6080/8883 (68.35%)]\t\tLoss: 0.76850\n",
      "Training Progress: \tEpoch 71 [6400/8883 (71.94%)]\t\tLoss: 0.91293\n",
      "Training Progress: \tEpoch 71 [6720/8883 (75.54%)]\t\tLoss: 1.03808\n",
      "Training Progress: \tEpoch 71 [7040/8883 (79.14%)]\t\tLoss: 0.71647\n",
      "Training Progress: \tEpoch 71 [7360/8883 (82.73%)]\t\tLoss: 0.69683\n",
      "Training Progress: \tEpoch 71 [7680/8883 (86.33%)]\t\tLoss: 0.90172\n",
      "Training Progress: \tEpoch 71 [8000/8883 (89.93%)]\t\tLoss: 0.83626\n",
      "Training Progress: \tEpoch 71 [8320/8883 (93.53%)]\t\tLoss: 0.71373\n",
      "Training Progress: \tEpoch 71 [8640/8883 (97.12%)]\t\tLoss: 1.09596\n",
      "\tTrain loss: 0.01551, Accuracy: 6990/8883 (78.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1527/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1023/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/8883 (0.00%)]\t\tLoss: 0.85856\n",
      "Training Progress: \tEpoch 72 [320/8883 (3.60%)]\t\tLoss: 0.92092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 72 [640/8883 (7.19%)]\t\tLoss: 0.72960\n",
      "Training Progress: \tEpoch 72 [960/8883 (10.79%)]\t\tLoss: 0.74757\n",
      "Training Progress: \tEpoch 72 [1280/8883 (14.39%)]\t\tLoss: 0.80411\n",
      "Training Progress: \tEpoch 72 [1600/8883 (17.99%)]\t\tLoss: 0.75100\n",
      "Training Progress: \tEpoch 72 [1920/8883 (21.58%)]\t\tLoss: 0.82103\n",
      "Training Progress: \tEpoch 72 [2240/8883 (25.18%)]\t\tLoss: 0.67320\n",
      "Training Progress: \tEpoch 72 [2560/8883 (28.78%)]\t\tLoss: 0.61924\n",
      "Training Progress: \tEpoch 72 [2880/8883 (32.37%)]\t\tLoss: 0.85786\n",
      "Training Progress: \tEpoch 72 [3200/8883 (35.97%)]\t\tLoss: 0.85992\n",
      "Training Progress: \tEpoch 72 [3520/8883 (39.57%)]\t\tLoss: 1.03463\n",
      "Training Progress: \tEpoch 72 [3840/8883 (43.17%)]\t\tLoss: 0.85072\n",
      "Training Progress: \tEpoch 72 [4160/8883 (46.76%)]\t\tLoss: 0.81173\n",
      "Training Progress: \tEpoch 72 [4480/8883 (50.36%)]\t\tLoss: 0.67430\n",
      "Training Progress: \tEpoch 72 [4800/8883 (53.96%)]\t\tLoss: 0.68844\n",
      "Training Progress: \tEpoch 72 [5120/8883 (57.55%)]\t\tLoss: 0.87267\n",
      "Training Progress: \tEpoch 72 [5440/8883 (61.15%)]\t\tLoss: 0.86072\n",
      "Training Progress: \tEpoch 72 [5760/8883 (64.75%)]\t\tLoss: 0.90726\n",
      "Training Progress: \tEpoch 72 [6080/8883 (68.35%)]\t\tLoss: 0.68204\n",
      "Training Progress: \tEpoch 72 [6400/8883 (71.94%)]\t\tLoss: 1.02981\n",
      "Training Progress: \tEpoch 72 [6720/8883 (75.54%)]\t\tLoss: 1.20646\n",
      "Training Progress: \tEpoch 72 [7040/8883 (79.14%)]\t\tLoss: 0.74723\n",
      "Training Progress: \tEpoch 72 [7360/8883 (82.73%)]\t\tLoss: 0.89866\n",
      "Training Progress: \tEpoch 72 [7680/8883 (86.33%)]\t\tLoss: 0.92993\n",
      "Training Progress: \tEpoch 72 [8000/8883 (89.93%)]\t\tLoss: 0.86169\n",
      "Training Progress: \tEpoch 72 [8320/8883 (93.53%)]\t\tLoss: 0.79911\n",
      "Training Progress: \tEpoch 72 [8640/8883 (97.12%)]\t\tLoss: 0.86319\n",
      "\tTrain loss: 0.01438, Accuracy: 7056/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1050/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/8883 (0.00%)]\t\tLoss: 0.77796\n",
      "Training Progress: \tEpoch 73 [320/8883 (3.60%)]\t\tLoss: 0.83288\n",
      "Training Progress: \tEpoch 73 [640/8883 (7.19%)]\t\tLoss: 0.73498\n",
      "Training Progress: \tEpoch 73 [960/8883 (10.79%)]\t\tLoss: 0.81205\n",
      "Training Progress: \tEpoch 73 [1280/8883 (14.39%)]\t\tLoss: 0.88585\n",
      "Training Progress: \tEpoch 73 [1600/8883 (17.99%)]\t\tLoss: 0.92780\n",
      "Training Progress: \tEpoch 73 [1920/8883 (21.58%)]\t\tLoss: 0.73962\n",
      "Training Progress: \tEpoch 73 [2240/8883 (25.18%)]\t\tLoss: 0.85492\n",
      "Training Progress: \tEpoch 73 [2560/8883 (28.78%)]\t\tLoss: 0.86452\n",
      "Training Progress: \tEpoch 73 [2880/8883 (32.37%)]\t\tLoss: 0.62187\n",
      "Training Progress: \tEpoch 73 [3200/8883 (35.97%)]\t\tLoss: 1.08906\n",
      "Training Progress: \tEpoch 73 [3520/8883 (39.57%)]\t\tLoss: 0.97105\n",
      "Training Progress: \tEpoch 73 [3840/8883 (43.17%)]\t\tLoss: 0.96514\n",
      "Training Progress: \tEpoch 73 [4160/8883 (46.76%)]\t\tLoss: 0.91739\n",
      "Training Progress: \tEpoch 73 [4480/8883 (50.36%)]\t\tLoss: 0.89451\n",
      "Training Progress: \tEpoch 73 [4800/8883 (53.96%)]\t\tLoss: 0.79537\n",
      "Training Progress: \tEpoch 73 [5120/8883 (57.55%)]\t\tLoss: 0.75338\n",
      "Training Progress: \tEpoch 73 [5440/8883 (61.15%)]\t\tLoss: 0.94388\n",
      "Training Progress: \tEpoch 73 [5760/8883 (64.75%)]\t\tLoss: 1.13573\n",
      "Training Progress: \tEpoch 73 [6080/8883 (68.35%)]\t\tLoss: 0.95157\n",
      "Training Progress: \tEpoch 73 [6400/8883 (71.94%)]\t\tLoss: 1.10301\n",
      "Training Progress: \tEpoch 73 [6720/8883 (75.54%)]\t\tLoss: 1.05766\n",
      "Training Progress: \tEpoch 73 [7040/8883 (79.14%)]\t\tLoss: 0.72490\n",
      "Training Progress: \tEpoch 73 [7360/8883 (82.73%)]\t\tLoss: 0.71315\n",
      "Training Progress: \tEpoch 73 [7680/8883 (86.33%)]\t\tLoss: 0.85617\n",
      "Training Progress: \tEpoch 73 [8000/8883 (89.93%)]\t\tLoss: 0.74818\n",
      "Training Progress: \tEpoch 73 [8320/8883 (93.53%)]\t\tLoss: 0.62019\n",
      "Training Progress: \tEpoch 73 [8640/8883 (97.12%)]\t\tLoss: 0.74849\n",
      "\tTrain loss: 0.01495, Accuracy: 7056/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1558/1692 (92.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1065/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/8883 (0.00%)]\t\tLoss: 0.75753\n",
      "Training Progress: \tEpoch 74 [320/8883 (3.60%)]\t\tLoss: 0.70721\n",
      "Training Progress: \tEpoch 74 [640/8883 (7.19%)]\t\tLoss: 0.79436\n",
      "Training Progress: \tEpoch 74 [960/8883 (10.79%)]\t\tLoss: 0.80614\n",
      "Training Progress: \tEpoch 74 [1280/8883 (14.39%)]\t\tLoss: 0.81148\n",
      "Training Progress: \tEpoch 74 [1600/8883 (17.99%)]\t\tLoss: 1.03809\n",
      "Training Progress: \tEpoch 74 [1920/8883 (21.58%)]\t\tLoss: 0.91839\n",
      "Training Progress: \tEpoch 74 [2240/8883 (25.18%)]\t\tLoss: 0.61769\n",
      "Training Progress: \tEpoch 74 [2560/8883 (28.78%)]\t\tLoss: 0.75559\n",
      "Training Progress: \tEpoch 74 [2880/8883 (32.37%)]\t\tLoss: 0.91406\n",
      "Training Progress: \tEpoch 74 [3200/8883 (35.97%)]\t\tLoss: 0.83313\n",
      "Training Progress: \tEpoch 74 [3520/8883 (39.57%)]\t\tLoss: 0.85255\n",
      "Training Progress: \tEpoch 74 [3840/8883 (43.17%)]\t\tLoss: 0.99005\n",
      "Training Progress: \tEpoch 74 [4160/8883 (46.76%)]\t\tLoss: 0.97505\n",
      "Training Progress: \tEpoch 74 [4480/8883 (50.36%)]\t\tLoss: 0.76924\n",
      "Training Progress: \tEpoch 74 [4800/8883 (53.96%)]\t\tLoss: 0.56194\n",
      "Training Progress: \tEpoch 74 [5120/8883 (57.55%)]\t\tLoss: 0.68120\n",
      "Training Progress: \tEpoch 74 [5440/8883 (61.15%)]\t\tLoss: 0.98685\n",
      "Training Progress: \tEpoch 74 [5760/8883 (64.75%)]\t\tLoss: 0.81969\n",
      "Training Progress: \tEpoch 74 [6080/8883 (68.35%)]\t\tLoss: 0.64730\n",
      "Training Progress: \tEpoch 74 [6400/8883 (71.94%)]\t\tLoss: 1.03825\n",
      "Training Progress: \tEpoch 74 [6720/8883 (75.54%)]\t\tLoss: 0.92246\n",
      "Training Progress: \tEpoch 74 [7040/8883 (79.14%)]\t\tLoss: 0.68431\n",
      "Training Progress: \tEpoch 74 [7360/8883 (82.73%)]\t\tLoss: 0.62754\n",
      "Training Progress: \tEpoch 74 [7680/8883 (86.33%)]\t\tLoss: 0.97883\n",
      "Training Progress: \tEpoch 74 [8000/8883 (89.93%)]\t\tLoss: 0.97848\n",
      "Training Progress: \tEpoch 74 [8320/8883 (93.53%)]\t\tLoss: 0.64904\n",
      "Training Progress: \tEpoch 74 [8640/8883 (97.12%)]\t\tLoss: 1.00417\n",
      "\tTrain loss: 0.01471, Accuracy: 7071/8883 (79.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1562/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1035/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/8883 (0.00%)]\t\tLoss: 0.78019\n",
      "Training Progress: \tEpoch 75 [320/8883 (3.60%)]\t\tLoss: 0.87896\n",
      "Training Progress: \tEpoch 75 [640/8883 (7.19%)]\t\tLoss: 0.68760\n",
      "Training Progress: \tEpoch 75 [960/8883 (10.79%)]\t\tLoss: 0.75504\n",
      "Training Progress: \tEpoch 75 [1280/8883 (14.39%)]\t\tLoss: 0.84226\n",
      "Training Progress: \tEpoch 75 [1600/8883 (17.99%)]\t\tLoss: 1.04617\n",
      "Training Progress: \tEpoch 75 [1920/8883 (21.58%)]\t\tLoss: 0.65920\n",
      "Training Progress: \tEpoch 75 [2240/8883 (25.18%)]\t\tLoss: 0.77969\n",
      "Training Progress: \tEpoch 75 [2560/8883 (28.78%)]\t\tLoss: 0.59101\n",
      "Training Progress: \tEpoch 75 [2880/8883 (32.37%)]\t\tLoss: 0.86569\n",
      "Training Progress: \tEpoch 75 [3200/8883 (35.97%)]\t\tLoss: 1.01186\n",
      "Training Progress: \tEpoch 75 [3520/8883 (39.57%)]\t\tLoss: 0.97508\n",
      "Training Progress: \tEpoch 75 [3840/8883 (43.17%)]\t\tLoss: 0.98762\n",
      "Training Progress: \tEpoch 75 [4160/8883 (46.76%)]\t\tLoss: 0.82725\n",
      "Training Progress: \tEpoch 75 [4480/8883 (50.36%)]\t\tLoss: 0.81903\n",
      "Training Progress: \tEpoch 75 [4800/8883 (53.96%)]\t\tLoss: 0.81096\n",
      "Training Progress: \tEpoch 75 [5120/8883 (57.55%)]\t\tLoss: 0.70798\n",
      "Training Progress: \tEpoch 75 [5440/8883 (61.15%)]\t\tLoss: 0.83544\n",
      "Training Progress: \tEpoch 75 [5760/8883 (64.75%)]\t\tLoss: 0.86197\n",
      "Training Progress: \tEpoch 75 [6080/8883 (68.35%)]\t\tLoss: 0.80603\n",
      "Training Progress: \tEpoch 75 [6400/8883 (71.94%)]\t\tLoss: 0.69114\n",
      "Training Progress: \tEpoch 75 [6720/8883 (75.54%)]\t\tLoss: 0.79233\n",
      "Training Progress: \tEpoch 75 [7040/8883 (79.14%)]\t\tLoss: 0.91608\n",
      "Training Progress: \tEpoch 75 [7360/8883 (82.73%)]\t\tLoss: 0.78303\n",
      "Training Progress: \tEpoch 75 [7680/8883 (86.33%)]\t\tLoss: 0.70703\n",
      "Training Progress: \tEpoch 75 [8000/8883 (89.93%)]\t\tLoss: 0.83699\n",
      "Training Progress: \tEpoch 75 [8320/8883 (93.53%)]\t\tLoss: 0.68264\n",
      "Training Progress: \tEpoch 75 [8640/8883 (97.12%)]\t\tLoss: 0.82121\n",
      "\tTrain loss: 0.01478, Accuracy: 7011/8883 (78.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1523/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1019/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/8883 (0.00%)]\t\tLoss: 0.78887\n",
      "Training Progress: \tEpoch 76 [320/8883 (3.60%)]\t\tLoss: 0.85154\n",
      "Training Progress: \tEpoch 76 [640/8883 (7.19%)]\t\tLoss: 0.62328\n",
      "Training Progress: \tEpoch 76 [960/8883 (10.79%)]\t\tLoss: 0.81676\n",
      "Training Progress: \tEpoch 76 [1280/8883 (14.39%)]\t\tLoss: 1.23358\n",
      "Training Progress: \tEpoch 76 [1600/8883 (17.99%)]\t\tLoss: 1.09135\n",
      "Training Progress: \tEpoch 76 [1920/8883 (21.58%)]\t\tLoss: 0.86110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 76 [2240/8883 (25.18%)]\t\tLoss: 0.77115\n",
      "Training Progress: \tEpoch 76 [2560/8883 (28.78%)]\t\tLoss: 0.70381\n",
      "Training Progress: \tEpoch 76 [2880/8883 (32.37%)]\t\tLoss: 0.74306\n",
      "Training Progress: \tEpoch 76 [3200/8883 (35.97%)]\t\tLoss: 0.91856\n",
      "Training Progress: \tEpoch 76 [3520/8883 (39.57%)]\t\tLoss: 0.91662\n",
      "Training Progress: \tEpoch 76 [3840/8883 (43.17%)]\t\tLoss: 0.93425\n",
      "Training Progress: \tEpoch 76 [4160/8883 (46.76%)]\t\tLoss: 0.81557\n",
      "Training Progress: \tEpoch 76 [4480/8883 (50.36%)]\t\tLoss: 0.63457\n",
      "Training Progress: \tEpoch 76 [4800/8883 (53.96%)]\t\tLoss: 0.71715\n",
      "Training Progress: \tEpoch 76 [5120/8883 (57.55%)]\t\tLoss: 1.15888\n",
      "Training Progress: \tEpoch 76 [5440/8883 (61.15%)]\t\tLoss: 1.01150\n",
      "Training Progress: \tEpoch 76 [5760/8883 (64.75%)]\t\tLoss: 0.66655\n",
      "Training Progress: \tEpoch 76 [6080/8883 (68.35%)]\t\tLoss: 0.72608\n",
      "Training Progress: \tEpoch 76 [6400/8883 (71.94%)]\t\tLoss: 0.87002\n",
      "Training Progress: \tEpoch 76 [6720/8883 (75.54%)]\t\tLoss: 0.76439\n",
      "Training Progress: \tEpoch 76 [7040/8883 (79.14%)]\t\tLoss: 0.67010\n",
      "Training Progress: \tEpoch 76 [7360/8883 (82.73%)]\t\tLoss: 0.91167\n",
      "Training Progress: \tEpoch 76 [7680/8883 (86.33%)]\t\tLoss: 0.95732\n",
      "Training Progress: \tEpoch 76 [8000/8883 (89.93%)]\t\tLoss: 0.70219\n",
      "Training Progress: \tEpoch 76 [8320/8883 (93.53%)]\t\tLoss: 0.68977\n",
      "Training Progress: \tEpoch 76 [8640/8883 (97.12%)]\t\tLoss: 0.71815\n",
      "\tTrain loss: 0.01464, Accuracy: 7020/8883 (79.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1521/1692 (89.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1008/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/8883 (0.00%)]\t\tLoss: 0.79792\n",
      "Training Progress: \tEpoch 77 [320/8883 (3.60%)]\t\tLoss: 0.99830\n",
      "Training Progress: \tEpoch 77 [640/8883 (7.19%)]\t\tLoss: 0.65441\n",
      "Training Progress: \tEpoch 77 [960/8883 (10.79%)]\t\tLoss: 0.65282\n",
      "Training Progress: \tEpoch 77 [1280/8883 (14.39%)]\t\tLoss: 0.96920\n",
      "Training Progress: \tEpoch 77 [1600/8883 (17.99%)]\t\tLoss: 0.81209\n",
      "Training Progress: \tEpoch 77 [1920/8883 (21.58%)]\t\tLoss: 0.78704\n",
      "Training Progress: \tEpoch 77 [2240/8883 (25.18%)]\t\tLoss: 0.75766\n",
      "Training Progress: \tEpoch 77 [2560/8883 (28.78%)]\t\tLoss: 0.77804\n",
      "Training Progress: \tEpoch 77 [2880/8883 (32.37%)]\t\tLoss: 0.73750\n",
      "Training Progress: \tEpoch 77 [3200/8883 (35.97%)]\t\tLoss: 1.14525\n",
      "Training Progress: \tEpoch 77 [3520/8883 (39.57%)]\t\tLoss: 1.07397\n",
      "Training Progress: \tEpoch 77 [3840/8883 (43.17%)]\t\tLoss: 1.05002\n",
      "Training Progress: \tEpoch 77 [4160/8883 (46.76%)]\t\tLoss: 1.00291\n",
      "Training Progress: \tEpoch 77 [4480/8883 (50.36%)]\t\tLoss: 0.73966\n",
      "Training Progress: \tEpoch 77 [4800/8883 (53.96%)]\t\tLoss: 0.82797\n",
      "Training Progress: \tEpoch 77 [5120/8883 (57.55%)]\t\tLoss: 0.74369\n",
      "Training Progress: \tEpoch 77 [5440/8883 (61.15%)]\t\tLoss: 1.07611\n",
      "Training Progress: \tEpoch 77 [5760/8883 (64.75%)]\t\tLoss: 0.98870\n",
      "Training Progress: \tEpoch 77 [6080/8883 (68.35%)]\t\tLoss: 0.64653\n",
      "Training Progress: \tEpoch 77 [6400/8883 (71.94%)]\t\tLoss: 0.93460\n",
      "Training Progress: \tEpoch 77 [6720/8883 (75.54%)]\t\tLoss: 0.73938\n",
      "Training Progress: \tEpoch 77 [7040/8883 (79.14%)]\t\tLoss: 0.74841\n",
      "Training Progress: \tEpoch 77 [7360/8883 (82.73%)]\t\tLoss: 0.83623\n",
      "Training Progress: \tEpoch 77 [7680/8883 (86.33%)]\t\tLoss: 0.86865\n",
      "Training Progress: \tEpoch 77 [8000/8883 (89.93%)]\t\tLoss: 0.66429\n",
      "Training Progress: \tEpoch 77 [8320/8883 (93.53%)]\t\tLoss: 0.91352\n",
      "Training Progress: \tEpoch 77 [8640/8883 (97.12%)]\t\tLoss: 0.85887\n",
      "\tTrain loss: 0.01423, Accuracy: 7086/8883 (79.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1551/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/8883 (0.00%)]\t\tLoss: 0.74422\n",
      "Training Progress: \tEpoch 78 [320/8883 (3.60%)]\t\tLoss: 0.68686\n",
      "Training Progress: \tEpoch 78 [640/8883 (7.19%)]\t\tLoss: 0.68254\n",
      "Training Progress: \tEpoch 78 [960/8883 (10.79%)]\t\tLoss: 0.70699\n",
      "Training Progress: \tEpoch 78 [1280/8883 (14.39%)]\t\tLoss: 0.80370\n",
      "Training Progress: \tEpoch 78 [1600/8883 (17.99%)]\t\tLoss: 0.89420\n",
      "Training Progress: \tEpoch 78 [1920/8883 (21.58%)]\t\tLoss: 1.02682\n",
      "Training Progress: \tEpoch 78 [2240/8883 (25.18%)]\t\tLoss: 0.74974\n",
      "Training Progress: \tEpoch 78 [2560/8883 (28.78%)]\t\tLoss: 0.71096\n",
      "Training Progress: \tEpoch 78 [2880/8883 (32.37%)]\t\tLoss: 0.94230\n",
      "Training Progress: \tEpoch 78 [3200/8883 (35.97%)]\t\tLoss: 0.82450\n",
      "Training Progress: \tEpoch 78 [3520/8883 (39.57%)]\t\tLoss: 1.04139\n",
      "Training Progress: \tEpoch 78 [3840/8883 (43.17%)]\t\tLoss: 1.08724\n",
      "Training Progress: \tEpoch 78 [4160/8883 (46.76%)]\t\tLoss: 0.85189\n",
      "Training Progress: \tEpoch 78 [4480/8883 (50.36%)]\t\tLoss: 0.80755\n",
      "Training Progress: \tEpoch 78 [4800/8883 (53.96%)]\t\tLoss: 0.64631\n",
      "Training Progress: \tEpoch 78 [5120/8883 (57.55%)]\t\tLoss: 1.02582\n",
      "Training Progress: \tEpoch 78 [5440/8883 (61.15%)]\t\tLoss: 0.85893\n",
      "Training Progress: \tEpoch 78 [5760/8883 (64.75%)]\t\tLoss: 0.81440\n",
      "Training Progress: \tEpoch 78 [6080/8883 (68.35%)]\t\tLoss: 0.81190\n",
      "Training Progress: \tEpoch 78 [6400/8883 (71.94%)]\t\tLoss: 0.87837\n",
      "Training Progress: \tEpoch 78 [6720/8883 (75.54%)]\t\tLoss: 1.03065\n",
      "Training Progress: \tEpoch 78 [7040/8883 (79.14%)]\t\tLoss: 0.68033\n",
      "Training Progress: \tEpoch 78 [7360/8883 (82.73%)]\t\tLoss: 0.72141\n",
      "Training Progress: \tEpoch 78 [7680/8883 (86.33%)]\t\tLoss: 1.00816\n",
      "Training Progress: \tEpoch 78 [8000/8883 (89.93%)]\t\tLoss: 0.83974\n",
      "Training Progress: \tEpoch 78 [8320/8883 (93.53%)]\t\tLoss: 0.72487\n",
      "Training Progress: \tEpoch 78 [8640/8883 (97.12%)]\t\tLoss: 0.92222\n",
      "\tTrain loss: 0.01453, Accuracy: 7080/8883 (79.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1577/1692 (93.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1072/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/8883 (0.00%)]\t\tLoss: 0.81224\n",
      "Training Progress: \tEpoch 79 [320/8883 (3.60%)]\t\tLoss: 0.86281\n",
      "Training Progress: \tEpoch 79 [640/8883 (7.19%)]\t\tLoss: 0.86400\n",
      "Training Progress: \tEpoch 79 [960/8883 (10.79%)]\t\tLoss: 0.87891\n",
      "Training Progress: \tEpoch 79 [1280/8883 (14.39%)]\t\tLoss: 0.71323\n",
      "Training Progress: \tEpoch 79 [1600/8883 (17.99%)]\t\tLoss: 0.90890\n",
      "Training Progress: \tEpoch 79 [1920/8883 (21.58%)]\t\tLoss: 0.67983\n",
      "Training Progress: \tEpoch 79 [2240/8883 (25.18%)]\t\tLoss: 0.84226\n",
      "Training Progress: \tEpoch 79 [2560/8883 (28.78%)]\t\tLoss: 0.81705\n",
      "Training Progress: \tEpoch 79 [2880/8883 (32.37%)]\t\tLoss: 0.72582\n",
      "Training Progress: \tEpoch 79 [3200/8883 (35.97%)]\t\tLoss: 0.86396\n",
      "Training Progress: \tEpoch 79 [3520/8883 (39.57%)]\t\tLoss: 1.01199\n",
      "Training Progress: \tEpoch 79 [3840/8883 (43.17%)]\t\tLoss: 0.89100\n",
      "Training Progress: \tEpoch 79 [4160/8883 (46.76%)]\t\tLoss: 0.69230\n",
      "Training Progress: \tEpoch 79 [4480/8883 (50.36%)]\t\tLoss: 0.64881\n",
      "Training Progress: \tEpoch 79 [4800/8883 (53.96%)]\t\tLoss: 0.65169\n",
      "Training Progress: \tEpoch 79 [5120/8883 (57.55%)]\t\tLoss: 0.80700\n",
      "Training Progress: \tEpoch 79 [5440/8883 (61.15%)]\t\tLoss: 0.93811\n",
      "Training Progress: \tEpoch 79 [5760/8883 (64.75%)]\t\tLoss: 1.09627\n",
      "Training Progress: \tEpoch 79 [6080/8883 (68.35%)]\t\tLoss: 0.66234\n",
      "Training Progress: \tEpoch 79 [6400/8883 (71.94%)]\t\tLoss: 0.80256\n",
      "Training Progress: \tEpoch 79 [6720/8883 (75.54%)]\t\tLoss: 0.74659\n",
      "Training Progress: \tEpoch 79 [7040/8883 (79.14%)]\t\tLoss: 0.85765\n",
      "Training Progress: \tEpoch 79 [7360/8883 (82.73%)]\t\tLoss: 0.61111\n",
      "Training Progress: \tEpoch 79 [7680/8883 (86.33%)]\t\tLoss: 0.74865\n",
      "Training Progress: \tEpoch 79 [8000/8883 (89.93%)]\t\tLoss: 0.91364\n",
      "Training Progress: \tEpoch 79 [8320/8883 (93.53%)]\t\tLoss: 0.76348\n",
      "Training Progress: \tEpoch 79 [8640/8883 (97.12%)]\t\tLoss: 0.88740\n",
      "\tTrain loss: 0.01420, Accuracy: 7058/8883 (79.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1566/1692 (92.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1057/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/8883 (0.00%)]\t\tLoss: 0.83295\n",
      "Training Progress: \tEpoch 80 [320/8883 (3.60%)]\t\tLoss: 0.91615\n",
      "Training Progress: \tEpoch 80 [640/8883 (7.19%)]\t\tLoss: 0.59031\n",
      "Training Progress: \tEpoch 80 [960/8883 (10.79%)]\t\tLoss: 0.62869\n",
      "Training Progress: \tEpoch 80 [1280/8883 (14.39%)]\t\tLoss: 0.77589\n",
      "Training Progress: \tEpoch 80 [1600/8883 (17.99%)]\t\tLoss: 0.75116\n",
      "Training Progress: \tEpoch 80 [1920/8883 (21.58%)]\t\tLoss: 0.71549\n",
      "Training Progress: \tEpoch 80 [2240/8883 (25.18%)]\t\tLoss: 0.69381\n",
      "Training Progress: \tEpoch 80 [2560/8883 (28.78%)]\t\tLoss: 0.61377\n",
      "Training Progress: \tEpoch 80 [2880/8883 (32.37%)]\t\tLoss: 0.93542\n",
      "Training Progress: \tEpoch 80 [3200/8883 (35.97%)]\t\tLoss: 0.91332\n",
      "Training Progress: \tEpoch 80 [3520/8883 (39.57%)]\t\tLoss: 0.71273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 80 [3840/8883 (43.17%)]\t\tLoss: 0.78756\n",
      "Training Progress: \tEpoch 80 [4160/8883 (46.76%)]\t\tLoss: 0.81850\n",
      "Training Progress: \tEpoch 80 [4480/8883 (50.36%)]\t\tLoss: 0.87232\n",
      "Training Progress: \tEpoch 80 [4800/8883 (53.96%)]\t\tLoss: 0.71968\n",
      "Training Progress: \tEpoch 80 [5120/8883 (57.55%)]\t\tLoss: 0.86574\n",
      "Training Progress: \tEpoch 80 [5440/8883 (61.15%)]\t\tLoss: 0.93458\n",
      "Training Progress: \tEpoch 80 [5760/8883 (64.75%)]\t\tLoss: 0.79094\n",
      "Training Progress: \tEpoch 80 [6080/8883 (68.35%)]\t\tLoss: 0.75713\n",
      "Training Progress: \tEpoch 80 [6400/8883 (71.94%)]\t\tLoss: 1.00782\n",
      "Training Progress: \tEpoch 80 [6720/8883 (75.54%)]\t\tLoss: 0.81859\n",
      "Training Progress: \tEpoch 80 [7040/8883 (79.14%)]\t\tLoss: 0.84065\n",
      "Training Progress: \tEpoch 80 [7360/8883 (82.73%)]\t\tLoss: 0.90823\n",
      "Training Progress: \tEpoch 80 [7680/8883 (86.33%)]\t\tLoss: 0.78005\n",
      "Training Progress: \tEpoch 80 [8000/8883 (89.93%)]\t\tLoss: 0.69247\n",
      "Training Progress: \tEpoch 80 [8320/8883 (93.53%)]\t\tLoss: 0.70109\n",
      "Training Progress: \tEpoch 80 [8640/8883 (97.12%)]\t\tLoss: 0.89798\n",
      "\tTrain loss: 0.01405, Accuracy: 7090/8883 (79.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1558/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1030/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/8883 (0.00%)]\t\tLoss: 0.75514\n",
      "Training Progress: \tEpoch 81 [320/8883 (3.60%)]\t\tLoss: 0.76256\n",
      "Training Progress: \tEpoch 81 [640/8883 (7.19%)]\t\tLoss: 0.80046\n",
      "Training Progress: \tEpoch 81 [960/8883 (10.79%)]\t\tLoss: 0.86717\n",
      "Training Progress: \tEpoch 81 [1280/8883 (14.39%)]\t\tLoss: 0.91855\n",
      "Training Progress: \tEpoch 81 [1600/8883 (17.99%)]\t\tLoss: 0.75796\n",
      "Training Progress: \tEpoch 81 [1920/8883 (21.58%)]\t\tLoss: 0.85648\n",
      "Training Progress: \tEpoch 81 [2240/8883 (25.18%)]\t\tLoss: 0.84521\n",
      "Training Progress: \tEpoch 81 [2560/8883 (28.78%)]\t\tLoss: 0.88349\n",
      "Training Progress: \tEpoch 81 [2880/8883 (32.37%)]\t\tLoss: 0.85026\n",
      "Training Progress: \tEpoch 81 [3200/8883 (35.97%)]\t\tLoss: 0.91380\n",
      "Training Progress: \tEpoch 81 [3520/8883 (39.57%)]\t\tLoss: 0.84862\n",
      "Training Progress: \tEpoch 81 [3840/8883 (43.17%)]\t\tLoss: 0.76334\n",
      "Training Progress: \tEpoch 81 [4160/8883 (46.76%)]\t\tLoss: 0.78657\n",
      "Training Progress: \tEpoch 81 [4480/8883 (50.36%)]\t\tLoss: 0.74420\n",
      "Training Progress: \tEpoch 81 [4800/8883 (53.96%)]\t\tLoss: 0.49158\n",
      "Training Progress: \tEpoch 81 [5120/8883 (57.55%)]\t\tLoss: 0.82983\n",
      "Training Progress: \tEpoch 81 [5440/8883 (61.15%)]\t\tLoss: 1.00191\n",
      "Training Progress: \tEpoch 81 [5760/8883 (64.75%)]\t\tLoss: 1.00431\n",
      "Training Progress: \tEpoch 81 [6080/8883 (68.35%)]\t\tLoss: 0.72855\n",
      "Training Progress: \tEpoch 81 [6400/8883 (71.94%)]\t\tLoss: 1.04960\n",
      "Training Progress: \tEpoch 81 [6720/8883 (75.54%)]\t\tLoss: 0.82752\n",
      "Training Progress: \tEpoch 81 [7040/8883 (79.14%)]\t\tLoss: 0.56396\n",
      "Training Progress: \tEpoch 81 [7360/8883 (82.73%)]\t\tLoss: 0.71741\n",
      "Training Progress: \tEpoch 81 [7680/8883 (86.33%)]\t\tLoss: 0.77119\n",
      "Training Progress: \tEpoch 81 [8000/8883 (89.93%)]\t\tLoss: 0.86744\n",
      "Training Progress: \tEpoch 81 [8320/8883 (93.53%)]\t\tLoss: 0.76372\n",
      "Training Progress: \tEpoch 81 [8640/8883 (97.12%)]\t\tLoss: 0.89382\n",
      "\tTrain loss: 0.01410, Accuracy: 7141/8883 (80.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1563/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1061/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/8883 (0.00%)]\t\tLoss: 0.93361\n",
      "Training Progress: \tEpoch 82 [320/8883 (3.60%)]\t\tLoss: 0.64719\n",
      "Training Progress: \tEpoch 82 [640/8883 (7.19%)]\t\tLoss: 0.65353\n",
      "Training Progress: \tEpoch 82 [960/8883 (10.79%)]\t\tLoss: 0.65299\n",
      "Training Progress: \tEpoch 82 [1280/8883 (14.39%)]\t\tLoss: 0.81666\n",
      "Training Progress: \tEpoch 82 [1600/8883 (17.99%)]\t\tLoss: 0.76188\n",
      "Training Progress: \tEpoch 82 [1920/8883 (21.58%)]\t\tLoss: 0.78275\n",
      "Training Progress: \tEpoch 82 [2240/8883 (25.18%)]\t\tLoss: 0.65693\n",
      "Training Progress: \tEpoch 82 [2560/8883 (28.78%)]\t\tLoss: 0.93979\n",
      "Training Progress: \tEpoch 82 [2880/8883 (32.37%)]\t\tLoss: 0.70961\n",
      "Training Progress: \tEpoch 82 [3200/8883 (35.97%)]\t\tLoss: 0.90440\n",
      "Training Progress: \tEpoch 82 [3520/8883 (39.57%)]\t\tLoss: 1.15926\n",
      "Training Progress: \tEpoch 82 [3840/8883 (43.17%)]\t\tLoss: 0.86286\n",
      "Training Progress: \tEpoch 82 [4160/8883 (46.76%)]\t\tLoss: 0.78242\n",
      "Training Progress: \tEpoch 82 [4480/8883 (50.36%)]\t\tLoss: 0.66952\n",
      "Training Progress: \tEpoch 82 [4800/8883 (53.96%)]\t\tLoss: 0.82281\n",
      "Training Progress: \tEpoch 82 [5120/8883 (57.55%)]\t\tLoss: 0.95964\n",
      "Training Progress: \tEpoch 82 [5440/8883 (61.15%)]\t\tLoss: 0.83507\n",
      "Training Progress: \tEpoch 82 [5760/8883 (64.75%)]\t\tLoss: 0.93638\n",
      "Training Progress: \tEpoch 82 [6080/8883 (68.35%)]\t\tLoss: 0.81982\n",
      "Training Progress: \tEpoch 82 [6400/8883 (71.94%)]\t\tLoss: 0.71181\n",
      "Training Progress: \tEpoch 82 [6720/8883 (75.54%)]\t\tLoss: 0.66380\n",
      "Training Progress: \tEpoch 82 [7040/8883 (79.14%)]\t\tLoss: 1.00366\n",
      "Training Progress: \tEpoch 82 [7360/8883 (82.73%)]\t\tLoss: 0.77107\n",
      "Training Progress: \tEpoch 82 [7680/8883 (86.33%)]\t\tLoss: 0.64671\n",
      "Training Progress: \tEpoch 82 [8000/8883 (89.93%)]\t\tLoss: 0.86158\n",
      "Training Progress: \tEpoch 82 [8320/8883 (93.53%)]\t\tLoss: 0.83282\n",
      "Training Progress: \tEpoch 82 [8640/8883 (97.12%)]\t\tLoss: 0.71322\n",
      "\tTrain loss: 0.01405, Accuracy: 7128/8883 (80.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1568/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/8883 (0.00%)]\t\tLoss: 0.69515\n",
      "Training Progress: \tEpoch 83 [320/8883 (3.60%)]\t\tLoss: 0.75961\n",
      "Training Progress: \tEpoch 83 [640/8883 (7.19%)]\t\tLoss: 0.76566\n",
      "Training Progress: \tEpoch 83 [960/8883 (10.79%)]\t\tLoss: 0.62317\n",
      "Training Progress: \tEpoch 83 [1280/8883 (14.39%)]\t\tLoss: 0.90796\n",
      "Training Progress: \tEpoch 83 [1600/8883 (17.99%)]\t\tLoss: 0.97768\n",
      "Training Progress: \tEpoch 83 [1920/8883 (21.58%)]\t\tLoss: 0.86666\n",
      "Training Progress: \tEpoch 83 [2240/8883 (25.18%)]\t\tLoss: 0.77736\n",
      "Training Progress: \tEpoch 83 [2560/8883 (28.78%)]\t\tLoss: 0.68992\n",
      "Training Progress: \tEpoch 83 [2880/8883 (32.37%)]\t\tLoss: 0.93612\n",
      "Training Progress: \tEpoch 83 [3200/8883 (35.97%)]\t\tLoss: 0.83494\n",
      "Training Progress: \tEpoch 83 [3520/8883 (39.57%)]\t\tLoss: 0.70516\n",
      "Training Progress: \tEpoch 83 [3840/8883 (43.17%)]\t\tLoss: 1.05671\n",
      "Training Progress: \tEpoch 83 [4160/8883 (46.76%)]\t\tLoss: 0.68052\n",
      "Training Progress: \tEpoch 83 [4480/8883 (50.36%)]\t\tLoss: 0.58310\n",
      "Training Progress: \tEpoch 83 [4800/8883 (53.96%)]\t\tLoss: 0.60738\n",
      "Training Progress: \tEpoch 83 [5120/8883 (57.55%)]\t\tLoss: 0.61215\n",
      "Training Progress: \tEpoch 83 [5440/8883 (61.15%)]\t\tLoss: 0.80406\n",
      "Training Progress: \tEpoch 83 [5760/8883 (64.75%)]\t\tLoss: 0.83712\n",
      "Training Progress: \tEpoch 83 [6080/8883 (68.35%)]\t\tLoss: 0.69777\n",
      "Training Progress: \tEpoch 83 [6400/8883 (71.94%)]\t\tLoss: 0.90936\n",
      "Training Progress: \tEpoch 83 [6720/8883 (75.54%)]\t\tLoss: 0.87084\n",
      "Training Progress: \tEpoch 83 [7040/8883 (79.14%)]\t\tLoss: 1.03654\n",
      "Training Progress: \tEpoch 83 [7360/8883 (82.73%)]\t\tLoss: 0.76121\n",
      "Training Progress: \tEpoch 83 [7680/8883 (86.33%)]\t\tLoss: 0.77533\n",
      "Training Progress: \tEpoch 83 [8000/8883 (89.93%)]\t\tLoss: 0.95040\n",
      "Training Progress: \tEpoch 83 [8320/8883 (93.53%)]\t\tLoss: 0.92043\n",
      "Training Progress: \tEpoch 83 [8640/8883 (97.12%)]\t\tLoss: 0.97873\n",
      "\tTrain loss: 0.01348, Accuracy: 7128/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1575/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/8883 (0.00%)]\t\tLoss: 0.68761\n",
      "Training Progress: \tEpoch 84 [320/8883 (3.60%)]\t\tLoss: 0.74955\n",
      "Training Progress: \tEpoch 84 [640/8883 (7.19%)]\t\tLoss: 0.67555\n",
      "Training Progress: \tEpoch 84 [960/8883 (10.79%)]\t\tLoss: 0.79951\n",
      "Training Progress: \tEpoch 84 [1280/8883 (14.39%)]\t\tLoss: 0.70787\n",
      "Training Progress: \tEpoch 84 [1600/8883 (17.99%)]\t\tLoss: 0.83725\n",
      "Training Progress: \tEpoch 84 [1920/8883 (21.58%)]\t\tLoss: 0.85144\n",
      "Training Progress: \tEpoch 84 [2240/8883 (25.18%)]\t\tLoss: 0.77122\n",
      "Training Progress: \tEpoch 84 [2560/8883 (28.78%)]\t\tLoss: 0.77742\n",
      "Training Progress: \tEpoch 84 [2880/8883 (32.37%)]\t\tLoss: 0.78561\n",
      "Training Progress: \tEpoch 84 [3200/8883 (35.97%)]\t\tLoss: 1.16986\n",
      "Training Progress: \tEpoch 84 [3520/8883 (39.57%)]\t\tLoss: 1.01665\n",
      "Training Progress: \tEpoch 84 [3840/8883 (43.17%)]\t\tLoss: 1.18411\n",
      "Training Progress: \tEpoch 84 [4160/8883 (46.76%)]\t\tLoss: 0.72186\n",
      "Training Progress: \tEpoch 84 [4480/8883 (50.36%)]\t\tLoss: 0.73446\n",
      "Training Progress: \tEpoch 84 [4800/8883 (53.96%)]\t\tLoss: 0.58435\n",
      "Training Progress: \tEpoch 84 [5120/8883 (57.55%)]\t\tLoss: 0.74876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [5440/8883 (61.15%)]\t\tLoss: 0.90867\n",
      "Training Progress: \tEpoch 84 [5760/8883 (64.75%)]\t\tLoss: 0.87256\n",
      "Training Progress: \tEpoch 84 [6080/8883 (68.35%)]\t\tLoss: 0.67030\n",
      "Training Progress: \tEpoch 84 [6400/8883 (71.94%)]\t\tLoss: 0.87608\n",
      "Training Progress: \tEpoch 84 [6720/8883 (75.54%)]\t\tLoss: 0.80141\n",
      "Training Progress: \tEpoch 84 [7040/8883 (79.14%)]\t\tLoss: 0.83694\n",
      "Training Progress: \tEpoch 84 [7360/8883 (82.73%)]\t\tLoss: 0.78898\n",
      "Training Progress: \tEpoch 84 [7680/8883 (86.33%)]\t\tLoss: 0.54245\n",
      "Training Progress: \tEpoch 84 [8000/8883 (89.93%)]\t\tLoss: 0.62552\n",
      "Training Progress: \tEpoch 84 [8320/8883 (93.53%)]\t\tLoss: 0.59037\n",
      "Training Progress: \tEpoch 84 [8640/8883 (97.12%)]\t\tLoss: 0.65197\n",
      "\tTrain loss: 0.01369, Accuracy: 7114/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1578/1692 (93.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/8883 (0.00%)]\t\tLoss: 0.64650\n",
      "Training Progress: \tEpoch 85 [320/8883 (3.60%)]\t\tLoss: 0.81584\n",
      "Training Progress: \tEpoch 85 [640/8883 (7.19%)]\t\tLoss: 0.66944\n",
      "Training Progress: \tEpoch 85 [960/8883 (10.79%)]\t\tLoss: 0.75475\n",
      "Training Progress: \tEpoch 85 [1280/8883 (14.39%)]\t\tLoss: 0.69157\n",
      "Training Progress: \tEpoch 85 [1600/8883 (17.99%)]\t\tLoss: 0.88101\n",
      "Training Progress: \tEpoch 85 [1920/8883 (21.58%)]\t\tLoss: 0.84314\n",
      "Training Progress: \tEpoch 85 [2240/8883 (25.18%)]\t\tLoss: 0.64550\n",
      "Training Progress: \tEpoch 85 [2560/8883 (28.78%)]\t\tLoss: 0.75976\n",
      "Training Progress: \tEpoch 85 [2880/8883 (32.37%)]\t\tLoss: 0.74515\n",
      "Training Progress: \tEpoch 85 [3200/8883 (35.97%)]\t\tLoss: 0.84780\n",
      "Training Progress: \tEpoch 85 [3520/8883 (39.57%)]\t\tLoss: 0.83823\n",
      "Training Progress: \tEpoch 85 [3840/8883 (43.17%)]\t\tLoss: 0.99693\n",
      "Training Progress: \tEpoch 85 [4160/8883 (46.76%)]\t\tLoss: 0.98179\n",
      "Training Progress: \tEpoch 85 [4480/8883 (50.36%)]\t\tLoss: 0.80393\n",
      "Training Progress: \tEpoch 85 [4800/8883 (53.96%)]\t\tLoss: 0.56566\n",
      "Training Progress: \tEpoch 85 [5120/8883 (57.55%)]\t\tLoss: 0.80323\n",
      "Training Progress: \tEpoch 85 [5440/8883 (61.15%)]\t\tLoss: 0.76900\n",
      "Training Progress: \tEpoch 85 [5760/8883 (64.75%)]\t\tLoss: 0.76637\n",
      "Training Progress: \tEpoch 85 [6080/8883 (68.35%)]\t\tLoss: 0.63963\n",
      "Training Progress: \tEpoch 85 [6400/8883 (71.94%)]\t\tLoss: 0.82615\n",
      "Training Progress: \tEpoch 85 [6720/8883 (75.54%)]\t\tLoss: 0.91511\n",
      "Training Progress: \tEpoch 85 [7040/8883 (79.14%)]\t\tLoss: 0.75439\n",
      "Training Progress: \tEpoch 85 [7360/8883 (82.73%)]\t\tLoss: 0.85197\n",
      "Training Progress: \tEpoch 85 [7680/8883 (86.33%)]\t\tLoss: 0.85295\n",
      "Training Progress: \tEpoch 85 [8000/8883 (89.93%)]\t\tLoss: 0.88764\n",
      "Training Progress: \tEpoch 85 [8320/8883 (93.53%)]\t\tLoss: 0.69035\n",
      "Training Progress: \tEpoch 85 [8640/8883 (97.12%)]\t\tLoss: 0.82498\n",
      "\tTrain loss: 0.01373, Accuracy: 7122/8883 (80.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1582/1692 (93.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/8883 (0.00%)]\t\tLoss: 0.89916\n",
      "Training Progress: \tEpoch 86 [320/8883 (3.60%)]\t\tLoss: 0.85518\n",
      "Training Progress: \tEpoch 86 [640/8883 (7.19%)]\t\tLoss: 0.67427\n",
      "Training Progress: \tEpoch 86 [960/8883 (10.79%)]\t\tLoss: 0.87427\n",
      "Training Progress: \tEpoch 86 [1280/8883 (14.39%)]\t\tLoss: 0.79100\n",
      "Training Progress: \tEpoch 86 [1600/8883 (17.99%)]\t\tLoss: 0.87876\n",
      "Training Progress: \tEpoch 86 [1920/8883 (21.58%)]\t\tLoss: 0.71613\n",
      "Training Progress: \tEpoch 86 [2240/8883 (25.18%)]\t\tLoss: 0.77217\n",
      "Training Progress: \tEpoch 86 [2560/8883 (28.78%)]\t\tLoss: 0.79320\n",
      "Training Progress: \tEpoch 86 [2880/8883 (32.37%)]\t\tLoss: 0.84490\n",
      "Training Progress: \tEpoch 86 [3200/8883 (35.97%)]\t\tLoss: 1.03280\n",
      "Training Progress: \tEpoch 86 [3520/8883 (39.57%)]\t\tLoss: 0.85913\n",
      "Training Progress: \tEpoch 86 [3840/8883 (43.17%)]\t\tLoss: 0.72524\n",
      "Training Progress: \tEpoch 86 [4160/8883 (46.76%)]\t\tLoss: 0.90972\n",
      "Training Progress: \tEpoch 86 [4480/8883 (50.36%)]\t\tLoss: 0.89330\n",
      "Training Progress: \tEpoch 86 [4800/8883 (53.96%)]\t\tLoss: 0.58969\n",
      "Training Progress: \tEpoch 86 [5120/8883 (57.55%)]\t\tLoss: 0.97465\n",
      "Training Progress: \tEpoch 86 [5440/8883 (61.15%)]\t\tLoss: 1.08030\n",
      "Training Progress: \tEpoch 86 [5760/8883 (64.75%)]\t\tLoss: 1.00273\n",
      "Training Progress: \tEpoch 86 [6080/8883 (68.35%)]\t\tLoss: 0.89418\n",
      "Training Progress: \tEpoch 86 [6400/8883 (71.94%)]\t\tLoss: 1.26476\n",
      "Training Progress: \tEpoch 86 [6720/8883 (75.54%)]\t\tLoss: 0.89689\n",
      "Training Progress: \tEpoch 86 [7040/8883 (79.14%)]\t\tLoss: 0.78479\n",
      "Training Progress: \tEpoch 86 [7360/8883 (82.73%)]\t\tLoss: 0.84204\n",
      "Training Progress: \tEpoch 86 [7680/8883 (86.33%)]\t\tLoss: 0.73597\n",
      "Training Progress: \tEpoch 86 [8000/8883 (89.93%)]\t\tLoss: 0.64546\n",
      "Training Progress: \tEpoch 86 [8320/8883 (93.53%)]\t\tLoss: 0.79544\n",
      "Training Progress: \tEpoch 86 [8640/8883 (97.12%)]\t\tLoss: 0.62984\n",
      "\tTrain loss: 0.01350, Accuracy: 7168/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1586/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1080/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/8883 (0.00%)]\t\tLoss: 0.65943\n",
      "Training Progress: \tEpoch 87 [320/8883 (3.60%)]\t\tLoss: 0.80481\n",
      "Training Progress: \tEpoch 87 [640/8883 (7.19%)]\t\tLoss: 0.52761\n",
      "Training Progress: \tEpoch 87 [960/8883 (10.79%)]\t\tLoss: 0.78232\n",
      "Training Progress: \tEpoch 87 [1280/8883 (14.39%)]\t\tLoss: 0.89787\n",
      "Training Progress: \tEpoch 87 [1600/8883 (17.99%)]\t\tLoss: 0.95125\n",
      "Training Progress: \tEpoch 87 [1920/8883 (21.58%)]\t\tLoss: 0.57932\n",
      "Training Progress: \tEpoch 87 [2240/8883 (25.18%)]\t\tLoss: 0.71286\n",
      "Training Progress: \tEpoch 87 [2560/8883 (28.78%)]\t\tLoss: 0.84764\n",
      "Training Progress: \tEpoch 87 [2880/8883 (32.37%)]\t\tLoss: 0.68503\n",
      "Training Progress: \tEpoch 87 [3200/8883 (35.97%)]\t\tLoss: 0.88868\n",
      "Training Progress: \tEpoch 87 [3520/8883 (39.57%)]\t\tLoss: 1.24449\n",
      "Training Progress: \tEpoch 87 [3840/8883 (43.17%)]\t\tLoss: 0.91492\n",
      "Training Progress: \tEpoch 87 [4160/8883 (46.76%)]\t\tLoss: 0.90802\n",
      "Training Progress: \tEpoch 87 [4480/8883 (50.36%)]\t\tLoss: 0.70630\n",
      "Training Progress: \tEpoch 87 [4800/8883 (53.96%)]\t\tLoss: 0.59172\n",
      "Training Progress: \tEpoch 87 [5120/8883 (57.55%)]\t\tLoss: 0.94619\n",
      "Training Progress: \tEpoch 87 [5440/8883 (61.15%)]\t\tLoss: 0.85483\n",
      "Training Progress: \tEpoch 87 [5760/8883 (64.75%)]\t\tLoss: 0.85619\n",
      "Training Progress: \tEpoch 87 [6080/8883 (68.35%)]\t\tLoss: 0.75911\n",
      "Training Progress: \tEpoch 87 [6400/8883 (71.94%)]\t\tLoss: 1.09621\n",
      "Training Progress: \tEpoch 87 [6720/8883 (75.54%)]\t\tLoss: 0.99992\n",
      "Training Progress: \tEpoch 87 [7040/8883 (79.14%)]\t\tLoss: 0.82361\n",
      "Training Progress: \tEpoch 87 [7360/8883 (82.73%)]\t\tLoss: 0.65543\n",
      "Training Progress: \tEpoch 87 [7680/8883 (86.33%)]\t\tLoss: 0.75336\n",
      "Training Progress: \tEpoch 87 [8000/8883 (89.93%)]\t\tLoss: 0.89605\n",
      "Training Progress: \tEpoch 87 [8320/8883 (93.53%)]\t\tLoss: 0.82909\n",
      "Training Progress: \tEpoch 87 [8640/8883 (97.12%)]\t\tLoss: 0.69970\n",
      "\tTrain loss: 0.01322, Accuracy: 7184/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1602/1692 (94.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1076/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/8883 (0.00%)]\t\tLoss: 0.64516\n",
      "Training Progress: \tEpoch 88 [320/8883 (3.60%)]\t\tLoss: 0.87501\n",
      "Training Progress: \tEpoch 88 [640/8883 (7.19%)]\t\tLoss: 0.74632\n",
      "Training Progress: \tEpoch 88 [960/8883 (10.79%)]\t\tLoss: 0.65576\n",
      "Training Progress: \tEpoch 88 [1280/8883 (14.39%)]\t\tLoss: 0.79926\n",
      "Training Progress: \tEpoch 88 [1600/8883 (17.99%)]\t\tLoss: 0.66028\n",
      "Training Progress: \tEpoch 88 [1920/8883 (21.58%)]\t\tLoss: 0.69967\n",
      "Training Progress: \tEpoch 88 [2240/8883 (25.18%)]\t\tLoss: 0.82867\n",
      "Training Progress: \tEpoch 88 [2560/8883 (28.78%)]\t\tLoss: 0.80523\n",
      "Training Progress: \tEpoch 88 [2880/8883 (32.37%)]\t\tLoss: 0.65820\n",
      "Training Progress: \tEpoch 88 [3200/8883 (35.97%)]\t\tLoss: 1.03253\n",
      "Training Progress: \tEpoch 88 [3520/8883 (39.57%)]\t\tLoss: 1.41221\n",
      "Training Progress: \tEpoch 88 [3840/8883 (43.17%)]\t\tLoss: 0.93273\n",
      "Training Progress: \tEpoch 88 [4160/8883 (46.76%)]\t\tLoss: 0.65982\n",
      "Training Progress: \tEpoch 88 [4480/8883 (50.36%)]\t\tLoss: 0.55831\n",
      "Training Progress: \tEpoch 88 [4800/8883 (53.96%)]\t\tLoss: 0.76430\n",
      "Training Progress: \tEpoch 88 [5120/8883 (57.55%)]\t\tLoss: 0.88175\n",
      "Training Progress: \tEpoch 88 [5440/8883 (61.15%)]\t\tLoss: 0.83744\n",
      "Training Progress: \tEpoch 88 [5760/8883 (64.75%)]\t\tLoss: 0.81375\n",
      "Training Progress: \tEpoch 88 [6080/8883 (68.35%)]\t\tLoss: 0.80500\n",
      "Training Progress: \tEpoch 88 [6400/8883 (71.94%)]\t\tLoss: 0.85213\n",
      "Training Progress: \tEpoch 88 [6720/8883 (75.54%)]\t\tLoss: 0.86469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 88 [7040/8883 (79.14%)]\t\tLoss: 0.68454\n",
      "Training Progress: \tEpoch 88 [7360/8883 (82.73%)]\t\tLoss: 0.87588\n",
      "Training Progress: \tEpoch 88 [7680/8883 (86.33%)]\t\tLoss: 0.76488\n",
      "Training Progress: \tEpoch 88 [8000/8883 (89.93%)]\t\tLoss: 0.87782\n",
      "Training Progress: \tEpoch 88 [8320/8883 (93.53%)]\t\tLoss: 0.63954\n",
      "Training Progress: \tEpoch 88 [8640/8883 (97.12%)]\t\tLoss: 0.93262\n",
      "\tTrain loss: 0.01345, Accuracy: 7188/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1595/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1051/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/8883 (0.00%)]\t\tLoss: 0.68327\n",
      "Training Progress: \tEpoch 89 [320/8883 (3.60%)]\t\tLoss: 0.67093\n",
      "Training Progress: \tEpoch 89 [640/8883 (7.19%)]\t\tLoss: 0.71874\n",
      "Training Progress: \tEpoch 89 [960/8883 (10.79%)]\t\tLoss: 0.67272\n",
      "Training Progress: \tEpoch 89 [1280/8883 (14.39%)]\t\tLoss: 0.82931\n",
      "Training Progress: \tEpoch 89 [1600/8883 (17.99%)]\t\tLoss: 0.90438\n",
      "Training Progress: \tEpoch 89 [1920/8883 (21.58%)]\t\tLoss: 0.82811\n",
      "Training Progress: \tEpoch 89 [2240/8883 (25.18%)]\t\tLoss: 0.76370\n",
      "Training Progress: \tEpoch 89 [2560/8883 (28.78%)]\t\tLoss: 0.66931\n",
      "Training Progress: \tEpoch 89 [2880/8883 (32.37%)]\t\tLoss: 0.59659\n",
      "Training Progress: \tEpoch 89 [3200/8883 (35.97%)]\t\tLoss: 1.03522\n",
      "Training Progress: \tEpoch 89 [3520/8883 (39.57%)]\t\tLoss: 1.07702\n",
      "Training Progress: \tEpoch 89 [3840/8883 (43.17%)]\t\tLoss: 0.98142\n",
      "Training Progress: \tEpoch 89 [4160/8883 (46.76%)]\t\tLoss: 1.20409\n",
      "Training Progress: \tEpoch 89 [4480/8883 (50.36%)]\t\tLoss: 0.68105\n",
      "Training Progress: \tEpoch 89 [4800/8883 (53.96%)]\t\tLoss: 0.59368\n",
      "Training Progress: \tEpoch 89 [5120/8883 (57.55%)]\t\tLoss: 1.08543\n",
      "Training Progress: \tEpoch 89 [5440/8883 (61.15%)]\t\tLoss: 0.77946\n",
      "Training Progress: \tEpoch 89 [5760/8883 (64.75%)]\t\tLoss: 0.76448\n",
      "Training Progress: \tEpoch 89 [6080/8883 (68.35%)]\t\tLoss: 0.74969\n",
      "Training Progress: \tEpoch 89 [6400/8883 (71.94%)]\t\tLoss: 0.72609\n",
      "Training Progress: \tEpoch 89 [6720/8883 (75.54%)]\t\tLoss: 0.71066\n",
      "Training Progress: \tEpoch 89 [7040/8883 (79.14%)]\t\tLoss: 0.63789\n",
      "Training Progress: \tEpoch 89 [7360/8883 (82.73%)]\t\tLoss: 0.83813\n",
      "Training Progress: \tEpoch 89 [7680/8883 (86.33%)]\t\tLoss: 0.75488\n",
      "Training Progress: \tEpoch 89 [8000/8883 (89.93%)]\t\tLoss: 0.69707\n",
      "Training Progress: \tEpoch 89 [8320/8883 (93.53%)]\t\tLoss: 0.65993\n",
      "Training Progress: \tEpoch 89 [8640/8883 (97.12%)]\t\tLoss: 0.70379\n",
      "\tTrain loss: 0.01296, Accuracy: 7187/8883 (80.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1614/1692 (95.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1062/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/8883 (0.00%)]\t\tLoss: 0.73833\n",
      "Training Progress: \tEpoch 90 [320/8883 (3.60%)]\t\tLoss: 0.78044\n",
      "Training Progress: \tEpoch 90 [640/8883 (7.19%)]\t\tLoss: 0.65058\n",
      "Training Progress: \tEpoch 90 [960/8883 (10.79%)]\t\tLoss: 0.72962\n",
      "Training Progress: \tEpoch 90 [1280/8883 (14.39%)]\t\tLoss: 1.03444\n",
      "Training Progress: \tEpoch 90 [1600/8883 (17.99%)]\t\tLoss: 0.85291\n",
      "Training Progress: \tEpoch 90 [1920/8883 (21.58%)]\t\tLoss: 0.79663\n",
      "Training Progress: \tEpoch 90 [2240/8883 (25.18%)]\t\tLoss: 0.78915\n",
      "Training Progress: \tEpoch 90 [2560/8883 (28.78%)]\t\tLoss: 0.61062\n",
      "Training Progress: \tEpoch 90 [2880/8883 (32.37%)]\t\tLoss: 0.83878\n",
      "Training Progress: \tEpoch 90 [3200/8883 (35.97%)]\t\tLoss: 0.93380\n",
      "Training Progress: \tEpoch 90 [3520/8883 (39.57%)]\t\tLoss: 0.69476\n",
      "Training Progress: \tEpoch 90 [3840/8883 (43.17%)]\t\tLoss: 0.77568\n",
      "Training Progress: \tEpoch 90 [4160/8883 (46.76%)]\t\tLoss: 0.67671\n",
      "Training Progress: \tEpoch 90 [4480/8883 (50.36%)]\t\tLoss: 0.66347\n",
      "Training Progress: \tEpoch 90 [4800/8883 (53.96%)]\t\tLoss: 0.62954\n",
      "Training Progress: \tEpoch 90 [5120/8883 (57.55%)]\t\tLoss: 0.67099\n",
      "Training Progress: \tEpoch 90 [5440/8883 (61.15%)]\t\tLoss: 0.83411\n",
      "Training Progress: \tEpoch 90 [5760/8883 (64.75%)]\t\tLoss: 0.85632\n",
      "Training Progress: \tEpoch 90 [6080/8883 (68.35%)]\t\tLoss: 0.71581\n",
      "Training Progress: \tEpoch 90 [6400/8883 (71.94%)]\t\tLoss: 0.77270\n",
      "Training Progress: \tEpoch 90 [6720/8883 (75.54%)]\t\tLoss: 0.95189\n",
      "Training Progress: \tEpoch 90 [7040/8883 (79.14%)]\t\tLoss: 0.73345\n",
      "Training Progress: \tEpoch 90 [7360/8883 (82.73%)]\t\tLoss: 0.71996\n",
      "Training Progress: \tEpoch 90 [7680/8883 (86.33%)]\t\tLoss: 0.84436\n",
      "Training Progress: \tEpoch 90 [8000/8883 (89.93%)]\t\tLoss: 1.00258\n",
      "Training Progress: \tEpoch 90 [8320/8883 (93.53%)]\t\tLoss: 0.91747\n",
      "Training Progress: \tEpoch 90 [8640/8883 (97.12%)]\t\tLoss: 0.73179\n",
      "\tTrain loss: 0.01325, Accuracy: 7178/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1607/1692 (94.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1049/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/8883 (0.00%)]\t\tLoss: 0.86673\n",
      "Training Progress: \tEpoch 91 [320/8883 (3.60%)]\t\tLoss: 0.88043\n",
      "Training Progress: \tEpoch 91 [640/8883 (7.19%)]\t\tLoss: 0.70200\n",
      "Training Progress: \tEpoch 91 [960/8883 (10.79%)]\t\tLoss: 0.73466\n",
      "Training Progress: \tEpoch 91 [1280/8883 (14.39%)]\t\tLoss: 0.60284\n",
      "Training Progress: \tEpoch 91 [1600/8883 (17.99%)]\t\tLoss: 0.93861\n",
      "Training Progress: \tEpoch 91 [1920/8883 (21.58%)]\t\tLoss: 0.69849\n",
      "Training Progress: \tEpoch 91 [2240/8883 (25.18%)]\t\tLoss: 0.63761\n",
      "Training Progress: \tEpoch 91 [2560/8883 (28.78%)]\t\tLoss: 0.64747\n",
      "Training Progress: \tEpoch 91 [2880/8883 (32.37%)]\t\tLoss: 0.80806\n",
      "Training Progress: \tEpoch 91 [3200/8883 (35.97%)]\t\tLoss: 0.96192\n",
      "Training Progress: \tEpoch 91 [3520/8883 (39.57%)]\t\tLoss: 0.96301\n",
      "Training Progress: \tEpoch 91 [3840/8883 (43.17%)]\t\tLoss: 0.84453\n",
      "Training Progress: \tEpoch 91 [4160/8883 (46.76%)]\t\tLoss: 0.70676\n",
      "Training Progress: \tEpoch 91 [4480/8883 (50.36%)]\t\tLoss: 0.87846\n",
      "Training Progress: \tEpoch 91 [4800/8883 (53.96%)]\t\tLoss: 0.78814\n",
      "Training Progress: \tEpoch 91 [5120/8883 (57.55%)]\t\tLoss: 1.11974\n",
      "Training Progress: \tEpoch 91 [5440/8883 (61.15%)]\t\tLoss: 1.12559\n",
      "Training Progress: \tEpoch 91 [5760/8883 (64.75%)]\t\tLoss: 0.90982\n",
      "Training Progress: \tEpoch 91 [6080/8883 (68.35%)]\t\tLoss: 0.62228\n",
      "Training Progress: \tEpoch 91 [6400/8883 (71.94%)]\t\tLoss: 0.88983\n",
      "Training Progress: \tEpoch 91 [6720/8883 (75.54%)]\t\tLoss: 0.91646\n",
      "Training Progress: \tEpoch 91 [7040/8883 (79.14%)]\t\tLoss: 0.74419\n",
      "Training Progress: \tEpoch 91 [7360/8883 (82.73%)]\t\tLoss: 0.60273\n",
      "Training Progress: \tEpoch 91 [7680/8883 (86.33%)]\t\tLoss: 0.79861\n",
      "Training Progress: \tEpoch 91 [8000/8883 (89.93%)]\t\tLoss: 0.81968\n",
      "Training Progress: \tEpoch 91 [8320/8883 (93.53%)]\t\tLoss: 0.67578\n",
      "Training Progress: \tEpoch 91 [8640/8883 (97.12%)]\t\tLoss: 0.94851\n",
      "\tTrain loss: 0.01348, Accuracy: 7167/8883 (80.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1585/1692 (93.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1062/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/8883 (0.00%)]\t\tLoss: 0.76178\n",
      "Training Progress: \tEpoch 92 [320/8883 (3.60%)]\t\tLoss: 0.92000\n",
      "Training Progress: \tEpoch 92 [640/8883 (7.19%)]\t\tLoss: 0.67562\n",
      "Training Progress: \tEpoch 92 [960/8883 (10.79%)]\t\tLoss: 0.72956\n",
      "Training Progress: \tEpoch 92 [1280/8883 (14.39%)]\t\tLoss: 0.84537\n",
      "Training Progress: \tEpoch 92 [1600/8883 (17.99%)]\t\tLoss: 0.98205\n",
      "Training Progress: \tEpoch 92 [1920/8883 (21.58%)]\t\tLoss: 0.82552\n",
      "Training Progress: \tEpoch 92 [2240/8883 (25.18%)]\t\tLoss: 0.60867\n",
      "Training Progress: \tEpoch 92 [2560/8883 (28.78%)]\t\tLoss: 0.57735\n",
      "Training Progress: \tEpoch 92 [2880/8883 (32.37%)]\t\tLoss: 0.65886\n",
      "Training Progress: \tEpoch 92 [3200/8883 (35.97%)]\t\tLoss: 0.95684\n",
      "Training Progress: \tEpoch 92 [3520/8883 (39.57%)]\t\tLoss: 0.80525\n",
      "Training Progress: \tEpoch 92 [3840/8883 (43.17%)]\t\tLoss: 0.93690\n",
      "Training Progress: \tEpoch 92 [4160/8883 (46.76%)]\t\tLoss: 0.81079\n",
      "Training Progress: \tEpoch 92 [4480/8883 (50.36%)]\t\tLoss: 0.65426\n",
      "Training Progress: \tEpoch 92 [4800/8883 (53.96%)]\t\tLoss: 0.57168\n",
      "Training Progress: \tEpoch 92 [5120/8883 (57.55%)]\t\tLoss: 0.92027\n",
      "Training Progress: \tEpoch 92 [5440/8883 (61.15%)]\t\tLoss: 0.97218\n",
      "Training Progress: \tEpoch 92 [5760/8883 (64.75%)]\t\tLoss: 0.72856\n",
      "Training Progress: \tEpoch 92 [6080/8883 (68.35%)]\t\tLoss: 0.69494\n",
      "Training Progress: \tEpoch 92 [6400/8883 (71.94%)]\t\tLoss: 0.88157\n",
      "Training Progress: \tEpoch 92 [6720/8883 (75.54%)]\t\tLoss: 0.84654\n",
      "Training Progress: \tEpoch 92 [7040/8883 (79.14%)]\t\tLoss: 0.75808\n",
      "Training Progress: \tEpoch 92 [7360/8883 (82.73%)]\t\tLoss: 0.62157\n",
      "Training Progress: \tEpoch 92 [7680/8883 (86.33%)]\t\tLoss: 0.62036\n",
      "Training Progress: \tEpoch 92 [8000/8883 (89.93%)]\t\tLoss: 0.76810\n",
      "Training Progress: \tEpoch 92 [8320/8883 (93.53%)]\t\tLoss: 0.70272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 92 [8640/8883 (97.12%)]\t\tLoss: 0.93967\n",
      "\tTrain loss: 0.01333, Accuracy: 7104/8883 (79.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1561/1692 (92.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1059/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/8883 (0.00%)]\t\tLoss: 0.68637\n",
      "Training Progress: \tEpoch 93 [320/8883 (3.60%)]\t\tLoss: 0.70940\n",
      "Training Progress: \tEpoch 93 [640/8883 (7.19%)]\t\tLoss: 0.50313\n",
      "Training Progress: \tEpoch 93 [960/8883 (10.79%)]\t\tLoss: 0.67459\n",
      "Training Progress: \tEpoch 93 [1280/8883 (14.39%)]\t\tLoss: 1.01568\n",
      "Training Progress: \tEpoch 93 [1600/8883 (17.99%)]\t\tLoss: 1.00628\n",
      "Training Progress: \tEpoch 93 [1920/8883 (21.58%)]\t\tLoss: 0.72227\n",
      "Training Progress: \tEpoch 93 [2240/8883 (25.18%)]\t\tLoss: 0.83520\n",
      "Training Progress: \tEpoch 93 [2560/8883 (28.78%)]\t\tLoss: 0.76712\n",
      "Training Progress: \tEpoch 93 [2880/8883 (32.37%)]\t\tLoss: 0.87016\n",
      "Training Progress: \tEpoch 93 [3200/8883 (35.97%)]\t\tLoss: 0.77181\n",
      "Training Progress: \tEpoch 93 [3520/8883 (39.57%)]\t\tLoss: 0.82533\n",
      "Training Progress: \tEpoch 93 [3840/8883 (43.17%)]\t\tLoss: 0.84582\n",
      "Training Progress: \tEpoch 93 [4160/8883 (46.76%)]\t\tLoss: 0.72607\n",
      "Training Progress: \tEpoch 93 [4480/8883 (50.36%)]\t\tLoss: 0.71914\n",
      "Training Progress: \tEpoch 93 [4800/8883 (53.96%)]\t\tLoss: 0.56496\n",
      "Training Progress: \tEpoch 93 [5120/8883 (57.55%)]\t\tLoss: 0.97756\n",
      "Training Progress: \tEpoch 93 [5440/8883 (61.15%)]\t\tLoss: 0.84568\n",
      "Training Progress: \tEpoch 93 [5760/8883 (64.75%)]\t\tLoss: 0.86707\n",
      "Training Progress: \tEpoch 93 [6080/8883 (68.35%)]\t\tLoss: 0.64863\n",
      "Training Progress: \tEpoch 93 [6400/8883 (71.94%)]\t\tLoss: 1.02692\n",
      "Training Progress: \tEpoch 93 [6720/8883 (75.54%)]\t\tLoss: 0.78025\n",
      "Training Progress: \tEpoch 93 [7040/8883 (79.14%)]\t\tLoss: 0.77127\n",
      "Training Progress: \tEpoch 93 [7360/8883 (82.73%)]\t\tLoss: 0.68749\n",
      "Training Progress: \tEpoch 93 [7680/8883 (86.33%)]\t\tLoss: 0.60429\n",
      "Training Progress: \tEpoch 93 [8000/8883 (89.93%)]\t\tLoss: 0.72894\n",
      "Training Progress: \tEpoch 93 [8320/8883 (93.53%)]\t\tLoss: 0.54201\n",
      "Training Progress: \tEpoch 93 [8640/8883 (97.12%)]\t\tLoss: 0.79182\n",
      "\tTrain loss: 0.01300, Accuracy: 7199/8883 (81.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1603/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/8883 (0.00%)]\t\tLoss: 0.70257\n",
      "Training Progress: \tEpoch 94 [320/8883 (3.60%)]\t\tLoss: 0.84065\n",
      "Training Progress: \tEpoch 94 [640/8883 (7.19%)]\t\tLoss: 0.63410\n",
      "Training Progress: \tEpoch 94 [960/8883 (10.79%)]\t\tLoss: 0.67748\n",
      "Training Progress: \tEpoch 94 [1280/8883 (14.39%)]\t\tLoss: 1.00761\n",
      "Training Progress: \tEpoch 94 [1600/8883 (17.99%)]\t\tLoss: 1.02925\n",
      "Training Progress: \tEpoch 94 [1920/8883 (21.58%)]\t\tLoss: 0.59715\n",
      "Training Progress: \tEpoch 94 [2240/8883 (25.18%)]\t\tLoss: 0.76761\n",
      "Training Progress: \tEpoch 94 [2560/8883 (28.78%)]\t\tLoss: 0.88298\n",
      "Training Progress: \tEpoch 94 [2880/8883 (32.37%)]\t\tLoss: 0.56529\n",
      "Training Progress: \tEpoch 94 [3200/8883 (35.97%)]\t\tLoss: 0.76016\n",
      "Training Progress: \tEpoch 94 [3520/8883 (39.57%)]\t\tLoss: 0.95810\n",
      "Training Progress: \tEpoch 94 [3840/8883 (43.17%)]\t\tLoss: 0.82087\n",
      "Training Progress: \tEpoch 94 [4160/8883 (46.76%)]\t\tLoss: 0.97326\n",
      "Training Progress: \tEpoch 94 [4480/8883 (50.36%)]\t\tLoss: 0.88437\n",
      "Training Progress: \tEpoch 94 [4800/8883 (53.96%)]\t\tLoss: 0.66478\n",
      "Training Progress: \tEpoch 94 [5120/8883 (57.55%)]\t\tLoss: 0.68512\n",
      "Training Progress: \tEpoch 94 [5440/8883 (61.15%)]\t\tLoss: 0.87489\n",
      "Training Progress: \tEpoch 94 [5760/8883 (64.75%)]\t\tLoss: 1.05330\n",
      "Training Progress: \tEpoch 94 [6080/8883 (68.35%)]\t\tLoss: 0.68637\n",
      "Training Progress: \tEpoch 94 [6400/8883 (71.94%)]\t\tLoss: 1.16706\n",
      "Training Progress: \tEpoch 94 [6720/8883 (75.54%)]\t\tLoss: 0.78555\n",
      "Training Progress: \tEpoch 94 [7040/8883 (79.14%)]\t\tLoss: 0.76445\n",
      "Training Progress: \tEpoch 94 [7360/8883 (82.73%)]\t\tLoss: 0.58585\n",
      "Training Progress: \tEpoch 94 [7680/8883 (86.33%)]\t\tLoss: 0.72576\n",
      "Training Progress: \tEpoch 94 [8000/8883 (89.93%)]\t\tLoss: 0.75430\n",
      "Training Progress: \tEpoch 94 [8320/8883 (93.53%)]\t\tLoss: 0.68822\n",
      "Training Progress: \tEpoch 94 [8640/8883 (97.12%)]\t\tLoss: 0.70987\n",
      "\tTrain loss: 0.01289, Accuracy: 7165/8883 (80.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1610/1692 (95.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/8883 (0.00%)]\t\tLoss: 0.86334\n",
      "Training Progress: \tEpoch 95 [320/8883 (3.60%)]\t\tLoss: 0.82459\n",
      "Training Progress: \tEpoch 95 [640/8883 (7.19%)]\t\tLoss: 0.58955\n",
      "Training Progress: \tEpoch 95 [960/8883 (10.79%)]\t\tLoss: 0.65239\n",
      "Training Progress: \tEpoch 95 [1280/8883 (14.39%)]\t\tLoss: 0.72308\n",
      "Training Progress: \tEpoch 95 [1600/8883 (17.99%)]\t\tLoss: 0.79119\n",
      "Training Progress: \tEpoch 95 [1920/8883 (21.58%)]\t\tLoss: 0.64721\n",
      "Training Progress: \tEpoch 95 [2240/8883 (25.18%)]\t\tLoss: 0.49881\n",
      "Training Progress: \tEpoch 95 [2560/8883 (28.78%)]\t\tLoss: 0.71884\n",
      "Training Progress: \tEpoch 95 [2880/8883 (32.37%)]\t\tLoss: 0.60914\n",
      "Training Progress: \tEpoch 95 [3200/8883 (35.97%)]\t\tLoss: 0.95831\n",
      "Training Progress: \tEpoch 95 [3520/8883 (39.57%)]\t\tLoss: 0.76704\n",
      "Training Progress: \tEpoch 95 [3840/8883 (43.17%)]\t\tLoss: 0.87604\n",
      "Training Progress: \tEpoch 95 [4160/8883 (46.76%)]\t\tLoss: 0.72488\n",
      "Training Progress: \tEpoch 95 [4480/8883 (50.36%)]\t\tLoss: 0.79371\n",
      "Training Progress: \tEpoch 95 [4800/8883 (53.96%)]\t\tLoss: 0.72522\n",
      "Training Progress: \tEpoch 95 [5120/8883 (57.55%)]\t\tLoss: 0.84108\n",
      "Training Progress: \tEpoch 95 [5440/8883 (61.15%)]\t\tLoss: 0.80750\n",
      "Training Progress: \tEpoch 95 [5760/8883 (64.75%)]\t\tLoss: 0.87120\n",
      "Training Progress: \tEpoch 95 [6080/8883 (68.35%)]\t\tLoss: 0.78003\n",
      "Training Progress: \tEpoch 95 [6400/8883 (71.94%)]\t\tLoss: 0.92621\n",
      "Training Progress: \tEpoch 95 [6720/8883 (75.54%)]\t\tLoss: 0.99190\n",
      "Training Progress: \tEpoch 95 [7040/8883 (79.14%)]\t\tLoss: 0.81054\n",
      "Training Progress: \tEpoch 95 [7360/8883 (82.73%)]\t\tLoss: 0.60853\n",
      "Training Progress: \tEpoch 95 [7680/8883 (86.33%)]\t\tLoss: 0.67690\n",
      "Training Progress: \tEpoch 95 [8000/8883 (89.93%)]\t\tLoss: 0.70256\n",
      "Training Progress: \tEpoch 95 [8320/8883 (93.53%)]\t\tLoss: 0.56566\n",
      "Training Progress: \tEpoch 95 [8640/8883 (97.12%)]\t\tLoss: 0.76941\n",
      "\tTrain loss: 0.01329, Accuracy: 7170/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1597/1692 (94.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1081/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/8883 (0.00%)]\t\tLoss: 0.84911\n",
      "Training Progress: \tEpoch 96 [320/8883 (3.60%)]\t\tLoss: 0.96291\n",
      "Training Progress: \tEpoch 96 [640/8883 (7.19%)]\t\tLoss: 0.61880\n",
      "Training Progress: \tEpoch 96 [960/8883 (10.79%)]\t\tLoss: 0.59247\n",
      "Training Progress: \tEpoch 96 [1280/8883 (14.39%)]\t\tLoss: 0.82028\n",
      "Training Progress: \tEpoch 96 [1600/8883 (17.99%)]\t\tLoss: 0.88065\n",
      "Training Progress: \tEpoch 96 [1920/8883 (21.58%)]\t\tLoss: 0.66297\n",
      "Training Progress: \tEpoch 96 [2240/8883 (25.18%)]\t\tLoss: 0.66773\n",
      "Training Progress: \tEpoch 96 [2560/8883 (28.78%)]\t\tLoss: 0.80027\n",
      "Training Progress: \tEpoch 96 [2880/8883 (32.37%)]\t\tLoss: 0.58964\n",
      "Training Progress: \tEpoch 96 [3200/8883 (35.97%)]\t\tLoss: 0.93387\n",
      "Training Progress: \tEpoch 96 [3520/8883 (39.57%)]\t\tLoss: 0.99184\n",
      "Training Progress: \tEpoch 96 [3840/8883 (43.17%)]\t\tLoss: 0.96370\n",
      "Training Progress: \tEpoch 96 [4160/8883 (46.76%)]\t\tLoss: 0.94870\n",
      "Training Progress: \tEpoch 96 [4480/8883 (50.36%)]\t\tLoss: 0.51682\n",
      "Training Progress: \tEpoch 96 [4800/8883 (53.96%)]\t\tLoss: 0.76563\n",
      "Training Progress: \tEpoch 96 [5120/8883 (57.55%)]\t\tLoss: 0.65081\n",
      "Training Progress: \tEpoch 96 [5440/8883 (61.15%)]\t\tLoss: 0.91982\n",
      "Training Progress: \tEpoch 96 [5760/8883 (64.75%)]\t\tLoss: 0.92687\n",
      "Training Progress: \tEpoch 96 [6080/8883 (68.35%)]\t\tLoss: 0.72730\n",
      "Training Progress: \tEpoch 96 [6400/8883 (71.94%)]\t\tLoss: 0.66360\n",
      "Training Progress: \tEpoch 96 [6720/8883 (75.54%)]\t\tLoss: 0.92969\n",
      "Training Progress: \tEpoch 96 [7040/8883 (79.14%)]\t\tLoss: 0.58105\n",
      "Training Progress: \tEpoch 96 [7360/8883 (82.73%)]\t\tLoss: 0.60204\n",
      "Training Progress: \tEpoch 96 [7680/8883 (86.33%)]\t\tLoss: 0.92920\n",
      "Training Progress: \tEpoch 96 [8000/8883 (89.93%)]\t\tLoss: 0.74662\n",
      "Training Progress: \tEpoch 96 [8320/8883 (93.53%)]\t\tLoss: 0.64737\n",
      "Training Progress: \tEpoch 96 [8640/8883 (97.12%)]\t\tLoss: 0.80411\n",
      "\tTrain loss: 0.01344, Accuracy: 7154/8883 (80.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1601/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1069/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/8883 (0.00%)]\t\tLoss: 0.83988\n",
      "Training Progress: \tEpoch 97 [320/8883 (3.60%)]\t\tLoss: 0.75448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 97 [640/8883 (7.19%)]\t\tLoss: 0.83094\n",
      "Training Progress: \tEpoch 97 [960/8883 (10.79%)]\t\tLoss: 0.63430\n",
      "Training Progress: \tEpoch 97 [1280/8883 (14.39%)]\t\tLoss: 0.73771\n",
      "Training Progress: \tEpoch 97 [1600/8883 (17.99%)]\t\tLoss: 0.73405\n",
      "Training Progress: \tEpoch 97 [1920/8883 (21.58%)]\t\tLoss: 0.59671\n",
      "Training Progress: \tEpoch 97 [2240/8883 (25.18%)]\t\tLoss: 0.59758\n",
      "Training Progress: \tEpoch 97 [2560/8883 (28.78%)]\t\tLoss: 0.59257\n",
      "Training Progress: \tEpoch 97 [2880/8883 (32.37%)]\t\tLoss: 0.64288\n",
      "Training Progress: \tEpoch 97 [3200/8883 (35.97%)]\t\tLoss: 0.75842\n",
      "Training Progress: \tEpoch 97 [3520/8883 (39.57%)]\t\tLoss: 0.75112\n",
      "Training Progress: \tEpoch 97 [3840/8883 (43.17%)]\t\tLoss: 0.67461\n",
      "Training Progress: \tEpoch 97 [4160/8883 (46.76%)]\t\tLoss: 0.71283\n",
      "Training Progress: \tEpoch 97 [4480/8883 (50.36%)]\t\tLoss: 0.66324\n",
      "Training Progress: \tEpoch 97 [4800/8883 (53.96%)]\t\tLoss: 0.75096\n",
      "Training Progress: \tEpoch 97 [5120/8883 (57.55%)]\t\tLoss: 0.78857\n",
      "Training Progress: \tEpoch 97 [5440/8883 (61.15%)]\t\tLoss: 0.82298\n",
      "Training Progress: \tEpoch 97 [5760/8883 (64.75%)]\t\tLoss: 0.99586\n",
      "Training Progress: \tEpoch 97 [6080/8883 (68.35%)]\t\tLoss: 0.88948\n",
      "Training Progress: \tEpoch 97 [6400/8883 (71.94%)]\t\tLoss: 0.70657\n",
      "Training Progress: \tEpoch 97 [6720/8883 (75.54%)]\t\tLoss: 0.71564\n",
      "Training Progress: \tEpoch 97 [7040/8883 (79.14%)]\t\tLoss: 0.64848\n",
      "Training Progress: \tEpoch 97 [7360/8883 (82.73%)]\t\tLoss: 0.59943\n",
      "Training Progress: \tEpoch 97 [7680/8883 (86.33%)]\t\tLoss: 0.84198\n",
      "Training Progress: \tEpoch 97 [8000/8883 (89.93%)]\t\tLoss: 0.74570\n",
      "Training Progress: \tEpoch 97 [8320/8883 (93.53%)]\t\tLoss: 0.98914\n",
      "Training Progress: \tEpoch 97 [8640/8883 (97.12%)]\t\tLoss: 0.78001\n",
      "\tTrain loss: 0.01286, Accuracy: 7197/8883 (81.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1608/1692 (95.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1112/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/8883 (0.00%)]\t\tLoss: 0.70031\n",
      "Training Progress: \tEpoch 98 [320/8883 (3.60%)]\t\tLoss: 0.77449\n",
      "Training Progress: \tEpoch 98 [640/8883 (7.19%)]\t\tLoss: 0.70445\n",
      "Training Progress: \tEpoch 98 [960/8883 (10.79%)]\t\tLoss: 0.57313\n",
      "Training Progress: \tEpoch 98 [1280/8883 (14.39%)]\t\tLoss: 0.76002\n",
      "Training Progress: \tEpoch 98 [1600/8883 (17.99%)]\t\tLoss: 0.87054\n",
      "Training Progress: \tEpoch 98 [1920/8883 (21.58%)]\t\tLoss: 0.54804\n",
      "Training Progress: \tEpoch 98 [2240/8883 (25.18%)]\t\tLoss: 0.62582\n",
      "Training Progress: \tEpoch 98 [2560/8883 (28.78%)]\t\tLoss: 0.75600\n",
      "Training Progress: \tEpoch 98 [2880/8883 (32.37%)]\t\tLoss: 0.65161\n",
      "Training Progress: \tEpoch 98 [3200/8883 (35.97%)]\t\tLoss: 0.89574\n",
      "Training Progress: \tEpoch 98 [3520/8883 (39.57%)]\t\tLoss: 0.78735\n",
      "Training Progress: \tEpoch 98 [3840/8883 (43.17%)]\t\tLoss: 1.33948\n",
      "Training Progress: \tEpoch 98 [4160/8883 (46.76%)]\t\tLoss: 0.74001\n",
      "Training Progress: \tEpoch 98 [4480/8883 (50.36%)]\t\tLoss: 0.74593\n",
      "Training Progress: \tEpoch 98 [4800/8883 (53.96%)]\t\tLoss: 0.49811\n",
      "Training Progress: \tEpoch 98 [5120/8883 (57.55%)]\t\tLoss: 0.71204\n",
      "Training Progress: \tEpoch 98 [5440/8883 (61.15%)]\t\tLoss: 0.86357\n",
      "Training Progress: \tEpoch 98 [5760/8883 (64.75%)]\t\tLoss: 0.83214\n",
      "Training Progress: \tEpoch 98 [6080/8883 (68.35%)]\t\tLoss: 0.85164\n",
      "Training Progress: \tEpoch 98 [6400/8883 (71.94%)]\t\tLoss: 0.79351\n",
      "Training Progress: \tEpoch 98 [6720/8883 (75.54%)]\t\tLoss: 0.92585\n",
      "Training Progress: \tEpoch 98 [7040/8883 (79.14%)]\t\tLoss: 0.83116\n",
      "Training Progress: \tEpoch 98 [7360/8883 (82.73%)]\t\tLoss: 1.10901\n",
      "Training Progress: \tEpoch 98 [7680/8883 (86.33%)]\t\tLoss: 0.54690\n",
      "Training Progress: \tEpoch 98 [8000/8883 (89.93%)]\t\tLoss: 0.75331\n",
      "Training Progress: \tEpoch 98 [8320/8883 (93.53%)]\t\tLoss: 0.67375\n",
      "Training Progress: \tEpoch 98 [8640/8883 (97.12%)]\t\tLoss: 0.96719\n",
      "\tTrain loss: 0.01318, Accuracy: 7217/8883 (81.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1603/1692 (94.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1040/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/8883 (0.00%)]\t\tLoss: 0.80454\n",
      "Training Progress: \tEpoch 99 [320/8883 (3.60%)]\t\tLoss: 0.62160\n",
      "Training Progress: \tEpoch 99 [640/8883 (7.19%)]\t\tLoss: 0.49489\n",
      "Training Progress: \tEpoch 99 [960/8883 (10.79%)]\t\tLoss: 0.72193\n",
      "Training Progress: \tEpoch 99 [1280/8883 (14.39%)]\t\tLoss: 0.76155\n",
      "Training Progress: \tEpoch 99 [1600/8883 (17.99%)]\t\tLoss: 0.80719\n",
      "Training Progress: \tEpoch 99 [1920/8883 (21.58%)]\t\tLoss: 0.72135\n",
      "Training Progress: \tEpoch 99 [2240/8883 (25.18%)]\t\tLoss: 0.62331\n",
      "Training Progress: \tEpoch 99 [2560/8883 (28.78%)]\t\tLoss: 0.55956\n",
      "Training Progress: \tEpoch 99 [2880/8883 (32.37%)]\t\tLoss: 0.64754\n",
      "Training Progress: \tEpoch 99 [3200/8883 (35.97%)]\t\tLoss: 0.86758\n",
      "Training Progress: \tEpoch 99 [3520/8883 (39.57%)]\t\tLoss: 0.92498\n",
      "Training Progress: \tEpoch 99 [3840/8883 (43.17%)]\t\tLoss: 1.01513\n",
      "Training Progress: \tEpoch 99 [4160/8883 (46.76%)]\t\tLoss: 0.87953\n",
      "Training Progress: \tEpoch 99 [4480/8883 (50.36%)]\t\tLoss: 0.72976\n",
      "Training Progress: \tEpoch 99 [4800/8883 (53.96%)]\t\tLoss: 0.51694\n",
      "Training Progress: \tEpoch 99 [5120/8883 (57.55%)]\t\tLoss: 1.27086\n",
      "Training Progress: \tEpoch 99 [5440/8883 (61.15%)]\t\tLoss: 0.81138\n",
      "Training Progress: \tEpoch 99 [5760/8883 (64.75%)]\t\tLoss: 0.93266\n",
      "Training Progress: \tEpoch 99 [6080/8883 (68.35%)]\t\tLoss: 0.94539\n",
      "Training Progress: \tEpoch 99 [6400/8883 (71.94%)]\t\tLoss: 0.75306\n",
      "Training Progress: \tEpoch 99 [6720/8883 (75.54%)]\t\tLoss: 0.81221\n",
      "Training Progress: \tEpoch 99 [7040/8883 (79.14%)]\t\tLoss: 0.74862\n",
      "Training Progress: \tEpoch 99 [7360/8883 (82.73%)]\t\tLoss: 0.56785\n",
      "Training Progress: \tEpoch 99 [7680/8883 (86.33%)]\t\tLoss: 0.81982\n",
      "Training Progress: \tEpoch 99 [8000/8883 (89.93%)]\t\tLoss: 0.87413\n",
      "Training Progress: \tEpoch 99 [8320/8883 (93.53%)]\t\tLoss: 0.88913\n",
      "Training Progress: \tEpoch 99 [8640/8883 (97.12%)]\t\tLoss: 0.81733\n",
      "\tTrain loss: 0.01254, Accuracy: 7230/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1620/1692 (95.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/8883 (0.00%)]\t\tLoss: 0.68305\n",
      "Training Progress: \tEpoch 100 [320/8883 (3.60%)]\t\tLoss: 0.82417\n",
      "Training Progress: \tEpoch 100 [640/8883 (7.19%)]\t\tLoss: 0.62920\n",
      "Training Progress: \tEpoch 100 [960/8883 (10.79%)]\t\tLoss: 0.55935\n",
      "Training Progress: \tEpoch 100 [1280/8883 (14.39%)]\t\tLoss: 0.77420\n",
      "Training Progress: \tEpoch 100 [1600/8883 (17.99%)]\t\tLoss: 0.72620\n",
      "Training Progress: \tEpoch 100 [1920/8883 (21.58%)]\t\tLoss: 0.63751\n",
      "Training Progress: \tEpoch 100 [2240/8883 (25.18%)]\t\tLoss: 0.90050\n",
      "Training Progress: \tEpoch 100 [2560/8883 (28.78%)]\t\tLoss: 0.64807\n",
      "Training Progress: \tEpoch 100 [2880/8883 (32.37%)]\t\tLoss: 0.82954\n",
      "Training Progress: \tEpoch 100 [3200/8883 (35.97%)]\t\tLoss: 0.77674\n",
      "Training Progress: \tEpoch 100 [3520/8883 (39.57%)]\t\tLoss: 0.80910\n",
      "Training Progress: \tEpoch 100 [3840/8883 (43.17%)]\t\tLoss: 0.92558\n",
      "Training Progress: \tEpoch 100 [4160/8883 (46.76%)]\t\tLoss: 0.69089\n",
      "Training Progress: \tEpoch 100 [4480/8883 (50.36%)]\t\tLoss: 0.71549\n",
      "Training Progress: \tEpoch 100 [4800/8883 (53.96%)]\t\tLoss: 0.70264\n",
      "Training Progress: \tEpoch 100 [5120/8883 (57.55%)]\t\tLoss: 1.10222\n",
      "Training Progress: \tEpoch 100 [5440/8883 (61.15%)]\t\tLoss: 0.77613\n",
      "Training Progress: \tEpoch 100 [5760/8883 (64.75%)]\t\tLoss: 0.69585\n",
      "Training Progress: \tEpoch 100 [6080/8883 (68.35%)]\t\tLoss: 0.69043\n",
      "Training Progress: \tEpoch 100 [6400/8883 (71.94%)]\t\tLoss: 0.90124\n",
      "Training Progress: \tEpoch 100 [6720/8883 (75.54%)]\t\tLoss: 0.80322\n",
      "Training Progress: \tEpoch 100 [7040/8883 (79.14%)]\t\tLoss: 0.92353\n",
      "Training Progress: \tEpoch 100 [7360/8883 (82.73%)]\t\tLoss: 0.96068\n",
      "Training Progress: \tEpoch 100 [7680/8883 (86.33%)]\t\tLoss: 0.63191\n",
      "Training Progress: \tEpoch 100 [8000/8883 (89.93%)]\t\tLoss: 0.67738\n",
      "Training Progress: \tEpoch 100 [8320/8883 (93.53%)]\t\tLoss: 0.79956\n",
      "Training Progress: \tEpoch 100 [8640/8883 (97.12%)]\t\tLoss: 0.66699\n",
      "\tTrain loss: 0.01261, Accuracy: 7216/8883 (81.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1627/1692 (96.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9615839243498818\n",
      "Best test accuracy:\n",
      "0.6275395033860045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_gru, cnn_gru_optimizer, data_loaders_wgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ca723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
