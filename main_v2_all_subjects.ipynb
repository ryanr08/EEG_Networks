{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92746a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from train_evaluate import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7aae3",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85a7dd",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0824322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/michael/Desktop/Home/研究所/ECE247/projects/data/'\n",
    "X_train_valid, y_train_valid, X_test, y_test = load_data(data_dir, subjects=[1,2,3,4,5,6,7,8,9]) # default subjects=[1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb1db",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68646e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (6768, 250, 1, 22)\n",
      "Shape of x_valid: (1692, 250, 1, 22)\n",
      "Shape of x_test: (1772, 250, 1, 22)\n",
      "Shape of y_train: torch.Size([6768, 4])\n",
      "Shape of y_valid: torch.Size([1692, 4])\n",
      "Shape of y_test: torch.Size([1772, 4])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = main_prep(X_train_valid,y_train_valid,X_test, y_test,2,2,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6fc9c",
   "metadata": {},
   "source": [
    "## PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders  = dataloader_setup(x_train, y_train, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af55041",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe828a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR      = 0.0005\n",
    "BETAS   = (0.9, 0.999)\n",
    "EPS     = 1e-08\n",
    "DECAY   = 0.0005\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7eae4",
   "metadata": {},
   "source": [
    "# Modeling (CNN, LSTM, GRU, CNN+LSTM, CNN+GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897c535",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464c4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the CNN model using sequential class\n",
    "class CNN(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(800,4)\n",
    "        \n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):  #input(22,250,1)\n",
    "        x = input.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "        x = self.flatten(x)\n",
    "        out = self.dense(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203a809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate LSTM model\n",
    "cnn = CNN()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fbcd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.46058\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.57104\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.49610\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.61831\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.57288\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.44407\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.55952\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.69810\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.41124\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.29225\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.34996\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.43967\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.46265\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.55334\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.41508\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.48072\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.49895\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.64448\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.40173\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.40148\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.27342\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.26221\n",
      "\tTrain loss: 0.04156, Accuracy: 2499/6768 (36.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 598/1692 (35.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 644/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.35304\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.66531\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.16983\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.30459\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.79487\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.02945\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.42522\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.63973\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.27245\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 0.95786\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.37013\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.31753\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.15976\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.39520\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.32156\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.34157\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.31306\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.70862\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.33166\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.37185\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.16101\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.35412\n",
      "\tTrain loss: 0.03747, Accuracy: 3100/6768 (45.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.16881\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.48553\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.42279\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.12780\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.09388\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.09808\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.44969\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.20931\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.15784\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.01204\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.21901\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.02420\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.04160\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.26786\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.13740\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.12389\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.35521\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.46005\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.18786\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.03360\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.20590\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.44382\n",
      "\tTrain loss: 0.03538, Accuracy: 3565/6768 (52.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 860/1692 (50.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 862/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.11399\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.19588\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.30453\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.39773\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.13389\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 0.82440\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.25137\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.12961\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 0.98581\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 0.96778\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 0.91117\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.02711\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 0.85589\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.07862\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 0.99248\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.18611\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.30188\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.42878\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.15680\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.16836\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.04471\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.17261\n",
      "\tTrain loss: 0.03572, Accuracy: 3473/6768 (51.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 891/1692 (52.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 782/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.19542\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.26016\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.28201\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.16676\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.31926\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 0.81758\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.23813\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.02291\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 0.90667\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 0.82183\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 0.87243\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.03789\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 0.99342\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.13289\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 0.95280\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.03726\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.21671\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.42785\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.23199\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.16270\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 0.92771\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.25207\n",
      "\tTrain loss: 0.03229, Accuracy: 3945/6768 (58.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 982/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 886/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.11258\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.19043\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.36673\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 0.97807\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.10951\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 0.63936\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.25698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.17823\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 0.91725\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 0.84757\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 0.83353\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.07420\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 0.97282\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.08034\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 0.82544\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 0.98334\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.13256\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.55104\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.08462\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.15319\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 0.99871\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.14440\n",
      "\tTrain loss: 0.03202, Accuracy: 3842/6768 (56.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 987/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 0.92940\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.29454\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.34733\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 0.95640\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.15591\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 0.78983\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.17299\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.00895\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 0.86137\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.10400\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 0.82715\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 0.87154\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 0.93767\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.17380\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 0.90851\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.01038\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.06621\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.16099\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.06055\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.31696\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.04677\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 0.87673\n",
      "\tTrain loss: 0.03037, Accuracy: 4093/6768 (60.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1028/1692 (60.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 915/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.15009\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.49134\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.16839\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.04000\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.25920\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.66198\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.24477\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 0.80363\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 0.89378\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 0.66341\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 0.72684\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 0.77544\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 0.93979\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.17059\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 0.89923\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 0.94050\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 0.93963\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.57701\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 0.89979\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.06499\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 0.85231\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.05111\n",
      "\tTrain loss: 0.02985, Accuracy: 4031/6768 (59.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1019/1692 (60.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 895/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 0.90755\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.17866\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.35470\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.07300\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.17082\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.92329\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.28826\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 0.70182\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.02297\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 0.89620\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 0.81938\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.06121\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 0.96772\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.03575\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 0.78536\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 0.98944\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 0.89224\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.36049\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.07479\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.14961\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 0.68195\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.06182\n",
      "\tTrain loss: 0.02887, Accuracy: 4243/6768 (62.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1066/1692 (63.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.01643\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.34016\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.15782\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.07192\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 0.91009\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 0.80025\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.01330\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 0.82081\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 0.88665\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 0.70861\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 0.70825\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 0.98479\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 0.75233\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.17456\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 0.81559\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 0.80664\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.14185\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.35561\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 0.93769\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 0.96973\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 0.77402\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 0.80447\n",
      "\tTrain loss: 0.02832, Accuracy: 4230/6768 (62.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1051/1692 (62.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 942/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 0.97734\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.34246\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.01708\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 0.91726\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 0.85768\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.66925\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 0.90690\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 0.64021\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 0.84207\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 0.65901\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.66052\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 0.75638\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 0.71664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 0.83125\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 0.87285\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 0.85606\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 0.86581\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.32205\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 0.89799\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.12713\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 0.82909\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 0.95762\n",
      "\tTrain loss: 0.02787, Accuracy: 4390/6768 (64.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1095/1692 (64.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 973/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.76999\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.35205\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.32626\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 0.86877\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.06208\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.48944\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 0.93596\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 0.60196\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 0.88995\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 0.63070\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.62982\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 0.86798\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.81658\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.05897\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 0.74182\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 0.85674\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.98850\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.34917\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.79107\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.24197\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 0.77520\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 0.98606\n",
      "\tTrain loss: 0.02791, Accuracy: 4369/6768 (64.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1089/1692 (64.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 981/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.99465\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.44701\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.11402\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 0.96264\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.14763\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.75306\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.17816\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 0.75095\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 0.75393\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 0.58185\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.80941\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 0.80406\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.66748\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 0.95378\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 0.70806\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 0.83263\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.13758\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.09346\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 0.74460\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.06136\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.82954\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 0.89348\n",
      "\tTrain loss: 0.02632, Accuracy: 4563/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1115/1692 (65.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.85452\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.14405\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.19117\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 0.93909\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 0.83924\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.56282\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 0.90651\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.64923\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 0.85336\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 0.81572\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.83228\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.89437\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.72720\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 0.79792\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 0.63668\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 0.74357\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.81671\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.01228\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.78465\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 0.81770\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 0.57948\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.64571\n",
      "\tTrain loss: 0.02609, Accuracy: 4526/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1101/1692 (65.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 991/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.69742\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.05296\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.33339\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.76734\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.03384\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.58666\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 0.94086\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.72188\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 0.74349\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.79119\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.64100\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 0.72258\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.61409\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 0.84664\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 0.78979\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.78225\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.99810\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.11914\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.91502\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.94154\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.64244\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.84690\n",
      "\tTrain loss: 0.02330, Accuracy: 4751/6768 (70.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1162/1692 (68.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1041/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.86876\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.24986\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.03818\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.79957\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 0.82095\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.61999\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 0.73877\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.51107\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.70869\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.52172\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.74717\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.92046\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.58309\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.64850\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.66462\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.90418\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.96743\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.42239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.71993\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.90007\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.62674\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.84553\n",
      "\tTrain loss: 0.02338, Accuracy: 4800/6768 (70.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1188/1692 (70.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.86521\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 0.81813\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.98331\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.97789\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.67420\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.70739\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 0.78738\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.74357\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.72100\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.60120\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.50354\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.54306\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.75014\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.69874\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.61853\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.58189\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.82429\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.21543\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.76888\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.09536\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.67083\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.78721\n",
      "\tTrain loss: 0.02300, Accuracy: 4695/6768 (69.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1140/1692 (67.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 991/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.71383\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.89326\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.00949\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.89165\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.94684\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.49718\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 0.75493\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.71139\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.71239\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.64736\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.94636\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.55807\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.58980\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.50392\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.82090\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.96819\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 0.77301\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.63351\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.10766\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.43943\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.64070\n",
      "\tTrain loss: 0.02341, Accuracy: 4836/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1187/1692 (70.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1027/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.66986\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 0.79998\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.82048\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.73880\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.83388\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.48991\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.77421\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.61776\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.65753\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.66759\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.59260\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.72079\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.38865\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.70993\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.87472\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.80110\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.76678\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.94637\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.75027\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 1.09004\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.29888\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.67704\n",
      "\tTrain loss: 0.02163, Accuracy: 5036/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1214/1692 (71.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.73485\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.81418\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.88234\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.87499\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.69486\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.46616\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.80553\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.33476\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.73258\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.70248\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.79110\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.51476\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.69970\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.82589\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.57833\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.74135\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.83351\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.21859\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.66471\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.11349\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.59537\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.59194\n",
      "\tTrain loss: 0.02104, Accuracy: 4936/6768 (72.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1197/1692 (70.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.61973\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.03490\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.96774\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.97715\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.91541\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.41308\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.69929\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.52016\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.68287\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.35910\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.83830\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.52433\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.49360\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.80358\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.72457\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.74918\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.76429\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.94181\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.78480\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.73727\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.68626\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.81484\n",
      "\tTrain loss: 0.02062, Accuracy: 5022/6768 (74.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00044, Accuracy: 1215/1692 (71.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.60063\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.91485\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.78569\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.71172\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.83234\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.36491\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.78550\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.40748\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.68612\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.46426\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.56391\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.53701\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.70932\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.92378\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.68323\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.72472\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.89181\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.01442\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.59517\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.85313\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.58822\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.61717\n",
      "\tTrain loss: 0.02224, Accuracy: 4940/6768 (72.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1023/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.62248\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.76094\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.64055\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.73692\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.61860\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.44317\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.55155\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.37214\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.67819\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.55431\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.58537\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.62149\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.42793\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.72842\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.57792\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.61545\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.79082\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.92661\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.87175\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.76351\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.48307\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.82021\n",
      "\tTrain loss: 0.01719, Accuracy: 5435/6768 (80.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1300/1692 (76.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1081/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.63890\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.90015\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.99377\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.56276\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.83382\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.48330\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.89874\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.47521\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.91843\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.42357\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.50433\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.49978\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.46249\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.79395\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.84074\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.60802\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.82407\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.92783\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.51015\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.60671\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.29930\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.81363\n",
      "\tTrain loss: 0.01919, Accuracy: 5183/6768 (76.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.69288\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.66013\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.82779\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.94126\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.94673\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.44532\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.53754\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.41419\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.58104\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.55357\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.52513\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.69900\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.52385\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.71226\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.51863\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.63035\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.57599\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.80729\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.62321\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.82696\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.45415\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.73439\n",
      "\tTrain loss: 0.01765, Accuracy: 5362/6768 (79.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.60779\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.66595\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 1.00869\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.88875\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.78976\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.48056\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.55465\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.43057\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.68568\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.50686\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.41767\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.76831\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.39684\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.65049\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.49884\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.54622\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.62274\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.05230\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.85414\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.90708\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.34354\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.66030\n",
      "\tTrain loss: 0.01904, Accuracy: 5103/6768 (75.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1211/1692 (71.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.75062\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.17655\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.73019\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.81836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.74546\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.42305\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.57351\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.33445\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.91514\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.59222\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.85216\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.56071\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.52368\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.63787\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.68962\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.79615\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.68363\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.71077\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.61897\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.75924\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.35389\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.46148\n",
      "\tTrain loss: 0.01628, Accuracy: 5349/6768 (79.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1042/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.53128\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.52942\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.88619\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.78546\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.63487\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.40384\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.58145\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.39875\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.63763\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.62119\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.40913\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.64956\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.44038\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.75426\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.65132\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.44044\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.50673\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.91296\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.47078\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.93358\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.53000\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.65146\n",
      "\tTrain loss: 0.01570, Accuracy: 5550/6768 (82.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1308/1692 (77.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.40842\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.57753\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.61831\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.46987\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.72923\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.53029\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.81467\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.40331\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.76227\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.41264\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.55900\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.43698\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.55171\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.61489\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.68682\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.48873\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.91607\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.63528\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.47950\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.60322\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.37633\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.65404\n",
      "\tTrain loss: 0.01748, Accuracy: 5433/6768 (80.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1291/1692 (76.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.57704\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.49904\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.85178\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.53352\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.74819\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.36906\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.69931\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.32156\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.50360\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.37840\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.57656\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.54831\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.36816\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.47701\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.42087\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.65920\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.71426\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.53871\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.63442\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.68984\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.33840\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.57253\n",
      "\tTrain loss: 0.01633, Accuracy: 5459/6768 (80.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1284/1692 (75.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1053/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.63808\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.69432\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.61059\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.50825\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.38899\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.33357\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.41377\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.57442\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.48586\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.54480\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.48478\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.50091\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.40311\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.64764\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.70609\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.54376\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.66709\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.84309\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.52115\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.71542\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.27726\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.66197\n",
      "\tTrain loss: 0.01464, Accuracy: 5591/6768 (82.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1319/1692 (77.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.39735\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.63324\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.56073\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.49397\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.62418\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.38436\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.43051\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.42208\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.38887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.47676\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.51061\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.58589\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.30969\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.52628\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.65695\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.60501\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.69879\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.50006\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.41784\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.66370\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.57842\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.70504\n",
      "\tTrain loss: 0.01606, Accuracy: 5576/6768 (82.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1311/1692 (77.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1056/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.64800\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.90156\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.72959\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.67766\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.69804\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.32512\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.47624\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.36778\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.54477\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.30788\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.73894\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.46288\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.49675\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.46614\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.52009\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.57175\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.58518\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.65784\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.51768\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.57660\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.28588\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.56124\n",
      "\tTrain loss: 0.01626, Accuracy: 5470/6768 (80.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1293/1692 (76.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.57186\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.43519\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.42750\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.71449\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.67684\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.24630\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.41420\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.32890\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.73898\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.56377\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.60362\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.39642\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.51775\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.47000\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.37098\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.52251\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.53573\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.89331\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.45492\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.50733\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.36174\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.54482\n",
      "\tTrain loss: 0.01416, Accuracy: 5672/6768 (83.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1039/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.75511\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.71679\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.46190\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.61043\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.71016\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.33557\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.51415\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.35765\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.40812\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.34855\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.31963\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.72047\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.44914\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.28948\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.60399\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.63413\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.72479\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.56280\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.58846\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.61455\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.58787\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.46244\n",
      "\tTrain loss: 0.01227, Accuracy: 5855/6768 (86.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1082/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.70670\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.80583\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.47217\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.33198\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.56082\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.23021\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.35283\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.48109\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.51262\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.39216\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.37511\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.39163\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.30637\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.52733\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.42003\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.42380\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.64815\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.68133\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.50510\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.48074\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.22768\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.32943\n",
      "\tTrain loss: 0.01171, Accuracy: 5947/6768 (87.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1406/1692 (83.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.31921\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.52102\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.72233\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.52501\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.32398\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.41010\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.46038\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.42840\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.55333\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.32546\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.30455\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.68700\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.45432\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.56185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.48214\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.55530\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.58860\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.80711\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.42600\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.65740\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.48513\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.74094\n",
      "\tTrain loss: 0.01337, Accuracy: 5684/6768 (83.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1350/1692 (79.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1053/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.78943\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.84139\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.40784\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.42759\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.34319\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.37124\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.44474\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.37462\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.48628\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.34041\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.58625\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.58405\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.47438\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.66655\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.53752\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.53111\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.57458\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.66402\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.63633\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.83622\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.32646\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.48468\n",
      "\tTrain loss: 0.01386, Accuracy: 5659/6768 (83.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.48821\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.82301\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.75204\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.60309\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.52892\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.33229\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.53933\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.32492\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.60103\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.55747\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.33151\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.60119\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.41732\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.51635\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.52314\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.64944\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.50592\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.35006\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.34858\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.55116\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.54589\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.46415\n",
      "\tTrain loss: 0.01193, Accuracy: 5871/6768 (86.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1389/1692 (82.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1060/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.33406\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.48842\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.42889\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.79265\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.76922\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.44586\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.58748\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.30930\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.45979\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.63153\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.57749\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.53824\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.44580\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.31478\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.51478\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.48758\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.50888\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.52579\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.61148\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.57361\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.58252\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.53415\n",
      "\tTrain loss: 0.01141, Accuracy: 5944/6768 (87.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1410/1692 (83.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.58221\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.50914\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.48011\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.34381\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.33744\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.31642\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.52831\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.41047\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.88252\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.54918\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.36488\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.37958\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.27350\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.60776\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.60087\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.44833\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.70389\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.45136\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.54438\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.56929\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.61813\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.52909\n",
      "\tTrain loss: 0.01242, Accuracy: 5812/6768 (85.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1383/1692 (81.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1063/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.27612\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.52679\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.62176\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.50622\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.49370\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.19110\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.39366\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.41939\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.36407\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.43475\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.46640\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.50205\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.40713\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.43325\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.25097\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.47257\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.30677\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.62222\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.55092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.56647\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.33646\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.51794\n",
      "\tTrain loss: 0.01102, Accuracy: 5907/6768 (87.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1408/1692 (83.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1054/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.63658\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.65690\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.50616\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.64885\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.83837\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.29780\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.52346\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.40741\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.40898\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.53961\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.37575\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.65183\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.46009\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.61637\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.44213\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.47818\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.59184\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.64178\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.44585\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.53538\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.39705\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.39194\n",
      "\tTrain loss: 0.01043, Accuracy: 6029/6768 (89.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1432/1692 (84.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.45423\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.21563\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.41986\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.33667\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.65329\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.34303\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.51946\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.33807\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.38462\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.41665\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.41775\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.53705\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.25057\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.51624\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.47196\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.51486\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.37523\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.51250\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.58022\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.45502\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.25410\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.50372\n",
      "\tTrain loss: 0.01096, Accuracy: 5853/6768 (86.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1372/1692 (81.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1062/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.41176\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.71275\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.39445\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.35963\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.39999\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.19967\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.37442\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.40582\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.22802\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.55491\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.37412\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.41325\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.46719\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.54765\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.51735\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.45890\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.52060\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.48990\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.48284\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.35256\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.41609\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.48755\n",
      "\tTrain loss: 0.01114, Accuracy: 5855/6768 (86.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1388/1692 (82.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1043/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.80061\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.78711\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.91271\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.73058\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.44110\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.37280\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.45435\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.43059\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.38627\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.44689\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.57619\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.27738\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.29821\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.46321\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.47104\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.37778\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.52735\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.51021\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.43055\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.43289\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.31751\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.57236\n",
      "\tTrain loss: 0.00933, Accuracy: 6055/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1435/1692 (84.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.62159\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.67267\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.48950\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.78703\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.42138\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.37986\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.48580\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.41423\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.30600\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.38072\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.45014\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.43623\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.26826\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.56773\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.41066\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.47652\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.35554\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.36774\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.72678\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.64944\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.43501\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.53361\n",
      "\tTrain loss: 0.00870, Accuracy: 6145/6768 (90.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1461/1692 (86.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00060, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.60909\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.53226\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.36114\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.53911\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.37711\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.20403\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.39005\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.51539\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.38428\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.18225\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.24575\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.36905\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.43800\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.50320\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.36740\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.52619\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.32258\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.53728\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.53614\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.50036\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.47044\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.36908\n",
      "\tTrain loss: 0.00939, Accuracy: 6080/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.31974\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.60279\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.67148\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.61155\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.53493\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.34476\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.38820\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.20964\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.32553\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.46567\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.29640\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.47520\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.36439\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.47188\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.49753\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.47430\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.38956\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.42948\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.41428\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.26337\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.50312\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.42594\n",
      "\tTrain loss: 0.00930, Accuracy: 6060/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1452/1692 (85.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.30237\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.40413\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.73673\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.24768\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.72127\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.16405\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.74832\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.28967\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.43620\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.21892\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.22692\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.48681\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.24926\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.64602\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.38651\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.41542\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.33040\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.63576\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.50877\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.62179\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.10927\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.39062\n",
      "\tTrain loss: 0.01057, Accuracy: 5961/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1419/1692 (83.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1090/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.33851\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.38980\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.60117\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.52637\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.54221\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.26181\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.30009\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.50363\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.46168\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.94566\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.24802\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.32281\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.37692\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.36997\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.38416\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.70917\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.40713\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.31711\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.40133\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.58194\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.23357\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.58075\n",
      "\tTrain loss: 0.00935, Accuracy: 6095/6768 (90.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1448/1692 (85.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.38582\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.72863\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.74040\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.41231\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.80389\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.21384\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.36238\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.71667\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.31104\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.49257\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.42871\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.35382\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.32655\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.26461\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.34716\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.34844\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.57696\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.47517\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.18422\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.50118\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.32619\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.35306\n",
      "\tTrain loss: 0.01003, Accuracy: 6046/6768 (89.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1446/1692 (85.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.22520\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.38657\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.54522\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.33725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.54119\n",
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.24725\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.49627\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.25776\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.39604\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.32849\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.62782\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.42826\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.41446\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.27414\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.42461\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.54746\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.52094\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.38368\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.29741\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.25855\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.39209\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.40564\n",
      "\tTrain loss: 0.00954, Accuracy: 6079/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1110/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.42429\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.63496\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.45521\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.53854\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.77004\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.44629\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.39081\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.41143\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.32579\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.38006\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.48102\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.32588\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.26538\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.32174\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.47339\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.51219\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.42957\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.44973\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.41771\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.51448\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.22071\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.52228\n",
      "\tTrain loss: 0.00884, Accuracy: 6129/6768 (90.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1110/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.28376\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.46082\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.24783\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.38980\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.25796\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.36754\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.60892\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.40395\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.23300\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.21407\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.53728\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.34087\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.29091\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.40844\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.37227\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.36658\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.57088\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.70059\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.24045\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.81313\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.26003\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.47729\n",
      "\tTrain loss: 0.01077, Accuracy: 5975/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1436/1692 (84.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.31490\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.76195\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.74618\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.40066\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.53316\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.30765\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.40058\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.22927\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.26476\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.65378\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.46106\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.43339\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.55040\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.29935\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.40432\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.40115\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.59914\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.65774\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.42479\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.48300\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.40878\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.48579\n",
      "\tTrain loss: 0.00747, Accuracy: 6207/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1125/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.41680\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.40824\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.48162\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.27494\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.45617\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.64613\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.23668\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.50040\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.28393\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.22687\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.44387\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.21555\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.46994\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.27266\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.40000\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.27140\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.71067\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.58750\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.39794\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.32369\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.28397\n",
      "\tTrain loss: 0.01069, Accuracy: 5867/6768 (86.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1404/1692 (82.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1058/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.53576\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.25551\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.43216\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.42473\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.42576\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.13113\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.26723\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.29780\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.41222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.34861\n",
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.27886\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.34040\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.56688\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.65819\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.48095\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.39673\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.54401\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.43256\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.29829\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.43824\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.33499\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.35309\n",
      "\tTrain loss: 0.00844, Accuracy: 6171/6768 (91.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1481/1692 (87.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.45040\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.43916\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.28322\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.41795\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.55539\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.17554\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.28539\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.23428\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.40068\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.39220\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.27307\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.56347\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.38816\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.64441\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.33974\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.49523\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.37386\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.21583\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.20288\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.39717\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.31463\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.51522\n",
      "\tTrain loss: 0.00930, Accuracy: 6035/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1465/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1094/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.40505\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.51340\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.61008\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.28667\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.74246\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.24864\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.07689\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.22740\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.37554\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.33297\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.30984\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.35264\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.36185\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.35221\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.49103\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.27775\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.54461\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.38105\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.57811\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.33850\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.69712\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.47795\n",
      "\tTrain loss: 0.00731, Accuracy: 6238/6768 (92.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.55099\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.41229\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.64633\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.48033\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.43243\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.28974\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.48569\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.43545\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.26378\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.55413\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.53411\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.33919\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.30529\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.42595\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.38955\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.52907\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.41521\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.50298\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.35156\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.48874\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.17268\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.27734\n",
      "\tTrain loss: 0.00812, Accuracy: 6158/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1455/1692 (85.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.47675\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.46402\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.74584\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.24848\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.57157\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.26141\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.25277\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.44733\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.36354\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.36214\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.31558\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.53499\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.22023\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.59422\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.56469\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.64566\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.17291\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.73295\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.27552\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.60354\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.35646\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.51713\n",
      "\tTrain loss: 0.00790, Accuracy: 6174/6768 (91.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1158/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.44668\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.79002\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.23550\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.26941\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.33112\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.18780\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.65433\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.16459\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.36353\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.33889\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.30141\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.31239\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.33407\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.31263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.28997\n",
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.46748\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.49482\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.46663\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.39155\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.30323\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.24510\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.49540\n",
      "\tTrain loss: 0.00694, Accuracy: 6223/6768 (91.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1498/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.27394\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.53561\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.38996\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.45277\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.44770\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.28691\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.53957\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.25093\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.24376\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.40648\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.64988\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.55691\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.23503\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.27108\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.43097\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.39400\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.27695\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.59831\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.45369\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.70726\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.38432\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.61131\n",
      "\tTrain loss: 0.00673, Accuracy: 6326/6768 (93.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1532/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1123/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.26286\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.43436\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.55908\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.27327\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.45071\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.16662\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.21743\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.26068\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.56880\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.29813\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.28496\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.29177\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.30919\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.38302\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.24958\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.28428\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.46531\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.63316\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.40504\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.45137\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.36257\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.23611\n",
      "\tTrain loss: 0.00853, Accuracy: 6085/6768 (89.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1458/1692 (86.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.23674\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.44075\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.54264\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.52090\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.36734\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.19153\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.31585\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.54570\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.34514\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.43056\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.47440\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.49844\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.39943\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.28234\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.33625\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.39736\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.39263\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.61463\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.42901\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.51909\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.17757\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.36775\n",
      "\tTrain loss: 0.00914, Accuracy: 6035/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1442/1692 (85.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1052/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.39657\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.22045\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.60285\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.25705\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.54364\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.22843\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.36095\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.38754\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.53101\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.25168\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.19968\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.49806\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.32856\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.37822\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.24061\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.71650\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.30442\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.41861\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.29880\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.26931\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.14628\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.23402\n",
      "\tTrain loss: 0.00873, Accuracy: 6006/6768 (88.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1449/1692 (85.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.38325\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.46932\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.36321\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.32724\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.22878\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.27404\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.30845\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.26062\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.29824\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.40493\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.46687\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.35814\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.37314\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.53198\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.34479\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.36860\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.50811\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.53214\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.38381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.42078\n",
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.19528\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.51360\n",
      "\tTrain loss: 0.00769, Accuracy: 6175/6768 (91.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1465/1692 (86.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.50604\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.64816\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.42003\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.26109\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.60085\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.53356\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.26525\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.17645\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.63022\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.25811\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.27989\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.26243\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.19720\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.48334\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.36574\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.34942\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.47775\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.49283\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.34755\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.54962\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.19692\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.54276\n",
      "\tTrain loss: 0.00718, Accuracy: 6218/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.34815\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.58156\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.42253\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.38464\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.62237\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.21629\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.31427\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.37179\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.32576\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.39440\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.40007\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.27629\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.46714\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.43245\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.45073\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.27954\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.33269\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.26606\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.21504\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.54229\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.41307\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.45342\n",
      "\tTrain loss: 0.00786, Accuracy: 6156/6768 (90.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.31083\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.54921\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.51594\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.37066\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.24810\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.19518\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.23164\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.20043\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.18751\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.42288\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.21465\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.32897\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.35756\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.59051\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.53105\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.36467\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.38493\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.51939\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.43414\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.31701\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.37559\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.59801\n",
      "\tTrain loss: 0.00612, Accuracy: 6324/6768 (93.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1510/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.26427\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.47441\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.35580\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.52981\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.39163\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.36404\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.32940\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.12250\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.52561\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.64284\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.19254\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.35574\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.26701\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.43883\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.31039\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.51360\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.22867\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.57775\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.38735\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.25384\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.29838\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.26801\n",
      "\tTrain loss: 0.00815, Accuracy: 6121/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1467/1692 (86.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.18718\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.47173\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.43345\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.39086\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.36114\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.38167\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.18820\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.16007\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.31072\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.57346\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.55521\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.57832\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.32483\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.70638\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.14267\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.30819\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.52569\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.59671\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.27809\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.33144\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.24061\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.41219\n",
      "\tTrain loss: 0.00588, Accuracy: 6375/6768 (94.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1532/1692 (90.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00063, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.54422\n",
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.39057\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.34139\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.23082\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.42880\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.39706\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.40163\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.40135\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.46437\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.26447\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.23024\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.25417\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.33177\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.49646\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.39595\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.30047\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.24547\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.43862\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.31720\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.32354\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.24663\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.18278\n",
      "\tTrain loss: 0.00689, Accuracy: 6262/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1111/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.30881\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.43892\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.28979\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.26265\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.19790\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.29301\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.18433\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.26444\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.23987\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.40789\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.16170\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.18989\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.36494\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.30679\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.45054\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.60354\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.42137\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.49329\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.39942\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.32565\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.25638\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.64232\n",
      "\tTrain loss: 0.00781, Accuracy: 6134/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.28201\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.39086\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.50986\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.29247\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.26172\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.15503\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.19254\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.27766\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.32903\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.54163\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.40207\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.29602\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.59650\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.27649\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.35214\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.43869\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.29190\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.26935\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.21647\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.33067\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.35511\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.28019\n",
      "\tTrain loss: 0.00671, Accuracy: 6281/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.32877\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.42443\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.43777\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.32782\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.34190\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.29171\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.41587\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.40504\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.49739\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.23781\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.29462\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.52425\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.26963\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.38113\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.55388\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.37962\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.35573\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.31913\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.25317\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.47383\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.16435\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.43065\n",
      "\tTrain loss: 0.00695, Accuracy: 6236/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.21528\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.47960\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.16903\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.37511\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.35310\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.24127\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.46319\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.33762\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.26869\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.45685\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.15866\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.21291\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.23593\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.40326\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.33995\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.20832\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.23868\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.49512\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.42761\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.33742\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.22590\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.35764\n",
      "\tTrain loss: 0.00648, Accuracy: 6304/6768 (93.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1514/1692 (89.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1066/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.29245\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.41083\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.37606\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.49218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.33042\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.14910\n",
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.33711\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.15556\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.30885\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.44914\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.35636\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.21176\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.23687\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.29874\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.23999\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.31195\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.21415\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.46132\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.40379\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.35774\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.18040\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.35648\n",
      "\tTrain loss: 0.00671, Accuracy: 6225/6768 (91.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1113/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.27137\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.48478\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.31457\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.35565\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.37994\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.23975\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.40205\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.48809\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.39492\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.07037\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.37915\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.41058\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.35433\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.33725\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.23773\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.27374\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.27338\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.50391\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.25345\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.41908\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.20599\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.43796\n",
      "\tTrain loss: 0.00588, Accuracy: 6363/6768 (94.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1537/1692 (90.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1088/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.56913\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.54578\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.27414\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.37648\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.45616\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.11484\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.28767\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.25856\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.30991\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.24747\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.45098\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.50578\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.27499\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.35893\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.22355\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.36518\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.25223\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.48634\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.32433\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.42386\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.22251\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.41275\n",
      "\tTrain loss: 0.00850, Accuracy: 6140/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1072/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.30535\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.23449\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.36638\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.39441\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.20321\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.15417\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.26128\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.24451\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.15402\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.22725\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.64743\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.36262\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.13553\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.11030\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.19046\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.38549\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.29473\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.39811\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.28863\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.31455\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.33374\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.34731\n",
      "\tTrain loss: 0.00720, Accuracy: 6193/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1475/1692 (87.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.27939\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.39910\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.44321\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.51876\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.28575\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.19175\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.54637\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.31750\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.23170\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.25313\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.30912\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.37149\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.16490\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.24925\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.48348\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.64043\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.22312\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.38161\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.38660\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.56236\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.43594\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.23682\n",
      "\tTrain loss: 0.00749, Accuracy: 6194/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1499/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1046/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.32081\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.28397\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.44253\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.25862\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.53789\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.31302\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.48989\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.21020\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.29335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.22948\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.31559\n",
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.62649\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.29833\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.27670\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.26052\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.31717\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.30758\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.32968\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.33310\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.34037\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.57163\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.30770\n",
      "\tTrain loss: 0.00732, Accuracy: 6129/6768 (90.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1475/1692 (87.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.22274\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.31901\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.54941\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.24284\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.57080\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.15519\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.26053\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.32675\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.43460\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.31415\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.49438\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.23890\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.66022\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.27950\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.30260\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.41837\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.22930\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.34684\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.20670\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.58950\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.32191\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.52467\n",
      "\tTrain loss: 0.00614, Accuracy: 6277/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1520/1692 (89.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.33560\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.30835\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.39418\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.32152\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.41757\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.21123\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.35352\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.10141\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.56885\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.27936\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.37671\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.28024\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.33209\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.48017\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.32755\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.43149\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.29447\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.40442\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.19823\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.40655\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.32296\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.23684\n",
      "\tTrain loss: 0.00707, Accuracy: 6228/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1501/1692 (88.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.47804\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.63870\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.60457\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.27457\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.48814\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.16916\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.37425\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.52208\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.19240\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.25257\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.25134\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.31606\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.21717\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.23079\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.26883\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.45092\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.46149\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.45472\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.19080\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.36854\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.23978\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.59644\n",
      "\tTrain loss: 0.00546, Accuracy: 6355/6768 (93.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1538/1692 (90.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.31553\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.39661\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.85855\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.15952\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.33195\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.05869\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.19698\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.28145\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.40182\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.78868\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.38180\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.39236\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.38489\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.25845\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.19566\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.32296\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.42480\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.63187\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.35061\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.54938\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.11177\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.23256\n",
      "\tTrain loss: 0.00495, Accuracy: 6432/6768 (95.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1559/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1117/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.49539\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.18885\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.46993\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.20263\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.25432\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.21631\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.33255\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.22299\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.25021\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.19930\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.28153\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.27612\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.28689\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.23182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.28786\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.22928\n",
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.24524\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.33083\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.34346\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.30325\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.18354\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.49297\n",
      "\tTrain loss: 0.00728, Accuracy: 6183/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.32034\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.60132\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.30019\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.49752\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.18101\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.25284\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.47594\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.16784\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.21494\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.43180\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.37124\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.29924\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.26134\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.28444\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.42568\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.35097\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.39133\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.47878\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.16653\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.35679\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.13376\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.30386\n",
      "\tTrain loss: 0.00657, Accuracy: 6250/6768 (92.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.11711\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.50131\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.40267\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.34835\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.32839\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.20444\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.24342\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.42161\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.27978\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.18309\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.38034\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.15922\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.18224\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.30054\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.43005\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.45256\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.23059\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.49841\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.22350\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.27266\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.36902\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.35102\n",
      "\tTrain loss: 0.00511, Accuracy: 6391/6768 (94.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.20804\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.18154\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.37436\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.32741\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.43043\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.19301\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.34172\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.46163\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.38274\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.21232\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.44126\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.42677\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.38862\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.30353\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.56309\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.23099\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.15584\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.36490\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.28279\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.55393\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.23800\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.42116\n",
      "\tTrain loss: 0.00610, Accuracy: 6284/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1533/1692 (90.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.20594\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.28285\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.38309\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.25587\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.43750\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.13030\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.31812\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.18229\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.15345\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.38172\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.21357\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.26655\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.26640\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.11720\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.34822\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.21616\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.40387\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.32429\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.21760\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.29941\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.21184\n",
      "\tTrain loss: 0.00642, Accuracy: 6226/6768 (91.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1503/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1099/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.32386\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.49038\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.58554\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.39920\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.19990\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.16501\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.38351\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.37015\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.32032\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.40866\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.48514\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.18064\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.26570\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.38910\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.42877\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.26810\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.45301\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.68012\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.28986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.47532\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.10538\n",
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.40657\n",
      "\tTrain loss: 0.00629, Accuracy: 6267/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1529/1692 (90.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1116/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.36557\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.22562\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.59652\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.18616\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.31027\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.25580\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.39550\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.21518\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.42360\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.24099\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.27927\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.20633\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.21237\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.49694\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.43429\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.39845\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.40706\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.71447\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.25798\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.38662\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.23900\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.34871\n",
      "\tTrain loss: 0.00579, Accuracy: 6323/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1534/1692 (90.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1128/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.43089\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.39778\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.18179\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.32345\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.44871\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.23743\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.26148\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.24147\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.17946\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.15444\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.22425\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.15985\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.32403\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.39629\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.36037\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.41958\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.20690\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.47138\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.49878\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.45142\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.19192\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.24334\n",
      "\tTrain loss: 0.00607, Accuracy: 6303/6768 (93.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1522/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.39017\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.35449\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.36240\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.31935\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.29630\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.09872\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.20254\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.19772\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.33799\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.46867\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.31061\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.61493\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.12332\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.44162\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.27415\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.38538\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.17199\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.30006\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.34770\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.41063\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.42430\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.26804\n",
      "\tTrain loss: 0.00579, Accuracy: 6324/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1529/1692 (90.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1101/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.20868\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.39126\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.37265\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.27341\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.39962\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.13624\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.32851\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.31503\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.32369\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.27682\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.35823\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.36178\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.23186\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.45195\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.29094\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.16734\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.26974\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.48882\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.22039\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.42575\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.64318\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.58569\n",
      "\tTrain loss: 0.00558, Accuracy: 6322/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1519/1692 (89.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1101/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.25717\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.32595\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.44790\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.23241\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.59749\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.36496\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.29389\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.35793\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.35821\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.24834\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.11690\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.36701\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.29687\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.30840\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.16316\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.24140\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.30873\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.39185\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.28091\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.33948\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.27845\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.36326\n",
      "\tTrain loss: 0.00477, Accuracy: 6426/6768 (94.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1570/1692 (92.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00064, Accuracy: 1115/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.61626\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.35466\n",
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.28302\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.18575\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.19916\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.14456\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.16907\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.15159\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.49588\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.21044\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.36522\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.26390\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.34161\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.36592\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.12651\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.54756\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.48740\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.53463\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.19791\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.42205\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.19154\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.33246\n",
      "\tTrain loss: 0.00527, Accuracy: 6376/6768 (94.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1545/1692 (91.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9278959810874704\n",
      "Best test accuracy:\n",
      "0.6534988713318285\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn, optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fe233",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9286f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38918f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate LSTM model\n",
    "lstm = LSTM()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db6a6b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.36766\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.35595\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.46475\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.39018\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.37832\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.45167\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.36223\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.41420\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.43996\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.32874\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.41226\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.45990\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.39559\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.42868\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.42906\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.37410\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.41727\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.35933\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.41635\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.37210\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.42096\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.37574\n",
      "\tTrain loss: 0.04309, Accuracy: 1980/6768 (29.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 481/1692 (28.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 471/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.36511\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.38696\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.39642\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.34696\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.40944\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.39222\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.34721\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.38606\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.41830\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.41940\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.43862\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.39522\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.38536\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.35754\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.41736\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.31049\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.39173\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.38274\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.39209\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.39066\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.41568\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.36042\n",
      "\tTrain loss: 0.04278, Accuracy: 2094/6768 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 521/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 501/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.36514\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.35763\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.39512\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.36538\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.42683\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.36665\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.34948\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.35276\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.39295\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.39516\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.34967\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.40173\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.38895\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.40475\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.35948\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.34288\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.33749\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.33349\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.41347\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.37952\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.51528\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.40920\n",
      "\tTrain loss: 0.04240, Accuracy: 2329/6768 (34.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 578/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.35681\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.37198\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.36471\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.36757\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.39694\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.36046\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.47410\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.36147\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.43152\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.41230\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.34330\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.39884\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.39168\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.37993\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.32728\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.33328\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.37917\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.28024\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.39937\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.31064\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.43015\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.28577\n",
      "\tTrain loss: 0.04215, Accuracy: 2281/6768 (33.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 584/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 474/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.27648\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.38737\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.35966\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.28831\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.34583\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.34913\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.32932\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.29058\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.28882\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.56612\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.31574\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.43814\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.33442\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.36010\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.33253\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.32392\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.38501\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.35707\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.34298\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.35544\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.46227\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.28579\n",
      "\tTrain loss: 0.04167, Accuracy: 2475/6768 (36.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 603/1692 (35.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 487/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.22344\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.39121\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.29426\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.29706\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.28293\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.31726\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.35815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.38702\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.22347\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.38781\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.30211\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.41932\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.30725\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.37513\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.35713\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.28423\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.34025\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.29774\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.28580\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.33128\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.36016\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.23617\n",
      "\tTrain loss: 0.04106, Accuracy: 2545/6768 (37.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 619/1692 (36.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.29518\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.40336\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.26582\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.23860\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.28779\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.31326\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.37916\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.39357\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.27230\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.43452\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.20028\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.39180\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.26667\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.37137\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.26752\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.16226\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.28862\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.19362\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.33890\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.33677\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.48129\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.17972\n",
      "\tTrain loss: 0.04084, Accuracy: 2632/6768 (38.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 632/1692 (37.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.13738\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.25783\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.31498\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.18498\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.30802\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.28996\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.53085\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.29609\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.19077\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.45504\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.20630\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.38160\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.22228\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.35752\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.20091\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.20107\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.15757\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.22286\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.42981\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.13945\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.39734\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.10808\n",
      "\tTrain loss: 0.04038, Accuracy: 2720/6768 (40.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 664/1692 (39.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.18797\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.37500\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.24800\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.09622\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.28155\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.28944\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.34229\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.16328\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.13034\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.43617\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.14233\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.26172\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.38497\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.30713\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.13853\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.15579\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.21349\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.27165\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.37978\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.25887\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.39316\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.25390\n",
      "\tTrain loss: 0.04024, Accuracy: 2726/6768 (40.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 653/1692 (38.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 500/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.02672\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.36513\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.20895\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.10020\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19890\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.14534\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.39898\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.21210\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.16686\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.25144\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.16488\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.35438\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.35191\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.28425\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.15283\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.02076\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.10332\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.26516\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.25065\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.24344\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.47379\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.14254\n",
      "\tTrain loss: 0.03970, Accuracy: 2852/6768 (42.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 680/1692 (40.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.07775\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.27875\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.16798\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.08722\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.15588\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.19572\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.28178\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.15127\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.15179\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.27149\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.00531\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.21045\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.23032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.20781\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.13126\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.02261\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.04881\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.04522\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.18079\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.13692\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.39959\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.03416\n",
      "\tTrain loss: 0.03920, Accuracy: 2943/6768 (43.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 681/1692 (40.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.96458\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.21620\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.11183\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.13072\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.23029\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.14524\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.29294\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.07184\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.08234\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.32262\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.08261\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.20338\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.26727\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.23836\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.05356\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.04992\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.89743\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.14058\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.18544\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.07495\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.42533\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.01922\n",
      "\tTrain loss: 0.03869, Accuracy: 3026/6768 (44.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 711/1692 (42.00%)\n",
      "\tTest loss: 0.00087, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.10409\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.20862\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.12757\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.07092\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.12626\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.02418\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.42037\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.24654\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.08295\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.20637\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.90959\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.17975\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.32449\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.32058\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.09046\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 0.99970\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.01370\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.24955\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.28585\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.07536\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.16684\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.03661\n",
      "\tTrain loss: 0.03869, Accuracy: 3103/6768 (45.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 737/1692 (43.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 520/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.89458\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.10168\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.14812\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.01764\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.27548\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.06588\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.29422\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.17985\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.14613\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.14932\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.93618\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.09009\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.01632\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.18377\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.05654\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 0.90084\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.96939\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.15581\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.09147\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.00950\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.26608\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.92838\n",
      "\tTrain loss: 0.03947, Accuracy: 3105/6768 (45.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 724/1692 (42.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.94220\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.09367\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.11723\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.98893\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.02678\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 1.01952\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.28423\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.02084\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.08783\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.04805\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.82222\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.10897\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.23549\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.17040\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.09560\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.93917\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.99560\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.14015\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.33023\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.86004\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.34102\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.89653\n",
      "\tTrain loss: 0.03956, Accuracy: 3045/6768 (44.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 731/1692 (43.00%)\n",
      "\tTest loss: 0.00093, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.06270\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.37969\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.11867\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.00865\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.14248\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.96601\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.17617\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.97846\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.96351\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.09299\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.95144\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.11306\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.88376\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.13487\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.95482\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.91241\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.88425\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.08721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.21838\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.88568\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.20201\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.85694\n",
      "\tTrain loss: 0.03646, Accuracy: 3414/6768 (50.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 779/1692 (46.00%)\n",
      "\tTest loss: 0.00094, Accuracy: 561/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.93307\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.07947\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.96990\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.91765\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.94435\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 1.01338\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.31783\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.87176\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.95026\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.16174\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.92678\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.05153\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 1.33075\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.05805\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.83238\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.86595\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.90338\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.01774\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.08505\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.80477\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 1.27260\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.84209\n",
      "\tTrain loss: 0.03658, Accuracy: 3496/6768 (51.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 811/1692 (47.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.85460\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.95799\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 0.88076\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.95183\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.97990\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.94714\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.23603\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.01298\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.01527\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.01748\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.82071\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.93787\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.90996\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.15846\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.92372\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 1.00706\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.85693\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.19477\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.00530\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.88500\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.20662\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.89830\n",
      "\tTrain loss: 0.03754, Accuracy: 3442/6768 (50.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 798/1692 (47.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 542/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.90963\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.05404\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.96905\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.76119\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.02273\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.93524\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.18214\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.88076\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.05017\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.01314\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.63987\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.10233\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.99373\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 1.12494\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.85152\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.92555\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.90627\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.96060\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.02369\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.90446\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 1.14355\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.92766\n",
      "\tTrain loss: 0.03517, Accuracy: 3695/6768 (54.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 832/1692 (49.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 1.00624\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.96666\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.20307\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.91227\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.93764\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.83931\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.07626\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.85738\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.76739\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.17863\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.72455\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.97472\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.93751\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.29068\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.78479\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.73755\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.89390\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 0.93298\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.02890\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.84603\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 1.19172\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.86542\n",
      "\tTrain loss: 0.03682, Accuracy: 3595/6768 (53.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 800/1692 (47.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.86272\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.90637\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.04143\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.04905\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.06638\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.76245\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.97367\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.93966\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.83850\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.88317\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.62784\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.82128\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 1.04101\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.98427\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.95579\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.79567\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.95981\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.00153\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.65237\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 1.24035\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.81353\n",
      "\tTrain loss: 0.03347, Accuracy: 3919/6768 (57.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00074, Accuracy: 872/1692 (51.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 568/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.90388\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.00677\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.08532\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.80185\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.90921\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.59404\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.97422\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.90883\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.80552\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.07044\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.71696\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.74873\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 1.23819\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.80139\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.79454\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.75059\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.79296\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.99984\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.92112\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.63625\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.91596\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.74766\n",
      "\tTrain loss: 0.03250, Accuracy: 4047/6768 (59.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 904/1692 (53.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 557/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 1.08213\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.78096\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.88979\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.73529\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.84342\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.73989\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.14843\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.88481\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.65875\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.86706\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.59150\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.84282\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 1.32662\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.98071\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.73629\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.73048\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.72678\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.14680\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.97613\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.70066\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 1.02507\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.83047\n",
      "\tTrain loss: 0.03530, Accuracy: 3973/6768 (58.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 878/1692 (51.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 565/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.97363\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.79596\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.76185\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.84057\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.98366\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.58238\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.96783\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.74022\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.70099\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.89763\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.69640\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.66611\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.92059\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.75654\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.61038\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.68143\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.87964\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.87429\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.89433\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.63147\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.93350\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.65774\n",
      "\tTrain loss: 0.03184, Accuracy: 4133/6768 (61.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 914/1692 (54.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 550/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.72565\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.68406\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.94637\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.70076\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.74083\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.78997\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.77208\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.59640\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.64595\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.82661\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.49757\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.63728\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 1.02323\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.77584\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.69156\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.65747\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.76749\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.08936\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.70323\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.54994\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.78097\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.50239\n",
      "\tTrain loss: 0.02986, Accuracy: 4311/6768 (63.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 959/1692 (56.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.79717\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.69071\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.86205\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.58899\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.81247\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.64129\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.75163\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.77545\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.74873\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.87260\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.54797\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.66574\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.96708\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.94961\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.88787\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.79723\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.61800\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.91772\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.98287\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.50239\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.03985\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.67252\n",
      "\tTrain loss: 0.03160, Accuracy: 4229/6768 (62.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 937/1692 (55.00%)\n",
      "\tTest loss: 0.00124, Accuracy: 581/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.97408\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.70512\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.72423\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.74997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.07341\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.58071\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.80009\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.68768\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.48561\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.89802\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.57713\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.57474\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.92610\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.84362\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.84082\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.49695\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.62441\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.93947\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.72852\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.67003\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.84433\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.48321\n",
      "\tTrain loss: 0.02680, Accuracy: 4626/6768 (68.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1023/1692 (60.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 579/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 1.03479\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.68376\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.72127\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.56509\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.54270\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.45885\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.70912\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.76254\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.62149\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.51744\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.56768\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.93784\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.71290\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.52559\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.55042\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.74685\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.02353\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.71067\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.45584\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.93539\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.55515\n",
      "\tTrain loss: 0.02952, Accuracy: 4323/6768 (63.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 962/1692 (56.00%)\n",
      "\tTest loss: 0.00122, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.90921\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.52170\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.81962\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.69620\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.50182\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.47446\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.58742\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.53164\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.49539\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.75638\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.41005\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.60194\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.70145\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.76752\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.46171\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.61852\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.60895\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.98821\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.83459\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.56525\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.79219\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.35221\n",
      "\tTrain loss: 0.03220, Accuracy: 4348/6768 (64.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 972/1692 (57.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.89043\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.42496\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.72051\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.50643\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.37388\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.59233\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.99181\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.63271\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.54959\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.63485\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.41999\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.59415\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.79811\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.62310\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.42393\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.59109\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.70248\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.97499\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.81623\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.49602\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.84182\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.60576\n",
      "\tTrain loss: 0.02981, Accuracy: 4446/6768 (65.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 996/1692 (58.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 598/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.63376\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.35739\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.70737\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.57877\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.53964\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.59020\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.73078\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.68665\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.44139\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.86146\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.31698\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.65637\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.79836\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.74210\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.61570\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.37193\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.55033\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.66171\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.80184\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.43839\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.73409\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.58815\n",
      "\tTrain loss: 0.02875, Accuracy: 4532/6768 (66.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 1010/1692 (59.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 591/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.77668\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.35348\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.75658\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.46343\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.53720\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.38454\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.81657\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.83212\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.43067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.62653\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.38091\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.66895\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.95314\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.58712\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.66480\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.46283\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.50343\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.88069\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.54017\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.51020\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.75453\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.42568\n",
      "\tTrain loss: 0.02762, Accuracy: 4589/6768 (67.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1014/1692 (59.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 574/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.62892\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.70750\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.64849\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.38951\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.59216\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.32011\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.45758\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.58343\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.41538\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.77483\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.54443\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.38890\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.59630\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.91008\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.40751\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.43239\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.72952\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.89479\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.61598\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.29027\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.61555\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.48456\n",
      "\tTrain loss: 0.02773, Accuracy: 4587/6768 (67.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1015/1692 (59.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.75216\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.53767\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.70567\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.41230\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.53440\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.44228\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.69149\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.48520\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.66655\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.54875\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.42815\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.40510\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.77629\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.49689\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.46912\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.36056\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.75694\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.82799\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.56063\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.52861\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.41193\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.39534\n",
      "\tTrain loss: 0.02417, Accuracy: 4806/6768 (71.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1056/1692 (62.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.67592\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.50467\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.51028\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.53375\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.65544\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.34113\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.60393\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.53343\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.30538\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.57058\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.41440\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.42649\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.79337\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.28295\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.36104\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.21599\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.38818\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.82611\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.63833\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.40832\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.59539\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.30694\n",
      "\tTrain loss: 0.02924, Accuracy: 4607/6768 (68.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.58478\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.43328\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.72216\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.40089\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.35366\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.36018\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.60778\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.48104\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.24426\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.44278\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.29995\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.44865\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.50758\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.38090\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.32894\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.36813\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.46948\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.81351\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.47521\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.43210\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.72068\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.43005\n",
      "\tTrain loss: 0.02506, Accuracy: 4863/6768 (71.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1086/1692 (64.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.46314\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.52936\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.64766\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.37214\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.58892\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.44415\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.59926\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.60931\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.45177\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.64079\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.50488\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.48496\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.45242\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.37924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.49129\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.38916\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.43490\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.68935\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.47855\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.29579\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.59939\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.45114\n",
      "\tTrain loss: 0.02652, Accuracy: 4845/6768 (71.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 1069/1692 (63.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 582/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.89732\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.46237\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.45645\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.42907\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.31806\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.34601\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.44835\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.42041\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.35040\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.66836\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.58465\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.31610\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.70013\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.59310\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.40313\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.31753\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.43695\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 1.17860\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.78798\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.45417\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.57681\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.26402\n",
      "\tTrain loss: 0.02443, Accuracy: 4937/6768 (72.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1108/1692 (65.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 588/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.50161\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.38105\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.71902\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.36443\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.52543\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.44106\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.65176\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.40685\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.53965\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.44807\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.27268\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.26734\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.64721\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.41157\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.32774\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.47117\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.50813\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.79793\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.74724\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.39906\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.63561\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.35502\n",
      "\tTrain loss: 0.02026, Accuracy: 5200/6768 (76.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1148/1692 (67.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.67103\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.29921\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.60765\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.51493\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.43756\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.43264\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.46830\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.43343\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.31034\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.60337\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.22663\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.60752\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.47928\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.36255\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.38843\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.54115\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.44863\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.61187\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.42485\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.26517\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.85218\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.33321\n",
      "\tTrain loss: 0.02069, Accuracy: 5119/6768 (75.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.55836\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.42883\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.36726\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.30906\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.54050\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.41947\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.67791\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.36918\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.27045\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.45183\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.27332\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.56517\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.54413\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.18569\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.24592\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.30666\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.34979\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.69343\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.53113\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.34812\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.51808\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.44127\n",
      "\tTrain loss: 0.01957, Accuracy: 5291/6768 (78.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1178/1692 (69.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 610/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.83246\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.22357\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.47856\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.31808\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.33959\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.45184\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.45208\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.40775\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.35815\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.53375\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.15655\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.25467\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.50997\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.49203\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.32865\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.34654\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.36089\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.69024\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.61046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.30180\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.68270\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.30435\n",
      "\tTrain loss: 0.01789, Accuracy: 5414/6768 (79.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 585/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.61258\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.42077\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.58335\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.58006\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.25699\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.23564\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.55074\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.45852\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.24617\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.32089\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.17373\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.44837\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.86099\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.33546\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.34913\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.39917\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.40260\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.49403\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.53407\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.34823\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.57232\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.42260\n",
      "\tTrain loss: 0.02617, Accuracy: 4810/6768 (71.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1080/1692 (63.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.79189\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.68741\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.41600\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.37885\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.32460\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.30297\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.44123\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.60455\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.28674\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.26943\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.27638\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.44440\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.63005\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.55183\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.20531\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.18875\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.67983\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.70251\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.60135\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.30001\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.72170\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.21452\n",
      "\tTrain loss: 0.02191, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.40063\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.21066\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.51608\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.36293\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.37477\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.20572\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.66834\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.30652\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.30073\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.44510\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.20987\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.27398\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.65597\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.50523\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.28949\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.18597\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.30606\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.41839\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.48926\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.30250\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.47674\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.36309\n",
      "\tTrain loss: 0.02091, Accuracy: 5268/6768 (77.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 602/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.69986\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.27440\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.48551\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.35482\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.32220\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.37014\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.32289\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.36548\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.22926\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.33355\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.18377\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.31697\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.42175\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.24505\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.30201\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.23754\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.30869\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.52209\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.36867\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.18968\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.43034\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.48596\n",
      "\tTrain loss: 0.01819, Accuracy: 5386/6768 (79.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1191/1692 (70.00%)\n",
      "\tTest loss: 0.00155, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.62044\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.32636\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.59447\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.15527\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.29231\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.29579\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.31746\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.26795\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.28848\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.60915\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.32458\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.42280\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.46451\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.33874\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.21216\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.38868\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.38927\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.72374\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.34005\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.23903\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.46086\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.32576\n",
      "\tTrain loss: 0.02217, Accuracy: 5132/6768 (75.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1135/1692 (67.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 601/1772 (33.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.53187\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.25414\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.41532\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.36441\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.42630\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.31447\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.34362\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.33430\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.11119\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.46936\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.24612\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.29397\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.56618\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.24541\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.20276\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.20017\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.40051\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.51870\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.29790\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.24019\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.61204\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.24227\n",
      "\tTrain loss: 0.01886, Accuracy: 5289/6768 (78.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1202/1692 (71.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.69774\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.34283\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.35776\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.37140\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.33499\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.28882\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.33687\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.42635\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.29402\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.60294\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.18620\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.23202\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.33564\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.50395\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.23692\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.11319\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.34425\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.51550\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.48401\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.19230\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.52367\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.35466\n",
      "\tTrain loss: 0.02006, Accuracy: 5309/6768 (78.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 601/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.47317\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.13026\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.56029\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.26717\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.32195\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.27619\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.12714\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.38978\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.20647\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.36329\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.26346\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.35425\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.34071\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.29372\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.15109\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.27064\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.31918\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.44130\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.46054\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.45159\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.40133\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.25230\n",
      "\tTrain loss: 0.01604, Accuracy: 5573/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1238/1692 (73.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.50774\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.30892\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.47175\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.19448\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.28998\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.21533\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.43022\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.13118\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.21973\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.28379\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.25465\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.33530\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.28419\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.26355\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.13920\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.37675\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.41366\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.51291\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.34819\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.34209\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.27435\n",
      "\tTrain loss: 0.01778, Accuracy: 5425/6768 (80.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 597/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.56546\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.31293\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.39138\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.38423\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.29526\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.26451\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.32888\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.28443\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.12147\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.24570\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.14721\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.23779\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.32713\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.18430\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.23378\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.38148\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.35396\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.74533\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.39642\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.30894\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.70678\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.30413\n",
      "\tTrain loss: 0.02403, Accuracy: 5135/6768 (75.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 565/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.54351\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.39474\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.49936\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.24841\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.26108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.21018\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.26193\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.23745\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.28922\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.35556\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.23149\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.11624\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.42560\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.23130\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.19364\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.32001\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.25661\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.50955\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.58354\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.33209\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.38418\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.46295\n",
      "\tTrain loss: 0.01742, Accuracy: 5416/6768 (80.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1235/1692 (72.00%)\n",
      "\tTest loss: 0.00160, Accuracy: 603/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.81353\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.22742\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.25238\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.18413\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.28744\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.57746\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.31608\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.18109\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.24688\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.41125\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.17563\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.12401\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.26792\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.34060\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.20147\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.21156\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.21228\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.50897\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.45740\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.18357\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.42224\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.09990\n",
      "\tTrain loss: 0.01868, Accuracy: 5426/6768 (80.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1199/1692 (70.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 572/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.45092\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.60331\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.26824\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.33319\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.33223\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.16716\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.55694\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.29050\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.35272\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.31519\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.14147\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.28573\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.24064\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.30624\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.23847\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.22174\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.18082\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.47014\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.20984\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.29052\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.30529\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.10617\n",
      "\tTrain loss: 0.01477, Accuracy: 5664/6768 (83.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1253/1692 (74.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 595/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.47959\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.34974\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.20439\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.33865\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.13232\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.44254\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.25382\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.18912\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.13760\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.35785\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.18741\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.22995\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.18669\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.25958\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.15303\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.30632\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.15459\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.55146\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.17166\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.28305\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.79577\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.09380\n",
      "\tTrain loss: 0.01922, Accuracy: 5386/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1205/1692 (71.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.22415\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.07467\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.44860\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.29913\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.10472\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.23179\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.25627\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.33881\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.41126\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.26731\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.33890\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.27499\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.46437\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.23846\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.36977\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.39295\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.22256\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.31493\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.31073\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.24165\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.55782\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.18050\n",
      "\tTrain loss: 0.01534, Accuracy: 5646/6768 (83.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1272/1692 (75.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.52664\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.34875\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.29118\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.29976\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.19196\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.34421\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.23795\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.27949\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.40526\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.22102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.16329\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.28106\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.49969\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.41523\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.11009\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.14009\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.13337\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.75720\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.33226\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.11261\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.30319\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.18393\n",
      "\tTrain loss: 0.01688, Accuracy: 5537/6768 (81.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00182, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.40257\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.19416\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.22075\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.14843\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.10450\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.15335\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.22112\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.24871\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.38154\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.32783\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.13473\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.27406\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.37253\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.43955\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.16656\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.23639\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.44645\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.42133\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.17829\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.25741\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.46001\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.27736\n",
      "\tTrain loss: 0.01588, Accuracy: 5586/6768 (82.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.47756\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.25548\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.24510\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.23005\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.10349\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.25660\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.12799\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.32043\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.19246\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.40748\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.09652\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.15544\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.40475\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.29898\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.20390\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.13332\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.30714\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.19455\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.18029\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.15916\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.35701\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.18542\n",
      "\tTrain loss: 0.01531, Accuracy: 5638/6768 (83.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00177, Accuracy: 650/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.36175\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.35822\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.15250\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.15847\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.30040\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.16429\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.12610\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.11178\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.30234\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.19976\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.32951\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.25906\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.23112\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.18787\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.35546\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.26742\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.32391\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.20897\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.16432\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.48750\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.16146\n",
      "\tTrain loss: 0.01948, Accuracy: 5382/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1221/1692 (72.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.20382\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.24088\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.47085\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.32199\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.22537\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.21136\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.15887\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.31820\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.21576\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.17183\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.24717\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.11166\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.47192\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.15348\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.14750\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.16805\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.20425\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.38880\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.22195\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.26222\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.51383\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.26693\n",
      "\tTrain loss: 0.01374, Accuracy: 5714/6768 (84.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1271/1692 (75.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.43359\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.32496\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.22058\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.18745\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.12790\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.39750\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.26901\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.42555\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.22088\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.47663\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.18061\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.26487\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.45434\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.11810\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.16307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.15420\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.21062\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.38434\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.22469\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.12736\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.48986\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.21407\n",
      "\tTrain loss: 0.01570, Accuracy: 5583/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.81950\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.20309\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.22260\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.46659\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.34659\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.26755\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.11494\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.21040\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.18856\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.14672\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.14445\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.18813\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.19302\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.11931\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.23594\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.19331\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.16493\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.38615\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.11773\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.09826\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.18818\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.19176\n",
      "\tTrain loss: 0.02131, Accuracy: 5298/6768 (78.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1176/1692 (69.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.32069\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.19076\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.29143\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.13439\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.12203\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.18773\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.34523\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.39093\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.14307\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.21321\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.28422\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.18704\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.26087\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.23283\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.15054\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.05922\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.16744\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.27240\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.45545\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.09502\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.35998\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.36310\n",
      "\tTrain loss: 0.01858, Accuracy: 5467/6768 (80.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1229/1692 (72.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.36727\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.51850\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.27211\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.12445\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.15850\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.17196\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.31891\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.24976\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.11482\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.12301\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.11290\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.07819\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.20291\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.14439\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.21522\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.21517\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.28443\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.38498\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.21176\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.31196\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.40561\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.41614\n",
      "\tTrain loss: 0.01643, Accuracy: 5590/6768 (82.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 597/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.43394\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.17768\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.52182\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.38734\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.07533\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.19511\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.03379\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.14579\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.19635\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.26756\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.09518\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.08847\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.25620\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.07652\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.06217\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.14271\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.22961\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.21239\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.39794\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.06595\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.52095\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.08412\n",
      "\tTrain loss: 0.01407, Accuracy: 5765/6768 (85.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1291/1692 (76.00%)\n",
      "\tTest loss: 0.00190, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.27218\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.17332\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.38184\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.28861\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.14820\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.18964\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.23026\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.22038\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.21328\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.12073\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.05027\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.12482\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.54795\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.09219\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.23995\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.17905\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.23760\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.34877\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.12310\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.27828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.36930\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.17927\n",
      "\tTrain loss: 0.01279, Accuracy: 5774/6768 (85.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1314/1692 (77.00%)\n",
      "\tTest loss: 0.00183, Accuracy: 571/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.31602\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.19372\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.27814\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.11019\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.12721\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.21112\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.13681\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.11068\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.22389\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.28824\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.09551\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.14025\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.44347\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.08739\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.21035\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.16980\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.23119\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.12981\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.23528\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.25223\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.22406\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.33308\n",
      "\tTrain loss: 0.02322, Accuracy: 5220/6768 (77.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1174/1692 (69.00%)\n",
      "\tTest loss: 0.00203, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.26104\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.45997\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.37994\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.22195\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.16835\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.14740\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.35765\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.35007\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.10294\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.50979\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.18085\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.20328\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.17137\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.25351\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.07298\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.09950\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.21765\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.37585\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.37847\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.10299\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.39079\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.18290\n",
      "\tTrain loss: 0.01556, Accuracy: 5611/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1259/1692 (74.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.55750\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.09281\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.28527\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.34819\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.29200\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.22515\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.11385\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.30842\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.10338\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.14360\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.10704\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.19350\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.18281\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.09077\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.15814\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.16955\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.24425\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.27344\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.13830\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.10510\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.39969\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.05896\n",
      "\tTrain loss: 0.01446, Accuracy: 5714/6768 (84.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1293/1692 (76.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.39094\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.11784\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.35574\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.08157\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.16651\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.13597\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.13571\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.15181\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.09383\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.10255\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.19950\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.60061\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.16314\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.17113\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.20455\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.19647\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.16946\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.27419\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.09720\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.31319\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.28944\n",
      "\tTrain loss: 0.01358, Accuracy: 5770/6768 (85.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1269/1692 (75.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 595/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.29932\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.35209\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.69390\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.20774\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.20175\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.14214\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.09321\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.17377\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.21433\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.13534\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.03962\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.20326\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.31602\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.38441\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.17099\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.05570\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.12162\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.08725\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.37845\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.16019\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.28102\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.06904\n",
      "\tTrain loss: 0.01451, Accuracy: 5744/6768 (84.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1299/1692 (76.00%)\n",
      "\tTest loss: 0.00197, Accuracy: 609/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.33686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.08569\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.20585\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.07160\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.17010\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.05262\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.06711\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.11412\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.11041\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.11576\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.08075\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.13594\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.42850\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.57510\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.32780\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.12096\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.25036\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.37125\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.12069\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.13889\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.17065\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.12440\n",
      "\tTrain loss: 0.01596, Accuracy: 5630/6768 (83.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1286/1692 (76.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 602/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.61859\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.24106\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.31940\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.13019\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.04339\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.18178\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.14673\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.32827\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.16769\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.27304\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.16421\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.22708\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.24355\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.18540\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.10382\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.10757\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.04444\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.27742\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.11345\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.06272\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.13956\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.06538\n",
      "\tTrain loss: 0.01575, Accuracy: 5661/6768 (83.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1282/1692 (75.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.42566\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.13983\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.57543\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.04828\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.16553\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.16302\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.14735\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.05926\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.16783\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.50271\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.12068\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.17862\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.30442\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.18031\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.08022\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.13434\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.22556\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.36274\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.06742\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.26213\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.31189\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.05938\n",
      "\tTrain loss: 0.01405, Accuracy: 5729/6768 (84.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1285/1692 (75.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 606/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.22367\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.07082\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.17655\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.06150\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.17366\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.26325\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.28436\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.10432\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.13779\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.13202\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.07570\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.31024\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.32918\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.07120\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.07234\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.04758\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.21553\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.32905\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.40940\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.12850\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.54032\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.19647\n",
      "\tTrain loss: 0.01346, Accuracy: 5787/6768 (85.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1311/1692 (77.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.31644\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.02814\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.17616\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.17207\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.10682\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.40567\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.21361\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.11960\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.16625\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.20022\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.09255\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.23235\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.25653\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.05326\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.11535\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.08011\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.19124\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.21429\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.26952\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.12440\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.55246\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.05236\n",
      "\tTrain loss: 0.01673, Accuracy: 5594/6768 (82.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1255/1692 (74.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.23758\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.26266\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.10210\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.07879\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.04234\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.21099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.33816\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.08914\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.05518\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.18578\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.12460\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.17977\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.26461\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.10348\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.11335\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.09485\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.24564\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.60554\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.12285\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.07699\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.15206\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.24306\n",
      "\tTrain loss: 0.01198, Accuracy: 5889/6768 (87.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1298/1692 (76.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 559/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.30712\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.07458\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.07814\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.13938\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.30816\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.38750\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.05773\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.04657\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.12524\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.13513\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.03234\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.17642\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.36084\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.36642\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.18683\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.14807\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.05316\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.12708\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.23390\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.11860\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.12653\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.05277\n",
      "\tTrain loss: 0.01282, Accuracy: 5842/6768 (86.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1304/1692 (77.00%)\n",
      "\tTest loss: 0.00204, Accuracy: 601/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.20126\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.20912\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.34569\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.14187\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.18578\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.16332\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.30488\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.26736\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.12165\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.21936\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.03278\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.07442\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.26117\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.13896\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.17863\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.09440\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.19842\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.14763\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.09369\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.17275\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.22715\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.03885\n",
      "\tTrain loss: 0.01297, Accuracy: 5822/6768 (86.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1315/1692 (77.00%)\n",
      "\tTest loss: 0.00199, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.20985\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.16428\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.10664\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.11032\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.13422\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.06289\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.05247\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.08412\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.08102\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.20161\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.27770\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.05902\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.25693\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.17032\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.09931\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.12531\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.09418\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.47837\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.29988\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.17883\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.25641\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.09122\n",
      "\tTrain loss: 0.01588, Accuracy: 5630/6768 (83.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1281/1692 (75.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 579/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.17161\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.06703\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.24708\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.23893\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.20092\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.17623\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.06860\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.05314\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.03019\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.05450\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.03691\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.09036\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.07764\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.08040\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.07979\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.17058\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.14607\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.18075\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.22887\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.05239\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.33211\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.04866\n",
      "\tTrain loss: 0.01090, Accuracy: 5944/6768 (87.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1355/1692 (80.00%)\n",
      "\tTest loss: 0.00197, Accuracy: 586/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.42589\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.20732\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.16953\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.07057\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.13614\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.23806\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.13639\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.03233\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.12889\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.10162\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.04028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.23571\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.26460\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.07654\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.06433\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.10053\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.24012\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.42229\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.07568\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.22194\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.14892\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.18692\n",
      "\tTrain loss: 0.02038, Accuracy: 5421/6768 (80.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00219, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.24715\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.16405\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.11583\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.12989\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.12276\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.20660\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.22984\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.12476\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.08500\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.11355\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.17901\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.09396\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.33339\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.11391\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.06017\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.08860\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.42940\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.05547\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.36080\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.10428\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.19567\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.05181\n",
      "\tTrain loss: 0.01643, Accuracy: 5662/6768 (83.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1269/1692 (75.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 596/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.51007\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.11784\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.24070\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.06004\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.09764\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.17350\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.23742\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.17614\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.15327\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.25350\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.06167\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.25739\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.31711\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.33733\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.07193\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.07498\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.21380\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.29851\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.23565\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.12507\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.33148\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.42104\n",
      "\tTrain loss: 0.01161, Accuracy: 5862/6768 (86.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1318/1692 (77.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.12856\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.13260\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.20113\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.16195\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.15019\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.06536\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.06447\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.15324\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.26106\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.09313\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.06887\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.10627\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.18353\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.06195\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.13852\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.17617\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.06733\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.08316\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.03027\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.09774\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.38445\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.12795\n",
      "\tTrain loss: 0.01307, Accuracy: 5804/6768 (85.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1318/1692 (77.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.13840\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.22184\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.28573\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.23157\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.16161\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.21667\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.14822\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.16735\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.14218\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.07094\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.03140\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.21392\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.12832\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.28658\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.20209\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.18703\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.13110\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.06990\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.30468\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.04726\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.22701\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.11344\n",
      "\tTrain loss: 0.01288, Accuracy: 5837/6768 (86.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1313/1692 (77.00%)\n",
      "\tTest loss: 0.00207, Accuracy: 584/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.32853\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.40384\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.13883\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.21468\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.31457\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.26779\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.07220\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.13560\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.07427\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.10005\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.03927\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.09906\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.11889\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.17441\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.09132\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.16633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.10084\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.17133\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.13793\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.17109\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.37053\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.23876\n",
      "\tTrain loss: 0.01413, Accuracy: 5794/6768 (85.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1305/1692 (77.00%)\n",
      "\tTest loss: 0.00212, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.21229\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.15890\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.34802\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.21227\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.19285\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.24211\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.27141\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.06592\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.29319\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.12836\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.02874\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.14259\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.18610\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.21407\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.27272\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.04438\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.09044\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.14020\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.03780\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.09245\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.17911\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.02955\n",
      "\tTrain loss: 0.01385, Accuracy: 5775/6768 (85.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1295/1692 (76.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 621/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.50072\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.08290\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.12432\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.06592\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.13479\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.11452\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.31427\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.03902\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.15976\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.04626\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.13277\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.20116\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.25712\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.22132\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.28647\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.12595\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.19527\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.43035\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.33677\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.08341\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.39854\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.19070\n",
      "\tTrain loss: 0.01177, Accuracy: 5868/6768 (86.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1353/1692 (79.00%)\n",
      "\tTest loss: 0.00207, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.13311\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.11307\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.35133\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.07988\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.10811\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.04594\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.05967\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.18354\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.05749\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.09132\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.11687\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.14506\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.29117\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.03692\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.10877\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.36134\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.13928\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.20849\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.06332\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.12294\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.20278\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.20460\n",
      "\tTrain loss: 0.01097, Accuracy: 5994/6768 (88.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1356/1692 (80.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 575/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.65473\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.23414\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.09157\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.18521\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.20503\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.43093\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.09991\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.06967\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.12933\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.17855\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.18063\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.11817\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.15209\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.15403\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.08582\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.13188\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.09916\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.36809\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.39314\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.13047\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.45715\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.26853\n",
      "\tTrain loss: 0.01197, Accuracy: 5878/6768 (86.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1326/1692 (78.00%)\n",
      "\tTest loss: 0.00209, Accuracy: 584/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.55907\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.12163\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.28048\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.04147\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.23431\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.12137\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.06975\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.08957\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.06920\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.02436\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.04333\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.05172\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.27706\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.13842\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.33631\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.02836\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.19214\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.07413\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.08021\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.13085\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.34137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.08854\n",
      "\tTrain loss: 0.01396, Accuracy: 5707/6768 (84.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.33387\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.04496\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.23930\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.04557\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.11069\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.03257\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.05360\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.07350\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.16929\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.06643\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.08172\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.06152\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.16793\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.06604\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.38620\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.04005\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.19508\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.27139\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.08133\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.03756\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.20493\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.18322\n",
      "\tTrain loss: 0.01563, Accuracy: 5769/6768 (85.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1289/1692 (76.00%)\n",
      "\tTest loss: 0.00215, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.21637\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.04071\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.30459\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.16543\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.18221\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.17404\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.04218\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.05074\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.04931\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.28772\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.20196\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.07981\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.08120\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.13389\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.19490\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.07423\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.10286\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.06367\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.03255\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.05111\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.16915\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.05527\n",
      "\tTrain loss: 0.01347, Accuracy: 5802/6768 (85.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1306/1692 (77.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.14069\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.08910\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.14829\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.09362\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.03098\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.26005\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.13956\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.03271\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.18557\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.16631\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.23773\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.11319\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.05538\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.21190\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.13496\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.08823\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.07812\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.18448\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.07262\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.08563\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.23610\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.04015\n",
      "\tTrain loss: 0.01297, Accuracy: 5837/6768 (86.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1324/1692 (78.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.25513\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.22047\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.15630\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.08545\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.19550\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.06047\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.13211\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.14764\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.14903\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.11001\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.02714\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.08506\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.19706\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.21216\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.04566\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.13471\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.13311\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.37908\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.27396\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.30023\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.12118\n",
      "\tTrain loss: 0.01229, Accuracy: 5937/6768 (87.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00228, Accuracy: 585/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.18718\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.25665\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.45362\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.09680\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.61041\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.07918\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.33221\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.16599\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.14945\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.16338\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.14046\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.11158\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.25057\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.09782\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.14210\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.02858\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.20129\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.17276\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.06407\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.16286\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.08569\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.09351\n",
      "\tTrain loss: 0.01151, Accuracy: 5964/6768 (88.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00216, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.13606\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.15872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.22040\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.05980\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.08897\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.02962\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.15988\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.09882\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.05009\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.27451\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.19678\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.05899\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.12777\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.09799\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.27889\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.19358\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.15415\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.20287\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.13915\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.15571\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.08062\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.15970\n",
      "\tTrain loss: 0.01245, Accuracy: 5997/6768 (88.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1346/1692 (79.00%)\n",
      "\tTest loss: 0.00210, Accuracy: 556/1772 (31.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.8014184397163121\n",
      "Best test accuracy:\n",
      "0.36681715575620766\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(lstm, optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95517be3",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13dede77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d762c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate LSTM model\n",
    "gru = GRU()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e8676c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.61701\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.36332\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.42349\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.47050\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.35153\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.43636\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.50179\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.34796\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.37872\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.50042\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.42771\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.46220\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.38592\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.42766\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.45746\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.37038\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.41778\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.38464\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.46940\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.42666\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.35407\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.35328\n",
      "\tTrain loss: 0.04293, Accuracy: 2060/6768 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 523/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 481/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.35607\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.38127\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.41767\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.34812\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.43826\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.35984\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.39115\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.36827\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.36243\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.43825\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.31810\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.37840\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.36802\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.46610\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.41845\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.38451\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.35336\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.36812\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.43365\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.39292\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.47538\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.35438\n",
      "\tTrain loss: 0.04255, Accuracy: 2277/6768 (33.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 544/1692 (32.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 497/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.44336\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.38399\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.35776\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.29525\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.38842\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.34309\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.41853\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.31803\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.32054\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.38609\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.40184\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.35653\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.43352\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.36629\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.34726\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.35097\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.31901\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.45615\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.38366\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.40131\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.41669\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.33918\n",
      "\tTrain loss: 0.04217, Accuracy: 2369/6768 (35.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 610/1692 (36.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 512/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.38251\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.41705\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.39798\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.32417\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.40191\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.33531\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.36344\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.41602\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.32426\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.40772\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.35302\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.36333\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.39286\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.32556\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.36691\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.37549\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.34740\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.32977\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.44440\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.29875\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.33190\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.27462\n",
      "\tTrain loss: 0.04191, Accuracy: 2386/6768 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 589/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.38390\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.39917\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.48786\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.32384\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.35998\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.31975\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.50512\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.37706\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.26499\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.39476\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.34279\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.35881\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.34838\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.38175\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.27775\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.37041\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.27168\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.36220\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.44350\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.33654\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.39998\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.26908\n",
      "\tTrain loss: 0.04152, Accuracy: 2396/6768 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 600/1692 (35.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.30605\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.44900\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.34953\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.25320\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.33739\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.22100\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.38611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.30357\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.22695\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.36559\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.29208\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.37809\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.36657\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.36338\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.32481\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.30098\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.35228\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.27706\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.42663\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.40146\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.34128\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.14436\n",
      "\tTrain loss: 0.04196, Accuracy: 2315/6768 (34.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 567/1692 (33.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 518/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.23721\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.42858\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.35478\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.26608\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.36782\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.28486\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.38923\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.30880\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.29521\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.36910\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.24288\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.43255\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.35873\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.31453\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.28569\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.31426\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.22183\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.34849\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.39057\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.23270\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.38930\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.17134\n",
      "\tTrain loss: 0.04122, Accuracy: 2545/6768 (37.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 617/1692 (36.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.21144\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.48593\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.32225\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.15085\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.33419\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.21560\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.50618\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.24168\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.11074\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.27660\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.19824\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.33630\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.39132\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.33524\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.24361\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.27110\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.35851\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.28072\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.40057\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.21859\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.23955\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.05071\n",
      "\tTrain loss: 0.04050, Accuracy: 2618/6768 (38.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 628/1692 (37.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.14194\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.39781\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.34079\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.22564\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.30729\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.13557\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.38695\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.25140\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.25045\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.34283\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.20468\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.33880\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.37424\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.32574\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.16900\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.22040\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.24608\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.32150\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.46026\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.29993\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.27880\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.08559\n",
      "\tTrain loss: 0.04087, Accuracy: 2572/6768 (38.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 626/1692 (36.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.10363\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.44555\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.25481\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.20611\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19657\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.18912\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.32386\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.28116\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.11670\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.37604\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.05072\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.41703\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.21380\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.16191\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.21457\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.25797\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.15348\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.26643\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.40545\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.24101\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.30425\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 0.91792\n",
      "\tTrain loss: 0.04124, Accuracy: 2644/6768 (39.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 645/1692 (38.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.08698\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.50665\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.19285\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.37177\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.21557\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.11972\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.36648\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.13913\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.10748\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.36187\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.00834\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.26553\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.26908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.27460\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.24953\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.22284\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.15224\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.17479\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.45608\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.10564\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.27319\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 0.95179\n",
      "\tTrain loss: 0.04083, Accuracy: 2821/6768 (41.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 686/1692 (40.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.14520\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.36199\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.22491\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.20451\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.31700\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.04437\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.33470\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.33126\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.01559\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.30485\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.96594\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.34943\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.24718\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.35698\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.28531\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.23237\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.16110\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.13965\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.26347\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.14968\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.30558\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.00005\n",
      "\tTrain loss: 0.03995, Accuracy: 2947/6768 (43.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 710/1692 (41.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 544/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.06824\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.39048\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.21162\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.17864\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.20839\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.98197\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.28502\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.16758\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.14110\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.27164\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 1.02601\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.20536\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.16393\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.13981\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.12980\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.10573\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.13168\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.26642\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.25728\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.18745\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.32376\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 0.92997\n",
      "\tTrain loss: 0.03930, Accuracy: 2984/6768 (44.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00088, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.07145\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.24098\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.17851\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.15451\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.25088\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.02774\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.20629\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.96144\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.05352\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.24378\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.93168\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.24410\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.18111\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.21239\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.19631\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.18991\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.07550\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.10356\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.26326\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.07638\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.20547\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.95784\n",
      "\tTrain loss: 0.04005, Accuracy: 3044/6768 (44.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 722/1692 (42.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.08145\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.19725\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.09449\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.21943\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.20613\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.98010\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.16307\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.10630\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.04007\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.10581\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.88999\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.25178\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.13394\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.11625\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.16552\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.98126\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.03610\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.23920\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.25438\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.93874\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.12897\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.91432\n",
      "\tTrain loss: 0.03839, Accuracy: 3139/6768 (46.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 725/1692 (42.00%)\n",
      "\tTest loss: 0.00091, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.03154\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.18819\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.08983\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.05366\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.25169\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.88989\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.16705\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 1.30554\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.94720\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.27791\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.88245\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.13716\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 1.12658\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.19586\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.91658\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.83348\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.04726\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.13159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.09841\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.92782\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.09265\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.91158\n",
      "\tTrain loss: 0.03860, Accuracy: 3247/6768 (47.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 763/1692 (45.00%)\n",
      "\tTest loss: 0.00093, Accuracy: 588/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.98963\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.13631\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.15564\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.15446\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.08535\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.87435\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.05345\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.94566\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.92126\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.26801\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.88075\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.08949\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.92635\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.04041\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.05949\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.89761\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.96845\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.13117\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.23037\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.01480\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 1.10264\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.82997\n",
      "\tTrain loss: 0.03947, Accuracy: 3151/6768 (46.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 745/1692 (44.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.93233\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.20690\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.22527\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.10327\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.16608\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 1.04666\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.12613\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.05680\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.98746\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.22716\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.94790\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.96913\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.04680\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.89210\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.98534\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.01130\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.13381\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.13722\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.90968\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.19276\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.89734\n",
      "\tTrain loss: 0.03988, Accuracy: 3327/6768 (49.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 785/1692 (46.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 556/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 1.05248\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.31615\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.04061\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.03080\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.98843\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.83730\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.15629\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 1.08260\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.78083\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.15556\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.69679\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.17706\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 1.05404\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.89546\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.90014\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.85148\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 1.01758\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.98668\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.28708\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.75097\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.89166\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.72530\n",
      "\tTrain loss: 0.03505, Accuracy: 3709/6768 (54.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 850/1692 (50.00%)\n",
      "\tTest loss: 0.00096, Accuracy: 559/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.87143\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.08718\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.10330\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 1.12156\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.91507\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.70937\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.90463\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 1.13799\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.92111\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.14936\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.77698\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.97891\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.85094\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.83842\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.82589\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.80843\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.06107\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.11223\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.07942\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.71509\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 1.01905\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.80205\n",
      "\tTrain loss: 0.03659, Accuracy: 3551/6768 (52.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 807/1692 (47.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.96693\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.04323\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.05353\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.01508\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.79557\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.73198\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.09569\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.84275\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.80543\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 1.08690\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.72046\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.90893\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.82185\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.99944\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.79976\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.85938\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.85970\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.08597\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 1.15470\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.85128\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.92623\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.90877\n",
      "\tTrain loss: 0.03499, Accuracy: 3638/6768 (53.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00075, Accuracy: 835/1692 (49.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.76583\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.20750\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.11462\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.80605\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.81190\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.52988\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.05602\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.98731\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.59503\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.20581\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.70271\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.80338\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.80062\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.84470\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.70864\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.81523\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.80675\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.02464\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.95899\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.80704\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.99169\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.63980\n",
      "\tTrain loss: 0.03980, Accuracy: 3317/6768 (49.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 776/1692 (45.00%)\n",
      "\tTest loss: 0.00112, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.82460\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 1.04626\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 1.00029\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.85602\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.76637\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.48569\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.68889\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.80053\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.64188\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 1.06810\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.54091\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.87966\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.68422\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.81862\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.55446\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.59651\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.83184\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.92637\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.70611\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.61176\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 1.07569\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.67941\n",
      "\tTrain loss: 0.03552, Accuracy: 3711/6768 (54.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 863/1692 (51.00%)\n",
      "\tTest loss: 0.00106, Accuracy: 589/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.77557\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 1.16489\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.96115\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.81533\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.79194\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.54835\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.78392\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.78646\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.72509\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 1.02417\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.47049\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.85714\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.66332\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.78149\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.50255\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.59616\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.68634\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.78829\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.88381\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.68007\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.72983\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.56677\n",
      "\tTrain loss: 0.03851, Accuracy: 3650/6768 (53.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 836/1692 (49.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.81138\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.00258\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.83112\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.79863\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.61192\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.50919\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.88401\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.79939\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.63083\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.87728\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.60381\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.87149\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.71987\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.75619\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.59050\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.72488\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.68871\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.94121\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.91205\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.57276\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.85755\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.59549\n",
      "\tTrain loss: 0.03811, Accuracy: 3609/6768 (53.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 812/1692 (47.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 571/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.76815\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.69848\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.76283\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.83649\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.80812\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.60127\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.84301\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.99774\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.58304\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 1.00284\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.47473\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.64194\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.63398\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.49813\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.47786\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.68685\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.93271\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.79328\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.57022\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.02986\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.73203\n",
      "\tTrain loss: 0.04126, Accuracy: 3578/6768 (52.00%)\n",
      "\tValidation loss: 0.00093, Accuracy: 795/1692 (46.00%)\n",
      "\tTest loss: 0.00131, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.63567\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.93057\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.77963\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.79181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.83851\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.49384\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.64829\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.99598\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.63213\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.63769\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.56403\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.82808\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.63535\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.73235\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.51843\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.58460\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.71169\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.93018\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.80573\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.67717\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.92638\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.47542\n",
      "\tTrain loss: 0.03802, Accuracy: 3773/6768 (55.00%)\n",
      "\tValidation loss: 0.00087, Accuracy: 855/1692 (50.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.55596\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.85588\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.85318\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.58816\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.53279\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.43704\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.83527\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.63433\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.51109\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.78508\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.54066\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.96426\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.68633\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.87062\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.38681\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.72429\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.62026\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.91036\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.85082\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.79475\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.60198\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.37718\n",
      "\tTrain loss: 0.03268, Accuracy: 4242/6768 (62.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 941/1692 (55.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 627/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.68444\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.69853\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.74627\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.74776\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.75744\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.30212\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.69930\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.71198\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.37089\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.79582\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.55347\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.60408\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.47544\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.59117\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.28767\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.49856\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.55297\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.80461\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.65186\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.47311\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.79298\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.37691\n",
      "\tTrain loss: 0.04425, Accuracy: 3583/6768 (52.00%)\n",
      "\tValidation loss: 0.00099, Accuracy: 810/1692 (47.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.54507\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.49534\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.79002\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.65074\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.59621\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.30983\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.61499\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.75642\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.55276\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.83541\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.34871\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.71331\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.62159\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.54359\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.39783\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.63142\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.56108\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.67286\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.58430\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.51162\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.92081\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.43942\n",
      "\tTrain loss: 0.03383, Accuracy: 3998/6768 (59.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 910/1692 (53.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 609/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.56692\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.65930\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.76464\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.49101\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.59738\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.28007\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.64867\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.63042\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.46374\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.69798\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.47831\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.72154\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.52570\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.69798\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.40524\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.43713\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.81934\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.50433\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.70336\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.38931\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.74011\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.36178\n",
      "\tTrain loss: 0.03701, Accuracy: 3964/6768 (58.00%)\n",
      "\tValidation loss: 0.00088, Accuracy: 872/1692 (51.00%)\n",
      "\tTest loss: 0.00129, Accuracy: 617/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.47163\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.74519\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.56168\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.57079\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.54025\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.24277\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.72634\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.82799\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.58737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.57453\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.43561\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.66068\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.64873\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.66504\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.54117\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.52813\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.70086\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.62547\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.51774\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.44197\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.73059\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.44593\n",
      "\tTrain loss: 0.02802, Accuracy: 4632/6768 (68.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 659/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.60942\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.74723\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.55135\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.61180\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.35638\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.26990\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.54567\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.74816\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.53564\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.71763\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.49109\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.30890\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.52550\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.42531\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.37901\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.41286\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.78473\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.85066\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.46063\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.42327\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.65408\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.45291\n",
      "\tTrain loss: 0.02691, Accuracy: 4503/6768 (66.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 995/1692 (58.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 645/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.63063\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.58364\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.59067\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.50587\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.56751\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.25987\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.62205\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.45597\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.38207\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.62982\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.30256\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.40987\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.38010\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.74832\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.34486\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.48428\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.80324\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.46319\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.55016\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.43819\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.66046\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.24589\n",
      "\tTrain loss: 0.02952, Accuracy: 4469/6768 (66.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 998/1692 (58.00%)\n",
      "\tTest loss: 0.00130, Accuracy: 610/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.47173\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.52461\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.66405\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.22539\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.61219\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.33025\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.49517\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.54298\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.47445\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.53523\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.36152\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.54322\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.45464\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.45224\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.45782\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.41497\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.47235\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.58226\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.81663\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.36027\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.56360\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.33370\n",
      "\tTrain loss: 0.03086, Accuracy: 4469/6768 (66.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 963/1692 (56.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 605/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.67900\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.56139\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.55898\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.65326\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.47516\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.20997\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.62984\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.50687\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.42018\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.52463\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.27178\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.76618\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.46367\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.64655\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.27153\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.35636\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.48582\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.50287\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.54248\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.38790\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.53047\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.30322\n",
      "\tTrain loss: 0.03392, Accuracy: 4307/6768 (63.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 938/1692 (55.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 631/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.61198\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.78651\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.58415\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.53449\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.53836\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.19739\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.37886\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.32150\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.48813\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.53818\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.48275\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.68137\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.48620\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.62141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.23404\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.26330\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.49805\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.44846\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.38167\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.44365\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.47922\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.27569\n",
      "\tTrain loss: 0.03011, Accuracy: 4682/6768 (69.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1031/1692 (60.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 639/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.46819\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.65417\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.41532\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.52571\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.43501\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.14242\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.40725\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.27571\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.40092\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.62420\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.31668\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.50290\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.49474\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.42123\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.31171\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.33165\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.70877\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.72470\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.71112\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.46996\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.63980\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.39061\n",
      "\tTrain loss: 0.02884, Accuracy: 4697/6768 (69.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1011/1692 (59.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 652/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.58013\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.48912\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.47551\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.53338\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.43798\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.20049\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.55608\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.52524\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.29502\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.45393\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.28957\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.82577\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.54835\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.53319\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.42106\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.35093\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.53285\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.46442\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.23191\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.47995\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.67982\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.20546\n",
      "\tTrain loss: 0.02405, Accuracy: 5056/6768 (74.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1110/1692 (65.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 671/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.62522\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.56948\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.37457\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.40427\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.44103\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.21122\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.52359\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.43669\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.55504\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.60885\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.21388\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.40557\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.30229\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.39941\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.47640\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.36965\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.40424\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.61850\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.31094\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.24028\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.67720\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.19374\n",
      "\tTrain loss: 0.03266, Accuracy: 4389/6768 (64.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 941/1692 (55.00%)\n",
      "\tTest loss: 0.00141, Accuracy: 622/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.54714\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.30052\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.59961\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.22366\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.25369\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.22519\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.56110\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.48522\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.37769\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.44456\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.24859\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.39611\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.49521\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.35853\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.40793\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.17388\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.51516\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.60369\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.57927\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.22845\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.57769\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.22752\n",
      "\tTrain loss: 0.03023, Accuracy: 4603/6768 (68.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1017/1692 (60.00%)\n",
      "\tTest loss: 0.00151, Accuracy: 628/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.61017\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.24658\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.47419\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.21332\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.31360\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.19461\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.41696\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.24000\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.36977\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.45675\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.31026\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.41003\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.25355\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.37911\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.29835\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.22392\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.42431\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.37960\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.26656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.41025\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.61278\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.13002\n",
      "\tTrain loss: 0.02951, Accuracy: 4769/6768 (70.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1067/1692 (63.00%)\n",
      "\tTest loss: 0.00148, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.49730\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.36515\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.51583\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.20608\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.35003\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.16427\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.20534\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.20710\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.24971\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.37799\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.21118\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.53090\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.41827\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.43883\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.38743\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.25343\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.45691\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.37919\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.38237\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.30628\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.62170\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.31503\n",
      "\tTrain loss: 0.02025, Accuracy: 5307/6768 (78.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1177/1692 (69.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 679/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.49448\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.23148\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.42418\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.35975\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.34535\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.08709\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.39774\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.32201\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.44704\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.61448\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.35025\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.81718\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.24776\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.52935\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.42824\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.29316\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.31151\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.55436\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.27107\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.35096\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.43246\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.23738\n",
      "\tTrain loss: 0.02257, Accuracy: 5186/6768 (76.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1138/1692 (67.00%)\n",
      "\tTest loss: 0.00146, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.47433\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.28139\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.27709\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.26071\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.43196\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.14943\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.54060\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.32063\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.30755\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.35787\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.19096\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.33679\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.29090\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.28387\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.19148\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.33773\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.43869\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.50697\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.30362\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.33704\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.55144\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.29054\n",
      "\tTrain loss: 0.02594, Accuracy: 4905/6768 (72.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 1071/1692 (63.00%)\n",
      "\tTest loss: 0.00147, Accuracy: 680/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.36987\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.16494\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.27365\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.22289\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.18188\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.31061\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.63858\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.30051\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.24997\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.24790\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.23151\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.48354\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.24666\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.17637\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.45535\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.23463\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.37278\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.17692\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.21850\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.24370\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.24291\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.13744\n",
      "\tTrain loss: 0.02437, Accuracy: 5115/6768 (75.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 1131/1692 (66.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.54604\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.39527\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.42118\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.25945\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.32131\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.13462\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.52301\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.32991\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.18292\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.53795\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.13823\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.63418\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.24953\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.26597\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.25683\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.19526\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.28309\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.24316\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.35212\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.25344\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.38902\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.27491\n",
      "\tTrain loss: 0.02590, Accuracy: 5062/6768 (74.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1128/1692 (66.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 652/1772 (36.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.49111\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.33719\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.44923\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.21714\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.18721\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.28798\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.30099\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.28233\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.35417\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.52675\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.20985\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.23257\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.26942\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.24132\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.26652\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.27869\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.37263\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.48486\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.38099\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.52421\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.26240\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.22509\n",
      "\tTrain loss: 0.02327, Accuracy: 5222/6768 (77.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1157/1692 (68.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 716/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.36848\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.51711\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.43573\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.17518\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.33112\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.12920\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.32497\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.30976\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.40840\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.45794\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.33074\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.55281\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.26726\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.31960\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.11314\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.37404\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.28833\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.45809\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.34196\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.43001\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.30115\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.12489\n",
      "\tTrain loss: 0.02046, Accuracy: 5338/6768 (78.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00156, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.67580\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.53579\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.23554\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.18487\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.17524\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.14343\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.48185\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.48711\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.21890\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.20092\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.09611\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.47697\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.17522\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.21211\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.26425\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.09689\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.61616\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.26045\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.20759\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.19698\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.51277\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.09258\n",
      "\tTrain loss: 0.02695, Accuracy: 4975/6768 (73.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1084/1692 (64.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 650/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.34998\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.28429\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.21978\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.33612\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.11497\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.15460\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.33403\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.29763\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.31932\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.33548\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.07697\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.34268\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.15633\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.15992\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.11955\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.30594\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.42261\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.36906\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.37428\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.22906\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.52805\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.09384\n",
      "\tTrain loss: 0.02489, Accuracy: 5144/6768 (76.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 1140/1692 (67.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.35641\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.08362\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.30248\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.28866\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.30886\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.03582\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.28537\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.13164\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.18695\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.32681\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.16201\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.38633\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.49226\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.16001\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.12971\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.21995\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.33319\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.31930\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.31086\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.27626\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.55583\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.24190\n",
      "\tTrain loss: 0.02198, Accuracy: 5267/6768 (77.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1156/1692 (68.00%)\n",
      "\tTest loss: 0.00156, Accuracy: 676/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.42010\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.18767\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.54213\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.23852\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.25943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.14419\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.28716\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.24268\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.14989\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.45854\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.11746\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.38390\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.10836\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.37021\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.16248\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.08541\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.23621\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.24174\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.24896\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.31438\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.19035\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.05220\n",
      "\tTrain loss: 0.02349, Accuracy: 5186/6768 (76.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1155/1692 (68.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 675/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.28316\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.39453\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.64057\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.31453\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.16882\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.09461\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.28204\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.19623\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.24235\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.29966\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.19731\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.56269\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.31982\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.15104\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.31329\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.17904\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.28087\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.36724\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.35107\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.36368\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.48935\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.16568\n",
      "\tTrain loss: 0.02252, Accuracy: 5278/6768 (77.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1158/1692 (68.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.42767\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.35170\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.34622\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.24222\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.22986\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.15293\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.19760\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.29755\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.30620\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.27191\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.19130\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.19146\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.17133\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.28039\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.22718\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.16189\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.16490\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.42224\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.15385\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.15350\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.32240\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.11456\n",
      "\tTrain loss: 0.02030, Accuracy: 5374/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1194/1692 (70.00%)\n",
      "\tTest loss: 0.00160, Accuracy: 683/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.51704\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.29141\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.47302\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.13123\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.28884\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.06556\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.13960\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.37039\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.29005\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.10135\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.16701\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.31495\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.11541\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.30102\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.10316\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.12313\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.42446\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.35223\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.15725\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.16092\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.24367\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.17304\n",
      "\tTrain loss: 0.02356, Accuracy: 5214/6768 (77.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 659/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.24810\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.13425\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.29688\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.26810\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.25061\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.07474\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.16891\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.25310\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.06060\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.26604\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.08523\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.39074\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.16474\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.13274\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.19185\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.06211\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.21008\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.24608\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.25782\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.13670\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.37092\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.17409\n",
      "\tTrain loss: 0.02596, Accuracy: 5087/6768 (75.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1122/1692 (66.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 647/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.31360\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.26302\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.26407\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.41891\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.20950\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.39567\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.22395\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.30771\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.35984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.10440\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.30265\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.25610\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.16828\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.19582\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.25274\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.11726\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.52153\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.41395\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.16201\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.35060\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.15403\n",
      "\tTrain loss: 0.01755, Accuracy: 5573/6768 (82.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1231/1692 (72.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 725/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.59962\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.25045\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.42651\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.17540\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.07306\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.24279\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.12497\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.24171\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.05552\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.30417\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.13506\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.18949\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.14445\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.24090\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.28232\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.14988\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.08547\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.22429\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.26304\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.17793\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.66511\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.07937\n",
      "\tTrain loss: 0.03097, Accuracy: 4891/6768 (72.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1092/1692 (64.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 636/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.37577\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.32631\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.16734\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.25934\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.20062\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.03813\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.14620\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.35642\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.12276\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.07039\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.13919\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.17736\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.26677\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.19948\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.19807\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.08118\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.27649\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.37160\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.24072\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.10813\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.31774\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.13979\n",
      "\tTrain loss: 0.02396, Accuracy: 5288/6768 (78.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1179/1692 (69.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 697/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.43332\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.15467\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.16618\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.27389\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.40610\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.05842\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.18565\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.13348\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.16147\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.20327\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.06579\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.26480\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.36871\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.19210\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.22467\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.11857\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.16637\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.43303\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.16661\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.16594\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.43756\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.13081\n",
      "\tTrain loss: 0.01414, Accuracy: 5732/6768 (84.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1278/1692 (75.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 726/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.44529\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.14333\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.11893\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.16772\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.03637\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.04100\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.39680\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.14128\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.23967\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.18913\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.07267\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.30294\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.26844\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.22575\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.13312\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.29156\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.30564\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.18640\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.19677\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.07122\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.25499\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.14923\n",
      "\tTrain loss: 0.02103, Accuracy: 5404/6768 (79.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1200/1692 (70.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 693/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.21468\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.16960\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.21707\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.33022\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.13195\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.03349\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.21598\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.10932\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.06750\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.19602\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.16847\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.13379\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.13480\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.17969\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.25175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.25464\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.15972\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.20260\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.18449\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.21797\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.37644\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.45870\n",
      "\tTrain loss: 0.02949, Accuracy: 4986/6768 (73.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 1104/1692 (65.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 688/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.49565\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.28470\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.26413\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.11708\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.26862\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.04511\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.05870\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.06477\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.10817\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.25356\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.09361\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.16512\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.22376\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.20473\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.22373\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.06111\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.15997\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.15948\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.18483\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.27013\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.32868\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.37054\n",
      "\tTrain loss: 0.02247, Accuracy: 5370/6768 (79.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.24581\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.03373\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.26207\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.20168\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.07669\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.08629\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.08383\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.13099\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.16416\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.10344\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.04452\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.23261\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.20714\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.34639\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.09051\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.06740\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.19459\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.30436\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.12213\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.09884\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.30091\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.10085\n",
      "\tTrain loss: 0.01941, Accuracy: 5539/6768 (81.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1248/1692 (73.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 715/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.45011\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.25678\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.25579\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.09600\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.09056\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.08959\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.11069\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.02865\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.26212\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.18191\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.06149\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.35868\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.18044\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.52612\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.12278\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.05314\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.15807\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.10763\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.17334\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.32146\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.25491\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.12648\n",
      "\tTrain loss: 0.02821, Accuracy: 5070/6768 (74.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 1135/1692 (67.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 653/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.15951\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.36507\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.30427\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.15073\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.18957\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.08277\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.21947\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.11267\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.27110\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.13277\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.12693\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.16667\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.27745\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.30116\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.18194\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.15197\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.07562\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.26382\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.26168\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.36145\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.42603\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.19649\n",
      "\tTrain loss: 0.02228, Accuracy: 5318/6768 (78.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1168/1692 (69.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 679/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.34164\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.07069\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.22684\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.28212\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.06480\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.09946\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.11267\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.08831\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.18278\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.24342\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.18796\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.28479\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.10021\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.09269\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.17821\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.08823\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.17709\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.13918\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.24871\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.15702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.33769\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.18267\n",
      "\tTrain loss: 0.02267, Accuracy: 5362/6768 (79.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 681/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.44891\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.06868\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.13287\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.03928\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.25318\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.09885\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.16951\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.14742\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.18647\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.20458\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.06457\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.25776\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.23090\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.21309\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.08501\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.26281\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.33248\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.16618\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.48295\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.23289\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.08941\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.20612\n",
      "\tTrain loss: 0.02452, Accuracy: 5262/6768 (77.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.32235\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.07789\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.18210\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.14409\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.08509\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.04826\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.07476\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.06382\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.10432\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.12004\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.15654\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.48645\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.25825\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.09318\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.14887\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.11274\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.09949\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.38233\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.17267\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.18128\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.18551\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.09087\n",
      "\tTrain loss: 0.01919, Accuracy: 5487/6768 (81.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1227/1692 (72.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 693/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.52532\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.49185\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.31389\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.07354\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.14602\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.07760\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.06140\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.11588\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.21187\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.14299\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.09957\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.21357\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.12561\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.11285\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.15188\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.04168\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.40214\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.20381\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.34278\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.03231\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.09819\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.21406\n",
      "\tTrain loss: 0.01799, Accuracy: 5569/6768 (82.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 713/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.31018\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.21007\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.18734\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.08395\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.20356\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.05774\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.14044\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.20814\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.32353\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.20916\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.06368\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.19125\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.13225\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.07468\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.13225\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.15840\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.26711\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.14805\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.26092\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.11946\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.21772\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.03559\n",
      "\tTrain loss: 0.01965, Accuracy: 5584/6768 (82.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 698/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.19365\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.42478\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.06455\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.16507\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.10962\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.34039\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.12233\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.06215\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.30077\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.15598\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.16636\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.23620\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.06547\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.10302\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.12002\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.13883\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.23538\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.18770\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.17632\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.15011\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.33609\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.14928\n",
      "\tTrain loss: 0.02339, Accuracy: 5345/6768 (78.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 694/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.20763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.25428\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.36649\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.06713\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.13598\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.04189\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.18772\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.07470\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.17468\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.15037\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.14853\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.24698\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.08473\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.14528\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.10657\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.06212\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.09727\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.11223\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.19095\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.13864\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.25657\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.04044\n",
      "\tTrain loss: 0.02742, Accuracy: 5097/6768 (75.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 658/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.40212\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.19527\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.12367\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.15835\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.12638\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.02795\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.11793\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.12108\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.05332\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.28669\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.05907\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.19144\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.07978\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.02648\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.05666\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.08108\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.24828\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.09283\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.03724\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.19452\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.34131\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.08846\n",
      "\tTrain loss: 0.02374, Accuracy: 5301/6768 (78.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.32997\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.22968\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.23877\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.16708\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.34300\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.03611\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.08069\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.06001\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.06872\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.19521\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.09410\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.14911\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.15565\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.14448\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.28831\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.36630\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.11270\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.16968\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.24131\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.11315\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.05523\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.15346\n",
      "\tTrain loss: 0.02344, Accuracy: 5405/6768 (79.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1205/1692 (71.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 686/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.21670\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.14941\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.23437\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.24730\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.14449\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.10814\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.04068\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.20395\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.16571\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.09762\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.18912\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.14394\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.34266\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.07568\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.49496\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.07097\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.28104\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.09186\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.08487\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.19807\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.08575\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.05670\n",
      "\tTrain loss: 0.02024, Accuracy: 5508/6768 (81.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1241/1692 (73.00%)\n",
      "\tTest loss: 0.00188, Accuracy: 725/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.34222\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.24352\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.13666\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.09034\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.07656\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.06574\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.30688\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.11350\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.16863\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.07005\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.04933\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.13868\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.17738\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.24805\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.09933\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.19212\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.08792\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.12495\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.10461\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.28886\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.16130\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.05532\n",
      "\tTrain loss: 0.01942, Accuracy: 5680/6768 (83.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1297/1692 (76.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.41181\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.06538\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.24119\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.13271\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.39677\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.14775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.11322\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.13698\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.17017\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.14120\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.26387\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.15409\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.10995\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.25015\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.04191\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.09340\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.26263\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.09968\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.14198\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.17475\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.17814\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.16434\n",
      "\tTrain loss: 0.03465, Accuracy: 5029/6768 (74.00%)\n",
      "\tValidation loss: 0.00098, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00228, Accuracy: 649/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.22838\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.04004\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.16486\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.09376\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.10253\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.06815\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.13355\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.04698\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.06529\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.25360\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.02319\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.31921\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.06214\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.09658\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.06931\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.20925\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.09573\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.10856\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.22073\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.09114\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.09991\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.17669\n",
      "\tTrain loss: 0.03246, Accuracy: 4934/6768 (72.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1078/1692 (63.00%)\n",
      "\tTest loss: 0.00216, Accuracy: 639/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.20361\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.25050\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.20733\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.12868\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.07152\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.04346\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.05959\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.06516\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.02959\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.27511\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.09658\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.19017\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.03628\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.17003\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.17424\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.02673\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.14609\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.08105\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.12083\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.51017\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.04942\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.18669\n",
      "\tTrain loss: 0.02204, Accuracy: 5320/6768 (78.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1186/1692 (70.00%)\n",
      "\tTest loss: 0.00199, Accuracy: 717/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.15083\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.12673\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.19309\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.17935\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.09022\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.05309\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.19200\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.09695\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.07125\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.09030\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.14365\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.07842\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.17610\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.07258\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.10915\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.12049\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.13332\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.08406\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.06995\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.26339\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.41961\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.18433\n",
      "\tTrain loss: 0.01557, Accuracy: 5723/6768 (84.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1259/1692 (74.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 698/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.34734\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.06895\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.25249\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.12603\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.01619\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.04786\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.28648\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.16585\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.08011\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.15783\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.08239\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.22929\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.02503\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.25445\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.10289\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.19277\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.04656\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.09701\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.22544\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.07671\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.52862\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.08680\n",
      "\tTrain loss: 0.01951, Accuracy: 5604/6768 (82.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1252/1692 (73.00%)\n",
      "\tTest loss: 0.00200, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.25462\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.11301\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.06207\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.19457\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.16752\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.15405\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.09669\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.06362\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.06705\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.17563\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.08747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.24614\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.22558\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.28303\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.15432\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.09444\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.43383\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.23886\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.10719\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.10676\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.16356\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.04025\n",
      "\tTrain loss: 0.02082, Accuracy: 5486/6768 (81.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1234/1692 (72.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.38097\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.20545\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.37770\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.02224\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.17001\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.08220\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.08368\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.13493\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.03928\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.19413\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.13261\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.36559\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.08618\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.14511\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.07484\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.07229\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.03729\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.07811\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.09792\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.12064\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.23280\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.06682\n",
      "\tTrain loss: 0.03370, Accuracy: 4983/6768 (73.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1102/1692 (65.00%)\n",
      "\tTest loss: 0.00234, Accuracy: 641/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.11135\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.38136\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.06973\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.01631\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.03555\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.11827\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.05507\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.03089\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.06358\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.09535\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.05967\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.14100\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.20472\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.13960\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.13051\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.15036\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.07073\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.31627\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.08118\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.20135\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.07001\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.03547\n",
      "\tTrain loss: 0.01905, Accuracy: 5597/6768 (82.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1238/1692 (73.00%)\n",
      "\tTest loss: 0.00198, Accuracy: 695/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.26876\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.11099\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.19814\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.07152\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.23101\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.04292\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.16894\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.18332\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.06791\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.20702\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.05227\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.33191\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.16223\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.09196\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.15575\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.20423\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.25908\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.07028\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.19305\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.06850\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.06353\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.14391\n",
      "\tTrain loss: 0.01915, Accuracy: 5551/6768 (82.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 1224/1692 (72.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 686/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.18653\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.06194\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.20303\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.06980\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.05542\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.11412\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.15509\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.06140\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.11450\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.14027\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.16406\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.14523\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.05514\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.05863\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.06081\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.31002\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.15341\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.15205\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.04884\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.10115\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.07293\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.02216\n",
      "\tTrain loss: 0.02923, Accuracy: 5155/6768 (76.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1160/1692 (68.00%)\n",
      "\tTest loss: 0.00211, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.24354\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.22065\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.04012\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.18526\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.02377\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.01039\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.14151\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.02821\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.07334\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.09943\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.03911\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.29628\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.05734\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.24604\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.03015\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.03194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.17545\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.33594\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.06727\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.09223\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.18005\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.17874\n",
      "\tTrain loss: 0.02943, Accuracy: 5238/6768 (77.00%)\n",
      "\tValidation loss: 0.00086, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00220, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.20638\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.10900\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.03591\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.24564\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.11289\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.06945\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.08106\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.12439\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.08698\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.21501\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.03253\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.36828\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.10063\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.15242\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.14127\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.02400\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.16113\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.34771\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.48712\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.10445\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.19009\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.20280\n",
      "\tTrain loss: 0.03477, Accuracy: 4932/6768 (72.00%)\n",
      "\tValidation loss: 0.00093, Accuracy: 1097/1692 (64.00%)\n",
      "\tTest loss: 0.00232, Accuracy: 642/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.39164\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.33135\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.19351\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.05032\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.07561\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.02355\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.15243\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.07277\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.22460\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.15384\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.04201\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.21532\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.07735\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.07180\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.11418\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.06566\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.24422\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.10339\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.39322\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.15571\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.10366\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.03677\n",
      "\tTrain loss: 0.02516, Accuracy: 5230/6768 (77.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1180/1692 (69.00%)\n",
      "\tTest loss: 0.00212, Accuracy: 643/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.22973\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.14955\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.13855\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.07726\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.03133\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.02025\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.14430\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.02900\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.17131\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.08519\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.19587\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.15657\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.21487\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.10221\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.07116\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.15680\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.28385\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.21123\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.13671\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.03021\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.28243\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.02720\n",
      "\tTrain loss: 0.03107, Accuracy: 5090/6768 (75.00%)\n",
      "\tValidation loss: 0.00087, Accuracy: 1141/1692 (67.00%)\n",
      "\tTest loss: 0.00224, Accuracy: 646/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.18522\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.17678\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.02695\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.03917\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.03737\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.04128\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.06245\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.06241\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.05514\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.08463\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.02863\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.17542\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.28609\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.03128\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.11834\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.04513\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.11432\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.20927\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.58272\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.17194\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.19575\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.20542\n",
      "\tTrain loss: 0.02625, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1146/1692 (67.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 637/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.12998\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.15309\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.12944\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.03521\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.03610\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.03495\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.09067\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.05924\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.08013\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.06959\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.05093\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.14524\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.28417\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.24802\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.06978\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.06494\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.08945\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.14904\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.32613\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.15183\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.15701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.08422\n",
      "\tTrain loss: 0.02059, Accuracy: 5673/6768 (83.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1292/1692 (76.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 720/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.40080\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.16652\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.26865\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.21446\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.02222\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.05673\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.10948\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.06328\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.12273\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.02470\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.06199\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.09973\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.15439\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.05700\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.06105\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.09578\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.05264\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.31589\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.07734\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.05210\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.15244\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.02939\n",
      "\tTrain loss: 0.02742, Accuracy: 5373/6768 (79.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 1209/1692 (71.00%)\n",
      "\tTest loss: 0.00231, Accuracy: 671/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.25911\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.14174\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.33817\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.13484\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.03682\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.09428\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.38131\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.19065\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.03379\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.25651\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.25597\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.17344\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.08929\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.18392\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.07810\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.24765\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.15438\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.11829\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.03181\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.02804\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.24721\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.17542\n",
      "\tTrain loss: 0.02839, Accuracy: 5257/6768 (77.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00210, Accuracy: 689/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.16417\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.26279\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.12543\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.08199\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.01124\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.02499\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.03614\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.06375\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.08970\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.03635\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.02836\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.33286\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.08825\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.14452\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.15710\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.07698\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.21015\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.09728\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.10955\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.12267\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.18218\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.04756\n",
      "\tTrain loss: 0.03302, Accuracy: 4978/6768 (73.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1106/1692 (65.00%)\n",
      "\tTest loss: 0.00231, Accuracy: 662/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.21840\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.04765\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.09792\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.23917\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.09578\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.12135\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.05302\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.12484\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.03962\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.15023\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.09466\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.20928\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.03638\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.25606\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.34955\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.15201\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.17520\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.11388\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.05491\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.21396\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.09308\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.00525\n",
      "\tTrain loss: 0.02462, Accuracy: 5389/6768 (79.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00213, Accuracy: 701/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.24113\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.08602\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.09809\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.01909\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.18482\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.12117\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.13199\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.01072\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.06135\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.08257\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.04884\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.13554\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.15769\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.20626\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.07694\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.02311\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.08656\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.16341\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.04103\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.31265\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.09627\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.06980\n",
      "\tTrain loss: 0.02287, Accuracy: 5585/6768 (82.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 1255/1692 (74.00%)\n",
      "\tTest loss: 0.00217, Accuracy: 687/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.23229\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.07206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.07209\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.16742\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.05213\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.02984\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.15736\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.19429\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.02379\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.13899\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.11065\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.09170\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.30002\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.07556\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.09697\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.13692\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.07852\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.20128\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.27405\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.02512\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.13930\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.10156\n",
      "\tTrain loss: 0.02479, Accuracy: 5395/6768 (79.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00218, Accuracy: 682/1772 (38.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.766548463356974\n",
      "Best test accuracy:\n",
      "0.40970654627539504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(gru, optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c17b8f",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa1e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "class CNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7c97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate LSTM model\n",
    "cnn_lstm = CNN_LSTM()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(cnn_lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088397f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.43890\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.46004\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.35402\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.39963\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.45418\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.46258\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.41949\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.42676\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.41432\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.41545\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.45960\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.40389\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.37963\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.38618\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.41861\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.34269\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.39094\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.40985\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.41452\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.44635\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.36851\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.38330\n",
      "\tTrain loss: 0.04222, Accuracy: 2375/6768 (35.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 567/1692 (33.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 602/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.40301\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.30820\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.38509\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.30576\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.41172\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.31527\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.28702\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.35393\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.32698\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.28250\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.35721\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.28766\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.32123\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.35331\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.33564\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.26061\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.37636\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.31639\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.31570\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.42934\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.26297\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.26963\n",
      "\tTrain loss: 0.04001, Accuracy: 2795/6768 (41.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 653/1692 (38.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 662/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.34594\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.40450\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.25266\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.19942\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.43451\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.12974\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.23669\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.28501\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.21081\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.20791\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.16386\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.25208\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.11122\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.27005\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.25079\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.16673\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.37977\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.44989\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.32745\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.41464\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.17124\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.25392\n",
      "\tTrain loss: 0.03828, Accuracy: 2995/6768 (44.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 696/1692 (41.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 679/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.31455\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.21357\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.34142\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.25227\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.32679\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.14541\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.29841\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.40108\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.10745\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.15490\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.17764\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.41677\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.06084\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.32025\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.31357\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.27155\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.30804\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.43722\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.17264\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.28060\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.18458\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.36062\n",
      "\tTrain loss: 0.03822, Accuracy: 2980/6768 (44.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 702/1692 (41.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.28553\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.22026\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.25364\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.12320\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.23183\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.06597\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.16227\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.27126\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.14725\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.01429\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.12849\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.23546\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.06027\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.28169\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.18707\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.11765\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.28929\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.52151\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.23161\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.52247\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.19790\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.31606\n",
      "\tTrain loss: 0.03880, Accuracy: 2923/6768 (43.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 674/1692 (39.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 685/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.38937\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.40956\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.21119\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.16455\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.23929\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.07190\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.09070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.16684\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.12466\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.19617\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.05129\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.20395\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 0.92148\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.18230\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.11447\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.26930\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.39789\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.38619\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.17461\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.38201\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.22048\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.27449\n",
      "\tTrain loss: 0.03644, Accuracy: 3185/6768 (47.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 742/1692 (43.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.11800\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.29387\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.13862\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 0.98972\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.22007\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.01396\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.17143\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.13270\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.18818\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.06364\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.00942\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.15079\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 0.98209\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.26400\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.12935\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.11643\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.08176\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.35673\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.16895\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.22671\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.26044\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.32623\n",
      "\tTrain loss: 0.03677, Accuracy: 3151/6768 (46.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 735/1692 (43.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 748/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.21995\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.14297\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.26853\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.02060\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.06467\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.96706\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.23171\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.10690\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.19268\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.20040\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.03574\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.18337\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 0.93191\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.33022\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.14821\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.28909\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.11423\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.28476\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.25174\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.19874\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.25055\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.25516\n",
      "\tTrain loss: 0.03675, Accuracy: 3159/6768 (46.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 744/1692 (43.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 718/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.14688\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.24144\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.06249\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.01903\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.17728\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.01433\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.17278\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.06855\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.05561\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.07199\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.12887\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.20450\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 0.96333\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.25198\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.04599\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.29742\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.03163\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.37054\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.19071\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.21437\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.16780\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.17867\n",
      "\tTrain loss: 0.03582, Accuracy: 3328/6768 (49.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 783/1692 (46.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 746/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.16185\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.32650\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.06596\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 0.96500\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19796\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 0.93374\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.21543\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 0.98854\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.10806\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.17348\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.10287\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 0.95982\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 0.94182\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.22937\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.02305\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.21889\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.12190\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.43225\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.15618\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.18368\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 0.99865\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.20875\n",
      "\tTrain loss: 0.03749, Accuracy: 3024/6768 (44.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 693/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.14409\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.11552\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.12697\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 0.92279\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.15998\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.02832\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.07680\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 0.92996\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.07951\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.03355\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.01198\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.19943\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 0.86068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.19978\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.04955\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.10988\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.08610\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.31067\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.12385\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.17605\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.03073\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.13496\n",
      "\tTrain loss: 0.03573, Accuracy: 3323/6768 (49.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 786/1692 (46.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 750/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.04491\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.09974\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.16142\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.00607\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.21626\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.02068\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.22583\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.03693\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 0.98098\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.16288\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.96762\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.02649\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.87447\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.20518\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 0.99495\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.01104\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.99500\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.29952\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.09989\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.34669\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.09627\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.06619\n",
      "\tTrain loss: 0.03589, Accuracy: 3303/6768 (48.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 778/1692 (45.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 738/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.13013\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.27874\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.20944\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 0.96487\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.25459\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.89747\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.30405\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.12363\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.05120\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 0.89765\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.88959\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 0.97943\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.90775\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.27080\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.09354\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.09501\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.05579\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.22785\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.08257\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.18704\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.12621\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.07418\n",
      "\tTrain loss: 0.03626, Accuracy: 3142/6768 (46.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 763/1692 (45.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 681/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.96390\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.29707\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.12146\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.09553\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 0.99190\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.87218\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.14025\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.98156\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 0.85656\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.17445\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.93508\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.85808\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.13889\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 0.97449\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.02044\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.02621\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.28992\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.97784\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.29875\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.10410\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.22144\n",
      "\tTrain loss: 0.03594, Accuracy: 3417/6768 (50.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 802/1692 (47.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 748/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.93341\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.14878\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.23099\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.90695\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.06768\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.92878\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.24464\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.91649\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.15486\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.07004\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.92024\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.05698\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.87667\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.12711\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.14616\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.92476\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.07849\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.30475\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.96630\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 1.33398\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.09855\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.14363\n",
      "\tTrain loss: 0.03474, Accuracy: 3452/6768 (51.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 817/1692 (48.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.90956\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.08873\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.09212\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.94145\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.01186\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.76111\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.29256\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.91882\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.84937\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.09364\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.89031\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.92092\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.86348\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.97234\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.95175\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.00987\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.10275\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.27839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.95908\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 1.14305\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.17313\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.94654\n",
      "\tTrain loss: 0.03365, Accuracy: 3562/6768 (52.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 835/1692 (49.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 1.00551\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.17229\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.16889\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.98438\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.97381\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.87251\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.21861\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.00836\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.91296\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.93128\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 1.03797\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.95645\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.86951\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.01306\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.05358\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.92687\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 1.02244\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.41518\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.89237\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.06269\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.98025\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.94345\n",
      "\tTrain loss: 0.03494, Accuracy: 3394/6768 (50.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 799/1692 (47.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 764/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.94844\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.03579\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.09617\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.81907\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.08851\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.87227\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.10319\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.00235\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.01743\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.90305\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.92571\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.95984\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.90900\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.01475\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.98592\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.91358\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.95663\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.42834\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.91685\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.09172\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.03320\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 1.15690\n",
      "\tTrain loss: 0.03530, Accuracy: 3383/6768 (49.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 795/1692 (46.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 705/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 1.00242\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.24193\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.22829\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.91320\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.08994\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.81971\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.05014\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.88787\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.99109\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.11471\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.80952\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.90904\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.90268\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.97950\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 1.18141\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.97489\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.91258\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 1.38034\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.95061\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 1.04702\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 1.02271\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.99856\n",
      "\tTrain loss: 0.03483, Accuracy: 3382/6768 (49.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 808/1692 (47.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 712/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 1.00867\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.09999\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.07121\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.93103\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 1.11183\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.89635\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.18139\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.94112\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.85856\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.99956\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.90854\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.80283\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.86262\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.93525\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 1.07724\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.86365\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.93217\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.35055\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.98015\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.02101\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.85327\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.08949\n",
      "\tTrain loss: 0.03303, Accuracy: 3589/6768 (53.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 854/1692 (50.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 1.00796\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.31709\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.98516\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.85560\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.00224\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.82044\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.98894\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.86482\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.95328\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 1.04791\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.82993\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.96163\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.77609\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.99736\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.84932\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.86379\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 1.23133\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.12642\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.85907\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 1.23068\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.95767\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 1.07776\n",
      "\tTrain loss: 0.03007, Accuracy: 3874/6768 (57.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00060, Accuracy: 924/1692 (54.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 832/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 1.04327\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.02833\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.16613\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.96660\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.92047\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.90716\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.08569\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.79247\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.97125\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.06635\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.80417\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.90239\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.78663\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.99413\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 1.04340\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 1.00565\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.96047\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.17375\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.89118\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 1.10768\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 1.05609\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.98944\n",
      "\tTrain loss: 0.03440, Accuracy: 3579/6768 (52.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 868/1692 (51.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 765/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.85554\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.95552\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 1.03231\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 1.00312\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 1.17736\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.71263\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.01742\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.90118\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.88980\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 1.05145\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.90838\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.75762\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.65360\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 1.00251\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.92334\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 1.05895\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 1.23615\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.07285\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.84261\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 1.06470\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.78916\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.95371\n",
      "\tTrain loss: 0.03354, Accuracy: 3526/6768 (52.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 835/1692 (49.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 766/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.93080\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 1.23507\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.97905\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.90762\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.93323\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.62021\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 1.08298\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.80091\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.74890\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.89665\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.96095\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.96406\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.69759\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.89933\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.85137\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.88282\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.89630\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 1.24603\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 1.06052\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 1.06766\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.98274\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.98564\n",
      "\tTrain loss: 0.03208, Accuracy: 3790/6768 (55.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 919/1692 (54.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 792/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.81643\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.77891\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.95083\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 1.00099\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 1.07198\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.66188\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 1.11737\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.75123\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.87769\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.81620\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.76052\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.91952\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.61437\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 1.04140\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.97005\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.91512\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 1.00782\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.23478\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.79187\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 1.15118\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.83881\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.91733\n",
      "\tTrain loss: 0.03035, Accuracy: 3925/6768 (57.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 908/1692 (53.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 802/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 1.07321\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 1.01544\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 1.14219\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.92131\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.96107\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.75623\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 1.00542\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.79778\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.92363\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.97498\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.90056\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 1.04069\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.76580\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.96884\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.96431\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 1.12643\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.91092\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.36467\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.75287\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 1.03385\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.05539\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 1.06631\n",
      "\tTrain loss: 0.03184, Accuracy: 3685/6768 (54.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 875/1692 (51.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 744/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.74130\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.90266\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.91223\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.82964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.03501\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.79594\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.94959\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.87544\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.72062\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.93612\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.85424\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.82892\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.74898\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.93914\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.86934\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 1.00825\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.91566\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.93198\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.87774\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 1.25243\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.93229\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.89730\n",
      "\tTrain loss: 0.03143, Accuracy: 3848/6768 (56.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 912/1692 (53.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 763/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.91339\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 1.13366\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 1.03045\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 1.01032\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.94226\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.60854\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 1.26650\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.76151\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.71912\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 1.14546\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.81093\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.79973\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.75554\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.97173\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.80890\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.99748\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.99467\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.27297\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.88088\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 1.05977\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.80292\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.98058\n",
      "\tTrain loss: 0.02969, Accuracy: 4088/6768 (60.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 944/1692 (55.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.83117\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.94551\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.96052\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.85566\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 1.17450\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.67653\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 1.08980\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.82854\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.66756\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 1.17362\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.74769\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.80831\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.86449\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.79989\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.73347\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 1.13023\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.94835\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 1.30178\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 1.01915\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.99640\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.77108\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.91001\n",
      "\tTrain loss: 0.02891, Accuracy: 3890/6768 (57.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 929/1692 (54.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.74127\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.82029\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.83387\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.81842\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 1.19227\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.93234\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 1.00118\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.65611\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.65130\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.87059\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.91756\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.68365\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.81204\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.87122\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.93705\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.90784\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.98714\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 1.21754\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.82324\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.97634\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.81685\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.98605\n",
      "\tTrain loss: 0.02748, Accuracy: 4180/6768 (61.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 989/1692 (58.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 801/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.91695\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 1.03461\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.93074\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.99122\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.95606\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.75699\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 1.02271\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.72725\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.75901\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.85634\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.76215\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.80223\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.61604\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.97837\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.85003\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 1.07727\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.98677\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 1.21085\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.90758\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.94184\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.59258\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.89927\n",
      "\tTrain loss: 0.02940, Accuracy: 4039/6768 (59.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 956/1692 (56.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 830/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.86891\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.92608\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 1.15130\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.91923\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.94373\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.58920\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.88979\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.63100\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.80580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.68484\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.91170\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.86495\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.71780\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 1.04383\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.81520\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.97593\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.84936\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 1.32249\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.85521\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.98268\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.70125\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 1.29430\n",
      "\tTrain loss: 0.02883, Accuracy: 4012/6768 (59.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 956/1692 (56.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 791/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.89057\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 1.12277\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.82289\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.80744\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.99042\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.72841\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.85311\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.75748\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.93764\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.89548\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.84760\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.64658\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.64344\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 1.02749\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.82800\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 1.00425\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.87045\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 1.14553\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.70106\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 1.12596\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.86084\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.89692\n",
      "\tTrain loss: 0.02752, Accuracy: 4154/6768 (61.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 977/1692 (57.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 793/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.93714\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.78396\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.65719\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.82667\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.88750\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.82351\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.83648\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.82152\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.91409\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.73776\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.82238\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.87961\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.64286\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.94749\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.68879\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 1.01593\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.94764\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 1.02822\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.90024\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 1.09438\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.92003\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 1.00720\n",
      "\tTrain loss: 0.02603, Accuracy: 4323/6768 (63.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 993/1692 (58.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.74466\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 1.10070\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.78055\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.87277\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 1.01528\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.57943\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.83834\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.63375\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.82011\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.94564\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.82671\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.82818\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.63217\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.93809\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.87586\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.89622\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.82928\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.94627\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.85827\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.80400\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.81789\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.91660\n",
      "\tTrain loss: 0.02971, Accuracy: 3989/6768 (58.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 927/1692 (54.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 744/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.76522\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 1.04585\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.77595\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.80104\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.76805\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.69549\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 1.08796\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.84505\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.66443\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.95106\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.81926\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.89653\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.71336\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 1.01371\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.94438\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.76196\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.94713\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 1.13084\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.85290\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.93054\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.80216\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.86901\n",
      "\tTrain loss: 0.02811, Accuracy: 4062/6768 (60.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 959/1692 (56.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 751/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.75101\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 1.08911\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.78000\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.87445\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.91663\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.64868\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.85110\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.58127\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.73968\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.89609\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.85957\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.73603\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.72029\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.98865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 1.09835\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.84595\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.90211\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 1.03137\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.75445\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.82159\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.61381\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.91924\n",
      "\tTrain loss: 0.02747, Accuracy: 4220/6768 (62.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 983/1692 (58.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 791/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.76203\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.93057\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.78765\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.75493\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.95186\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.62387\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.76531\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.60770\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.83742\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.71182\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.58880\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.84646\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.74495\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.86747\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.67577\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.85556\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 1.11101\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 1.22816\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.85397\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.86983\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.82450\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.83392\n",
      "\tTrain loss: 0.02794, Accuracy: 4165/6768 (61.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 979/1692 (57.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 1.07082\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 1.02985\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.70543\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.83121\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.94954\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.59116\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.91022\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.68353\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.86331\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.77569\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.79823\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.79405\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.63940\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.77704\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.62139\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.82608\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.90810\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 1.03849\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.94129\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.98669\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.89651\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.96390\n",
      "\tTrain loss: 0.02519, Accuracy: 4351/6768 (64.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1024/1692 (60.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 797/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.94061\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 1.08779\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.89363\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.85396\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.89845\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.69595\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.96792\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.59144\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.63442\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.82012\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.79387\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.76362\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.74622\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.89028\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.92869\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.77887\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.67640\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 1.18636\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.70872\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 1.17543\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.83432\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.70014\n",
      "\tTrain loss: 0.02439, Accuracy: 4432/6768 (65.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1037/1692 (61.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.76390\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.81676\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 1.07716\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.92876\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.96414\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.56841\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 1.08365\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.69380\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.71379\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.80557\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.73834\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.63594\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.66028\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.68200\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.73301\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.99457\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.71800\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 1.08965\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.91539\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.97398\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.71166\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.74198\n",
      "\tTrain loss: 0.02418, Accuracy: 4584/6768 (67.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1058/1692 (62.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.76753\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 1.10597\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.80694\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.78834\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.74342\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.90932\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.88016\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.83322\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.83973\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.71651\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.73519\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.63369\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.76633\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.90143\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 1.05725\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.80454\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.77626\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 1.03154\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.91415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.94697\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.72022\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.80825\n",
      "\tTrain loss: 0.02547, Accuracy: 4326/6768 (63.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1022/1692 (60.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 828/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.83045\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.89806\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.95353\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.81282\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.85829\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.54561\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.55546\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.88668\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.59114\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.65952\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.98348\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.82370\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.64692\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.95773\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.73465\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.91781\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.89307\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 1.05137\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.77960\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.83829\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.78531\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.87997\n",
      "\tTrain loss: 0.02709, Accuracy: 4327/6768 (63.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1017/1692 (60.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 798/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.94771\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.75662\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.67876\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.86426\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 1.01324\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.68710\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.73734\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.62946\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.88447\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.82536\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.68229\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.62871\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.72556\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.75620\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.90437\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.96951\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.74762\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 1.29857\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.86716\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.79124\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.76343\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.79320\n",
      "\tTrain loss: 0.02469, Accuracy: 4474/6768 (66.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1046/1692 (61.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.64469\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.87256\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.99341\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.94875\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.70552\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.52404\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 1.07273\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.84495\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.87495\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.66637\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.76296\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.73334\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.53997\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.68829\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.87124\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.82606\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.97816\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 1.07503\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.79055\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 1.01032\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.72176\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.93525\n",
      "\tTrain loss: 0.02664, Accuracy: 4276/6768 (63.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 999/1692 (59.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 771/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.76426\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.95648\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.86697\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.69193\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 1.05419\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.71545\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.82744\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.53552\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.61307\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.75154\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.65672\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.65251\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.64126\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 1.08244\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.86587\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.55431\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.92314\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.96147\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.79899\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 1.06358\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.65396\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.86209\n",
      "\tTrain loss: 0.02616, Accuracy: 4265/6768 (63.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 997/1692 (58.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.72863\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.97003\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 1.01110\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.69074\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.67072\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.47221\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.90923\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.62718\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.56889\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.85765\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.79888\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.69241\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.65272\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.76400\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.89518\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.80110\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.86883\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.85190\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.91096\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.97596\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.82485\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.71968\n",
      "\tTrain loss: 0.02314, Accuracy: 4483/6768 (66.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1075/1692 (63.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 810/1772 (45.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.71075\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.99317\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.63430\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.72822\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 1.10761\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.39535\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.84098\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.61215\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.65909\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.92523\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.61541\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.77943\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.57832\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.83113\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.71749\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.82776\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.88862\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 1.23266\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.74972\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.81639\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.61270\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.75001\n",
      "\tTrain loss: 0.02290, Accuracy: 4644/6768 (68.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1103/1692 (65.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 840/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.75768\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 1.00917\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.62779\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.67991\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.93542\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.55163\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.92806\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.62261\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.82713\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.79166\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.64754\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.69252\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.66227\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.68512\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.70359\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.84617\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.83955\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.89005\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.90258\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.92402\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.74024\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.83662\n",
      "\tTrain loss: 0.02149, Accuracy: 4724/6768 (69.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1095/1692 (64.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 835/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.81557\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.86631\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.81262\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.75037\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.87548\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.54809\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.87257\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.63591\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.59422\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 1.17711\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.75514\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.79945\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.81980\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.94793\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.70039\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.66524\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.82030\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.94540\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.79085\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 1.07419\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.65842\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.67012\n",
      "\tTrain loss: 0.02537, Accuracy: 4414/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1045/1692 (61.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 788/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.71197\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 1.02445\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.67981\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.81163\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.85510\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.67746\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.81181\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.55594\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.62718\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.69562\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.55494\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.67689\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.47729\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.57535\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.69625\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.73550\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.70694\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 1.09161\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.65836\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.86690\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.66997\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.96004\n",
      "\tTrain loss: 0.02360, Accuracy: 4559/6768 (67.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1077/1692 (63.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.80558\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.83213\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.77648\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.70122\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.99807\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.51950\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.88804\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.62521\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.87768\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.92399\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.82579\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.95920\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.57124\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.99586\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.66268\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.73400\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.85822\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 1.00542\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.69013\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.73994\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.64026\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 1.11248\n",
      "\tTrain loss: 0.02544, Accuracy: 4436/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1051/1692 (62.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.84824\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.62985\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.67261\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.66995\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.91994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.65749\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.71394\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.60746\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.67083\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.83148\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.76786\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.71380\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.71384\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.81836\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.63256\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.63364\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.71004\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.79624\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.66218\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.69349\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.89634\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.83354\n",
      "\tTrain loss: 0.02235, Accuracy: 4573/6768 (67.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1080/1692 (63.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.87556\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.87482\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.68203\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.71810\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.85010\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.52487\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.93885\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.60721\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.67581\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.82383\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.68910\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.56259\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.68722\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.94790\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.68472\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.79039\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.74753\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 1.15308\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.87422\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.92701\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.62004\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.67779\n",
      "\tTrain loss: 0.02255, Accuracy: 4721/6768 (69.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1107/1692 (65.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 803/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.75371\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.84700\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.59302\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.72130\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 1.00491\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.64800\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 1.02823\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.69970\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.56317\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.80372\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.61663\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.55762\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.55550\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.64887\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.65586\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.80706\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.93451\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.90700\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.70608\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.70360\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.64159\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.67457\n",
      "\tTrain loss: 0.02663, Accuracy: 4206/6768 (62.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1001/1692 (59.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 773/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.85523\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 1.14458\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.57326\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.74253\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.95826\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.51118\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.83671\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.52921\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.56613\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.75594\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.60159\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.70651\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.60805\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.73164\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.69285\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.88288\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.72518\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.91653\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.88145\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.85137\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.53844\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.82401\n",
      "\tTrain loss: 0.02493, Accuracy: 4616/6768 (68.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1083/1692 (64.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 753/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.78317\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.88794\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.97928\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.61143\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.93556\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.51633\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 1.05026\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.88126\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.58065\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.81323\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.58327\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.57686\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.74864\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.89419\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.88435\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.71319\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.89990\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.99442\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.81451\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 1.15063\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.66349\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 1.05249\n",
      "\tTrain loss: 0.02479, Accuracy: 4594/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1078/1692 (63.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 785/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.88302\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.84324\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.90340\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.73001\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.73213\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.54565\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.60576\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.71143\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.59233\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.78868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.90653\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.76379\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.64738\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.73960\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.83170\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.57645\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.69140\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 1.03388\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.71776\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.75442\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.69783\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.79182\n",
      "\tTrain loss: 0.02551, Accuracy: 4463/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1049/1692 (61.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 749/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.66136\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.80691\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.72029\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.76177\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.78710\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.72993\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.85372\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.60874\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.66473\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.84108\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 1.02266\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.81533\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.70434\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.64806\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.57271\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.59310\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.66187\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.76488\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 1.00838\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.92375\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.58341\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.60903\n",
      "\tTrain loss: 0.02090, Accuracy: 4823/6768 (71.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1112/1692 (65.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 802/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.77021\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 1.03061\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.63512\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.79200\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.81632\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.61362\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.86204\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.65128\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.49709\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.66993\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.65500\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.62407\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.61661\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.85687\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.62974\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.62983\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.77102\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.91985\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.69347\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.95782\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.70641\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.80717\n",
      "\tTrain loss: 0.02061, Accuracy: 4902/6768 (72.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.73610\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.86173\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.73239\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.73195\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.73172\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.73170\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.67157\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.57989\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.65515\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.91866\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.79787\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.58345\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.68367\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.61904\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.63767\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.72910\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.66518\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.89117\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.90307\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.98188\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.65266\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.73597\n",
      "\tTrain loss: 0.02089, Accuracy: 4873/6768 (72.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1135/1692 (67.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 800/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.75465\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.82871\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.73016\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.61047\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.85415\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.59381\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.70355\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.58616\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.73013\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.76838\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.67609\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.74505\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.59338\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.93258\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.62927\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.79981\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.83319\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.75785\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.76646\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.84121\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.67853\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.73557\n",
      "\tTrain loss: 0.01990, Accuracy: 5052/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 849/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.68400\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.83226\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.71965\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.54128\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.84935\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.72655\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.64483\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.65066\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.57424\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.80494\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.57685\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.62423\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.75102\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.73823\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.76918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.57809\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.89060\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.86637\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.75176\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 1.06335\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.88459\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.73359\n",
      "\tTrain loss: 0.01953, Accuracy: 5009/6768 (74.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1160/1692 (68.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 816/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.65951\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.74577\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.53548\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.65166\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.82124\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.52481\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.79095\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.73231\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.64162\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.70893\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.80416\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.86648\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.56600\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.69483\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.58669\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.86152\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.94323\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.69165\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.86801\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.84640\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.74573\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.77043\n",
      "\tTrain loss: 0.02250, Accuracy: 4714/6768 (69.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1118/1692 (66.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 769/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.79952\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.73288\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.67201\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.64885\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.90813\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.67016\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.79738\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.68584\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.49311\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.59997\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.67168\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.72134\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.65832\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 1.00219\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.52753\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.76105\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.87792\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.81641\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.62804\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.83906\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.72691\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.69890\n",
      "\tTrain loss: 0.01862, Accuracy: 5113/6768 (75.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1209/1692 (71.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 831/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.62329\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.73696\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.57737\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.72451\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.90698\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.67777\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.63292\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.60312\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.44351\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.63191\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.78312\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.69751\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.66517\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.57798\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.66253\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.76321\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 1.07688\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.98939\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.65298\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.61012\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.64864\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.72144\n",
      "\tTrain loss: 0.02094, Accuracy: 4912/6768 (72.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1158/1692 (68.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 839/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.73690\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.76776\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.77972\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.64474\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.75850\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.59105\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.84583\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.50749\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.58027\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.66411\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.54043\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.70324\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.53931\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.85737\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.58888\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.45411\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 1.18628\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.87812\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.86479\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.85440\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.62573\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.75008\n",
      "\tTrain loss: 0.01936, Accuracy: 4999/6768 (73.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1171/1692 (69.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.80494\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.76585\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.54588\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.58431\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.74526\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.52705\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.57385\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.72659\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.68660\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.70609\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.89240\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.66745\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.61637\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.78229\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.56327\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.50018\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.65489\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.88767\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.80642\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.69662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.56786\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.87127\n",
      "\tTrain loss: 0.02310, Accuracy: 4551/6768 (67.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1097/1692 (64.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.54681\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.82884\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.69217\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.63091\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 1.22149\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.41247\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 1.01151\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.68256\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.70298\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.67847\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.54463\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.65880\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.55690\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.69336\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.58514\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.66129\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.86507\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.88259\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.77553\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.81815\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.55683\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.61977\n",
      "\tTrain loss: 0.02044, Accuracy: 4896/6768 (72.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.82743\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.73827\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.69493\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.67663\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.55078\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.50261\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.83631\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.50748\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.45169\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.55773\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.53268\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.43836\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.59127\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.67827\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.87343\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.70197\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.70466\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.92498\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.70702\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.90560\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.50243\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.62916\n",
      "\tTrain loss: 0.02040, Accuracy: 4906/6768 (72.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1167/1692 (68.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 809/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.67657\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.91162\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.80775\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.61472\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.81857\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.49371\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.65490\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.80641\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.53817\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.77868\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.67426\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.73655\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.69329\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.77050\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.66676\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.79006\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.71907\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.78473\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.93848\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.99082\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.50971\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.67012\n",
      "\tTrain loss: 0.02191, Accuracy: 4709/6768 (69.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1111/1692 (65.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 790/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.52394\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.92937\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.59038\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.63901\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.91810\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.58390\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.82665\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.73457\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.44900\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.72704\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.76950\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.54099\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.65055\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.53625\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.51410\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.59439\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.86967\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.99957\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.61684\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.79235\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.48562\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.69745\n",
      "\tTrain loss: 0.01855, Accuracy: 5187/6768 (76.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1212/1692 (71.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 837/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.65584\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.81280\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.65286\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.57614\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.82415\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.58530\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.75753\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.52553\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.71103\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.79079\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.54113\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.47853\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.62203\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.76879\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.64011\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.62207\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.69350\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 1.04800\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.63701\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 1.00627\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.57152\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.73513\n",
      "\tTrain loss: 0.01999, Accuracy: 4940/6768 (72.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1146/1692 (67.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 824/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.76205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.68991\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.51828\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.62564\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.76990\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.39392\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.69644\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.53505\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.59059\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.62098\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.66676\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.66406\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.60745\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.77436\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.57591\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.73564\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.79439\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.67351\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.48275\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.67546\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.65157\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.62065\n",
      "\tTrain loss: 0.01979, Accuracy: 5075/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1177/1692 (69.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 841/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.81591\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.68659\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.65406\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.91199\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.65135\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.42098\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.75589\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.45260\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.60971\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.78179\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.41190\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.47295\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.53818\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.68605\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.59698\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.62876\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.67305\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 1.01686\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.72321\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.52227\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.45252\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.85775\n",
      "\tTrain loss: 0.01990, Accuracy: 4982/6768 (73.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1160/1692 (68.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 829/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.73853\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.83130\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.53246\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.78736\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.85422\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.51870\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.70093\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.51755\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.53632\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.53626\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.45769\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.57089\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.58196\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.71203\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.62334\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.57895\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.73018\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.86623\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.78299\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.75508\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.60952\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.71643\n",
      "\tTrain loss: 0.02141, Accuracy: 4897/6768 (72.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1161/1692 (68.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 822/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.65240\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.70238\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.63763\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.65390\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.57261\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.42826\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.54568\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.62201\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.54441\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.75032\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.61723\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.56911\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.65902\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.96448\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.67424\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.79601\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 1.06642\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 1.04116\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.79427\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 1.12781\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.76943\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.59670\n",
      "\tTrain loss: 0.02021, Accuracy: 5055/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 847/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.56792\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.78129\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.68791\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.71219\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.57942\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.48458\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.77527\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.37863\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.74957\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.84118\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.65112\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.65481\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.51653\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.84042\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.60011\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.48082\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.75877\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.90675\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.43539\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.74710\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.58963\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.80777\n",
      "\tTrain loss: 0.01696, Accuracy: 5361/6768 (79.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1266/1692 (74.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 839/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.67175\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.77460\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.93491\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.62432\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.86460\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.54347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.65686\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.63253\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.55107\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.97368\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.72132\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.58190\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.76764\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.73166\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.50430\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.73633\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.57121\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 1.16318\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.60680\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.62720\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.54845\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.59637\n",
      "\tTrain loss: 0.01963, Accuracy: 4804/6768 (70.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1159/1692 (68.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 794/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.89338\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.81451\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.67897\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.72238\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 1.01405\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.52390\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.59369\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.49493\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.80128\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.57025\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.52051\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.50971\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.48048\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.75416\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.49207\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.62163\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.63160\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 1.21112\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.59342\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.71887\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.60681\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.54472\n",
      "\tTrain loss: 0.01899, Accuracy: 5109/6768 (75.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1208/1692 (71.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 839/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.65408\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 1.04293\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.57375\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.69838\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.87774\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.53706\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.63418\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.49321\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.78869\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.73826\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.57345\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.45772\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.61023\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.47539\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.58311\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.68796\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.85963\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 1.00923\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.65812\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.71433\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.48158\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.90862\n",
      "\tTrain loss: 0.02002, Accuracy: 5053/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1192/1692 (70.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 840/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.48870\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.77723\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.51204\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.53854\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.98735\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.52661\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.56623\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.51666\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.52586\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.70455\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.86786\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.63030\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.70186\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.76973\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.48046\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.35962\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.68487\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.64561\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.70846\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.69131\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.60002\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.71689\n",
      "\tTrain loss: 0.02040, Accuracy: 5081/6768 (75.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1176/1692 (69.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.81071\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.73556\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.55770\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.53486\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.54816\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.45512\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.87250\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.54453\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.49949\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.75303\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.60185\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.64483\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.42809\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.71630\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.77407\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.65363\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 1.01802\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 1.02219\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.74796\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.88870\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.90999\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.57913\n",
      "\tTrain loss: 0.01756, Accuracy: 5197/6768 (76.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1240/1692 (73.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 863/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.93645\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.77651\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.69790\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.61343\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.78223\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.35775\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.85639\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.54452\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.58476\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.52103\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.75502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.47350\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.52813\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.63682\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.74832\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.68161\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 1.21687\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.94472\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.85515\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.82533\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.50896\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.60273\n",
      "\tTrain loss: 0.02024, Accuracy: 4710/6768 (69.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1137/1692 (67.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 804/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.47905\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.70539\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.59944\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.55297\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.76761\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.66466\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.71402\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.63256\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.59864\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.55320\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.69686\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.62535\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.57661\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.69070\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.60074\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.53591\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.85653\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.68284\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.62314\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.70081\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.72378\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.68744\n",
      "\tTrain loss: 0.01884, Accuracy: 5009/6768 (74.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1210/1692 (71.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.63522\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.77995\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.55428\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.60946\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.63835\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.45734\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.61961\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.56836\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.67494\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 1.01378\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.57672\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.66698\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.49566\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.83607\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.57954\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.72206\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 1.03032\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.85044\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.62034\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.69412\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.65529\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.65025\n",
      "\tTrain loss: 0.01645, Accuracy: 5258/6768 (77.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1236/1692 (73.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 867/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.76696\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.80628\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.59665\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.59791\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.84973\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.44518\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.64741\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.62479\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.72174\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.49838\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.56886\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.69524\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.46319\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.63464\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.55107\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.70238\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.72521\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.83551\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.69471\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.67493\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.50951\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.52603\n",
      "\tTrain loss: 0.01772, Accuracy: 5397/6768 (79.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1255/1692 (74.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 849/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.75753\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.85748\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.85233\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.48263\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.84777\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.37968\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.78278\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.65992\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.53480\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.84779\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.70960\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.55751\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.71471\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.59138\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.63578\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.72758\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.72457\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 1.02327\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.97393\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.84064\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.45070\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.46178\n",
      "\tTrain loss: 0.01872, Accuracy: 5150/6768 (76.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1204/1692 (71.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 853/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.60475\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 1.05837\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.53894\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.45116\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.94647\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.39713\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.69167\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.44672\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.55006\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.61050\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.47423\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.49941\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.41108\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.69492\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.46081\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.58568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.66825\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 1.36842\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.60701\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.85237\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.54655\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.66989\n",
      "\tTrain loss: 0.01792, Accuracy: 5266/6768 (77.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1237/1692 (73.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 878/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.61232\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.72989\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.48178\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.50165\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.78755\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.45492\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.66393\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.40897\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.49936\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.58039\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.53323\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.76598\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.59995\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.74040\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.52649\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.69325\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.89347\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.63829\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.39352\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.74851\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.54347\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.78733\n",
      "\tTrain loss: 0.01994, Accuracy: 4895/6768 (72.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1159/1692 (68.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.53281\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.80356\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.58035\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.46200\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.80556\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.44993\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.76374\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.46374\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.39348\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.68846\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.47552\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.62221\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.63737\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.72879\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.53781\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.66440\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.71772\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.76251\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.56660\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.75833\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.45818\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.65261\n",
      "\tTrain loss: 0.01767, Accuracy: 5040/6768 (74.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1201/1692 (70.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 843/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.76822\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.73667\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.75370\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.46760\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.88644\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.40603\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.49535\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.76801\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.51047\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.70133\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.52241\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.53056\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.62734\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.64898\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.65418\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.59610\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.88549\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.86661\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.60544\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.76703\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.84073\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.78111\n",
      "\tTrain loss: 0.01536, Accuracy: 5394/6768 (79.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1275/1692 (75.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 866/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.58881\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.83042\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.55799\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.52655\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.85187\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.51845\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 1.03682\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.56712\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.55112\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.49885\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.57644\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.67760\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.49883\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.91366\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.47705\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.61935\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 1.00015\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.81583\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.60182\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.72367\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.58820\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.75948\n",
      "\tTrain loss: 0.01755, Accuracy: 4983/6768 (73.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1188/1692 (70.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 836/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.74363\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 1.21023\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.65772\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.57026\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.75804\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.50787\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.72598\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.40136\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.53700\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.70479\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.55663\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.58154\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.53746\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.78744\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.47052\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.45726\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.77113\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.88583\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.67621\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.74089\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.54516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.61130\n",
      "\tTrain loss: 0.01669, Accuracy: 5288/6768 (78.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1256/1692 (74.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 870/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.71383\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.92588\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.82058\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.53921\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.61703\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.34351\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.58793\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.62530\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.54325\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.51365\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.59120\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.63404\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.37241\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.91207\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.51373\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.85394\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.88239\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 1.11683\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.67162\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.75551\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.42086\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.70334\n",
      "\tTrain loss: 0.01677, Accuracy: 5233/6768 (77.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1246/1692 (73.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.77969\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.65962\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.58871\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.59818\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.72756\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.51866\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.59650\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.55117\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.48288\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.69880\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.58993\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.55620\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.62309\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.74643\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.44505\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.65902\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.81141\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.84406\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.87658\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.68235\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.51443\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.59088\n",
      "\tTrain loss: 0.01816, Accuracy: 5030/6768 (74.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1216/1692 (71.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 832/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.53313\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.77123\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.53492\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.54738\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.54969\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.46351\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.66216\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.45254\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.58960\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.55620\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.44865\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.63875\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.48264\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.72505\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.63475\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.50721\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.49969\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.77159\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.42644\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.79611\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.46104\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.43634\n",
      "\tTrain loss: 0.01445, Accuracy: 5383/6768 (79.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1273/1692 (75.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 881/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.58803\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.61471\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.54476\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.71728\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.92608\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.34805\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.40899\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.51175\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.76283\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.62088\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.41049\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.39201\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.51337\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.75687\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.91316\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.50155\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.69978\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.78516\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.56688\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.55583\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.55890\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.84987\n",
      "\tTrain loss: 0.01981, Accuracy: 4835/6768 (71.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1168/1692 (69.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 830/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.43991\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.59630\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.69486\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.49942\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.63773\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.40320\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.66975\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.48912\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.39640\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.55334\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.65635\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.40040\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.43133\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.65928\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.56933\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.55623\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.61587\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.89329\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.58630\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.70685\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.47022\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.52474\n",
      "\tTrain loss: 0.01643, Accuracy: 5288/6768 (78.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1271/1692 (75.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 876/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.55669\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.82173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.72518\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.43038\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 1.11675\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.28628\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.67887\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.55181\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.44861\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.57427\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.44887\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.52548\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.43677\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.49897\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.58455\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.68718\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.51387\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.93148\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.78579\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.97066\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.60788\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.33781\n",
      "\tTrain loss: 0.01858, Accuracy: 5127/6768 (75.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1223/1692 (72.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 849/1772 (47.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.7535460992907801\n",
      "Best test accuracy:\n",
      "0.4971783295711061\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_lstm, optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9c135",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2879a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.gru = nn.GRU(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e562daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate LSTM model\n",
    "cnn_gru = CNN_GRU()\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(cnn_gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af8419f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.41579\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.58733\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.38961\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.54189\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.39062\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.35340\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.41957\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.36078\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.39113\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.43237\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.37879\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.38133\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.36395\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.31951\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.41831\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.37384\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.41622\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.34776\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.46206\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.39176\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.43637\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.40556\n",
      "\tTrain loss: 0.04198, Accuracy: 2127/6768 (31.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 530/1692 (31.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 556/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.43717\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.32513\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.34393\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.29302\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.40666\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.21358\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.33836\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.36959\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.33160\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.22814\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.21897\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.35132\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.27915\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.42043\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.34385\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.25294\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.34068\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.41489\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.29000\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.37558\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.28913\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.37783\n",
      "\tTrain loss: 0.03999, Accuracy: 2513/6768 (37.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 650/1692 (38.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 639/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.43864\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.28244\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.21909\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.27978\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.40539\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.22070\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.33040\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.43554\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.20316\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.10165\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.19603\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.32178\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.12325\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.43177\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.23526\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.17132\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.28669\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.30512\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.20171\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.38186\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.12476\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.31957\n",
      "\tTrain loss: 0.03847, Accuracy: 2891/6768 (42.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 687/1692 (40.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 743/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.27326\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.43164\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.30850\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.20468\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.40264\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.05336\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.22485\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.28285\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.23194\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.18307\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.15083\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.24497\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.12386\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.31586\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.16031\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.31020\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.35651\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.36114\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.24024\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.26089\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.14749\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.33990\n",
      "\tTrain loss: 0.03813, Accuracy: 2950/6768 (43.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 703/1692 (41.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.29665\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.31222\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.25636\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.12988\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.23830\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.07502\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.21580\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.20721\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.21918\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.21880\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.06551\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.23105\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.12765\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.33038\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.17387\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.20753\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.27171\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.22684\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.29389\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.19417\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.19487\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.27735\n",
      "\tTrain loss: 0.03742, Accuracy: 3040/6768 (44.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 706/1692 (41.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 727/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.13936\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.25969\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.27643\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.14651\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.34376\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 0.92157\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.07985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.18436\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.14928\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.05320\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.07682\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.25832\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.03469\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.35234\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.07639\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.15955\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.27010\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.33134\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.21183\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.20702\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.28491\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.32289\n",
      "\tTrain loss: 0.03763, Accuracy: 3005/6768 (44.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 721/1692 (42.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 708/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.19626\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.20654\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.08712\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.09824\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.13633\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.02203\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.17612\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.19746\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.12748\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.14369\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.03505\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.22019\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.01474\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.25406\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.07228\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.17640\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.22754\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.26147\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.06010\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.14212\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.41296\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.21768\n",
      "\tTrain loss: 0.03705, Accuracy: 3020/6768 (44.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 726/1692 (42.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 714/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.28524\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.24767\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.07858\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.09055\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.18629\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.94396\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.12271\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.13294\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.07339\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.05601\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.05701\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.21187\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 0.96775\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.24072\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.12896\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.24983\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.06093\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.38426\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.01441\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.11118\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.23910\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.31036\n",
      "\tTrain loss: 0.03767, Accuracy: 2927/6768 (43.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 698/1692 (41.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.12881\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.20745\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.10995\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.04605\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.08162\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.99619\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.20231\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.05780\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.16321\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.06142\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.02276\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.16660\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.05082\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.33145\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.13303\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.01665\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.23649\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.39317\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.12308\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.20670\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.09301\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.26547\n",
      "\tTrain loss: 0.03802, Accuracy: 2905/6768 (42.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 706/1692 (41.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 676/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.12304\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.33315\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.18271\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 0.94275\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.22221\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.01720\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.32138\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 0.94022\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.24636\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.01561\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 0.91930\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.18301\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.04822\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.34791\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.11383\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.00175\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.20289\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.18367\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.04719\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.08304\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.17393\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.33242\n",
      "\tTrain loss: 0.03520, Accuracy: 3265/6768 (48.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 775/1692 (45.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 741/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.12899\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.11454\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.08536\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.02377\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.13150\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.91565\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.26682\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 0.98900\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.15207\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.17260\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.95020\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.11912\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.02455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.22107\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 0.93878\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.21955\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.37546\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.34402\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 0.91321\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.07545\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.09386\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.20530\n",
      "\tTrain loss: 0.03347, Accuracy: 3533/6768 (52.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 829/1692 (48.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 821/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.17506\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.22816\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.14690\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 0.96395\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.16242\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.92376\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.26825\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.18117\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.17436\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.22341\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.01946\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.09068\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.87667\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.13329\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.08282\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.15033\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.20954\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.29644\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.98040\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.08428\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.09002\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.41793\n",
      "\tTrain loss: 0.03458, Accuracy: 3429/6768 (50.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 770/1692 (45.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 771/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.95942\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.17569\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.14880\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.03243\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.12938\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.79593\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.33240\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.00722\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.07418\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.04939\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.97636\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 0.96676\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.94983\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.18392\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.01489\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.10242\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.19390\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.27821\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 0.90992\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.08546\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.10609\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.30099\n",
      "\tTrain loss: 0.03471, Accuracy: 3295/6768 (48.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 783/1692 (46.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 719/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.03176\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.23577\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.11742\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 0.95365\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.07129\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.84819\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.23171\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.88827\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 0.93382\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.14810\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.97951\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.92913\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.01960\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.20170\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.08758\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.19140\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.12174\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.26187\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.95422\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.12084\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.09083\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.20701\n",
      "\tTrain loss: 0.03273, Accuracy: 3613/6768 (53.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 834/1692 (49.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 811/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.15738\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.11889\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.21118\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.01352\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.14257\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.86327\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.15944\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.97293\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.00111\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.98813\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.99395\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.00230\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.96404\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.04765\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.10293\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.97722\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.90706\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.44522\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.03731\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 1.22846\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.94104\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.08226\n",
      "\tTrain loss: 0.03328, Accuracy: 3471/6768 (51.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 818/1692 (48.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.97974\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.19808\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.15979\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.94440\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.00571\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.89603\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.17607\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.95840\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.13907\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.86959\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.88619\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.92229\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.92751\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.20970\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 1.00377\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.08876\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.13088\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 0.97502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.89293\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 1.17219\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.98387\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.16549\n",
      "\tTrain loss: 0.03343, Accuracy: 3419/6768 (50.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 804/1692 (47.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 785/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.89529\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.14608\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.00455\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.91845\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.23365\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.70286\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.15571\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.76041\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.85816\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.94686\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.81812\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.94755\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.93272\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.10923\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.98265\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.26205\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 1.18610\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.08524\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.89599\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.00932\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.97407\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.21958\n",
      "\tTrain loss: 0.03356, Accuracy: 3447/6768 (50.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 804/1692 (47.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 765/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.94850\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.93936\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.09158\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.93682\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.17166\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.81967\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.12555\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.96134\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.87415\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.98599\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 1.01861\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.88275\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.82304\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.33039\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 1.00201\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.94486\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.18624\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.27445\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.88351\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.11339\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.91809\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 1.10139\n",
      "\tTrain loss: 0.03245, Accuracy: 3571/6768 (52.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 863/1692 (51.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 774/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.94368\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.12608\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.14324\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.86356\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.20355\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.73951\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.17340\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 1.09660\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.13165\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.02366\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.97449\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.92596\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.95965\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.98579\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.93286\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 1.07370\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 1.06391\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 1.37810\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.88993\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 1.08680\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.97470\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 1.25868\n",
      "\tTrain loss: 0.03218, Accuracy: 3644/6768 (53.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 867/1692 (51.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 736/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.89302\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.16771\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.00024\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.93446\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 1.17770\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.77639\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.18624\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.75562\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 1.06003\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.87947\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.94440\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.84073\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.84966\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.14028\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.92296\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.94028\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.17203\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.05462\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.92690\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.14020\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.92476\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.08365\n",
      "\tTrain loss: 0.03010, Accuracy: 3851/6768 (56.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 897/1692 (53.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 768/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.96911\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.94868\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.98709\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.76150\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.94910\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.80948\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.04722\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.81976\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.89493\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 1.01731\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 1.05495\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 1.01853\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.88576\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 1.14010\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.96234\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 1.03754\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 1.14747\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.33630\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.81431\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 1.20566\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.92481\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 1.05705\n",
      "\tTrain loss: 0.02995, Accuracy: 3861/6768 (57.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00060, Accuracy: 900/1692 (53.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 795/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.87375\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.99288\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.95629\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.79051\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 1.08275\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.67573\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.27650\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.89560\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.81777\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.86889\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 1.11415\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.88778\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.75126\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.92989\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.97499\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 1.02735\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 1.10055\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.13663\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.91223\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.97403\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.92145\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 1.09962\n",
      "\tTrain loss: 0.03027, Accuracy: 3852/6768 (56.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 891/1692 (52.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 791/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.93824\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 1.00786\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.94071\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.93296\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.94054\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.64806\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.10059\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.82422\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.99990\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.79291\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.80460\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.77543\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.65696\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.82276\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.92814\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.99906\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.38308\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.85669\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 1.12917\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.95125\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.93603\n",
      "\tTrain loss: 0.02930, Accuracy: 3922/6768 (57.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 932/1692 (55.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 797/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.85460\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 1.06380\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 1.02574\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.94305\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 1.06280\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.65096\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 1.16183\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.75977\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 1.00812\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.91068\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.86722\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.75217\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 1.00494\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.96932\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.96001\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 1.02262\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.85724\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 1.43055\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.91875\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 1.10148\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.76386\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.95635\n",
      "\tTrain loss: 0.02881, Accuracy: 3964/6768 (58.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 931/1692 (55.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 799/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.90272\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.04503\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 1.09382\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.84247\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 1.03661\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.76103\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.97743\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.83169\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.70207\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.95162\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 1.09962\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.88421\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.74619\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.94709\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.93212\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.93504\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.95303\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.09725\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.90961\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 1.12208\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.79364\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 1.02208\n",
      "\tTrain loss: 0.02976, Accuracy: 3940/6768 (58.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 942/1692 (55.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 773/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.96024\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.96648\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 1.04737\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.89000\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 1.22815\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.78095\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 1.20347\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.75554\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.91327\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.92460\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.86357\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.81789\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.86839\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.97387\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.88954\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 1.02335\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 1.12702\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.28071\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.94345\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.87845\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.75957\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.99315\n",
      "\tTrain loss: 0.02795, Accuracy: 4043/6768 (59.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 958/1692 (56.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 800/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 1.09255\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.04241\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.92414\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.93459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.05740\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.58405\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.95573\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.88484\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.83759\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.95351\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.86432\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.83991\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.83263\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 1.03856\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 1.07745\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.92894\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.99061\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 1.07842\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.86717\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.93282\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 1.10429\n",
      "\tTrain loss: 0.02790, Accuracy: 4036/6768 (59.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 945/1692 (55.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.82507\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 1.18128\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 1.03348\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.71330\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 1.08491\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.62186\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.94437\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.73957\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.88354\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.94580\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.93652\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.95951\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.75203\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.94212\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.96330\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.90251\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.90737\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.12610\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.77460\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 1.02816\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.85054\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.98472\n",
      "\tTrain loss: 0.02698, Accuracy: 4167/6768 (61.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 998/1692 (58.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 800/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.96691\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.97174\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.83624\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.80412\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.97762\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.65420\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 1.22262\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.84333\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.82280\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.90471\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.77516\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.87325\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.76712\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.91186\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 1.09186\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.92028\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.92369\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 1.11670\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.80115\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.92568\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.86062\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 1.14158\n",
      "\tTrain loss: 0.02827, Accuracy: 4094/6768 (60.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 957/1692 (56.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.76398\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 1.04299\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.84863\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.88543\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.81204\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.58958\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.87809\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.71386\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.88665\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.99115\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.64371\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.88984\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.73749\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.96713\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.86970\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 1.07107\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 1.07678\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 1.04697\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.92255\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.97988\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.76706\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 1.15425\n",
      "\tTrain loss: 0.02818, Accuracy: 4075/6768 (60.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 955/1692 (56.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.70611\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 1.18849\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 1.04190\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.85873\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.89270\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.60627\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 1.03117\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.84174\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.82409\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.99594\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.69655\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.81019\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.75383\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.88106\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 1.09232\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 1.02618\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.82726\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 1.00227\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.83361\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.86174\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.69856\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.99756\n",
      "\tTrain loss: 0.02658, Accuracy: 4220/6768 (62.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 978/1692 (57.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 823/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.94121\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.95019\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.84912\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.91036\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.95805\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.72280\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 1.04254\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.69357\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.85505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.76310\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.74331\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.76064\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.79030\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.86735\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.74988\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.96841\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.97027\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 1.07649\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 1.03262\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.82507\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.67757\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.84922\n",
      "\tTrain loss: 0.02498, Accuracy: 4450/6768 (65.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1025/1692 (60.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 848/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.83627\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.96699\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.73038\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.69964\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.94804\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.64406\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.88331\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.76724\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.84316\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.98375\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.91487\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.93462\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.65998\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.98459\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 1.07074\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.83104\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.94662\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 1.08764\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.74850\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.94458\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.67844\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.94514\n",
      "\tTrain loss: 0.02489, Accuracy: 4423/6768 (65.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1027/1692 (60.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.85121\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.77013\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.94023\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.95841\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.57737\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.89254\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.72515\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.83501\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.73627\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.76765\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.92794\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.69629\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.89157\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.85220\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.79112\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.81615\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.92549\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.75957\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.90658\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.95745\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 1.01609\n",
      "\tTrain loss: 0.02523, Accuracy: 4318/6768 (63.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 999/1692 (59.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 784/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 1.03055\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.94019\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.77409\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.90746\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.87247\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.65839\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.87819\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.59734\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.87522\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.96584\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.67270\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.69212\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.76726\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.81106\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.82137\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.86893\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.89065\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 1.18650\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.94322\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.86456\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.55390\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.82660\n",
      "\tTrain loss: 0.02431, Accuracy: 4519/6768 (66.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1064/1692 (62.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 838/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.74996\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 1.03400\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.87298\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.79081\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.89209\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.64858\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.99570\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.58964\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.66815\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.85195\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.84697\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.69974\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.73408\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 1.12631\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 1.02152\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.95665\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.95039\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.92778\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.78469\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.81538\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.76176\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 1.03531\n",
      "\tTrain loss: 0.02443, Accuracy: 4508/6768 (66.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1039/1692 (61.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 818/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.87707\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.81620\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.84491\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.84844\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.97829\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.69103\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.95184\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.53426\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.92222\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.96412\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.65509\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.73829\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.68428\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.96048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.87153\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.70406\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 1.00823\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 1.01446\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.79077\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.92913\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.65808\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.71729\n",
      "\tTrain loss: 0.02412, Accuracy: 4539/6768 (67.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1065/1692 (62.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 831/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.90432\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.91909\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.75058\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.86776\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 1.10963\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.67607\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.72563\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.69910\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.64819\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.84189\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.83561\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.83809\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.54810\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.83247\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.85716\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 1.07027\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.98033\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 1.14074\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.73673\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 1.05593\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.82765\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.94626\n",
      "\tTrain loss: 0.02419, Accuracy: 4551/6768 (67.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1049/1692 (61.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 836/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.78845\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 1.10419\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.71762\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.81347\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.79238\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.62237\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.83873\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.71605\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.81528\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.92811\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.76101\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.90214\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.54299\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.89693\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.80572\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.98412\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.94380\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.91641\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.91028\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.82132\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.71377\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 1.01437\n",
      "\tTrain loss: 0.02477, Accuracy: 4478/6768 (66.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1039/1692 (61.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 806/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.82169\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.87987\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.68995\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.83118\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.95768\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.68902\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.83527\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.66002\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.72075\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.75757\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.77000\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.63189\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.69718\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.65887\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.58587\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.80598\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.92728\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.88292\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.82831\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 1.04631\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.81883\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.81691\n",
      "\tTrain loss: 0.02397, Accuracy: 4575/6768 (67.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1064/1692 (62.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.85883\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.96292\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.59043\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.85553\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.88772\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.62542\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.78614\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.64370\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.86082\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.84574\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.71245\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.78802\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.69801\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.71300\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.78145\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 1.05166\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.69436\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 1.04501\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.88473\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.73810\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.70283\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.69324\n",
      "\tTrain loss: 0.02448, Accuracy: 4472/6768 (66.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1067/1692 (63.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 813/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.80664\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.93704\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.96410\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.71062\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.89713\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.77050\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.87654\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.66257\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.75896\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.84577\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.55246\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.64187\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.67628\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 1.02306\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.96173\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.87664\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.78241\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.98711\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.75764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.93376\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.71059\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.99583\n",
      "\tTrain loss: 0.02426, Accuracy: 4610/6768 (68.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1078/1692 (63.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 805/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.81613\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 1.00431\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.90005\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.71970\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.87453\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.55567\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.84085\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.57622\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.65172\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.79866\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.69586\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.61426\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.70220\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 1.02019\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.86109\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.52238\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.81969\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 1.05216\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.80579\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.73588\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.68636\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.80860\n",
      "\tTrain loss: 0.02235, Accuracy: 4794/6768 (70.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1113/1692 (65.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.83715\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.76152\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.71962\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.71521\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.78699\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.56176\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.75249\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.60327\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.71014\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 1.03350\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.70369\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.59895\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.67399\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.76400\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.87782\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.65959\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.71410\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 1.06755\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.67937\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.77161\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.63975\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 1.05700\n",
      "\tTrain loss: 0.02241, Accuracy: 4726/6768 (69.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1103/1692 (65.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 857/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.76898\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.79578\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.78303\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 1.10586\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.63678\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.71339\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.91286\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.74268\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.78161\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.84835\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.68223\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.90364\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.57600\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.73003\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.89652\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.70526\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.77658\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 1.06141\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.92732\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.92234\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.81262\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 1.05723\n",
      "\tTrain loss: 0.02314, Accuracy: 4791/6768 (70.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1124/1692 (66.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 800/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.81133\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 1.04742\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.91265\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.84803\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.88436\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.59120\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.69727\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.50513\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.71262\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.87373\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.69834\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.79221\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.70320\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.73655\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.69341\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.92435\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.87132\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.73940\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.87054\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.80948\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.58469\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.71734\n",
      "\tTrain loss: 0.02338, Accuracy: 4565/6768 (67.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1090/1692 (64.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 812/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.80548\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.82081\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.52950\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.88949\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.86295\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.54873\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.74625\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.53817\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.62585\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.70319\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 1.02074\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.87717\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.70328\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 1.11323\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.72077\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.60912\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.69458\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.95631\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.69552\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.99512\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.76592\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.93504\n",
      "\tTrain loss: 0.02171, Accuracy: 4802/6768 (70.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1118/1692 (66.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 846/1772 (47.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.81976\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.85635\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.74914\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.79193\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.85264\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.46267\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.78293\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.53399\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.68901\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.75093\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.76841\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.77053\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.66554\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.77395\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.64335\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.98143\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.97801\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.81684\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.73915\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.95668\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.59422\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.74162\n",
      "\tTrain loss: 0.02137, Accuracy: 4927/6768 (72.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1145/1692 (67.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 866/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.83349\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.76892\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.91792\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.70952\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.75053\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.45681\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.56193\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.61665\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 1.01094\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.70310\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.70749\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.64697\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.63677\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 1.05182\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.84866\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.64090\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.77837\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 1.06421\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.79447\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.72974\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.94871\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.78066\n",
      "\tTrain loss: 0.02219, Accuracy: 4928/6768 (72.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1149/1692 (67.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 848/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.79505\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.78024\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.76675\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.70433\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.89998\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.47041\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.75932\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.58231\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.88968\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.70332\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.80583\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.59378\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.41084\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.89017\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.73151\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.74435\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.69964\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 1.04891\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.77301\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.93190\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.61478\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.66687\n",
      "\tTrain loss: 0.02102, Accuracy: 4859/6768 (71.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 862/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.83830\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.82633\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.93774\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.65989\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 1.01107\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.42843\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.78163\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.52545\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.73077\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.81381\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.79249\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.60133\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.52195\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.90016\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.65802\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.60604\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.68733\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 1.16228\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.69946\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.99332\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.78477\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.77021\n",
      "\tTrain loss: 0.02273, Accuracy: 4856/6768 (71.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1165/1692 (68.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.65540\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.85962\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.46862\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.88799\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.96188\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.63597\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.58350\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.69037\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.63936\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.82031\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.43158\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.46610\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.50727\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.84187\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.85422\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 1.00713\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.68408\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.78672\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.76902\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.89195\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.70787\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.79754\n",
      "\tTrain loss: 0.02115, Accuracy: 4961/6768 (73.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.68260\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.99912\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.79629\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.73449\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.99327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.61723\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.62171\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.70633\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.76422\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.88093\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.63082\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.57361\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.62941\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.80131\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.77310\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.83954\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.75088\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.76937\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.77405\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.84489\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.70871\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.80900\n",
      "\tTrain loss: 0.02102, Accuracy: 4986/6768 (73.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1175/1692 (69.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.67771\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.89553\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.76803\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.79099\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.93542\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.49194\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.80264\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.43247\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.72012\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.94554\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.54836\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.54026\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.51511\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.81310\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.79824\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.74566\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.69918\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.89631\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.90515\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.85137\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.68241\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.59477\n",
      "\tTrain loss: 0.02078, Accuracy: 5015/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1166/1692 (68.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 844/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.71700\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 1.13460\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.68900\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.65411\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.77935\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.49873\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.81084\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.66926\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.55403\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.73449\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.86337\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.61522\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.53452\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.76017\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.81067\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.74121\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.74026\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 1.04589\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.99984\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.72107\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.79697\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.87829\n",
      "\tTrain loss: 0.01953, Accuracy: 5216/6768 (77.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1221/1692 (72.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 850/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.69139\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.80986\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.89270\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.68097\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.59644\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.66492\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.75033\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.65328\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.65305\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.84174\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.61142\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.60481\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.65356\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.73341\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.75310\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.51522\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.75331\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.91428\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.56047\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.80764\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.59400\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.89828\n",
      "\tTrain loss: 0.02029, Accuracy: 5118/6768 (75.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1191/1692 (70.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 861/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.82459\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 1.02959\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.74981\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.81642\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.76029\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.47840\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.69444\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.44531\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.51543\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.85490\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.66366\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.64913\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.51110\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.64834\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.72658\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.76027\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.69668\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.77794\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 1.02687\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.81366\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.65120\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.70522\n",
      "\tTrain loss: 0.02002, Accuracy: 5101/6768 (75.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1185/1692 (70.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 832/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.67966\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.84461\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.69326\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.64941\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.79568\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.45336\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.82194\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.44870\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.62509\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.96813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.48265\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.52789\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.53463\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.86713\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.75696\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.71074\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.74336\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.88063\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.93534\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.80162\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.62043\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.68130\n",
      "\tTrain loss: 0.02106, Accuracy: 5005/6768 (73.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1153/1692 (68.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 813/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.87371\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.87052\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.89107\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.81979\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.81634\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.64070\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.70408\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.56757\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.69188\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.78321\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.66987\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.51687\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.61949\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.94592\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.78323\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.90395\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.89934\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 1.19562\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.67659\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.92417\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.63757\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.61113\n",
      "\tTrain loss: 0.01978, Accuracy: 5097/6768 (75.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1204/1692 (71.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 848/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.64555\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.70223\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.78290\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.89829\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.96515\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.41033\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.79181\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.86921\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.69305\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.53600\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.70361\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.46453\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.47513\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.74822\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.81455\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.65838\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.69060\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.77653\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.61987\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.92478\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.64047\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.75215\n",
      "\tTrain loss: 0.01989, Accuracy: 5165/6768 (76.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1217/1692 (71.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 862/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.85557\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.90357\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.64636\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.76438\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.85342\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.51068\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.68469\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.51679\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.77473\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.79537\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.75797\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.59842\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.48824\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.75257\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.62856\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.62731\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.86042\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.81021\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.87044\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.92411\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.68127\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.79636\n",
      "\tTrain loss: 0.01913, Accuracy: 5332/6768 (78.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1237/1692 (73.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 874/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.69551\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.91627\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.75404\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.89206\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.74026\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.61829\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 1.22871\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.65101\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.66467\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.89210\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.67230\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.75117\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.68456\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.69826\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.76315\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.51756\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.61090\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.97208\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.96821\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.74307\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.74273\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.62823\n",
      "\tTrain loss: 0.01681, Accuracy: 5474/6768 (80.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1276/1692 (75.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 899/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.51191\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.89491\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.59568\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.69314\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.65160\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.78882\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.70779\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.53452\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.59115\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.80142\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.79883\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.64556\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.61680\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.66922\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.77051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.64121\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.70809\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.73891\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.79427\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.91427\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.50897\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.67596\n",
      "\tTrain loss: 0.01886, Accuracy: 5238/6768 (77.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1244/1692 (73.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 893/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.71042\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 1.02855\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.78826\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.62128\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.67014\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.43886\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.64275\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.52566\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.85663\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.72027\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.48430\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.61678\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.55878\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.93668\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.80843\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.59310\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.77485\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.72276\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.93090\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.61355\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.61533\n",
      "\tTrain loss: 0.01733, Accuracy: 5377/6768 (79.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1272/1692 (75.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 851/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.57332\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.82659\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.75961\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.76335\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.66660\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.43921\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.52866\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.52752\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.42697\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.73503\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.55491\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.52471\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.65262\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.58997\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.70004\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.84655\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.82518\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.68630\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.75595\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.88703\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.41667\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.76187\n",
      "\tTrain loss: 0.01649, Accuracy: 5499/6768 (81.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1287/1692 (76.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 884/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.75677\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.72319\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.60721\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.84756\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.71501\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.67385\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.70175\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.46256\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.40477\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.74416\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.49466\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.57840\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.51635\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.81570\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.72478\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.73699\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.68395\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.93286\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.64475\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.97993\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.45295\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.67854\n",
      "\tTrain loss: 0.01728, Accuracy: 5372/6768 (79.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1277/1692 (75.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 895/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.69545\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.71025\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.61040\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.90806\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.50473\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.44529\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.83426\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.60165\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.68410\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.60809\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.60610\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.70886\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.38359\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.62322\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.79130\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.62672\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.85215\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.39676\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 1.06388\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.43636\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.76750\n",
      "\tTrain loss: 0.01733, Accuracy: 5438/6768 (80.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1270/1692 (75.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 883/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.77845\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.94751\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.62233\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.82569\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.77549\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.45488\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.76589\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.44401\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.72887\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.56150\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.53586\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.64740\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.45269\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.88828\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.62958\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.85225\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.73002\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.84343\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.54172\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.95335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.49997\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.84673\n",
      "\tTrain loss: 0.01623, Accuracy: 5511/6768 (81.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1302/1692 (76.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 858/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.70824\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.84323\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.55603\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.69518\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.63135\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.46113\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.57917\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.67849\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.67863\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.73431\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.47376\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.73318\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.39886\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.85783\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.85013\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.75990\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.77213\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.89374\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.64330\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.89950\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.75976\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.65742\n",
      "\tTrain loss: 0.01760, Accuracy: 5449/6768 (80.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1268/1692 (74.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 903/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.55525\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.84506\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.89123\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.67164\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.57755\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.49669\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.87152\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.59390\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.51860\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.74858\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.46552\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.60491\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.43573\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.67961\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.60588\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.57114\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.75404\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.96531\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.67000\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 1.11128\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.59051\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.67869\n",
      "\tTrain loss: 0.01668, Accuracy: 5471/6768 (80.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1265/1692 (74.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 932/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.71735\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 1.01444\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.88940\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.55054\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.73133\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.33466\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.68940\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.51467\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.70043\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.65843\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.51468\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.77747\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.53181\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.81322\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.64462\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.66283\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.48584\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.94857\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.39750\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.91550\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.45091\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.69320\n",
      "\tTrain loss: 0.01662, Accuracy: 5597/6768 (82.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1314/1692 (77.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 888/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.50614\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.95653\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.62645\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.62288\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.75793\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.33709\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.64209\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.43137\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.81017\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.47840\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.63921\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.46111\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.41638\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.60613\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.65838\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.65624\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 1.00194\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.87539\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.42825\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.72025\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.50958\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.93721\n",
      "\tTrain loss: 0.01778, Accuracy: 5427/6768 (80.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1258/1692 (74.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.58025\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.86988\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.74765\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.67223\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.86518\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.42027\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.56538\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.57665\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.46345\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.73915\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.40724\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.46973\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.52133\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.49456\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.81133\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.64479\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.84312\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.85632\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.52038\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.45168\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.74441\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.59885\n",
      "\tTrain loss: 0.01708, Accuracy: 5535/6768 (81.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1292/1692 (76.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 924/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.67853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.77976\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.68071\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.63451\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.89736\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.38859\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.67514\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.60185\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.58487\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.59986\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.73652\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.40071\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.65367\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.49831\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.51868\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.60785\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.76226\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.97607\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.58682\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.76811\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.34206\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.76328\n",
      "\tTrain loss: 0.01716, Accuracy: 5525/6768 (81.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1281/1692 (75.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 896/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.56879\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.76324\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.44020\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.61607\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.66866\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.25591\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.48541\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.39985\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.81947\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.72697\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.52897\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.48012\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.41053\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.64979\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.70392\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.59601\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.71361\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.55797\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.45349\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.69738\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.32680\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.57254\n",
      "\tTrain loss: 0.01452, Accuracy: 5775/6768 (85.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1346/1692 (79.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 966/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.51427\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.91485\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.57528\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.70597\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.90105\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.41325\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.46253\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.47718\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.40488\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.59816\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.54548\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.58057\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.41407\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.52519\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.80890\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.58037\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.51047\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 1.08604\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.66287\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.60208\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.29150\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.61531\n",
      "\tTrain loss: 0.01445, Accuracy: 5767/6768 (85.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1345/1692 (79.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 978/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.58556\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.75795\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.81064\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.65922\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.53442\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.34507\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.58324\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.47196\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.82877\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.60127\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.60901\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.63375\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.43109\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.45707\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.63553\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.60942\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.71689\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.63623\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.52222\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.71500\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.72967\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.64998\n",
      "\tTrain loss: 0.01661, Accuracy: 5489/6768 (81.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1284/1692 (75.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 929/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.56904\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.89884\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.82355\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.84870\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.58486\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.28609\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.76977\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.42696\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.49188\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.73143\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.59581\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.65450\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.50138\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.61236\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.59705\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.63270\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.75051\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.64281\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.59816\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.63516\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.42997\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.56803\n",
      "\tTrain loss: 0.01475, Accuracy: 5652/6768 (83.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1316/1692 (77.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 1.00199\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 1.07409\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.62613\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.50349\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.81313\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.32464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.47562\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.53111\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.45645\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.68345\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.65877\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.50851\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.45007\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.59004\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.60525\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.50347\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.73807\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.75011\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.40078\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.50691\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.50278\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.72050\n",
      "\tTrain loss: 0.01467, Accuracy: 5719/6768 (84.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1328/1692 (78.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.74907\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.79298\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.80179\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.60455\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.82675\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.34741\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.39441\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.46533\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.86570\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.81946\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.49225\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.32468\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.56491\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.71905\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.45619\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.61229\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.54512\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.83653\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.58990\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.67497\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.59815\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.45843\n",
      "\tTrain loss: 0.01456, Accuracy: 5736/6768 (84.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1321/1692 (78.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.68881\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.84939\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.79206\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.79645\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.56131\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.56960\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.49855\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.53419\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.43006\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.74930\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.60516\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.60170\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.52620\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.45685\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.68384\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.60593\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.64580\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.94882\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.49781\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.71265\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.47390\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.59788\n",
      "\tTrain loss: 0.01422, Accuracy: 5715/6768 (84.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1325/1692 (78.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 937/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.49758\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.77425\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.71153\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.61240\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.83650\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.34395\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.62453\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.37008\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.71629\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.38901\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.38177\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.59357\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.33552\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.73618\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.65703\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.60413\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.45653\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 1.07878\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.49732\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.81561\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.35362\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.63248\n",
      "\tTrain loss: 0.01507, Accuracy: 5736/6768 (84.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 927/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.51345\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.91559\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.60297\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.52459\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.72858\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.25796\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.39534\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.46958\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.63436\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.62536\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.43705\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.65752\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.30635\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.59410\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.48786\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.79792\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.66969\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.70629\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.50943\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.84808\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.38256\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.49702\n",
      "\tTrain loss: 0.01499, Accuracy: 5785/6768 (85.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1351/1692 (79.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 962/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.63298\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.76749\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.54984\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.79570\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.67637\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.25386\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.70155\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.48619\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.54737\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.49872\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.56682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.32582\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.45078\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.48301\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.65514\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.54674\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.58829\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.74238\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.61527\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.67553\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.45520\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.70636\n",
      "\tTrain loss: 0.01410, Accuracy: 5685/6768 (83.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1331/1692 (78.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 890/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.71603\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.84997\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.45493\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.75527\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.75048\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.42040\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.53558\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.60919\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.36096\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.63049\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.50967\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.46661\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.48764\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.56956\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.64751\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.53069\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.56051\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.99035\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.57589\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.75842\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.40410\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.55078\n",
      "\tTrain loss: 0.01298, Accuracy: 5773/6768 (85.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1342/1692 (79.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 918/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.69649\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.79199\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.82052\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.71824\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.62248\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.29014\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.65116\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.20612\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.57047\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.47258\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.32262\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.71750\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.34871\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.47517\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.55690\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.45842\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.63814\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.63813\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.39512\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.49292\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.55558\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.52942\n",
      "\tTrain loss: 0.01185, Accuracy: 5913/6768 (87.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1390/1692 (82.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 969/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.69633\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 1.02353\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.62001\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.64644\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.58314\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.57269\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.56363\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.31549\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.59549\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.50208\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.25693\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.38209\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.44923\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.39663\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.60280\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.57475\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.54795\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 1.00697\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.46318\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.67950\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.27560\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.45279\n",
      "\tTrain loss: 0.01242, Accuracy: 5852/6768 (86.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1365/1692 (80.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 903/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.56663\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.91675\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.57738\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.68041\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.67719\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.28123\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.54610\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.45524\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.37839\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.55485\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.67382\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.52358\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.39113\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.92124\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.62972\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.44945\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.62428\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.65205\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.50570\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.52194\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.66286\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.61992\n",
      "\tTrain loss: 0.01298, Accuracy: 5842/6768 (86.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1378/1692 (81.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1008/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.66571\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.84104\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.48522\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.67458\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.84838\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.38804\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.75707\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.46008\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.38752\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.56784\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.35537\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.26236\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.41891\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.77832\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.60600\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.49268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.66050\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.68106\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.61595\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.79980\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.48536\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.38198\n",
      "\tTrain loss: 0.01241, Accuracy: 5797/6768 (85.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1350/1692 (79.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 951/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.93661\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.77639\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.50607\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.61590\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.61648\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.43745\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.68617\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.69591\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.47013\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.65142\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.44882\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.44839\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.36342\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.33118\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.74689\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.59958\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.62341\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.88240\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.73658\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.89365\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.34372\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.51126\n",
      "\tTrain loss: 0.01346, Accuracy: 5739/6768 (84.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1339/1692 (79.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 973/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.50365\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.81318\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.87865\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.59925\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.55294\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.19982\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.69544\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.27367\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.47440\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.56722\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.40687\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.51996\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.34994\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.46713\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.45795\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.42476\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.63491\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.67935\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.49759\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.80314\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.31902\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.62282\n",
      "\tTrain loss: 0.01311, Accuracy: 5776/6768 (85.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1343/1692 (79.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 890/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.51726\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.76974\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.49247\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.50957\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.51080\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.25219\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.50769\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.41371\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.56154\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.38679\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.46841\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.45177\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.24889\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.75720\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.69655\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.35641\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.45649\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.54912\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.73432\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.35816\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.55727\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.33709\n",
      "\tTrain loss: 0.01268, Accuracy: 5857/6768 (86.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1386/1692 (81.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 957/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.77982\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.81610\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.42599\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.43623\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.82233\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.23963\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.54803\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.28767\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.51929\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.53787\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.54659\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.45542\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.29519\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.41867\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.53866\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.50432\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.68319\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.53510\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.31629\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.60075\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.37499\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.38835\n",
      "\tTrain loss: 0.01249, Accuracy: 5769/6768 (85.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1364/1692 (80.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 916/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.66843\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.57396\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.40494\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.52481\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.60151\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.45074\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.35857\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.36956\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.39293\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.39770\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.35152\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.25930\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.32115\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.39349\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.32054\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.53511\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.51189\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.90604\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.47438\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.58612\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.37883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.49332\n",
      "\tTrain loss: 0.01237, Accuracy: 5900/6768 (87.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1013/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.53156\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.59032\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.73755\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.56260\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.68426\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.26609\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.25162\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.47875\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.62556\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.61592\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.81667\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.41756\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.31275\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.29156\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.47684\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.41535\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.78399\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.71915\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.49516\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.38645\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.32582\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.72396\n",
      "\tTrain loss: 0.01073, Accuracy: 5895/6768 (87.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 975/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.63706\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.74959\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.49858\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.72510\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.46320\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.30794\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.38434\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.42629\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.58893\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.47256\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.41719\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.47013\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.38466\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.44107\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.63845\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.67085\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.77318\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.78048\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.49916\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.58254\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.53373\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.44002\n",
      "\tTrain loss: 0.01055, Accuracy: 5996/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1413/1692 (83.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1012/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.61040\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.87493\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.47673\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.49272\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.62343\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.34482\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.54195\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.44166\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.75830\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.43434\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.31349\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.24063\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.44758\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.43104\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.45566\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.66795\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.45952\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.49410\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.77133\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.54242\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.50561\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.45835\n",
      "\tTrain loss: 0.01046, Accuracy: 5966/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1406/1692 (83.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 987/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.39271\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.58184\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.62212\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.48425\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.50457\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.34282\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.77585\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.27431\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.39907\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.37545\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.37398\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.67244\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.39708\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.73967\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.40591\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.63759\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.61615\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.60774\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.56219\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.60526\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.38371\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.56888\n",
      "\tTrain loss: 0.01169, Accuracy: 5924/6768 (87.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1414/1692 (83.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.66067\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.75909\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.54021\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.34584\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.60559\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.23597\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.45681\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.59179\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.46161\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.48743\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.50481\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.51741\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.38267\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.35151\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.70572\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.34575\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.75875\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.88136\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.44734\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.50759\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.31941\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.55849\n",
      "\tTrain loss: 0.01075, Accuracy: 5959/6768 (88.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1410/1692 (83.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 993/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.65435\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.62708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.46378\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.44905\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.56168\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.34737\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.40309\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.28881\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.53347\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.40638\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.32503\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.47302\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.33588\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.49761\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.47269\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.90441\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.65027\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.78998\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.53923\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.46237\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.34260\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.46210\n",
      "\tTrain loss: 0.01120, Accuracy: 5950/6768 (87.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 959/1772 (54.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.8356973995271868\n",
      "Best test accuracy:\n",
      "0.5716704288939052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_gru, optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc453a",
   "metadata": {},
   "source": [
    "# GAN Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3d10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_WGAN = np.load('./GAN/WGAN_generate_X_subject1.npy')\n",
    "fake_label = np.load('./GAN/generate_label_subject1.npy')\n",
    "\n",
    "\n",
    "fake_WGAN = np.swapaxes(fake_WGAN, 1,2)\n",
    "fake_WGAN = np.swapaxes(fake_WGAN,2,3)\n",
    "\n",
    "\n",
    "#Add 8460/4 fake data\n",
    "x_train_plus = np.vstack((x_train, fake_WGAN[0:fake_WGAN.shape[0]//4]))\n",
    "y_train_plus = np.vstack((y_train, fake_label[0:fake_WGAN.shape[0]//4]))\n",
    "p = np.random.permutation(x_train.shape[0])\n",
    "x_train_plus, y_train_plus = x_train_plus[p], y_train_plus[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e77b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_gan = dataloader_setup(x_train_plus, y_train_plus, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658728a2",
   "metadata": {},
   "source": [
    "## CNN+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9abc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn, optimizer, data_loaders_gan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6695d6",
   "metadata": {},
   "source": [
    "## LSTM+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(lstm, optimizer, data_loaders_gan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee026a3",
   "metadata": {},
   "source": [
    "## GRU+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14083be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(gru, optimizer, data_loaders_gan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7dc10",
   "metadata": {},
   "source": [
    "## CNN+LSTM+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a3710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_lstm, optimizer, data_loaders_gan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d4020",
   "metadata": {},
   "source": [
    "## CNN+GRU+GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(cnn_gru, optimizer, data_loaders_gan, num_epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
