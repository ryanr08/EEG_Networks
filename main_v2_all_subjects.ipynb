{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92746a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from train_evaluate import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7aae3",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85a7dd",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0824322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/michael/Desktop/Home/研究所/ECE247/projects/data/'\n",
    "X_train_valid, y_train_valid, X_test, y_test = load_data(data_dir, subjects=[1,2,3,4,5,6,7,8,9]) # default subjects=[1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb1db",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68646e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (6768, 250, 1, 22)\n",
      "Shape of x_valid: (1692, 250, 1, 22)\n",
      "Shape of x_test: (1772, 250, 1, 22)\n",
      "Shape of y_train: torch.Size([6768, 4])\n",
      "Shape of y_valid: torch.Size([1692, 4])\n",
      "Shape of y_test: torch.Size([1772, 4])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = main_prep(X_train_valid,y_train_valid,X_test, y_test,2,2,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6fc9c",
   "metadata": {},
   "source": [
    "## PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders  = dataloader_setup(x_train, y_train, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af55041",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe828a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR      = 0.0005\n",
    "BETAS   = (0.9, 0.999)\n",
    "EPS     = 1e-08\n",
    "DECAY   = 0.0005\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7eae4",
   "metadata": {},
   "source": [
    "# Modeling (CNN, LSTM, GRU, CNN+LSTM, CNN+GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd979d2c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a276d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the CNN model using sequential class\n",
    "class CNN(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(800,4)\n",
    "        \n",
    "    # forward method\n",
    "    def forward(self, input):  #input(22,250,1)\n",
    "        x = input.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "        x = self.flatten(x)\n",
    "        out = self.dense(x) \n",
    "        return out\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(22, 64, 3, batch_first=True, dropout=0.4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, W).permute(0, 2, 1)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "class CNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # LSTM\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "    \n",
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(22, 25, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(25, 50, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(50, 100, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(0, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(100, 200, (10, 1), padding=(5, 0), stride=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d((3, 1), stride=(3, 1),padding=(1, 0))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        self.gru = nn.GRU(4, 64, 3, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 54),\n",
    "            nn.BatchNorm1d(num_features=54, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(54, 44),\n",
    "            nn.BatchNorm1d(num_features=44, eps=1e-05, momentum=0.2, affine=True),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(44, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # CNN\n",
    "        x = x.permute(0,1,3,2)\n",
    "        x = self.dropout(self.bn1(self.pool1(F.elu(self.conv1(x)))))\n",
    "        x = self.dropout(self.bn2(self.pool2(F.elu(self.conv2(x)))))\n",
    "        x = self.dropout(self.bn3(self.pool3(F.elu(self.conv3(x)))))\n",
    "        x = self.dropout(self.bn4(self.pool4(F.elu(self.conv4(x)))))\n",
    "\n",
    "        # GRU\n",
    "        N, C, H, W = x.size()\n",
    "        x = x.view(N, C, H).permute(0, 1, 2)\n",
    "        out, _ = self.gru(x)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28777c7c",
   "metadata": {},
   "source": [
    "## Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d74747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate CNN model\n",
    "cnn = CNN()\n",
    "# create your cnn optimizer\n",
    "cnn_optimizer = optim.Adam(cnn.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate LSTM model\n",
    "lstm = LSTM()\n",
    "# create your lstm optimizer\n",
    "lstm_optimizer = optim.Adam(lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate GRU model\n",
    "gru = GRU()\n",
    "# create your gru optimizer\n",
    "gru_optimizer = optim.Adam(gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate CNN_LSTM model\n",
    "cnn_lstm = CNN_LSTM()\n",
    "# create your cnn_lstm optimizer\n",
    "cnn_lstm_optimizer = optim.Adam(cnn_lstm.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)\n",
    "\n",
    "# initiate CNN_GRU model\n",
    "cnn_gru = CNN_GRU()\n",
    "# create your cnn_gru optimizer\n",
    "cnn_gru_optimizer = optim.Adam(cnn_gru.parameters(), lr=LR, betas=BETAS, eps=EPS, weight_decay=DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897c535",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fbcd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.46058\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.57104\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.49610\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.61831\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.57288\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.44407\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.55952\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.69810\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.41124\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.29225\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.34996\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.43967\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.46265\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.55334\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.41508\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.48072\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.49895\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.64448\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.40173\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.40148\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.27342\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.26221\n",
      "\tTrain loss: 0.04156, Accuracy: 2499/6768 (36.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 598/1692 (35.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 644/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.35304\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.66531\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.16983\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.30459\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.79487\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.02945\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.42522\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.63973\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.27245\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 0.95786\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.37013\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.31753\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.15976\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.39520\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.32156\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.34157\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.31306\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.70862\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.33166\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.37185\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.16101\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.35412\n",
      "\tTrain loss: 0.03747, Accuracy: 3100/6768 (45.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.16881\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.48553\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.42279\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.12780\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.09388\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.09808\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.44969\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.20931\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.15784\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.01204\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.21901\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.02420\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.04160\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.26786\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.13740\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.12389\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.35521\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.46005\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.18786\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.03360\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.20590\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.44382\n",
      "\tTrain loss: 0.03538, Accuracy: 3565/6768 (52.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 860/1692 (50.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 862/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.11399\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.19588\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.30453\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.39773\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.13389\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 0.82440\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.25137\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.12961\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 0.98581\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 0.96778\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 0.91117\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.02711\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 0.85589\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.07862\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 0.99248\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.18611\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.30188\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.42878\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.15680\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.16836\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.04471\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.17261\n",
      "\tTrain loss: 0.03572, Accuracy: 3473/6768 (51.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 891/1692 (52.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 782/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.19542\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.26016\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.28201\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.16676\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.31926\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 0.81758\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.23813\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.02291\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 0.90667\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 0.82183\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 0.87243\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.03789\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 0.99342\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.13289\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 0.95280\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.03726\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.21671\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.42785\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.23199\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.16270\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 0.92771\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.25207\n",
      "\tTrain loss: 0.03229, Accuracy: 3945/6768 (58.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 982/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 886/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.11258\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.19043\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.36673\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 0.97807\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.10951\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 0.63936\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.25698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.17823\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 0.91725\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 0.84757\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 0.83353\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.07420\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 0.97282\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.08034\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 0.82544\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 0.98334\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.13256\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.55104\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.08462\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.15319\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 0.99871\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.14440\n",
      "\tTrain loss: 0.03202, Accuracy: 3842/6768 (56.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 987/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 0.92940\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.29454\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.34733\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 0.95640\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.15591\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 0.78983\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.17299\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.00895\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 0.86137\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.10400\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 0.82715\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 0.87154\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 0.93767\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.17380\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 0.90851\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.01038\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.06621\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.16099\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.06055\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.31696\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.04677\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 0.87673\n",
      "\tTrain loss: 0.03037, Accuracy: 4093/6768 (60.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1028/1692 (60.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 915/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.15009\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.49134\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.16839\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.04000\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.25920\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.66198\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.24477\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 0.80363\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 0.89378\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 0.66341\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 0.72684\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 0.77544\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 0.93979\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.17059\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 0.89923\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 0.94050\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 0.93963\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.57701\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 0.89979\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.06499\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 0.85231\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.05111\n",
      "\tTrain loss: 0.02985, Accuracy: 4031/6768 (59.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1019/1692 (60.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 895/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 0.90755\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.17866\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.35470\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.07300\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.17082\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.92329\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.28826\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 0.70182\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.02297\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 0.89620\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 0.81938\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.06121\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 0.96772\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.03575\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 0.78536\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 0.98944\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 0.89224\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.36049\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.07479\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.14961\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 0.68195\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.06182\n",
      "\tTrain loss: 0.02887, Accuracy: 4243/6768 (62.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1066/1692 (63.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.01643\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.34016\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.15782\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.07192\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 0.91009\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 0.80025\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.01330\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 0.82081\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 0.88665\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 0.70861\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 0.70825\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 0.98479\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 0.75233\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.17456\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 0.81559\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 0.80664\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.14185\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.35561\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 0.93769\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 0.96973\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 0.77402\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 0.80447\n",
      "\tTrain loss: 0.02832, Accuracy: 4230/6768 (62.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1051/1692 (62.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 942/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 0.97734\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.34246\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.01708\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 0.91726\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 0.85768\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.66925\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 0.90690\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 0.64021\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 0.84207\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 0.65901\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.66052\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 0.75638\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 0.71664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 0.83125\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 0.87285\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 0.85606\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 0.86581\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.32205\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 0.89799\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.12713\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 0.82909\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 0.95762\n",
      "\tTrain loss: 0.02787, Accuracy: 4390/6768 (64.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1095/1692 (64.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 973/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.76999\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.35205\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.32626\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 0.86877\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.06208\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.48944\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 0.93596\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 0.60196\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 0.88995\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 0.63070\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.62982\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 0.86798\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.81658\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.05897\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 0.74182\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 0.85674\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.98850\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.34917\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.79107\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.24197\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 0.77520\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 0.98606\n",
      "\tTrain loss: 0.02791, Accuracy: 4369/6768 (64.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1089/1692 (64.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 981/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.99465\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.44701\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.11402\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 0.96264\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.14763\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.75306\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.17816\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 0.75095\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 0.75393\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 0.58185\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.80941\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 0.80406\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.66748\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 0.95378\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 0.70806\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 0.83263\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.13758\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.09346\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 0.74460\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.06136\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.82954\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 0.89348\n",
      "\tTrain loss: 0.02632, Accuracy: 4563/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1115/1692 (65.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.85452\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.14405\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.19117\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 0.93909\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 0.83924\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.56282\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 0.90651\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.64923\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 0.85336\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 0.81572\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.83228\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.89437\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.72720\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 0.79792\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 0.63668\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 0.74357\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.81671\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.01228\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.78465\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 0.81770\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 0.57948\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.64571\n",
      "\tTrain loss: 0.02609, Accuracy: 4526/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1101/1692 (65.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 991/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.69742\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.05296\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.33339\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.76734\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.03384\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.58666\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 0.94086\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.72188\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 0.74349\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.79119\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.64100\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 0.72258\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.61409\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 0.84664\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 0.78979\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.78225\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.99810\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.11914\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.91502\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.94154\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.64244\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.84690\n",
      "\tTrain loss: 0.02330, Accuracy: 4751/6768 (70.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1162/1692 (68.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1041/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.86876\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.24986\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.03818\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.79957\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 0.82095\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.61999\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 0.73877\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.51107\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.70869\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.52172\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.74717\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.92046\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.58309\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.64850\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.66462\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.90418\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.96743\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.42239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.71993\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.90007\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.62674\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.84553\n",
      "\tTrain loss: 0.02338, Accuracy: 4800/6768 (70.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1188/1692 (70.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.86521\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 0.81813\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.98331\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.97789\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.67420\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.70739\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 0.78738\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.74357\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.72100\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.60120\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.50354\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.54306\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.75014\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.69874\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.61853\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.58189\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.82429\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.21543\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.76888\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.09536\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.67083\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.78721\n",
      "\tTrain loss: 0.02300, Accuracy: 4695/6768 (69.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1140/1692 (67.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 991/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.71383\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.89326\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.00949\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.89165\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.94684\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.49718\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 0.75493\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.71139\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.71239\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.64736\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.94636\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.55807\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.58980\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.50392\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.82090\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.96819\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 0.77301\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.63351\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.10766\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.43943\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.64070\n",
      "\tTrain loss: 0.02341, Accuracy: 4836/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1187/1692 (70.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1027/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.66986\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 0.79998\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.82048\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.73880\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.83388\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.48991\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.77421\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.61776\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.65753\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.66759\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.59260\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.72079\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.38865\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.70993\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.87472\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.80110\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.76678\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.94637\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.75027\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 1.09004\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.29888\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.67704\n",
      "\tTrain loss: 0.02163, Accuracy: 5036/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1214/1692 (71.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.73485\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.81418\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.88234\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.87499\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.69486\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.46616\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.80553\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.33476\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.73258\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.70248\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.79110\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.51476\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.69970\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.82589\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.57833\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.74135\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.83351\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.21859\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.66471\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.11349\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.59537\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.59194\n",
      "\tTrain loss: 0.02104, Accuracy: 4936/6768 (72.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1197/1692 (70.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.61973\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.03490\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.96774\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.97715\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.91541\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.41308\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.69929\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.52016\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.68287\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.35910\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.83830\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.52433\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.49360\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.80358\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.72457\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.74918\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.76429\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.94181\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.78480\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.73727\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.68626\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.81484\n",
      "\tTrain loss: 0.02062, Accuracy: 5022/6768 (74.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00044, Accuracy: 1215/1692 (71.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.60063\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.91485\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.78569\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.71172\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.83234\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.36491\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.78550\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.40748\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.68612\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.46426\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.56391\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.53701\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.70932\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.92378\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.68323\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.72472\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.89181\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.01442\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.59517\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.85313\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.58822\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.61717\n",
      "\tTrain loss: 0.02224, Accuracy: 4940/6768 (72.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1023/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.62248\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.76094\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.64055\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.73692\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.61860\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.44317\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.55155\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.37214\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.67819\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.55431\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.58537\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.62149\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.42793\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.72842\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.57792\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.61545\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.79082\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.92661\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.87175\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.76351\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.48307\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.82021\n",
      "\tTrain loss: 0.01719, Accuracy: 5435/6768 (80.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1300/1692 (76.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1081/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.63890\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.90015\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.99377\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.56276\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.83382\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.48330\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.89874\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.47521\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.91843\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.42357\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.50433\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.49978\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.46249\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.79395\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.84074\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.60802\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.82407\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.92783\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.51015\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.60671\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.29930\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.81363\n",
      "\tTrain loss: 0.01919, Accuracy: 5183/6768 (76.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.69288\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.66013\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.82779\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.94126\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.94673\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.44532\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.53754\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.41419\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.58104\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.55357\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.52513\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.69900\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.52385\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.71226\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.51863\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.63035\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.57599\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.80729\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.62321\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.82696\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.45415\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.73439\n",
      "\tTrain loss: 0.01765, Accuracy: 5362/6768 (79.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.60779\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.66595\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 1.00869\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.88875\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.78976\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.48056\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.55465\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.43057\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.68568\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.50686\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.41767\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.76831\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.39684\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.65049\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.49884\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.54622\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.62274\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.05230\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.85414\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.90708\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.34354\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.66030\n",
      "\tTrain loss: 0.01904, Accuracy: 5103/6768 (75.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1211/1692 (71.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.75062\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.17655\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.73019\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.81836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.74546\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.42305\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.57351\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.33445\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.91514\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.59222\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.85216\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.56071\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.52368\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.63787\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.68962\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.79615\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.68363\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.71077\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.61897\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.75924\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.35389\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.46148\n",
      "\tTrain loss: 0.01628, Accuracy: 5349/6768 (79.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1042/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.53128\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.52942\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.88619\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.78546\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.63487\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.40384\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.58145\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.39875\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.63763\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.62119\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.40913\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.64956\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.44038\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.75426\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.65132\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.44044\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.50673\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.91296\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.47078\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.93358\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.53000\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.65146\n",
      "\tTrain loss: 0.01570, Accuracy: 5550/6768 (82.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1308/1692 (77.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.40842\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.57753\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.61831\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.46987\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.72923\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.53029\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.81467\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.40331\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.76227\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.41264\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.55900\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.43698\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.55171\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.61489\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.68682\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.48873\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.91607\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.63528\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.47950\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.60322\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.37633\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.65404\n",
      "\tTrain loss: 0.01748, Accuracy: 5433/6768 (80.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1291/1692 (76.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.57704\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.49904\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.85178\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.53352\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.74819\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.36906\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.69931\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.32156\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.50360\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.37840\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.57656\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.54831\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.36816\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.47701\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.42087\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.65920\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.71426\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.53871\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.63442\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.68984\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.33840\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.57253\n",
      "\tTrain loss: 0.01633, Accuracy: 5459/6768 (80.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1284/1692 (75.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1053/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.63808\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.69432\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.61059\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.50825\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.38899\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.33357\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.41377\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.57442\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.48586\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.54480\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.48478\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.50091\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.40311\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.64764\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.70609\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.54376\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.66709\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.84309\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.52115\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.71542\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.27726\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.66197\n",
      "\tTrain loss: 0.01464, Accuracy: 5591/6768 (82.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1319/1692 (77.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.39735\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.63324\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.56073\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.49397\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.62418\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.38436\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.43051\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.42208\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.38887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.47676\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.51061\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.58589\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.30969\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.52628\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.65695\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.60501\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.69879\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.50006\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.41784\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.66370\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.57842\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.70504\n",
      "\tTrain loss: 0.01606, Accuracy: 5576/6768 (82.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1311/1692 (77.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1056/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.64800\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.90156\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.72959\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.67766\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.69804\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.32512\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.47624\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.36778\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.54477\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.30788\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.73894\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.46288\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.49675\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.46614\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.52009\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.57175\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.58518\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.65784\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.51768\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.57660\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.28588\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.56124\n",
      "\tTrain loss: 0.01626, Accuracy: 5470/6768 (80.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1293/1692 (76.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.57186\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.43519\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.42750\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.71449\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.67684\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.24630\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.41420\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.32890\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.73898\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.56377\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.60362\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.39642\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.51775\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.47000\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.37098\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.52251\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.53573\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.89331\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.45492\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.50733\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.36174\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.54482\n",
      "\tTrain loss: 0.01416, Accuracy: 5672/6768 (83.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1039/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.75511\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.71679\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.46190\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.61043\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.71016\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.33557\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.51415\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.35765\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.40812\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.34855\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.31963\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.72047\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.44914\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.28948\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.60399\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.63413\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.72479\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.56280\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.58846\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.61455\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.58787\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.46244\n",
      "\tTrain loss: 0.01227, Accuracy: 5855/6768 (86.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1393/1692 (82.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1082/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.70670\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.80583\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.47217\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.33198\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.56082\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.23021\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.35283\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.48109\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.51262\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.39216\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.37511\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.39163\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.30637\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.52733\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.42003\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.42380\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.64815\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.68133\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.50510\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.48074\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.22768\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.32943\n",
      "\tTrain loss: 0.01171, Accuracy: 5947/6768 (87.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1406/1692 (83.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.31921\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.52102\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.72233\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.52501\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.32398\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.41010\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.46038\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.42840\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.55333\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.32546\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.30455\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.68700\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.45432\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.56185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.48214\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.55530\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.58860\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.80711\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.42600\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.65740\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.48513\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.74094\n",
      "\tTrain loss: 0.01337, Accuracy: 5684/6768 (83.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1350/1692 (79.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1053/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.78943\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.84139\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.40784\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.42759\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.34319\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.37124\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.44474\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.37462\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.48628\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.34041\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.58625\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.58405\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.47438\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.66655\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.53752\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.53111\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.57458\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.66402\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.63633\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.83622\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.32646\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.48468\n",
      "\tTrain loss: 0.01386, Accuracy: 5659/6768 (83.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.48821\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.82301\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.75204\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.60309\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.52892\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.33229\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.53933\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.32492\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.60103\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.55747\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.33151\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.60119\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.41732\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.51635\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.52314\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.64944\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.50592\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.35006\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.34858\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.55116\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.54589\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.46415\n",
      "\tTrain loss: 0.01193, Accuracy: 5871/6768 (86.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1389/1692 (82.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1060/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.33406\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.48842\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.42889\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.79265\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.76922\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.44586\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.58748\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.30930\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.45979\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.63153\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.57749\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.53824\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.44580\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.31478\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.51478\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.48758\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.50888\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.52579\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.61148\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.57361\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.58252\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.53415\n",
      "\tTrain loss: 0.01141, Accuracy: 5944/6768 (87.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1410/1692 (83.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.58221\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.50914\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.48011\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.34381\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.33744\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.31642\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.52831\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.41047\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.88252\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.54918\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.36488\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.37958\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.27350\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.60776\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.60087\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.44833\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.70389\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.45136\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.54438\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.56929\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.61813\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.52909\n",
      "\tTrain loss: 0.01242, Accuracy: 5812/6768 (85.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1383/1692 (81.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1063/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.27612\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.52679\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.62176\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.50622\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.49370\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.19110\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.39366\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.41939\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.36407\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.43475\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.46640\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.50205\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.40713\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.43325\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.25097\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.47257\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.30677\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.62222\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.55092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.56647\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.33646\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.51794\n",
      "\tTrain loss: 0.01102, Accuracy: 5907/6768 (87.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1408/1692 (83.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1054/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.63658\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.65690\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.50616\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.64885\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.83837\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.29780\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.52346\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.40741\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.40898\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.53961\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.37575\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.65183\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.46009\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.61637\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.44213\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.47818\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.59184\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.64178\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.44585\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.53538\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.39705\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.39194\n",
      "\tTrain loss: 0.01043, Accuracy: 6029/6768 (89.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1432/1692 (84.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.45423\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.21563\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.41986\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.33667\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.65329\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.34303\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.51946\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.33807\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.38462\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.41665\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.41775\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.53705\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.25057\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.51624\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.47196\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.51486\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.37523\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.51250\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.58022\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.45502\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.25410\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.50372\n",
      "\tTrain loss: 0.01096, Accuracy: 5853/6768 (86.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1372/1692 (81.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1062/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.41176\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.71275\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.39445\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.35963\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.39999\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.19967\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.37442\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.40582\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.22802\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.55491\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.37412\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.41325\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.46719\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.54765\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.51735\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.45890\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.52060\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.48990\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.48284\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.35256\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.41609\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.48755\n",
      "\tTrain loss: 0.01114, Accuracy: 5855/6768 (86.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1388/1692 (82.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1043/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.80061\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.78711\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.91271\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.73058\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.44110\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.37280\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.45435\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.43059\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.38627\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.44689\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.57619\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.27738\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.29821\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.46321\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.47104\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.37778\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.52735\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.51021\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.43055\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.43289\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.31751\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.57236\n",
      "\tTrain loss: 0.00933, Accuracy: 6055/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1435/1692 (84.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.62159\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.67267\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.48950\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.78703\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.42138\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.37986\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.48580\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.41423\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.30600\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.38072\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.45014\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.43623\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.26826\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.56773\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.41066\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.47652\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.35554\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.36774\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.72678\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.64944\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.43501\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.53361\n",
      "\tTrain loss: 0.00870, Accuracy: 6145/6768 (90.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1461/1692 (86.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00060, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.60909\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.53226\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.36114\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.53911\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.37711\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.20403\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.39005\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.51539\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.38428\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.18225\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.24575\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.36905\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.43800\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.50320\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.36740\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.52619\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.32258\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.53728\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.53614\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.50036\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.47044\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.36908\n",
      "\tTrain loss: 0.00939, Accuracy: 6080/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.31974\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.60279\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.67148\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.61155\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.53493\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.34476\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.38820\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.20964\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.32553\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.46567\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.29640\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.47520\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.36439\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.47188\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.49753\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.47430\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.38956\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.42948\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.41428\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.26337\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.50312\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.42594\n",
      "\tTrain loss: 0.00930, Accuracy: 6060/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1452/1692 (85.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.30237\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.40413\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.73673\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.24768\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.72127\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.16405\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.74832\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.28967\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.43620\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.21892\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.22692\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.48681\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.24926\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.64602\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.38651\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.41542\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.33040\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.63576\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.50877\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.62179\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.10927\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.39062\n",
      "\tTrain loss: 0.01057, Accuracy: 5961/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1419/1692 (83.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1090/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.33851\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.38980\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.60117\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.52637\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.54221\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.26181\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.30009\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.50363\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.46168\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.94566\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.24802\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.32281\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.37692\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.36997\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.38416\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.70917\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.40713\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.31711\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.40133\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.58194\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.23357\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.58075\n",
      "\tTrain loss: 0.00935, Accuracy: 6095/6768 (90.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1448/1692 (85.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1102/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.38582\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.72863\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.74040\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.41231\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.80389\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.21384\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.36238\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.71667\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.31104\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.49257\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.42871\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.35382\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.32655\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.26461\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.34716\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.34844\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.57696\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.47517\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.18422\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.50118\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.32619\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.35306\n",
      "\tTrain loss: 0.01003, Accuracy: 6046/6768 (89.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1446/1692 (85.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.22520\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.38657\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.54522\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.33725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.54119\n",
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.24725\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.49627\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.25776\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.39604\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.32849\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.62782\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.42826\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.41446\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.27414\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.42461\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.54746\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.52094\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.38368\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.29741\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.25855\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.39209\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.40564\n",
      "\tTrain loss: 0.00954, Accuracy: 6079/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1110/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.42429\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.63496\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.45521\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.53854\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.77004\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.44629\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.39081\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.41143\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.32579\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.38006\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.48102\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.32588\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.26538\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.32174\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.47339\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.51219\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.42957\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.44973\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.41771\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.51448\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.22071\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.52228\n",
      "\tTrain loss: 0.00884, Accuracy: 6129/6768 (90.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1110/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.28376\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.46082\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.24783\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.38980\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.25796\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.36754\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.60892\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.40395\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.23300\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.21407\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.53728\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.34087\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.29091\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.40844\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.37227\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.36658\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.57088\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.70059\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.24045\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.81313\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.26003\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.47729\n",
      "\tTrain loss: 0.01077, Accuracy: 5975/6768 (88.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1436/1692 (84.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.31490\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.76195\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.74618\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.40066\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.53316\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.30765\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.40058\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.22927\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.26476\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.65378\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.46106\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.43339\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.55040\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.29935\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.40432\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.40115\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.59914\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.65774\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.42479\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.48300\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.40878\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.48579\n",
      "\tTrain loss: 0.00747, Accuracy: 6207/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1125/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.72865\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.41680\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.40824\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.48162\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.27494\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.45617\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.64613\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.23668\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.50040\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.28393\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.22687\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.44387\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.21555\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.46994\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.27266\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.40000\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.27140\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.71067\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.58750\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.39794\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.32369\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.28397\n",
      "\tTrain loss: 0.01069, Accuracy: 5867/6768 (86.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1404/1692 (82.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1058/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.53576\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.25551\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.43216\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.42473\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.42576\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.13113\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.26723\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.29780\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.41222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.34861\n",
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.27886\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.34040\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.56688\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.65819\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.48095\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.39673\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.54401\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.43256\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.29829\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.43824\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.33499\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.35309\n",
      "\tTrain loss: 0.00844, Accuracy: 6171/6768 (91.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1481/1692 (87.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.45040\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.43916\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.28322\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.41795\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.55539\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.17554\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.28539\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.23428\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.40068\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.39220\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.27307\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.56347\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.38816\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.64441\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.33974\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.49523\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.37386\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.21583\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.20288\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.39717\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.31463\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.51522\n",
      "\tTrain loss: 0.00930, Accuracy: 6035/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1465/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1094/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.40505\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.51340\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.61008\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.28667\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.74246\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.24864\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.07689\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.22740\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.37554\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.33297\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.30984\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.35264\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.36185\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.35221\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.49103\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.27775\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.54461\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.38105\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.57811\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.33850\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.69712\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.47795\n",
      "\tTrain loss: 0.00731, Accuracy: 6238/6768 (92.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.55099\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.41229\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.64633\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.48033\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.43243\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.28974\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.48569\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.43545\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.26378\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.55413\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.53411\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.33919\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.30529\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.42595\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.38955\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.52907\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.41521\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.50298\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.35156\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.48874\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.17268\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.27734\n",
      "\tTrain loss: 0.00812, Accuracy: 6158/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1455/1692 (85.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.47675\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.46402\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.74584\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.24848\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.57157\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.26141\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.25277\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.44733\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.36354\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.36214\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.31558\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.53499\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.22023\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.59422\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.56469\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.64566\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.17291\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.73295\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.27552\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.60354\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.35646\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.51713\n",
      "\tTrain loss: 0.00790, Accuracy: 6174/6768 (91.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1158/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.44668\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.79002\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.23550\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.26941\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.33112\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.18780\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.65433\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.16459\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.36353\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.33889\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.30141\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.31239\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.33407\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.31263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.28997\n",
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.46748\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.49482\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.46663\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.39155\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.30323\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.24510\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.49540\n",
      "\tTrain loss: 0.00694, Accuracy: 6223/6768 (91.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1498/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.27394\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.53561\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.38996\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.45277\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.44770\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.28691\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.53957\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.25093\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.24376\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.40648\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.64988\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.55691\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.23503\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.27108\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.43097\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.39400\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.27695\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.59831\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.45369\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.70726\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.38432\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.61131\n",
      "\tTrain loss: 0.00673, Accuracy: 6326/6768 (93.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1532/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1123/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.26286\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.43436\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.55908\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.27327\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.45071\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.16662\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.21743\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.26068\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.56880\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.29813\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.28496\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.29177\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.30919\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.38302\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.24958\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.28428\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.46531\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.63316\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.40504\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.45137\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.36257\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.23611\n",
      "\tTrain loss: 0.00853, Accuracy: 6085/6768 (89.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1458/1692 (86.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.23674\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.44075\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.54264\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.52090\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.36734\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.19153\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.31585\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.54570\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.34514\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.43056\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.47440\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.49844\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.39943\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.28234\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.33625\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.39736\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.39263\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.61463\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.42901\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.51909\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.17757\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.36775\n",
      "\tTrain loss: 0.00914, Accuracy: 6035/6768 (89.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1442/1692 (85.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1052/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.39657\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.22045\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.60285\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.25705\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.54364\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.22843\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.36095\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.38754\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.53101\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.25168\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.19968\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.49806\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.32856\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.37822\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.24061\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.71650\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.30442\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.41861\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.29880\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.26931\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.14628\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.23402\n",
      "\tTrain loss: 0.00873, Accuracy: 6006/6768 (88.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1449/1692 (85.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.38325\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.46932\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.36321\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.32724\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.22878\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.27404\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.30845\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.26062\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.29824\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.40493\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.46687\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.35814\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.37314\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.53198\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.34479\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.36860\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.50811\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.53214\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.38381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.42078\n",
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.19528\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.51360\n",
      "\tTrain loss: 0.00769, Accuracy: 6175/6768 (91.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1465/1692 (86.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.50604\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.64816\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.42003\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.26109\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.60085\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.53356\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.26525\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.17645\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.63022\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.25811\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.27989\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.26243\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.19720\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.48334\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.36574\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.34942\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.47775\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.49283\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.34755\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.54962\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.19692\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.54276\n",
      "\tTrain loss: 0.00718, Accuracy: 6218/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.34815\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.58156\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.42253\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.38464\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.62237\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.21629\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.31427\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.37179\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.32576\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.39440\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.40007\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.27629\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.46714\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.43245\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.45073\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.27954\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.33269\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.26606\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.21504\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.54229\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.41307\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.45342\n",
      "\tTrain loss: 0.00786, Accuracy: 6156/6768 (90.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.31083\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.54921\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.51594\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.37066\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.24810\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.19518\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.23164\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.20043\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.18751\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.42288\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.21465\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.32897\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.35756\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.59051\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.53105\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.36467\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.38493\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.51939\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.43414\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.31701\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.37559\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.59801\n",
      "\tTrain loss: 0.00612, Accuracy: 6324/6768 (93.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1510/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.26427\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.47441\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.35580\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.52981\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.39163\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.36404\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.32940\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.12250\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.52561\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.64284\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.19254\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.35574\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.26701\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.43883\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.31039\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.51360\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.22867\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.57775\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.38735\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.25384\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.29838\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.26801\n",
      "\tTrain loss: 0.00815, Accuracy: 6121/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1467/1692 (86.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.18718\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.47173\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.43345\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.39086\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.36114\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.38167\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.18820\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.16007\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.31072\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.57346\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.55521\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.57832\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.32483\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.70638\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.14267\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.30819\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.52569\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.59671\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.27809\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.33144\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.24061\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.41219\n",
      "\tTrain loss: 0.00588, Accuracy: 6375/6768 (94.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1532/1692 (90.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00063, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.54422\n",
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.39057\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.34139\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.23082\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.42880\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.39706\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.40163\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.40135\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.46437\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.26447\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.23024\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.25417\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.33177\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.49646\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.39595\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.30047\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.24547\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.43862\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.31720\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.32354\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.24663\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.18278\n",
      "\tTrain loss: 0.00689, Accuracy: 6262/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1111/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.30881\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.43892\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.28979\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.26265\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.19790\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.29301\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.18433\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.26444\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.23987\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.40789\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.16170\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.18989\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.36494\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.30679\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.45054\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.60354\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.42137\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.49329\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.39942\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.32565\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.25638\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.64232\n",
      "\tTrain loss: 0.00781, Accuracy: 6134/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 1033/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.28201\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.39086\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.50986\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.29247\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.26172\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.15503\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.19254\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.27766\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.32903\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.54163\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.40207\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.29602\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.59650\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.27649\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.35214\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.43869\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.29190\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.26935\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.21647\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.33067\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.35511\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.28019\n",
      "\tTrain loss: 0.00671, Accuracy: 6281/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.32877\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.42443\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.43777\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.32782\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.34190\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.29171\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.41587\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.40504\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.49739\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.23781\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.29462\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.52425\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.26963\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.38113\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.55388\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.37962\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.35573\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.31913\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.25317\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.47383\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.16435\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.43065\n",
      "\tTrain loss: 0.00695, Accuracy: 6236/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.21528\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.47960\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.16903\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.37511\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.35310\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.24127\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.46319\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.33762\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.26869\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.45685\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.15866\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.21291\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.23593\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.40326\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.33995\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.20832\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.23868\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.49512\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.42761\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.33742\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.22590\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.35764\n",
      "\tTrain loss: 0.00648, Accuracy: 6304/6768 (93.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1514/1692 (89.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1066/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.29245\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.41083\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.37606\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.49218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.33042\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.14910\n",
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.33711\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.15556\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.30885\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.44914\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.35636\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.21176\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.23687\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.29874\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.23999\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.31195\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.21415\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.46132\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.40379\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.35774\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.18040\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.35648\n",
      "\tTrain loss: 0.00671, Accuracy: 6225/6768 (91.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1113/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.27137\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.48478\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.31457\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.35565\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.37994\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.23975\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.40205\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.48809\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.39492\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.07037\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.37915\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.41058\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.35433\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.33725\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.23773\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.27374\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.27338\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.50391\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.25345\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.41908\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.20599\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.43796\n",
      "\tTrain loss: 0.00588, Accuracy: 6363/6768 (94.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1537/1692 (90.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1088/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.56913\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.54578\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.27414\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.37648\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.45616\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.11484\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.28767\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.25856\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.30991\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.24747\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.45098\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.50578\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.27499\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.35893\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.22355\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.36518\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.25223\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.48634\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.32433\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.42386\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.22251\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.41275\n",
      "\tTrain loss: 0.00850, Accuracy: 6140/6768 (90.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1072/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.30535\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.23449\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.36638\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.39441\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.20321\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.15417\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.26128\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.24451\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.15402\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.22725\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.64743\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.36262\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.13553\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.11030\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.19046\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.38549\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.29473\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.39811\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.28863\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.31455\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.33374\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.34731\n",
      "\tTrain loss: 0.00720, Accuracy: 6193/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1475/1692 (87.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.27939\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.39910\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.44321\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.51876\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.28575\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.19175\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.54637\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.31750\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.23170\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.25313\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.30912\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.37149\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.16490\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.24925\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.48348\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.64043\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.22312\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.38161\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.38660\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.56236\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.43594\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.23682\n",
      "\tTrain loss: 0.00749, Accuracy: 6194/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1499/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1046/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.32081\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.28397\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.44253\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.25862\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.53789\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.31302\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.48989\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.21020\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.29335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.22948\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.31559\n",
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.62649\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.29833\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.27670\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.26052\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.31717\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.30758\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.32968\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.33310\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.34037\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.57163\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.30770\n",
      "\tTrain loss: 0.00732, Accuracy: 6129/6768 (90.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1475/1692 (87.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.22274\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.31901\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.54941\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.24284\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.57080\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.15519\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.26053\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.32675\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.43460\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.31415\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.49438\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.23890\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.66022\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.27950\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.30260\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.41837\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.22930\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.34684\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.20670\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.58950\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.32191\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.52467\n",
      "\tTrain loss: 0.00614, Accuracy: 6277/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1520/1692 (89.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.33560\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.30835\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.39418\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.32152\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.41757\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.21123\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.35352\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.10141\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.56885\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.27936\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.37671\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.28024\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.33209\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.48017\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.32755\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.43149\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.29447\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.40442\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.19823\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.40655\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.32296\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.23684\n",
      "\tTrain loss: 0.00707, Accuracy: 6228/6768 (92.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1501/1692 (88.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.47804\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.63870\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.60457\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.27457\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.48814\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.16916\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.37425\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.52208\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.19240\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.25257\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.25134\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.31606\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.21717\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.23079\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.26883\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.45092\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.46149\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.45472\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.19080\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.36854\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.23978\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.59644\n",
      "\tTrain loss: 0.00546, Accuracy: 6355/6768 (93.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1538/1692 (90.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1097/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.31553\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.39661\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.85855\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.15952\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.33195\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.05869\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.19698\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.28145\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.40182\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.78868\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.38180\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.39236\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.38489\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.25845\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.19566\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.32296\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.42480\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.63187\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.35061\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.54938\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.11177\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.23256\n",
      "\tTrain loss: 0.00495, Accuracy: 6432/6768 (95.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1559/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1117/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.49539\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.18885\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.46993\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.20263\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.25432\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.21631\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.33255\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.22299\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.25021\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.19930\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.28153\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.27612\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.28689\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.23182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.28786\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.22928\n",
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.24524\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.33083\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.34346\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.30325\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.18354\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.49297\n",
      "\tTrain loss: 0.00728, Accuracy: 6183/6768 (91.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.32034\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.60132\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.30019\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.49752\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.18101\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.25284\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.47594\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.16784\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.21494\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.43180\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.37124\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.29924\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.26134\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.28444\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.42568\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.35097\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.39133\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.47878\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.16653\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.35679\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.13376\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.30386\n",
      "\tTrain loss: 0.00657, Accuracy: 6250/6768 (92.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1114/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.11711\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.50131\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.40267\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.34835\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.32839\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.20444\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.24342\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.42161\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.27978\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.18309\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.38034\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.15922\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.18224\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.30054\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.43005\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.45256\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.23059\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.49841\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.22350\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.27266\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.36902\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.35102\n",
      "\tTrain loss: 0.00511, Accuracy: 6391/6768 (94.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1124/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.20804\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.18154\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.37436\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.32741\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.43043\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.19301\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.34172\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.46163\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.38274\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.21232\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.44126\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.42677\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.38862\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.30353\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.56309\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.23099\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.15584\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.36490\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.28279\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.55393\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.23800\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.42116\n",
      "\tTrain loss: 0.00610, Accuracy: 6284/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1533/1692 (90.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1109/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.20594\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.28285\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.38309\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.25587\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.43750\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.13030\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.31812\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.18229\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.15345\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.38172\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.21357\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.26655\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.26640\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.11720\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.34822\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.21616\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.40387\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.32429\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.21760\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.29941\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.21184\n",
      "\tTrain loss: 0.00642, Accuracy: 6226/6768 (91.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1503/1692 (88.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 1099/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.32386\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.49038\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.58554\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.39920\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.19990\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.16501\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.38351\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.37015\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.32032\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.40866\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.48514\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.18064\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.26570\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.38910\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.42877\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.26810\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.45301\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.68012\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.28986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.47532\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.10538\n",
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.40657\n",
      "\tTrain loss: 0.00629, Accuracy: 6267/6768 (92.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1529/1692 (90.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1116/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.36557\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.22562\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.59652\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.18616\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.31027\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.25580\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.39550\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.21518\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.42360\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.24099\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.27927\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.20633\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.21237\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.49694\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.43429\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.39845\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.40706\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.71447\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.25798\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.38662\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.23900\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.34871\n",
      "\tTrain loss: 0.00579, Accuracy: 6323/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1534/1692 (90.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1128/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.43089\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.39778\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.18179\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.32345\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.44871\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.23743\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.26148\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.24147\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.17946\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.15444\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.22425\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.15985\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.32403\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.39629\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.36037\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.41958\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.20690\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.47138\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.49878\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.45142\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.19192\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.24334\n",
      "\tTrain loss: 0.00607, Accuracy: 6303/6768 (93.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1522/1692 (89.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1068/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.39017\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.35449\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.36240\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.31935\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.29630\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.09872\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.20254\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.19772\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.33799\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.46867\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.31061\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.61493\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.12332\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.44162\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.27415\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.38538\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.17199\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.30006\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.34770\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.41063\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.42430\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.26804\n",
      "\tTrain loss: 0.00579, Accuracy: 6324/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1529/1692 (90.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1101/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.20868\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.39126\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.37265\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.27341\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.39962\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.13624\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.32851\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.31503\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.32369\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.27682\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.35823\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.36178\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.23186\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.45195\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.29094\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.16734\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.26974\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.48882\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.22039\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.42575\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.64318\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.58569\n",
      "\tTrain loss: 0.00558, Accuracy: 6322/6768 (93.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1519/1692 (89.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1101/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.25717\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.32595\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.44790\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.23241\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.59749\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.36496\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.29389\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.35793\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.35821\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.24834\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.11690\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.36701\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.29687\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.30840\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.16316\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.24140\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.30873\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.39185\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.28091\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.33948\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.27845\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.36326\n",
      "\tTrain loss: 0.00477, Accuracy: 6426/6768 (94.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1570/1692 (92.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00064, Accuracy: 1115/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.61626\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.35466\n",
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.28302\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.18575\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.19916\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.14456\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.16907\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.15159\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.49588\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.21044\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.36522\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.26390\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.34161\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.36592\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.12651\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.54756\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.48740\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.53463\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.19791\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.42205\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.19154\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.33246\n",
      "\tTrain loss: 0.00527, Accuracy: 6376/6768 (94.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1545/1692 (91.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1108/1772 (62.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9278959810874704\n",
      "Best test accuracy:\n",
      "0.6534988713318285\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAEICAYAAACd/8f0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABnJ0lEQVR4nO3dd3zU9f3A8dfnLuOy996MsEcggIslOOsuKNghrT+tttbRWqvVWmu10y7raN3VWnFWUVFUFEVlQ1ghQAghi+y9c7nP74/PZRASEiDJJfB+Ph555O677n0Hl/f3s5XWGiGEEEIMbRZXByCEEEKI3knCFkIIIYYBSdhCCCHEMCAJWwghhBgGJGELIYQQw4AkbCGEEGIYkIQthBBCDAOSsE9TSqlspdRCV8chhDiSUmqNUqpCKeXp6ljE0CIJWwghhgilVCIwG9DAZYP4um6D9VrixEnCFu2UUp5Kqb8ppQqcP39ru8tXSoUqpd5TSlUqpcqVUmuVUhbnvp8rpfKVUjVKqb1KqQWufSdCDFvfBdYDLwDXtW1USsUppd5SSpUopcqUUo912neDUmqP8/uXrpSa5tyulVKjOh33glLqIefjeUqpPOd3txB4XikV5PyOlzhL+O8ppWI7nR+slHre+behQin1tnP7LqXUpZ2Oc1dKlSqlUgbqQzpdScIWnd0LnAFMBaYAM4H7nPt+CuQBYUAE8AtAK6XGALcAM7TWfsAFQPagRi3EqeO7wMvOnwuUUhFKKSvwHnAISARigOUASqnFwAPO8/wxpfKyPr5WJBAMJAA3YvLB887n8UAD8Fin418CvIEJQDjwV+f2F4FvdzruYuCw1npbH+MQfSTVIKKzbwE/1loXAyilfg38C/gl0AJEAQla60xgrfOYVsATGK+UKtFaZ7sicCGGO6XUOZhk+ZrWulQpdQC4FlPijgZ+prW2Ow//0vn7/4A/aq03OZ9nHsdLOoBfaa2bnM8bgDc7xfMw8JnzcRRwERCita5wHvK58/d/gF8qpfy11tXAdzDJXfQzKWGLzqIxd/FtDjm3AfwJ88fgI6VUllLqbgBn8r4dc5dfrJRarpSKRghxvK4DPtJalzqf/9e5LQ441ClZdxYHHDjB1yvRWje2PVFKeSul/qWUOqSUqga+AAKdJfw4oLxTsm6ntS4AvgK+qZQKxCT2l08wJnEMkrBFZwWYO/w28c5taK1rtNY/1VqPwFS7/aStrVpr/V+tdVvpQAN/GNywhRjelFJewNXAXKVUobNd+Q5M01QREN9Dx7BcYGQPl63HVGG3ieyyv+tSjT8FxgCztNb+wJy28JyvE+xMyN35N6ZafDGwTmud38Nx4iRIwj69uSulbG0/wCvAfUqpMKVUKHA/proLpdQlSqlRSikFVAGtgEMpNUYpda6zc1ojplrN4Zq3I8SwdQXmOzUe04dkKjAO0/R0BXAY+L1Sysf5fT3bed4zwJ1KqenKGKWUarvpTgOuVUpZlVIXAnN7icEP8/2tVEoFA79q26G1Pgx8ADzh7JzmrpSa0+nct4FpwG2YNm0xACRhn95WYr6gbT82YDOwA9gJbAUech47GvgEqAXWAU9orT/DtF//HigFCjGdUe4ZvLcgxCnhOuB5rXWO1rqw7QfT6WspcCkwCsjBdP68BkBr/TrwMKb6vAaTOIOd17zNeV4lpn/K273E8DfAC/NdXg982GX/dzB9WTKAYkxTGM442tq/k4C3+v62xfFQWnetFRFCCCGOj1LqfiBZa/3tXg8WJ0R6iQshhDgpzir06zGlcDFApEpcCCHECVNK3YDplPaB1voLV8dzKpMqcSGEEGIYkBK2EEIIMQwMuTbs0NBQnZiY6OowhBjytmzZUqq1DnN1HMci32ch+qYv3+chl7ATExPZvHmzq8MQYshTSh3q/SjXku+zEH3Tl++zVIkLIYQQw4AkbCGEEGIYkIQthBBCDANDrg1bDH8tLS3k5eXR2NjY+8GiVzabjdjYWNzd3V0dihDChSRhi36Xl5eHn58fiYmJmLVCxInSWlNWVkZeXh5JSUmuDkcI4UJSJS76XWNjIyEhIZKs+4FSipCQEKmtEEJIwhYDQ5J1/5HPUggBwzBhF1Q28JeP9nKwtM7VoQghhDgF7C+q4bO9xf16zfK6Zvp76u9hl7Ar61t49NNMMg5XuzoUMUSVlZUxdepUpk6dSmRkJDExMe3Pm5ubj3nu5s2bufXWWwcpUiGEqzkcmh+/so1bX9nWbwk2q6SWM363mt+u3NMv12sz7Dqdhfl5AlBS2+TiSMRQFRISQlpaGgAPPPAAvr6+3Hnnne377XY7bm7d/9dPTU0lNTV1MMIUQgwBH6UXklFYA5i8Eu5na9/33o4CZiQGE+Fv6+n0br21NZ9mu4On1x5kekIQF06M6pdYh10JO9jHA6tFUVwtCVv03bJly7jpppuYNWsWd911Fxs3buTMM88kJSWFs846i7179wKwZs0aLrnkEsAk++9///vMmzePESNG8Oijj7ryLQhx2tiQVcZfPt53XOc4HJrGltbjOkdrzd9XZ+JhNakwq6SjqTW/soFb/ruNR1btPe44/rctnzNHhDAlNoCfvb6D7H5qwh12JWyrRRHi40FJjSTs4eDX7+4mvaB/my/GR/vzq0snHPd5eXl5fP3111itVqqrq1m7di1ubm588skn/OIXv+DNN9886pyMjAw+++wzampqGDNmDDfffLOMhxZigC3flMv/tuVzZUoMSaE+fTrn6bVZPPPlQdbfswCrpW8dNVfvKWbP4Wp+cl4yf/l4H1kldZwxIgSAL/eXAPDhrkJ+c8VEbO7WPl1zY3Y5+ZUN3HXhGKYnBPHDl7fScJw3Ej0ZdiVsgHB/T4prZJiLOD6LFy/GajVfuqqqKhYvXszEiRO544472L17d7fnfOMb38DT05PQ0FDCw8MpKioazJCFGHYcDs323MrjPi+/sqH9cZazRPr+joI+n79y52FKapo4VNb30uw72wsI9fXgprkjsblbyCqpbd+3dn8pVouipsnOpxlHd0hraXVQUdfRJ6a4upFd+VW8vCEHHw8r54+PJDbIm3d+dDbjovz7HNOx9KmErZS6EPg7YAWe0Vr/vst+T+BFYDpQBlyjtc7utD8eSAce0Fo/crJBh/l6Shv2MHEiJeGB4uPTcaf+y1/+kvnz5/O///2P7Oxs5s2b1+05np6e7Y+tVit2u32gwxRiWHttcy53v7WTVbfPYUykX5/O+SqzlG89s4H3bz2H8VH+7YnzvR2HueXc0T2el1VSy4gwXyrqmtmRXwXAviKzrTctrQ7W7C3mwgmReLhZSAzxab9RcDg0Xx8o45LJUaw7UMbb2/K5eFJHO7TWmtuXp/FReiFLZsTj4WbhxXXZtLSaTmuLp8fi5WEKB/05LLPXhK2UsgKPA+cBecAmpdQKrXV6p8OuByq01qOUUkuAPwDXdNr/F+CD/go63M/G7n6uZhWnl6qqKmJiYgB44YUXXBuMEKeQ1zbnApBRWH1cCRtga04lEf42ahrtJIX6kFFYQ2ZxLaPCTQJuaG7F5m5BKcWK7QXc+so2Hrs2Ba2hrYP3vqIaLpwY2etrbs6uoKbRzoJxEQCMDPdllzPppx+upryumbnJYYT6evLiumwq65sJ9PYAYMX2At7feZgZiUG8sjEHh9Ysnh7HuePCaWxpZfboMHA4wNK/ldh9udpMIFNrnaW1bgaWA5d3OeZy4N/Ox28AC5TztkIpdQVwEOi+zvEEhPl5UlbXTKujf8e4idPHXXfdxT333ENKSoqUmoXoJwdL69iaUwnAgeLaYx/cyZZDFQCkF1S3d/y6ae4IlIL3dxwGwN7qYO6fPuOOV9NobGnlDx9kAPDYp5l8vq8Ef5sbMYFe7C2q6fF1iqobufBvX/D1gVJW7ynCw2ph9uhQAEaG+pBbXk+TvZW1+80NxDmjQrkyJYaWVs1zXx4EoLCqkV++vYtp8YEsv/FMvrhrPp//bD5/WDSZCyZEcvnUGIJVLfwhEdLf6fuH1wd9qRKPAXI7Pc8DZvV0jNbarpSqAkKUUo3AzzGl8zvpgVLqRuBGgPj4+F4DCvPzpNWhKa9rbh/mJUR3HnjggW63n3nmmezb19EL9aGHHgJg3rx57dXjXc/dtWvXQIQoxCnjra15WBQEeXuQWdK3hN3S6mB7XiVgSrZT4wIAOHNEKDMSgvlg12FuWziaHflVFNc08XZaAfuKasmvbOCa1Dhe3ZxLZnEt50+IoNnuYF9hzwn704xiMgpruOW/27C5WThzZAg+niYNjgjzxaEhp6yetftLGBvpR7i/jXB/G1emxPDEmgOcPSqUB95Nx+7Q/PnqqVgtiuhAr6NfqGArNFXBhn/B+K7l2xM30J3OHgD+qrU+5r+c1voprXWq1jo1LCys14uGt43Flp7iQggx6O54NY2fvrb9iG0Oh+atrfmcMzqMlPhADhQfu/NX2yQlew5X09jiICbQi4zD1ewvqsXDaiEmyIvzJ0SQUVhDXkU96w6UAXDRxEjSD1ezcFwED185kbhgL+wOzZzRYSRH+HGwtI5mu6Pb1/z6QBmB3u40tbRSUNXIwnHhZse6J5i3824A3kkr4OsDZZw/oaNa/f5LxhPo7c6Sp9eTWVzDP789/di91wt3mt+HvoLyrGN+DsejLwk7H4jr9DzWua3bY5RSbkAApvPZLOCPSqls4HbgF0qpW04uZJk8RQghBlKrQ/PaplxaWo9OfFprPtlTxJtb89jbqTT78oZD5Fc2sGh6LCPDfTlYWoe9m/MBDpTUcs4fPuOZtVnt1eHXzoqnye7g04xiEkK8sVoU5441CfWzjGK+PlDK2Eg//r4khXsvHsfDV07EzWrhtgXJeLpZmDsmjDGRftgdutupq7XWrDtQyrzkMP5yzVRig7xMUm6qgTW/J+Dge3jSzBNrMgn19eCG2R2r4wX5ePDwlZPw9XDjb9ekMCe5l4Jl4U7wCgJlgbT/9vp591VfqsQ3AaOVUkmYxLwEuLbLMSuA64B1wCLgU21un2a3HaCUegCo1Vo/drJBt81EU1wtQ7uEEKK/rd1fwl1v7iDE16O9U1abvIoGahpNv48n12TytyUpbM2p4MH30jl3bDiXTIqisaWV5lYHeRUNJHYpiRbXNHLdcxvJr2zgj6v2MiHan+gAGwvGhfOnVXvJKq3j/PHmNUeE+ZIU6sPKnYVszang22ck4OFm4YY5I9qvt2h6LBdPisTbw43R4aaTW0ZhNe+k5fPJniLsDs2P5o1iUmwApbXNnDUylAsmRHJBWwl63XPQVIUCZvkW80VtLHeePwY/25HzLVwwIZKFv4ro2xjvwp0Qfxa0NpmEPe8esPRtHPex9FrC1lrbgVuAVcAe4DWt9W6l1INKqcuchz2LabPOBH4C3H3SkR1DqJ/pqSclbCGE6H/7i0wrZoFzbPTWnAoefj8drTXpznUcZiYFs2J7AX/9eB83vriZyAAbf716KhaLau/VndlNx7PbXkmjrLaZp7+biofVwracSqYlBDEyzLd9xrGksI4kf+7YcNZlldFkd3DWyJBu4/X2aGuH9sFqUTy55gBPrDlAkLcHbhbFL9/Zxf+2mYrhMztfo9UO65+AwAQA5gWWMDHGn8WpcWBvhsfPgLV/aT+8T8m6uR7KMiFyEqR8G6rzIWtN7+f1QZ/asLXWK7XWyVrrkVrrh53b7tdar3A+btRaL9Zaj9Jaz9RaH1Vpr7XulzHYYP5xfD3dZHpSIYToxqrdhRRWnXgN5P5iU9V92HmNd7bl8/TagxwoqSW9oBqLgj9+czJuFgt/X72fpFAfnvnuDAK8Tal0pHMc9IEuHc+qG1tYf7CMG+aM4LzxEfzkvGQApicE4W61kBxpzhsZ2jGOeoGzWtxqUcxMCu456OIMbNueIzHEm4zCGmYmBfPfG87gme/OwO7QPLnmAHHBXsQFe3ecs2cFVOXCBb8FNxvfGVHHaz840yTm/M1QsgdW/xq2L+/+NbtbLKR4D2iHSdhjLoYrnoS4rv20T8ywnOkMTMczKWEL0X+UUhcqpfYqpTKVUkfVkimlEpRSq5VSO5RSa5RSsa6IUxzb/qIafvDSFu54Na3Pq085HJpHVu1ly6Fycw1nybgtYedVmJL2mr0lpB+uJinUh8RQH5b/4AxW3jqb1286q2PMdV0ZAV//nkhf61El7LScSrSGmYkm8V53ViJ//OZkFk03/5XGO2cEm6CyTA9rIDUxGD9PNybHBhxVTX2EL/8KK+9kVoTG3+bG364xvbjjQ7y5cbapQj9rROiR5+z9AHzCTWING4N76Z720jpZa0wbdNwZ8M4tcLhTJ7tDX8NzF8E/pkNdaduHaBJ44Q7zPHISuHnC1GvBs/eJXPpi2CbsUD9PSqSELboxf/58Vq1adcS2v/3tb9x8883dHj9v3jw2b94MwMUXX0xlZeVRxzzwwAM88sixK4jefvtt0tM75hO6//77+eSTT44zetfoNEHSRcB4YKlSanyXwx4BXtRaTwYeBH43uFGKvnhx3SEA1mWV8cGuwm6P6ZrIH/8sk8c+y+TZLw+itSazS5V4W8L+Yn8p6QXVjI82Q6+mxQcxPrrLtJvbXoS1j3B+QN5RJewthyqwKJjiHLpltSiunhHXnohTE4PxdLMwKvu/8MFdULIPDzcLj1w9hfu+Ma7nN601HPwCgHtTmvnw9jlHDLf64fyRnD8+gsWpsUefkzTbTHASPgGKOk0XkrUGolNg6StHdh7b8To8f5Hp/V2dD68vg5wN8Nh0+O/VkLcZPAMgsPchysdr2CZsKWGLnixdupTly4+swlq+fDlLly7t9dyVK1cSGBh4Qq/bNWE/+OCDLFy48ISu5QJ9mSBpPPCp8/Fn3ewXLlbd2MKbW/O4MiWGsZF+PPz+HuqajpwY6NkvD7Lgz59T5vz7+XVmKX/9ZB/uVsWGrHIKqxupabKjFBRWN6K1Jq+iHqVg/YEy8isb2kvC3cpcDcBEn0oyi2uPuDnYmlNBcoRfjyXlRdNiWXvXfDyrDjpPMPNxXTAhkukJx6gOLzsANWbecZ+yHUeNjfb2cOOp76aSmhgMhbtMsi7dD7WFkDTHHBQxHuqKTYm5sdok3hHzwDsYRp4LGe+b8zY9DaFj4NZtcMlfIXstPHc+NNfB/o8g7T8QORH6cUrSNsM2YYf5eUovcdGtRYsW8f7779PcbCbmz87OpqCggFdeeYXU1FQmTJjAr371q27PTUxMpLTUVHE9/PDDJCcnc84557Qvvwnw9NNPM2PGDKZMmcI3v/lN6uvr+frrr1mxYgU/+9nPmDp1KgcOHGDZsmW88cYbAKxevZqUlBQmTZrE97//fZqamtpf71e/+hXTpk1j0qRJZGRkDORHcyzdTZAU0+WY7cBVzsdXAn5KqaN6ASmlblRKbVZKbS4pKRmQYEX33tqSR31zK987O5EHLptAfmUDC//yOa9tym1PnP/blkdWaR23Lt/G5/tK+MFLWxgR5su9F4+jrK6ZD3aaUvnkmAAOVzVSUd9CXbOZbrPZOUzrqFJ1m6YayFkPwCTvSqob7TzrnCGs1aFJy6lkekJQj/FbLIpwf5tJwGBKtfZjFMzszsU3Dn5ufnv6Q0Faz8cXbIN/ng1bnu84py1hhzsrlIp2mypv3WoSNsC4S01b9+63IHeDqeb28Da/5/0Cpn4bfrQBLnbWwkVO7jmGkzDsltdsE+bnSV1zK3VN9vaZasQQ9MHdHZMI9JfISXDR73vcHRwczMyZM/nggw+4/PLLWb58OVdffTW/+MUvCA4OprW1lQULFrBjxw4mT+7+i7VlyxaWL19OWloadrudadOmMX36dACuuuoqbrjhBgDuu+8+nn32WX784x9z2WWXcckll7Bo0aIjrtXY2MiyZctYvXo1ycnJfPe73+XJJ5/k9ttvByA0NJStW7fyxBNP8Mgjj/DMM8/0w4c0IO4EHlNKLQO+wAzzPGrdQK31U8BTAKmpqTJ/8CBavimXKXGBTI4NBOCVG87gDx9mcNebOwj182BiTAC78quZEhfIV5llfJVZxthIP55bNoOWVgcPvJvOfzaYKvU5yWFsz6tqn4Xsm9Ni2ODsrd1jCfvgWnC0ADDWVsEFEyL4/QcZpMQH4uPpRk2TvfuErbUp0cbNgIZKqC81yTJrDWS8BxO/efQ5WWvglWvhqn+Zqm3/GIg/w1RP98R5M8EXfzal4IA4CHKOt45wLlRUnA4Vh8DNBrEzzbYxF4Gywvs/BRRMWtxxzXk/73g88wYIGdVxrX42bEvY7WOxZbYz0Y3O1eJt1eGvvfYa06ZNIyUlhd27dx9Rfd3V2rVrufLKK/H29sbf35/LLrusfd+uXbuYPXs2kyZN4uWXX+5xac42e/fuJSkpieRk0yP2uuuu44svvmjff9VVptA6ffp0srOzT/Qtn6xeJ0jSWhdora/SWqcA9zq3VQ5ahKLd1pwKfvJa2hHrKdQ12dlbVMP8MR2Tepw5MoTXbzqTUF8Plm/MZe0+Z+3RFRP54byRfGNyFK/ddCbRgV7EB3sTFWAjq6SOIG93JjjbqTcdNB3RRoX7csaIECL9bT1PCX1gNbj7QHQKqvIQf1w0hehAL254cQtPf2FK2tPiu0nYO1+HZxfCoXVQ7ixdp15v2oG3/afjuOIMqC8HRyt8eA+01MG7t5nSctIciJ4G1XlQe/RymADkbwGrhzlm34fmnLaqa98I8A6Bzc/B9lcg/kxwN3kG72BIOAsaKmDEXAjoWvnUycj54Bve8/6TMGyLpiOd4/T2F9X0eYFz4QLHKAkPpMsvv5w77riDrVu3Ul9fT3BwMI888gibNm0iKCiIZcuW0dh4Yk0qy5Yt4+2332bKlCm88MILrFmz5qRibVvC08XLd/Y6QZJSKhQo11o7gHuA5wY9SgGYduj3dxzmxjkjGBtpSrsZhdVoTXuibeNutXDVtFie+/IgtU12Qn09GR/lz8SYI49TSjErKZi30woYHe5HdKBJVpuyTcKODfLmoSsmUtXQ0n1QWsP+j00S9PSD3PUEeLnz/Pdm8IOXtvDm1jxCfDxICPE++ty0l83vnK9NqRcgNBlGLYRdb5lrt7bA0+eaGcQmXGFKwvN+AWv/bCYoSZwNQYnm3II0SD7fPG6oBA8fsLqbUvzo8007de76jupw8wGYHuH7PzJJeUGXZrNxl5n26im994UZKMO2hD020h+Lon0QvxCd+fr6Mn/+fL7//e+zdOlSqqur8fHxISAggKKiIj744Nirvc6ZM4e3336bhoYGampqePfdd9v31dTUEBUVRUtLCy+//HL7dj8/P2pqjl54YMyYMWRnZ5OZmQnASy+9xNy5c/vpnfaPPk6QNA/Yq5TaB0QAD7sk2NNck72VNRmmBLnNuTIW0L7k8IRu2pevTo3D7lzjeW5yGJYeJgCZNcJ0SRgV4UtUgOm4tT2vCj+bGwFe7sQFe5tE31RjOlm1Kc6Ad2+FykMwaoEpGVflQ6udkWG+vPOjs/nOGQn83+wRHetDN1SYRFxdAFnO9uTcTc72a2WSb9hYaKw0JeayTFOiri2EdY+Z5Dr3Llj4AFg9Tck2arI5t2CbuV5LIzw+y5TG68uh4iDEpsJ5vzZt1iMXHPkBLH4efp4N337Tea1OUr4FF/wOJlyFqwzbEraXh5XEUB/SZV1s0YOlS5dy5ZVXsnz5csaOHUtKSgpjx44lLi6Os88++5jnTps2jWuuuYYpU6YQHh7OjBkz2vf95je/YdasWYSFhTFr1qz2JL1kyRJuuOEGHn300fbOZgA2m43nn3+exYsXY7fbmTFjBjfddNPAvOmToLVeCazssu3+To/fwCyfK1zo6wNl1DWbrgNpOZUsnWmGD6UXVBPo7U5UgO2oc0aF+zI9IYgthyqY26nKvKsznQl7XKQfIT4euFsVzXZH+0Qo7V66CnxCzZCn+nJT8tWtMO27MPVbsOsN87w6D4IS8fF04zdXTOw4v6ES/jbFtDnHzQC0qYLO22Q6cwXGmerosDHm+JIMqHN2YLz2VUhfAWfcbErFZ/7QvG7bWOfQ0R0Je8+7JsFvfwUSnd/5mOnmdX+47ugPwM3T/HTHw8e8lgsN24QNZpB9Wm6lq8MQQ9QVV1xxxJCSF154odvjOldpd25Dvvfee7n33nuPOv7mm2/udkz32WeffUS7eOfXW7BgAdu2bTvqnM6vl5qaetLV62L4WXegjHd3FPDwFRM7Sp9ODofmsc8y+cbkqPak+dHuInw8rEyJC2RbbkX7sbsLqpkQ7X/UNdpcf04Sh8rqmTu654SdGOrDWz88i/FR/liqcogMsJFb3kBcUKdhUuVZkLfRtFW3tkDOOlPyve49M6YZ2qf6pOJQRzV1Z/s+NMtP7l9lqqBjZ8Lkq+G9O0xpO3KSOS5srPldshdqi0zHr8TZpqq8s84Tk8SfCTteg/KDZliYpz80VcMnvwYURE3t8f0PdcO2ShzM0IK8ioae21SEEGKI+8OHGfx3Qw5ldc1H7Xt/52H+8vG+9qFRDodZKWvemHBmJYWwv7iWmsYWWlod7C2sOar9urOLJ0Wx+b6F7dOH9mRafBC2ojT4+2TmeJnXne6ZD5//yVRhp68wB7bUmbbiQ1+bKum4mR0XCXIm7MpD3b9I+jumV/f5DwEapn2no0d2fanpaQ2mI5gtwJSwi/eY7T2VgNvM/blZaOP160yb89m3mervioPmBsB2jDHkQ9zwTtjOoQV7pB1bCDEM7cyraq8lzO6yJKS91cFfP94HwJqMYrTWbMutpKSmifMnRDA1PhCtYUdeFZnFtTS3Orptvz4hOaa6eKK7mYzknNoP4bOHTG/uPSs6hkId+sr8xM44MpH6x5rScEU3CbupxkyuMu4yOOvHcGsapHwHwseBh7OkHDLS/FbKJNmSvVC820xu0puAGNNh7PB2sLiZa09fZvbFTD/+z2IIGd4JO1oS9lDV1zmMRe/kszx1/Wf9Idr6f3Vdw/mtrflkldaxcFw4BVWN7Cuq5fXNuXi5W5k/NpypzrHWabmVx+xwdkIOpwEQbzU9xMNai8z2D35uhkZNv8704t63yiTGxC59QqxuJnF2V8Let8r06h7vnCgvOMkkZosVYqaZbW0lbDDt2IU7oSK7Y3KT3sy4HpLmwuQl4BdhqtsD4iH5gr6dP0QN64Qd7mcj1NeT9IJqPtpdyEvre6h+EYPKZrNRVlYmiaYfaK0pKyvDZju6I5EY3qrqW3hnez5XpsTiZlFklx2ZsJ/8/ABTYgPaO2u9t6OAFdsLuHRKFP42dwK83RkR5sOX+0tZu78Em7uFpFDf7l7q+DlnC4vUpqOXX9Nh0xbdYBI44y6DhLPNMCztMGOUuwpM6L6Enf42+EZ2v4JVW7V4cMd614SNNe3dYErhfWGxwnUr4IrHzXOvILhjJ4y/7NjnDXHDutMZmFL2B7sKeX1LHn42N75zRoKrQzrtxcbGkpeXh0xL2T9sNhuxsbIw1qlmVXohjS0Orjsrga05FWSX1rfvq6xv5mBpHXdfNJaoAC/GRvrxz88P0NKquXZWx9+41IQgXtucB5j1qa0WZdqZV/7MlGDbOoEdS9kB06taKRgx34xXLjNDEOOs5dy2YDS2Lfkw6WpT7V2631RZJzin+LS4dSTazoISzLjszop2Q8ZK08Pb0k15ccb/mclLOifs0DEdj/tawj5FDfuEPSHany/2lRDq60lpbRO1TXZ8ZapSl3J3dycpKcnVYQgxpKXlVuJnc2NidACJId5HVIlnFJqhgmOdS1bOHxtORmEN46P8mRLb0bHsnovGcf74SHw83dqPJXeDWaBi3yozv7VHNxOVtCk/CP+aC83O+QMmXQ2p3wM0eAXhUZvPHeeEw1dVZmz12bd2nNtWqo6e1v1rBI8wPbvLD5pqb61h5V2m09fsn3Yfj3/U0UOn2oZ2uXl13+P8NDKsq8QBfjBnBE9+axq/vMRUlRx2LgcnhBBD2Y68SibHBmCxKBJDfcguq2tvRspw9ssZ5+xYu3Ccmery2lnxRwzbCvLxYOH4CM4cGUKQj4fZuP0VsLhDVQ58+RdY9zg8mmISJ5gFMyoOmUU13rzeLB15/ceQ8m1TXZ3pXBJ2zMVm8pO2dujAzjPXYtqox1wMU3uY+WvKtaYT2Yf3mOc7X4dDX5oOYd7HWHmrq4BYc52wMaaq+zQ27Iuigd4eXDQpqn36vIKqRkZH+Lk4KiGE6FljSysZh2u4YY6p+k0K9aG+uZXimiYi/G1kFNYQ5O1OuHPO7ukJwbx581mkxAUe+8ItjbDrfzDxKjPf9hd/6tiX/jaccwes/rWZKczNBvZGWPxvMyTLFmDm7f76MfCLMh3A0l4203lC9+s7L32l51j8o8xMZB/fD/+7ySTs6GlmkpPjoRRMvsaU0k9zwz5ht2mb3UdK2EKIoS79cDV2h2aKs6d3YohZD+FgaR0R/jb2FNYwNvLISVCOtSxlu30fmA5aU5ZA2DiozDErS2190bQnn3Ub7HrTJM6oyabaesIV5tywMWZSkuy1EDXF9KoGM2wLOiZDOR6zbjY3AdtfMW3qlz56YqXkS/5y/Oecgk6ZhB3hb8OioEASthBiiNvhHHs9Jc60R7ctYJRdWseMxGD2FdawZGZcT6cbWnesNNVm+6umdJw01yTG/3N2+qothC//Zqq7aw7DeQ+aoU5dzfg/Z8KeaqqiwUyM4uZlOoMdLzcPWPIKlOyBsZccHa84LsO+DbuNu9VCuJ+NgqoTW4FJCCEGy468KsL8PIn0NzWD0YFeeFgtHCyrI6e8noaWVsZFHmNM9daX4M9jTBV4G0erWRd67CVHl2JHn2/m9v7w52Z5yeQLu7/u2G+YDmFTl3Yk7JrDpjr8RJNt6CgYd6kk635wyiRsgKhAG4erpIQthBja0vIqmRIb2F7lbbUo4oK9yC6ta+9wNjaqh744WsPX/zA9sMv2d2wv3W+mC+1uNq+YVNNGXZ5lVqjqaXpOqzssuN/0xrb5m3Og+/ZrMehOqYQdHeDF4UopYQshhq7qxhaySuqOGJ4FkBTqy9acSj7cXYhFwejwHhJ27gYo3Wsel+7r2N62QlV0ytHnWN06lpJsm2GsL9rasbv2EBcucUol7KgAGwVVDTLDlhBiyNqVb2btmtQlYd84ZwQOh+adtAISQ33w8uihc9aWF5xzbiso6ZKw3X3M8pLdmXqtmfJzzEV9D7atWlxK2EPCqZWwA71obHFQWS+rdwkhhqb09nm/j0zYM5OC+fgnc7l2VjzXnZloNra2wDu3QN4W87yhAnb/z3QYC0roKGmDSdhRU3ruhT36PPjxFvAK7HuwbSVrSdhDwimVsGMCTQeOAmnHFkIMUemHqwn38yTMOcaa9U/CP2eDw0Gwjwe/vXIS152V6Dz4Hdj2kpkABWDnG2bs9LTvmik7S51t2K12s0BGd9XhJ6OthB0gCXsoOKUSdlSAWWRd2rGFEENBVklte4m6TXpBdftKg+RsgFX3QuEOqMw+8mSt4etHzeP9H0FDpRnTHDHJJOawZJOwHa2mpG1vgOip/fsGkuaaDmvhY/v3uuKEnFoJW0rYQoghQmvNd5/byMWPruWSf6xlZ14VjS2tZBbXmmUwGyrN1KDuznm4izOOvED2WrN05bTroLUZ1vzeLHuZ8m2zPzTZLFNZmXPsDmcnI3oq3LAaPGX2yKHglErYoT6euFsVBVLCFkIMosaWVrblVByxbX9xLXkVDXxjUhSHKxv5w4cZZBbXYndoxkcFmLboqlxY/II5oTj9yIt+/Q/wCYOL/mBmJNvwpBlD3TbhSdsqVqX7TML28IPgkQP7RoVLnVIJ22JRRAbIWGwhxOB6dVMuVz7xNfuLatq3rdlbDMC93xjHt85I4KsDpXyWYbaNj/aHkr2mdD3yXNNGXNKphG1vhgOfmTm03b3MKlpgeni3LZzR1hs8dyPsfhviz+h+yUpxyjjl/nVHh/vx0e4i/rshR4Z3CSEGxfa8SgBW7ixs3/ZZRgljI/2IDvTiiqnRaA1PfZGFt4eVhGBvk6BDk02SDR8LxXs6Lli6DxwtHVXcU5aAVzDM/EHHMd7BpgT+1d+hvgwW/HIQ3qlwpVMuYf/2yklMSwjkF//byeOfZbo6HCHEaaCtY9kHuw4DUNPYwqbscuaNMctijgjzZUpcIDVNdsZF+WOxKJOU29Z6Dh9nnrfazfOiXeZ3xETzOzgJfn4QEs8+8oVDx5jEnvp9M6RLnNJOuYQdGWDjpe/PIjUhiA92FfZ+ghBCnIQmu+lIFuLjQUZhDQdL6/gqsxS7QzN/TFj7cVdMjQZgfJQ/NFZDdX5Hwg4bZzqWlWeZ54U7weppJjo5lpgUU8o+976BeGtiiDnlEjaYtuyzRoWy53A1NY0yiYoQfaGUulAptVcplamUurub/fFKqc+UUtuUUjuUUhe7Is6hZn+R6Uh201zT4euVjTk891U2fjY3pnVaEvPSKdEE+3hwzujQjvHToZ1K2GBWtQIo2m2qya29LKh47v1wy+aOdm1xSjslEzbAzMRgHBq2HKro/WAhTnNKKSvwOHARMB5YqpQa3+Ww+4DXtNYpwBLgicGN0jVWbC/gtU25Pe7fXWCmGl04PoIpcYE89UUW23IquOeicbhbO/7Ehvp6suW+hVwwIbJjhrK2EnZoMqA62rGLdpnx1r1x8zi+mcvEsHbKJuyU+ECsFsWm7HJXhyLEcDATyNRaZ2mtm4HlQNdVIjTQtsxTAFAwiPG5zEvrsnl8Tc/9YdILqvFxdiT7wZwRzB4dyrs/PodrZx09O1jb6lyU7AWLOwQlmece3qadungP1BRBXQlEThyItyOGsV7qW4YvH083Jkb7s+mglLCF6IMYoHMxMg+Y1eWYB4CPlFI/BnyAhd1dSCl1I3AjQHz88J/SsryumdzyeprsrXi6HT1P9+6C6vaOZBdPiuLiSVG9X7Rkr2mf7lzlHT4e8jZB3kbzPEIStjjSKVvCBpiRGExaXiVN9lZXhyLEqWAp8ILWOha4GHhJKXXU3xCt9VNa61StdWpYWNhRFxluKutbcGjIKasHoK7JTqvDDBl1ODR7DlebmcuOR+leM7VoZzNvgOoCePd28zxiwklGLk41fUrYfeiM4qmUetW5f4NSKtG5faZSKs35s10pdWU/x39MM5KCabY72JFXNZgvK8RwlA90XvQ41rmts+uB1wC01usAGxA6KNG5iMOhqahvBiCrtI5mu4M5f/yMF77OBiCnvJ665tajVt46ppZGqMiGsC7zc4+YB+fcDvWl4B8jHcnEUXpN2H3sjHI9UKG1HgX8FfiDc/suIFVrPRW4EPiXUmrQquFTE4JQCt7a2vXvjhCii03AaKVUklLKA9OpbEWXY3KABQBKqXGYhF0yqFEOsupGU7oGyCqpI6OwmrK6ZrbnVgKQUWhmNhsbdRxzbZdlgnY4O5p1Mf9eSJwNoxacZOTiVNSX5NneGQVAKdXWGaXzxLeXY9q3AN4AHlNKKa11fadjbJhOK4MmxNeT75+dxLNfHmTO6FAu6kvbkhCnIa21XSl1C7AKsALPaa13K6UeBDZrrVcAPwWeVkrdgfkuL9On+HSCFfUdw0KzSmrx8TRt2NlldQAcKKkFzMQofZa/2fyOmnr0Pqs7XPcutHVOE6KTviTsvnRGaT/G+cWvAkKAUqXULOA5IAH4jtba3vUFBrKTys8vHMvmQxXc9eYOJsUGEBvk3a/XF+JUobVeCazssu3+To/TgbO7nncqK68z1eEWBQdL62h13p8cLKlDa82Bkloi/W34eh5HxWHOBvAOhZAeFuqQZC16MOCdzrTWG7TWE4AZwD1KKVs3xwxYJxUPNwuPLU2hobmVF77K7tdrCyFObRXOhD020p+s0jrSnFXhNU12SmubySqpY2S4T88XWHErvHE9NNV2bMtZZxbqkMQsjlNfEnZfOqO0H+Nsow4AyjofoLXeA9QCgz5WIS7Ym/MnRPDm1jwaW6THuBCib9o6nKUmBlFeZxL0jEQze9nB0joOlNQyIrSH6vCmGkh7GXa9Ac9dAFX5UFsMFQchrmslpRC960vC7ktnlBXAdc7Hi4BPtdbaeY4bgFIqARgLZPdL5Mdp6cx4KupbWLVb5hcXQvRNW8Ke3mmK0StSYgDYlF1OTaOdkWE+cHgHvHbdkSXp7C/BYYc5P4OKQ7Dix5Cz3uyLP3PQ3oM4dfSasJ1tzm2dUfZgpibcrZR6UCl1mfOwZ4EQpVQm8BOgbejXOcB2pVQa8D/gh1rr0n5+D31y9shQ4oO9eWVjjiteXggxDJXXtRBobWRKp8FrF0+Mwt2q+GRPEeDscLbrDUh/2yx12ebAp2a96zk/g7l3wYHV8PWj4GaTlbXECelTG7bWeqXWOllrPVJr/bBz2/3OnqNorRu11ou11qO01jPbepRrrV/SWk/QWk/VWk/TWr89YO+kFxaLYsnMONZnlbcPyRBCiGOprG/mzx5PE//xjbhZFCPCfAjy8SAu2Lu9PXtkuC/kbTEnfP0PU/UNJmEnngNunmZSFL9oM5NZ9DQzB7gQx+mUnumsq2/NTCDS38Ydr6ZR33xUZ3UhhDhCeV0zSeowlvwtTIv1Y26y6RQ7ItQHrcHmbiHK1x0KtkHyhWZ89cf3myrwskwYea65kLuXKWWD6XAmxAk4ZecS706Atzt/uWYK33pmA795L53fXTXZ1SEJIYawivpmgqkCewOvLI5EOdenTgo1PcNHhPpiKdsHLXUw4UqInARf/KljPvAR8zsulvJtswb21G8N9tsQp4jTqoQNcNbIUH4wZySvbMyV9mwhxDFV1TXi7zBTG1tL0rFYzFCsJGfP8JHhvpDvrA6PmW5mKrvwD1BTaKYXbVs+E8ykKOfeZ1blEuIEnFYl7DZ3np9MRmE19729C3erBV9PN0aF+zIq/DhmKxJCnPrqSrHgMI+LdsN4s+JoYqiZgGlEqI9J2LYACB5pxlafcZOZWrS1RcZai351WiZsN6uFx66dxqInv+bO17cDMCrcl09+MtfFkQkhhgqHQ+PeWAJt/cOKdrfvGx/lT3SAjbNGhsBHm01HMkunCsvQ0YMbrDgtnJYJG8DX043XbzqTbTmVfL6vhGe/PEhxTSPhfkdNxCaEOA3VNNoJxbnSn38sFO1q3xfo7cHX9yyA5nooSodz7nBRlOJ0ctq1YXfmZ3NnTnIYl0w2i4JsPFju4oiEEK5QXN141Lby+mbCVKV5MnK+WRKzugCeOAu2LzfbC3eAboWYaYMWqzh9ndYJu83EmAC8PaxsyJKELcTpJqukljN+t5r3dhQAUFTdyGubcimvayasrYQ90tnb+43vQ/Fu2LfKPC/caX53t/KWEP1MEjbgbrUwPSGIDQfLej9YCHFK2ZpTiUPDy+vNqJHff5DBXW/u4IOdhwlTlbS6+0LsDHNwzjrzu616vHAneAWBf7QLIhenG0nYTmeMCGFfUW37cnpCiNPD7gJTil6XVcbm7HLe3W5K2i+uO0SYqsThEw4BceDpb5LzjP8zk6K0NJjEHTFReoOLQSEJ2+mMEcEAbJRSthCnlfSCauKCvVAKbvrPFlq1ZsmMOJpbHYSpKix+ESYhX/AwXPUMJM0xM5oVpUPxHpOwhRgEkrCdJsUEYnO38OqmXKoaWlwdjhBiEGitST9czZzRYZwzKpTS2mYuGB/JPRePw9fTrSNhA0z7Loxe2JGg96yAlnqIlIQtBockbCcPNws/mjeKNftKWPDnNe0T+wshTl255Q3UNNqZEB3At2YloBTcMCeJAC93bp43kkhLNco34siTgpLA3Qd2vGaeSwlbDBJJ2J38eMFo3r3lHBpbHLyxJdfV4QghBlj6YdN+PSHanwsnRrLhngVMTzDNYz86JxYfXQu+4UeeZLFAxHioKQBlhbCxgx22OE1Jwu5iYkwAI8N8OFRW7+pQhBADbHdBNVaLYkykHwDh/p0mTqorNr+7lrCho1QdOhrcZbIlMTgkYXcjIcSH7LI6V4chhBhguwuqGRnqje3garA3mY2VObDhKbOABxxdwoaOdmupDheDSBJ2NxJDvMmvaKDZ7nB1KEKIAbS7oIorA/bDfxfD5380G1fcCh/8DNY/YZ53l7AjJpnf0uFMDKLTdi7xY0kI8cGhIa+inhFhsoKXEKeiw1UNFFU3sSDkC7Nh3WMQMhKyPgM3L9j9P7O9uyrxmGlwxg9h4qLBC1ic9qSE3Y22pfOkHVuIU8s7afl8tNtUda/dX4onzYws/RRGngtaw9s3m3Wsl/yn4ySfsKMvZHWHC38HgXGDFLkQkrC7lRDiA8AhaccWpxGl1IVKqb1KqUyl1N3d7P+rUirN+bNPqbaVMYaHzzKKuW15Gve8tZOWVgdr95dyhc8urC21cPZtcOYPzYHz7oZRC2HsJRAQb5KzEEOAVIl3I8THA19PN7KlhC1OE0opK/A4cB6QB2xSSq3QWqe3HaO1vqPT8T8GUgY90BOUW17P7a+mkexVQ0WdnS/2lfBVZikveG8AHQmJsyH+TEg4G0YuMCd981loqHBt4EJ0Igm7G0op4oO9pYQtTiczgUytdRaAUmo5cDmQ3sPxS4FfDVJsJ+3vq/djb23l3dC/sV1b+OWH8dTX1TBBr4eZN4DFan5Gn9dxkrsN3KNcF7QQXUiVeA8SQ72lDVucTmKAzrMF5Tm3HUUplQAkAZ/2sP9GpdRmpdTmkpKSfg/0RKQXVLMougzP8j1MsWSxv6iK8eoQVkeLmRtciGFAEnYPEkJ8yK2ox94qQ7uE6GIJ8IbWurW7nVrrp7TWqVrr1LCwbjpsDbJWhyazpJZvaNMb3NPRQJI6zMKAfHOArGUthglJ2D1IDPGmpVVzuKrR1aEIMRjygc5dnmOd27qzBHhlwCPqJ4fK6nDYm5lc8TGEjQPgqogSFgYWgG8k+Eu1txgeJGH3oK2n+AMrdnPDi5vJKKwG4Jm1Wdz00ha01q4MT4j+tgkYrZRKUkp5YJLyiq4HKaXGAkHAukGO74TtK6pljmUHtuZyOPdecPPiR2PrSLZnQvSw6TcnhHQ660lyhB82dwsbD5ZjsSiWPLWeK1NieP6rbMCs8hMf4u3aIIXoJ1pru1LqFmAVYAWe01rvVko9CGzWWrcl7yXAcj2M7lj3F9VwkWUj2isIlXyhmZ3s0JdQug8mftPV4QnRZ5KwexDs48G2X56Pp5uF3Ip6rn16A89/lc20+EC25lSyNadCErY4pWitVwIru2y7v8vzBwYzpv6wt6iGBR4FqKgpZkx11FTY9LTZKSVsMYxIlfgxeHlYsVgUCSE+vHbTmTxw6Xj+e8MZeHtY2ZYj4zOFGA72F9aQpAsgZLTZED21Y2fnx0IMcVLC7qOYQC+WnZ0EwJTYQLblVro2ICHEMWmtsTs0tWW5eLnXQ2iy2dHWK9w/pvuFPYQYoqSEfQJS4gNJL6imseXoUS09bRdCDJ6NB8tJ+c3HPL02i3hdYDaGOkvYYWPAzSbDucSwIwn7BEyLD8Lu0OzMrzpi++GqBi75x1re3JrnosiEEACf7S2msr6FP364l5GqLWE7S9hWd7j8cZh7l+sCFOIESJX4CZgaHwjAtpwKZiQGt2/fkFWOQ0OhjN0WwqV25VcxMsyHIG8Pxhw+jHb3QflHdxwwSZbFFMOPJOwTEOrrSXywN9tyKo/YvuFgOQDldc0uiEoIAabtendBNeeNi+DBKybgeLEB1TIalHJ1aEKcFKkSP0FnjQzh04xiDpZ2LBCyKdsk7Ip6SdhCuEpBVSPldc1MjPHH082KV1VWR3W4EMOYJOwTdMd5yXi4Wfj5mztwODSltU1kFtcCUFHX4uLohDh97cqv4hzLTpZ8fi7kboKqXEnY4pQgCfsERfjb+OU3xrPxYDkvrT/EZmfpOszPU0rYQrjQ7vwqvuv2Me6NZbD8WrOxrYe4EMOYJOyTsDg1lvljwnjwvXSe/DwLm7uF2aNCpQ1bCBc6mJPDfEuamcWsrthslBK2OAVIwj4JSin+ce00JsYEsD23kpS4IML9bVTUN8viIEIMIq01z36+j/VZZcQVfIg7drjsHzDlWvDwheARrg5RiJPWp4StlLpQKbVXKZWplLq7m/2eSqlXnfs3KKUSndvPU0ptUUrtdP4+t5/jdzlfTzdeWDaDOclhLJkZR5C3Oy2tmrpmmTxFiMGyNzuXb346j6bnr+BS+yrKfZMhcpJJ2j/aCO42V4coxEnrNWErpazA48BFwHhgqVJqfJfDrgcqtNajgL8Cf3BuLwUu1VpPAq4DXuqvwIeSIB8PXvz+TC6fGkOQjwcAFVItLsSgydvxBYGqjnPcdjPOkouefI3ZYXWDgBjXBidEP+lLCXsmkKm1ztJaNwPLgcu7HHM58G/n4zeABUoppbXepnXbvIDsBryUUp79EfhQFextEra0YwsxeJoObcSBwvrD9bDgV4TM/YGrQxKi3/UlYccAuZ2e5zm3dXuM1toOVAEhXY75JrBVa93U9QWUUjcqpTYrpTaXlJT0NfYhqa2EXS49xYUYFFprgiu2U+iZBGHJMPsn4Onn6rCE6HeD0ulMKTUBU03e7W2v1voprXWq1jo1LCxsMEIaMMFSJS7EoMopq2OcYz91YVNdHYoQA6ovCTsfiOv0PNa5rdtjlFJuQABQ5nweC/wP+K7W+sDJBjzUtVWJV9TL5ClCDKiCNCjOIH3XNgJVHX4jz3B1REIMqL4k7E3AaKVUklLKA1gCrOhyzApMpzKARcCnWmutlAoE3gfu1lp/1U8xD2l+NjcsypSwc8rquW35Nuqb7YCpupPhXkL0j+ZXl9H8/GU0ZHwMQPi4c1wckRADq9eE7WyTvgVYBewBXtNa71ZKPaiUusx52LNAiFIqE/gJ0Db06xZgFHC/UirN+XNKrxhvsSiCvD0or2/m/Z2HeSetgC/3lwLw+w8z+OaTX7s4QiFOAQ0VeFQdxKOhiIsOP0mj8sISPtbVUQkxoPq0WpfWeiWwssu2+zs9bgQWd3PeQ8BDJxnjsBPk40FFXTPVDaZafF1WGeeNj+DdtAKKappoaXXgbpU5a4Q4UU052/AEsj3HkNi0l8qIM7FZrK4OS4gBJVljAAR7e1BR38zugmoA1meVk1lcS0FVI60OTV5Fg4sjFGJ4K9u/AYDsBf+C8AkEplzp4oiEGHiyHvYACPJxZ1d+NfmVDQR5u7PncDVvp3X008suqyMp1MeFEQoxvLXmbSHbEUHSyGSYKc1M4vQgJewBEOTtQX6lKUV/+4wEAJ77MptQXzNnzKFOa2gLIY6fb/ku9qgRxAV5uzoUIQaNJOwB0DZ5CsC1s+LxcrfS0NLKZVOi8fGwkl1W78LohBjm6soIaj5Mse84LBbl6miEGDSSsAdA21jscD9PogK8SE0MAmDumDASQnw4VCYlbDH09LbIj/OYq5VS6Uqp3Uqp/w5qgGn/RT9xJnrnawA0R0wZ1JcXwtUkYQ+AthL2xJgAAC6aGEWYnyezkoJJCvU5ooStteaZtVkclGpy4UJ9WeRHKTUauAc4W2s9Abh9MGPM/+LfqOJ01IfmXsInYfpgvrwQLicJewAE+7gDMDHaH4ClM+PYcM8CbO5WEkK8yS2vx97qACCvooGH3t/DP1bvd1m8QtC3RX5uAB7XWlcAaK2LBy26VjshFdv5tHUqxSqUdEcCI+OiB+3lhRgKpJf4AIgO9AIgJcFUhSulUM6mtsQQH+wOTUFlI/Eh3mzNqQDgo/QiGltasbnLWFLhEt0t8jOryzHJAEqprwAr8IDW+sOuF1JK3QjcCBAfH98/0RXvxqYbeMdxNrc2pOBJC59G+vfPtYUYJqSEPQDGRvrz4e2zmZd89EImCSGmV2u2sx17W04lALVNdtbsHd4rlYlTnhswGpgHLAWedk4/fISBWMzHcWg9AFGT5uPrH4wtMJIAb/d+ubYQw4WUsAfI2B7u/tvGX5uOZ2Fsy60kNSGIg6V1vLejgAsnRg5ilEK068siP3nABq11C3BQKbUPk8A3DXRwjVlfUamDiU8awwvzAqlptA/0Swox5EgJe5CF+Xni5W7lYGk9jS2tpBdUkZoYzIUTI1m9p7h9oRAhBllfFvl5G1O6RikViqkizxrwyLTGmrueLY5kRoT5MDbSnxmJwQP+skIMNZKwB5lSioQQb3YXVLG7oIqWVk1KfCCXTI6moaWVL/Z1VIuX1jbx25V7eOi9dBdGLE4HfVzkZxVQppRKBz4Dfqa1Lhvw4Kpy8WwoYpNjDCPCZIZAcfqSKnEXuGxqNH/8cC9/WrUXgJS4QAK9PfB0s7DxYAUXToxiQ1YZy57fRENLK0rBHecl4+Mp/1xi4PRhkR+NWY3vJ4MRT0lNE79f/hEPe7+KDUh3G0+Yc7ZAIU5HUsJ2gRtnj2BSTADrs8qJCfQi3N+Gh5uFKXGBbDlUDsDyTbnY3C3ce/E4tIaMwhoXRy3E4MpI387Ded/DLXMV7/gtpTl0AkrJzGbi9CUJ2wXcrBYeWTwFd6tiunPoF0BqQhC7C6qpb7azdn8ps0eHcdEk0wkt/XC1q8IVwiWshduwqRZ+5PEgf2hezIgwX1eHJIRLSR2ri4yJ9OO1H5xJZICtfVtqYhBPrNEs35hLaW0Tc5LDiAn0wt/mxh5J2OI0o6sLAFhXFUI1jZKwxWlPErYLpcQHHfF8mvP5E2sOADB7dChKKcZH+5NeIAlbnF6stUXUa09aPfyg2SEdzsRpT6rEh5BAbw9Gh/tSWtvE2Eg/IvxN6Xt8VAAZhdW0OrSLIxRi8Hg2FFFmCeaiSWYK0hGhUsIWpzcpYQ8xqYlB7C+uZfbo0PZt46L8aGxxcLC0jlHh8kdLnB68m4qptIZy87yR2NwtJEfI/31xepMS9hDTNiHEnE7Tmo53LiIi7djidBLQUkKtRxgjw3x56IpJuFnlz5U4vUkJe4i5dEo03h5unDOqo4Q9OtwPd6si/XA1l06RFYrEaUBrghzlpNvCXR2JEEOGJOwhxt1qOWo+cQ83C6PC/aTjmTh9NFTgSQt2nwhXRyLEkCF1TMPEhGh/duVXYSabOlKrQ7PuQFm3+4QYjprK8wDQflEujkSIoUMS9jAxJTaAsrpm8isbjtr3u5V7WPr0etYdGPhpnYUYDLUlOQBYAmJcHIkQQ4ck7GFicmwgADvzqo7Y/k5aPs98eRCAbbmVgxyVEAOj0VnC9gyShC1EG0nYw8TYKNPxbHunhF1W28TP39zBzKRg4oK9jkrmQgxXLZVmljPfUEnYQrSRhD1MeLpZGRvpz468yvZtGw6W09ji4O6LxjI1Loid+ZKwxalBVxdQqv0J9JOx10K0kYQ9jEyODWBnfhUO54xnm7LLsblbmBgdwOSYAPIrGyitbXJxlEKcPGtdIUU6iBAfWU5TiDaSsIeRybEB1DTayS6rA2BzdgVT4wLxcLMwMSYAQErZ4pTgWV9EEcH4e8nIUyHaSMIeRto6nu3Iq6K2yc7ugqr2mdEmxpjZ0HZ1asfOKKxm48HyQY9TiJPl3VxCpTVU1r8WohO5fR1GRof7YnO3sPlQOaG+njg0pDoTtp/NnRFhPuxwlrC11ty+PI2SmiY23bsQi0X+8Imhr6i6kWAb+NkrqPUM6/0EIU4jkrCHETerhQsnRPLqplxKapqwKJgWH9i+f3JMAOuzTIl6Z34VGYU1AOwuqGZSbIArQhaizxwOzfl//YIfTGjlh0C9l8xyJkRnUiU+zPzq0gkEeXuwancR46L88bO5t++bHBtIYXUjm7PLeXVTLh5u5p/3i/0lrgpXiD6rbmyhqqGFup0rASgITHVxREIMLZKwh5kgHw/+tHgK0LGyV5tFqbHEB3vz41e2sSKtgEsmRTEh2p/P90nCFkNfWV0zALP1JjIccejARNcGJMQQIwl7GJqbHMZ/rp/FLeeOOmK7v82dx6+dRlltMzVNdq6eEcec5DC2HqqgprHFRdEK0Tfldc0EUsMMlcHHjukE+Xi4OiQhhhRJ2MPUOaNDCfU9eozqpNgA/rR4MoumxzIrKZg5o8OwOzRfd5lnvLbJzmd7i9vHdAvhamW1zZxr2YZVaT5unU6IJGwhjiAJ+xR0+dQYHlk8BaUU0xOC8PGwsnpPEWA69vz1432c9bvVfO/5TXyUXuTiaIUwyuuaOc+6BbtPJCMnn83ZndaEF0JIwj7lebhZuDwlhje25JGWW8m/vsji76v3c8aIEDysFrbmVJz0a+zKr+KtrXntz6saWmhsaT3p64rBpZS6UCm1VymVqZS6u5v9y5RSJUqpNOfP//Xn65fXNjLbshOVfD5/XTqdUeEyLakQnUnCPg3cfdFYIvxt/Ojlrfz5o718Y1IU//rOdMZH+5OWU3nMc1/46iDrs469bOdjn2byi//tbF+Pe+lT63nwvfT+Cl8MAqWUFXgcuAgYDyxVSo3v5tBXtdZTnT/P9GcMNTWV+KpGrKGjej9YiNNQnxJ2H+68PZVSrzr3b1BKJTq3hyilPlNK1SqlHuvn2EUf+dvc+e1Vk8ivbCAywMZvr5qEUoqpcYHsyK/E3uro9rzyumYefC+dR1btPeb1d+ZX0djioKK+hVaHZl9RDVsPnXzJXQyqmUCm1jpLa90MLAcuH8wAmmqcN4ZeQYP5skIMG70m7D7eeV8PVGitRwF/Bf7g3N4I/BK4s98iFidk/phwnvzWNF66fhYBXmbsdkp8II0tjvYJVrr6NKMYh4YtORUUVzcCHNVJray2ifzKBgAKKhsormnE7tAcKKmlpYcbATEkxQC5nZ7nObd19U2l1A6l1BtKqbjuLqSUulEptVkptbmkpO9DCltqnTd5krCF6FZfSth9ufO+HPi38/EbwAKllNJa12mtv8QkbuFiF02KIinUp/15Spz5w5iWW9nt8R+nF+LtYUVr+Ci9iF35VaQ+/AlvbOlor+682EheRQP5FSZ5t7RqskvrBuBdCBd6F0jUWk8GPqbjO38ErfVTWutUrXVqWFjfpxd11DnnvZeELUS3+pKw+3Ln3X6M1toOVAEhfQ3iRO/IxcmJC/Yi2Mej24Td2NLKF/tKuWpaDEmhPqzaXciv391NeV0zv3hrJ9ud5+zstNhIQWVDe2kb6LHkLoakfKBziTnWua2d1rpMa922fuszwPR+jaBRSthCHMuQ6HR2onfk4uS0tWN3l7C/yiyloaWV88ZHcsGESNbuL2VTdgV3XTiGMD9PbvrPFirrm9mRX8WIMB9s7hYKKhvIc5awLQr2FUnCHkY2AaOVUklKKQ9gCbCi8wFKqahOTy8D9vTXi2utcWusNE9sgf11WSFOKX1Z/KPXO+9Ox+QppdyAAODYXYvFkDA1LpBPM4qpqGsmyMeDh95L582tedjcrfh6unHGiGD8bW788/MDjI/y5wdzRjJ7VBiXP/4lj67OZGdeFWeMMFOkFlQ1UN/SSqC3O6G+nmQU1tDS6uDptVlcnRrX7UQvYmjQWtuVUrcAqwAr8JzWerdS6kFgs9Z6BXCrUuoywA6UA8v66/Vrm+z46lrzRErYQnSrLyXsXu+8nc+vcz5eBHyq28b4iCHtvPERKAV/+2Qfew5X89xXB4kP8SHYx4PrzkrA083KlNhAbp43kkcWT8FqUUyKDeCaGXH8e102hdWNTIoNJCbQi3xnG3ZskBdjIvzYV1TDu9sL+OOHe/nf1q73eD2rbmzhRy9vZc3e4gF856IrrfVKrXWy1nqk1vph57b7nckarfU9WusJWuspWuv5WuuM/nrt8rpmAlQdrRYPcPfqr8sKcUrptYTdxzvvZ4GXlFKZmDvvJW3nK6WyAX/AQyl1BXC+1loG6Q4R46L8+e4ZCby0/hDrs8rxs7nz7+/NINC7Y1pIi0Xx8wvHHnHeHeclsyKtgLrmVibHBrC/qIY9h2uoa25lZJgPyRF+rNx1mCfWHABgd0EVffX7DzJ4f+dhPt5TxLPXpTJ7tDSTnOrK6poJoBa7RyBWJWu3C9GdPrVh9+HOu1FrvVhrPUprPVNrndXp3EStdbDW2ldrHSvJeuj56QVjCPH1ZG9RDbcvHH1Esu5JuJ+NO85LJsDLnQnR/kQHelFa20RueT0xgd6MifRDa8gsrsXbw3pEb/JjWZ9Vxn835LB0ZhwjQn244cXNZBbXnuxbFENceW0zgaoWh7RfC9GjIdHpTLiWv82dv1w9hatTY/n2GQl9Pu//Zo9g070L8fZwIybQVGM22R3EBHkxJtIPgDA/T5adlUhWaR11TfZjXk9rzX1v7yI+2JtfXjKeF78/k1aH5pWNOSf+5vqgsaWV25Zv46AMQ3MZs1JXHRbvQFeHIsSQJQlbADB7dBh/XDQFd+vx/ZfwcDPHRwd2tDvGBHoRH+zNyDAfbj13FNMTgtAa9hyuPua18isbyCyu5fpzkvD2cCPc38bCcRG8vS3/mJOwOBy6fVrUE7Erv4p30gr474ZDJ3wNcXLK6kwJ2+oT3PvBQpymJGGLfhHTKWHHBnlhtShW/3Qe3zkzkYkxAYCZZOWT9CJm//FT/v119lFJuG142bT4jl7C35wWS1ldM2v29jw+/+JH1/Lo6swTjj3LWbJevUc6ublKeV0TAaoON58+T98gxGlHErboF5EBNtr6CnVO3gAR/jbC/DzZmVfF7z/MoKi6iV+t2M3if66jtdNUp9tyKvF0szA2yq9929wxYYT6evDGlo65e1btLuTZLw8CUFLTREZhzVELlNQ12Vm+MeeI6/ekrSo8q7SOrBJpL3cFU8KukyFdQhyDJGzRLzzcLIT7eeLtYSXQ2/2o/ROj/Xlvx2Eyi2v506LJ/OrS8aTlVvLF/o6S87acCibFBBxRLe9utXDF1Bg+zSgmt7yestom7nx9O39alUFLq6O993nXSVqeWXuQu9/aySd7el/v+2BJXXvMn2ZIKdsVqmtr8aYRvAJdHYoQQ5YkbNFvogO9iAn0QnUzLGdiTADNrQ6SQn24ZHI035qVQKivBy+vN+3GzXYHuwqqmRoXeNS5y85OxOZu5Ycvb+X3H2RQ02inscXBnsPV7C4w7eJldc2U1ppZM1taHbzsbI9ekVbQa9wHS+tITQhmTIQfH+0u4i8f7eXcP6/hjx9mUFgl0+APhuZa5zzi0ktciB5Jwhb95vaFyfzsgjHd7psSGwjAzXNHYrUoPNwsXJ0ax6cZxeRXNrDncDXNdgcp8UdXicYGefOXq6eyM7+K17fkcdHESAC2HKo4Ynz3Pufc5at2F1Jc00RyhC+f7CmiprGlx5gdDs3BsjpGhPmwYFw4G7PLefTTTHw8zOxulz32Jc32nju85VXUs+DPa9jVx2Fronu6XuYRF6I3krBFv5mbHMb5EyK73Td/bDjPf28Gi6bHtm9bOjMeDby8/hDbcswf7JT4wG7PP298BLcvHE1iiDe/u2oS0QE2tuZUsrugmtQE80d+r7Na/N9fZxMf7M1vr5xEk93Bqt1FHCqrY0s3a3QXVDXQbDcl/ytTYhgZ5sMji6fw7o/P4YlvTae4pomvDpQCcMeraTz26f4jzt94sJwDJXU8+F76SfVUP91ZGp03PJKwhehRX+YSF+KkWS2K+WPCj9gWF+zNeeMieGLNAQK93Qn38yQqwNbjNW5fmMxtC0ajlCIlIYivM0spq2tm8fRYDpTUsq+ohozCajZlV3DvxeOYnhBEXLAXf/loL6W1zbRqzb+/N5NzRodSXN2Ij6cbWSWmw1lSqA+jI/xY/dN57a83f2wYfp5urNxxmEh/G//blk9CiDe3nDu6/Zh9RaaT2saD5Xyyp5jzxkf046d2enA4NO4tleCOJGwhjkFK2MKl/nrNVH5yXjL2Vs05o0O7bf/urG3/9PggyuqaAZgQHUByhB97C2t4dVMuHlYLi6bHopTim9NiKahq5IKJkYwK8+WHL2/h/nd2cdbvPz1ispQRYT5HvZanm5XzxkfwUXoRzzl7pR8qqz+iXXt/UQ0jw3wYEebD7z7Yg/0Y48VF92qb7QRo56Q1krCF6JGUsIVL+Xi6ceuC0dw4ZwSW45hDelpCxx/2CdH+jIn0462t+RwsreO8CREE+ZjpVX80fxSXTI5mVLgvueX1XPbYl7y0/hBjI/35ZE8xNY12fD3dCOthJbGLJkXx1rZ8Xt+Sx+TYAHbkVbExu5zLpkQDsK+4himxgZw3PoLblqexNaeSmUlm8o+6JjsPvZ9OUXUTF02M5NIp0djcrSf6UZ2yqhtaCFRtK3UFujQWIYYyKWGLIcHmbm2fNa0vxkf54+lmIdTXk3B/G2Mi/ahtslNR38LVqR2rwbpbLYwK9wVMFfzbPzqbD2+bw/IbzsDHw8qGg+Ukhfr0WLKfPToUX09zX/vwFZPw9XRj40Ez5ru+2U5ueQPJEX7MGxOORcGXmaa9u6CygcX/XMerm3LJOFzNz97Ywfee39ReAi+paZI2b6fqBjv+qg6NAs8AV4cjxJAlJWwxLHm4WZg9OgxfT1NiHRNhJluJDrBxzqjQHs9LCOmo+v7WGQk89UUWSaFHV4e3sblbWTozjtzyBibFBjA9IYgNWWYIUtuiJMkRvgR4uTM5NpCvMkv5yXnJ3P3WTnLK63lu2QzmJofxysZcfvG/nfzugwya7Q5eWn+Ib0yK4rdXTSLA6+hx66eT6sYWAqnF7hGAu0XKEEL0RBK2GLb+9Z3ptJWLkyP98HCzsGRmPFZL36rWrz8niZfWHWJCtP8xj7v3G+PbH89MCuZPq/ZSVtvU3uFstPNm4ZxRoTz5+QHSC6r5Yl8Jty8czTxnR7trZ8Wzq6CqfYa2heMiWLW7kG05Ffz8orFcOjkaSx/jPtVUNbQQqOpkpa4hqKWlhby8PBobZT6C/mKz2YiNjcXd/fhv1CVhi2Grc2L2t7mz+idzj9nLvKsIfxuf3zWPoD4sJ9pmlrN9elN2BfuLavCwWkgI9gbg7FGhPPZZJne9uR2L4oiqeYBfXToeb3crs5PDmJscRlpuJfe8tZPblqfx/FfZvPqDM/B0O/3auKsbWginVjqcDUF5eXn4+fmRmJjYa4dQ0TutNWVlZeTl5ZGUlHTc50v9kzhlxAV743acq42F+9mOa4WySbEBeHtYeWl9NhmFNYwI82l/zWkJgdjcLezKr2ZuctgRK5iB6XV+3yXjmZscBsDUuEDe//E5/OaKiaTlVvLmlnwA1h0oa18I5XRQ3WgnQNVi8ZaEPdQ0NjYSEhIiybqfKKUICQk54RoLSdhCHAdPNyu/vGQ8X2WW8fm+EpIj/I7YNzPJrDa1ZGZ8n65nsSi+PSueKbEB/PPzA+SW1/Oj/27lvrd34ujDwiWnguqGFgKpk6U1hyhJ1v3rZD5PSdhCHKclM+JY7JyxLTnC94h9V6fGcuaIEM4dG97dqd1SSnHLuaPJKa/n8se/otnu4NElKadNm3Z1YwvBllos3rK0phDHIm3YQhwnpRS/uWIiwb4eXD415oh9l0yO5pLJ0cd9zQVjwxkb6UdGYQ3//PY0RoT59n7SKaKmvgF/6sBbStjiSGVlZSxYsACAwsJCrFYrYWGmSWnjxo14ePTc/2Tz5s28+OKLPProo4MS62CQhC3ECbC5W7nnonH9dj2LRfHYtSlkFtdy4cSofrvucNBa55zjXUrYoouQkBDS0tIAeOCBB/D19eXOO+9s32+323Fz6z6NpaamkpqaOhhhDhpJ2EIMEaPC/RgV7tf7gaeaejMRjfQSH9p+/e5u0p3L2faX8dH+/OrSCcd1zrJly7DZbGzbto2zzz6bJUuWcNttt9HY2IiXlxfPP/88Y8aMYc2aNTzyyCO89957PPDAA+Tk5JCVlUVOTg633347t956a7++l8EgCVsI4VKWxrYStlSJi77Jy8vj66+/xmq1Ul1dzdq1a3Fzc+OTTz7hF7/4BW+++eZR52RkZPDZZ59RU1PDmDFjuPnmm09oLLQrScIWQgCglLoQ+DtgBZ7RWv++h+O+CbwBzNBabz7Z13VvNDPHSZX40Ha8JeGBtHjxYqxWM2dBVVUV1113Hfv370cpRUtLS7fnfOMb38DT0xNPT0/Cw8MpKioiNja222OHKuklLoRAKWUFHgcuAsYDS5VS47s5zg+4DdjQX6/t2dK2FraUsEXf+Ph0TCf8y1/+kvnz57Nr1y7efffdHsc4e3p2LPBjtVqx2+0DHmd/k4QthACYCWRqrbO01s3AcuDybo77DfAHoF/mqrS3OvC2OxO2lLDFCaiqqiImxozWeOGFF1wbzACThC2EAIgBcjs9z3Nua6eUmgbEaa3fP9aFlFI3KqU2K6U2l5SUHPNFa5vsBKoa7BZP8PA+wdDF6eyuu+7innvuISUlZViWmo+HtGELIXqllLIAfwGW9Xas1vop4CmA1NTUY07XVt1gJ5gamj2C5I+ROKYHHnig2+1nnnkm+/bta3/+0EMPATBv3jzmzZvX7bm7du0aiBAHnJSwhRAA+UDn1Upindva+AETgTVKqWzgDGCFUuqkBrqalbpqaZWVuoTolSRsIQTAJmC0UipJKeUBLAFWtO3UWldprUO11ola60RgPXDZyfYSr25sIUjV4rBJhzMheiMJWwiB1toO3AKsAvYAr2mtdyulHlRKXTZQr1vd0EIQNSgZgy1Er6TZSAgBgNZ6JbCyy7b7ezh2Xn+8pilh12DxDe2PywlxSpOELYRwmZr6JgKpw+4nCVuI3kiVuBDCZRpryrEojbskbCF6JQlbCOEyrXWlACiZNEV0Y/78+axateqIbX/729+4+eabuz1+3rx5bN5s+kFefPHFVFZWHnXMAw88wCOPPHLM13377bdJT09vf37//ffzySefHGf0/U8SthDCZXRd2zzi0ulMHG3p0qUsX778iG3Lly9n6dKlvZ67cuVKAgMDT+h1uybsBx98kIULF57QtfqTtGELIVzG0rbwh8wjPvR9cDcU7uzfa0ZOgou6XWMGgEWLFnHffffR3NyMh4cH2dnZFBQU8Morr/CTn/yEhoYGFi1axK9//eujzk1MTGTz5s2Ehoby8MMP8+9//5vw8HDi4uKYPn06AE8//TRPPfUUzc3NjBo1ipdeeom0tDRWrFjB559/zkMPPcSbb77Jb37zGy655BIWLVrE6tWrufPOO7Hb7cyYMYMnn3wST09PEhMTue6663j33XdpaWnh9ddfZ+zYsf36cQ2/ErajFRoqXB2FEKIfLEhwLm8oVeKiG8HBwcycOZMPPvgAMKXrq6++mocffpjNmzezY8cOPv/8c3bs2NHjNbZs2cLy5ctJS0tj5cqVbNq0qX3fVVddxaZNm9i+fTvjxo3j2Wef5ayzzuKyyy7jT3/6E2lpaYwcObL9+MbGRpYtW8arr77Kzp07sdvtPPnkk+37Q0ND2bp1KzfffHOv1e4nYviVsEsy4MmzIHgERE6GgFjzZdcO8PCB0NHgFwVWT/D0BVsguHmCUq6OXAjRxYRA51KIUiU+9B2jJDyQ2qrFL7/8cpYvX86zzz7La6+9xlNPPYXdbufw4cOkp6czefLkbs9fu3YtV155Jd7eZq76yy7rmFZg165d3HfffVRWVlJbW8sFF1xwzFj27t1LUlISycnJAFx33XU8/vjj3H777YC5AQCYPn06b7311sm+9aP0KWH3tk6uUsoTeBGYDpQB12its5377gGuB1qBW7XWR/YgOF5eQbDgfijYBoU7YN+HYO9t4SAFVg9w9wJ3b3DzAIt7RxLXDpPgvYJMcgfwCYPA+I7nSoHW4LCb63kHgy3AXFNZQbdCUw00VpubhthUaKqF+jLwCjSv21Rtzg2IBWunhdOb68313WxyYyFOL/Xl5rvo4evqSMQQdfnll3PHHXewdetW6uvrCQ4O5pFHHmHTpk0EBQWxbNmyHpfU7M2yZct4++23mTJlCi+88AJr1qw5qVjblvAcqOU7e03YndbJPQ+zgs8mpdQKrXV6p8OuByq01qOUUkswy+9d41xPdwkwAYgGPlFKJWutW084Yv9omP3TjudaQ0sDWKwmWZbuhboSsDdDcw00VIK9CVqbzHEt9Wafo6XjfGUxxzRWQmOVSeAle6E6Hzjm2gUnRllMm527l4m5qapju3eIqRVorjUJXzvMDYZPGPhGmNoDW4C5AWmsgtoi897AXNM7xFwXOm4QfMLA5m/+MDrs5nNAmxsNnxDwDjUrJVncoKURWpvNY3uDaX5QFvDwM+c4Ws0+Nw9zg6E11Bw21w0fZ65ZssecExBnjm1tMTdDPiHmc7Y3muOsHuDpZ+K1upv3U5VnfrfUd9xkefiaH3dbR4wWi7mmwwF1xeaGyD+m4303VEBznXnfnv4mTgCrm4mt7SbO6m6Orc53Jg7vjv8XFquzpsbPHNcWk1vHuro4HOZ8i9XU8HS+EWu7DsiNWE/qy8z/Wfl8RA98fX2ZP38+3//+91m6dCnV1dX4+PgQEBBAUVERH3zwQfsiH92ZM2cOy5Yt45577sFut/Puu+/ygx/8AICamhqioqJoaWnh5Zdfbl+m08/Pj5qamqOuNWbMGLKzs8nMzGxv8547d+6AvO/u9KWE3b5OLoBSqm2d3M4J+3LgAefjN4DHlFLKuX251roJOKiUynReb13/hI/5orf9kfUNMz/9pdVuEmbnpG1xN6XphgpnYmkwxyhl/rB7+JqOGYe3m5K1d4hJyi31JtG2tkDlIfOHqqXBnOMXCSiTYOpLzU2Gp69JkharSXB1JVBbDLkbTEne3mSu5xtuEicainZDQ7nZpx3OROUwr3US90jDh+K4b7CUxflvfBxsAR3/NrVFR9bweAWZfY1V0FLnrJHB3GhY3DpqYxx2502Lu4nbYjX/XlZ3c8MUNQWWvHx8cQ1HDRVSHS56tXTpUq688kqWL1/O2LFjSUlJYezYscTFxXH22Wcf89xp06ZxzTXXMGXKFMLDw5kxY0b7vt/85jfMmjWLsLAwZs2a1Z6klyxZwg033MCjjz7KG2+80X68zWbj+eefZ/Hixe2dzm666aaBedPdUFof+w+cUmoRcKHW+v+cz78DzNJa39LpmF3OY/Kczw8AszBJfL3W+j/O7c8CH2it36AHqampum0cnegnDocpLdubTEJw8zKJymE3ybyuxNw8OFqcpV0PU5J2czYTaG1uEpQzsThaO0rKWoN/FKCgeA+gIczZM7Iq1yRDi7u5kagvMzcXbjaz3d5kakFaGsyNjKevKZW31T44WsxNTHO9qXGwN5qY3WzOzofl5n34hJnjqvNNUvT0M3G7e5vE2VRj4gZzvqPVxNnabF7bO8SUznVrR/MEdMTYWGXOc/fuuHlqrjPn+0aYmHWr2VZbbGK1BZgSt8XN2ZTSYl5XO0zMFjdzTqvdGUuL8z02mfcemgzn3H7Mf1al1Bat9UmtljXQev0+f/GIed8LHxi0mETf7dmzh3Hjxrk6jFNOd59rX77PQ6LTmVLqRuBGgPj4eBdHcwqyWEzy8PDpst3DJFv/qD5cpA/HBMQc+TwwrvvjhGgz505XRyDEsNGXYV29rZN7xDFKKTcgANP5rC/norV+SmudqrVODQvrxyptIYQQ4hTRl4R9zHVynVYA1zkfLwI+1aaufQWwRCnlqZRKAkYDG/sndCGEEAOtt2ZTcXxO5vPstUpca21XSrWtk2sFnmtbJxfYrLVeATwLvOTsVFaOSeo4j3sN00HNDvzopHqICyGEGDQ2m42ysjJCQkJQ0pP/pGmtKSsrw2azndD5fWrD7m2dXK11I7C4h3MfBh4+oeiEEEK4TGxsLHl5eZSUlLg6lFOGzWYjNjb2hM4dEp3OhBBCDD3u7u4kJSW5OgzhNPzmEhdCCCFOQ5KwhRBCiGFAErYQQggxDPQ609lgU0qVAIf6cGgoUDrA4RwvialvhmJMMDTjOlZMCVrrIT1xQR+/z8Ptc3eloRiXxNQ3vcXU6/d5yCXsvlJKbR5q0zJKTH0zFGOCoRnXUIypvw3F9zgUY4KhGZfE1Df9EZNUiQshhBDDgCRsIYQQYhgYzgn7KVcH0A2JqW+GYkwwNOMaijH1t6H4HodiTDA045KY+uakYxq2bdhCCCHE6WQ4l7CFEEKI04YkbCGEEGIYGHYJWyl1oVJqr1IqUyl1t4tiiFNKfaaUSldK7VZK3ebcHqyU+lgptd/5O8gFsVmVUtuUUu85nycppTY4P69XnUukDnZMgUqpN5RSGUqpPUqpM139WSml7nD+2+1SSr2ilLK54rNSSj2nlCpWSu3qtK3bz0YZjzrj26GUmjbQ8Q00+T73GtuQ+j7Ld/mYcQz4d3lYJWyllBV4HLgIGA8sVUqNd0EoduCnWuvxwBnAj5xx3A2s1lqPBlY7nw+224A9nZ7/Afir1noUUAFc74KY/g58qLUeC0xxxueyz0opFQPcCqRqrSdilo1dgms+qxeAC7ts6+mzuQizpvxo4EbgyUGIb8DI97lPhtr3Wb7LPXuBgf4ua62HzQ9wJrCq0/N7gHuGQFzvAOcBe4Eo57YoYO8gxxHr/E9xLvAeoDAz67h19/kNUkwBwEGcHRw7bXfZZwXEALlAMGbFuveAC1z1WQGJwK7ePhvgX8DS7o4bjj/yfe41jiH1fZbvcp/iGdDv8rAqYdPxj9Mmz7nNZZRSiUAKsAGI0Fofdu4qBCIGOZy/AXcBDufzEKBSa213PnfF55UElADPO6v2nlFK+eDCz0prnQ88AuQAh4EqYAuu/6za9PTZDLn//ydpyL0f+T4fk3yXj1+/fpeHW8IeUpRSvsCbwO1a6+rO+7S5bRq0MXNKqUuAYq31lsF6zT5yA6YBT2qtU4A6ulSZueCzCgIux/wBigZ8OLoqa0gY7M/mdCbf517Jd/kk9MdnM9wSdj4Q1+l5rHPboFNKuWO+3C9rrd9ybi5SSkU590cBxYMY0tnAZUqpbGA5phrt70CgUsrNeYwrPq88IE9rvcH5/A3Ml96Vn9VC4KDWukRr3QK8hfn8XP1Ztenpsxky///7yZB5P/J97hP5Lh+/fv0uD7eEvQkY7ewB6IHpXLBisINQSingWWCP1vovnXatAK5zPr4O0xY2KLTW92itY7XWiZjP5VOt9beAz4BFrojJGVchkKuUGuPctABIx4WfFab67AyllLfz37ItJpd+Vp309NmsAL7r7GF6BlDVqbptOJLvcw+G4vdZvssnpH+/y4PVOaAfG/UvBvYBB4B7XRTDOZiqjR1AmvPnYkwb02pgP/AJEOyi+OYB7zkfjwA2ApnA64CnC+KZCmx2fl5vA0Gu/qyAXwMZwC7gJcDTFZ8V8Aqm7a0FU4K5vqfPBtPp6HHn//2dmJ6xg/7/q5/fv3yfe49vyHyf5bt8zDgG/LssU5MKIYQQw8BwqxIXQgghTkuSsIUQQohhQBK2EEIIMQxIwhZCCCGGAUnYQgghxDAgCVsIIYQYBiRhCyGEEMPA/wMmIQ1R77ka1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn, cnn_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fe233",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db6a6b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.36766\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.35595\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.46475\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.39018\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.37832\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.45167\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.36223\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.41420\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.43996\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.32874\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.41226\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.45990\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.39559\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.42868\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.42906\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.37410\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.41727\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.35933\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.41635\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.37210\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.42096\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.37574\n",
      "\tTrain loss: 0.04309, Accuracy: 1980/6768 (29.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 481/1692 (28.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 471/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.36511\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.38696\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.39642\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.34696\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.40944\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.39222\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.34721\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.38606\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.41830\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.41940\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.43862\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.39522\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.38536\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.35754\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.41736\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.31049\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.39173\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.38274\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.39209\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.39066\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.41568\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.36042\n",
      "\tTrain loss: 0.04278, Accuracy: 2094/6768 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 521/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 501/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.36514\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.35763\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.39512\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.36538\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.42683\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.36665\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.34948\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.35276\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.39295\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.39516\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.34967\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.40173\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.38895\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.40475\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.35948\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.34288\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.33749\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.33349\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.41347\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.37952\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.51528\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.40920\n",
      "\tTrain loss: 0.04240, Accuracy: 2329/6768 (34.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 578/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.35681\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.37198\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.36471\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.36757\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.39694\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.36046\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.47410\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.36147\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.43152\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.41230\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.34330\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.39884\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.39168\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.37993\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.32728\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.33328\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.37917\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.28024\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.39937\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.31064\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.43015\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.28577\n",
      "\tTrain loss: 0.04215, Accuracy: 2281/6768 (33.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 584/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 474/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.27648\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.38737\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.35966\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.28831\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.34583\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.34913\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.32932\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.29058\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.28882\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.56612\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.31574\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.43814\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.33442\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.36010\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.33253\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.32392\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.38501\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.35707\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.34298\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.35544\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.46227\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.28579\n",
      "\tTrain loss: 0.04167, Accuracy: 2475/6768 (36.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 603/1692 (35.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 487/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.22344\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.39121\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.29426\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.29706\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.28293\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.31726\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.35815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.38702\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.22347\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.38781\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.30211\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.41932\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.30725\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.37513\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.35713\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.28423\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.34025\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.29774\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.28580\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.33128\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.36016\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.23617\n",
      "\tTrain loss: 0.04106, Accuracy: 2545/6768 (37.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 619/1692 (36.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.29518\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.40336\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.26582\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.23860\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.28779\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.31326\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.37916\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.39357\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.27230\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.43452\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.20028\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.39180\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.26667\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.37137\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.26752\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.16226\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.28862\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.19362\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.33890\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.33677\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.48129\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.17972\n",
      "\tTrain loss: 0.04084, Accuracy: 2632/6768 (38.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 632/1692 (37.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 507/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.13738\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.25783\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.31498\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.18498\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.30802\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.28996\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.53085\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.29609\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.19077\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.45504\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.20630\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.38160\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.22228\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.35752\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.20091\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.20107\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.15757\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.22286\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.42981\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.13945\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.39734\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.10808\n",
      "\tTrain loss: 0.04038, Accuracy: 2720/6768 (40.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 664/1692 (39.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.18797\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.37500\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.24800\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.09622\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.28155\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.28944\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.34229\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.16328\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.13034\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.43617\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.14233\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.26172\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.38497\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.30713\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.13853\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.15579\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.21349\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.27165\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.37978\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.25887\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.39316\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.25390\n",
      "\tTrain loss: 0.04024, Accuracy: 2726/6768 (40.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 653/1692 (38.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 500/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.02672\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.36513\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.20895\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.10020\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19890\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.14534\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.39898\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.21210\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.16686\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.25144\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.16488\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.35438\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.35191\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.28425\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.15283\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.02076\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.10332\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.26516\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.25065\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.24344\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.47379\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.14254\n",
      "\tTrain loss: 0.03970, Accuracy: 2852/6768 (42.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 680/1692 (40.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.07775\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.27875\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.16798\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.08722\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.15588\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.19572\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.28178\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.15127\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.15179\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.27149\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.00531\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.21045\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.23032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.20781\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.13126\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.02261\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.04881\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.04522\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.18079\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.13692\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.39959\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.03416\n",
      "\tTrain loss: 0.03920, Accuracy: 2943/6768 (43.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 681/1692 (40.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.96458\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.21620\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.11183\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.13072\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.23029\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.14524\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.29294\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.07184\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.08234\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.32262\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.08261\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.20338\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.26727\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.23836\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.05356\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.04992\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.89743\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.14058\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.18544\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.07495\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.42533\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.01922\n",
      "\tTrain loss: 0.03869, Accuracy: 3026/6768 (44.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 711/1692 (42.00%)\n",
      "\tTest loss: 0.00087, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.10409\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.20862\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.12757\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.07092\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.12626\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.02418\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.42037\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.24654\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.08295\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.20637\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.90959\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.17975\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.32449\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.32058\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.09046\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 0.99970\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.01370\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.24955\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.28585\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.07536\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.16684\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.03661\n",
      "\tTrain loss: 0.03869, Accuracy: 3103/6768 (45.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 737/1692 (43.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 520/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.89458\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.10168\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.14812\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.01764\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.27548\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.06588\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.29422\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.17985\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.14613\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.14932\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.93618\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.09009\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.01632\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.18377\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.05654\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 0.90084\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.96939\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.15581\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.09147\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.00950\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.26608\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.92838\n",
      "\tTrain loss: 0.03947, Accuracy: 3105/6768 (45.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 724/1692 (42.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.94220\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.09367\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.11723\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.98893\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.02678\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 1.01952\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.28423\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.02084\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.08783\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.04805\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.82222\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.10897\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.23549\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.17040\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.09560\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.93917\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.99560\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.14015\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.33023\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.86004\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.34102\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.89653\n",
      "\tTrain loss: 0.03956, Accuracy: 3045/6768 (44.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 731/1692 (43.00%)\n",
      "\tTest loss: 0.00093, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.06270\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.37969\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.11867\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.00865\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.14248\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.96601\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.17617\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.97846\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.96351\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.09299\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.95144\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.11306\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.88376\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.13487\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.95482\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.91241\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.88425\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.08721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.21838\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.88568\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.20201\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.85694\n",
      "\tTrain loss: 0.03646, Accuracy: 3414/6768 (50.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 779/1692 (46.00%)\n",
      "\tTest loss: 0.00094, Accuracy: 561/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.93307\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.07947\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.96990\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.91765\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.94435\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 1.01338\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.31783\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.87176\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.95026\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.16174\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.92678\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.05153\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 1.33075\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.05805\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.83238\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.86595\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.90338\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.01774\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.08505\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.80477\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 1.27260\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.84209\n",
      "\tTrain loss: 0.03658, Accuracy: 3496/6768 (51.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 811/1692 (47.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.85460\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.95799\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 0.88076\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.95183\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.97990\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.94714\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.23603\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.01298\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.01527\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.01748\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.82071\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.93787\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.90996\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.15846\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.92372\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 1.00706\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.85693\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.19477\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.00530\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.88500\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.20662\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.89830\n",
      "\tTrain loss: 0.03754, Accuracy: 3442/6768 (50.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 798/1692 (47.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 542/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.90963\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.05404\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.96905\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.76119\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.02273\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.93524\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.18214\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.88076\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.05017\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.01314\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.63987\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.10233\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.99373\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 1.12494\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.85152\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.92555\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.90627\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.96060\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.02369\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.90446\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 1.14355\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.92766\n",
      "\tTrain loss: 0.03517, Accuracy: 3695/6768 (54.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 832/1692 (49.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 1.00624\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.96666\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.20307\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.91227\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.93764\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.83931\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.07626\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.85738\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.76739\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.17863\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.72455\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.97472\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.93751\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.29068\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.78479\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.73755\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.89390\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 0.93298\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.02890\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.84603\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 1.19172\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.86542\n",
      "\tTrain loss: 0.03682, Accuracy: 3595/6768 (53.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 800/1692 (47.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.86272\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.90637\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.04143\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.04905\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.06638\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.76245\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.97367\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.93966\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.83850\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.88317\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.62784\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.82128\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 1.04101\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.98427\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.95579\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.79567\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.95981\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.00153\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.65237\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 1.24035\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.81353\n",
      "\tTrain loss: 0.03347, Accuracy: 3919/6768 (57.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00074, Accuracy: 872/1692 (51.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 568/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.90388\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.00677\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.08532\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.80185\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.90921\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.59404\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.97422\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.90883\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.80552\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.07044\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.71696\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.74873\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 1.23819\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.80139\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.79454\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.75059\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.79296\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.99984\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.92112\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.63625\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.91596\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.74766\n",
      "\tTrain loss: 0.03250, Accuracy: 4047/6768 (59.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 904/1692 (53.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 557/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 1.08213\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.78096\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.88979\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.73529\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.84342\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.73989\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.14843\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.88481\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.65875\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.86706\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.59150\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.84282\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 1.32662\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.98071\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.73629\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.73048\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.72678\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.14680\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.97613\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.70066\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 1.02507\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.83047\n",
      "\tTrain loss: 0.03530, Accuracy: 3973/6768 (58.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 878/1692 (51.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 565/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.97363\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.79596\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.76185\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.84057\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.98366\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.58238\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.96783\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.74022\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.70099\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.89763\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.69640\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.66611\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.92059\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.75654\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.61038\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.68143\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.87964\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.87429\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.89433\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.63147\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.93350\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.65774\n",
      "\tTrain loss: 0.03184, Accuracy: 4133/6768 (61.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 914/1692 (54.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 550/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.72565\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.68406\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.94637\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.70076\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.74083\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.78997\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.77208\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.59640\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.64595\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.82661\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.49757\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.63728\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 1.02323\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.77584\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.69156\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.65747\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.76749\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.08936\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.70323\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.54994\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.78097\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.50239\n",
      "\tTrain loss: 0.02986, Accuracy: 4311/6768 (63.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 959/1692 (56.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.79717\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.69071\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.86205\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.58899\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.81247\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.64129\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.75163\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.77545\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.74873\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.87260\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.54797\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.66574\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.96708\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.94961\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.88787\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.79723\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.61800\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.91772\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.98287\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.50239\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.03985\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.67252\n",
      "\tTrain loss: 0.03160, Accuracy: 4229/6768 (62.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 937/1692 (55.00%)\n",
      "\tTest loss: 0.00124, Accuracy: 581/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.97408\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.70512\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.72423\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.74997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.07341\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.58071\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.80009\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.68768\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.48561\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.89802\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.57713\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.57474\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.92610\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.84362\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.84082\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.49695\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.62441\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.93947\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.72852\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.67003\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.84433\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.48321\n",
      "\tTrain loss: 0.02680, Accuracy: 4626/6768 (68.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1023/1692 (60.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 579/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 1.03479\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.68376\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.72127\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.56509\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.54270\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.45885\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.70912\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.76254\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.62149\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.51744\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.56768\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.93784\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.71290\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.52559\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.55042\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.74685\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.02353\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.71067\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.45584\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.93539\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.55515\n",
      "\tTrain loss: 0.02952, Accuracy: 4323/6768 (63.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 962/1692 (56.00%)\n",
      "\tTest loss: 0.00122, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.90921\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.52170\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.81962\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.69620\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.50182\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.47446\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.58742\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.53164\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.49539\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.75638\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.41005\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.60194\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.70145\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.76752\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.46171\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.61852\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.60895\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.98821\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.83459\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.56525\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.79219\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.35221\n",
      "\tTrain loss: 0.03220, Accuracy: 4348/6768 (64.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 972/1692 (57.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.89043\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.42496\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.72051\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.50643\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.37388\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.59233\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.99181\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.63271\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.54959\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.63485\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.41999\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.59415\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.79811\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.62310\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.42393\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.59109\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.70248\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.97499\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.81623\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.49602\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.84182\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.60576\n",
      "\tTrain loss: 0.02981, Accuracy: 4446/6768 (65.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 996/1692 (58.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 598/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.63376\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.35739\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.70737\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.57877\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.53964\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.59020\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.73078\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.68665\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.44139\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.86146\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.31698\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.65637\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.79836\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.74210\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.61570\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.37193\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.55033\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.66171\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.80184\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.43839\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.73409\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.58815\n",
      "\tTrain loss: 0.02875, Accuracy: 4532/6768 (66.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 1010/1692 (59.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 591/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.77668\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.35348\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.75658\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.46343\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.53720\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.38454\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.81657\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.83212\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.43067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.62653\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.38091\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.66895\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.95314\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.58712\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.66480\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.46283\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.50343\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.88069\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.54017\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.51020\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.75453\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.42568\n",
      "\tTrain loss: 0.02762, Accuracy: 4589/6768 (67.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1014/1692 (59.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 574/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.62892\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.70750\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.64849\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.38951\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.59216\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.32011\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.45758\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.58343\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.41538\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.77483\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.54443\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.38890\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.59630\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.91008\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.40751\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.43239\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.72952\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.89479\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.61598\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.29027\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.61555\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.48456\n",
      "\tTrain loss: 0.02773, Accuracy: 4587/6768 (67.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1015/1692 (59.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.75216\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.53767\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.70567\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.41230\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.53440\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.44228\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.69149\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.48520\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.66655\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.54875\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.42815\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.40510\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.77629\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.49689\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.46912\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.36056\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.75694\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.82799\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.56063\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.52861\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.41193\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.39534\n",
      "\tTrain loss: 0.02417, Accuracy: 4806/6768 (71.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1056/1692 (62.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.67592\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.50467\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.51028\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.53375\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.65544\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.34113\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.60393\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.53343\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.30538\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.57058\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.41440\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.42649\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.79337\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.28295\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.36104\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.21599\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.38818\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.82611\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.63833\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.40832\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.59539\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.30694\n",
      "\tTrain loss: 0.02924, Accuracy: 4607/6768 (68.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.58478\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.43328\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.72216\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.40089\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.35366\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.36018\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.60778\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.48104\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.24426\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.44278\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.29995\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.44865\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.50758\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.38090\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.32894\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.36813\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.46948\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.81351\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.47521\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.43210\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.72068\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.43005\n",
      "\tTrain loss: 0.02506, Accuracy: 4863/6768 (71.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1086/1692 (64.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.46314\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.52936\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.64766\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.37214\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.58892\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.44415\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.59926\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.60931\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.45177\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.64079\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.50488\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.48496\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.45242\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.37924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.49129\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.38916\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.43490\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.68935\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.47855\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.29579\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.59939\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.45114\n",
      "\tTrain loss: 0.02652, Accuracy: 4845/6768 (71.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 1069/1692 (63.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 582/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.89732\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.46237\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.45645\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.42907\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.31806\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.34601\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.44835\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.42041\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.35040\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.66836\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.58465\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.31610\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.70013\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.59310\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.40313\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.31753\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.43695\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 1.17860\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.78798\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.45417\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.57681\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.26402\n",
      "\tTrain loss: 0.02443, Accuracy: 4937/6768 (72.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1108/1692 (65.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 588/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.50161\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.38105\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.71902\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.36443\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.52543\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.44106\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.65176\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.40685\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.53965\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.44807\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.27268\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.26734\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.64721\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.41157\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.32774\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.47117\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.50813\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.79793\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.74724\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.39906\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.63561\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.35502\n",
      "\tTrain loss: 0.02026, Accuracy: 5200/6768 (76.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1148/1692 (67.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.67103\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.29921\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.60765\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.51493\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.43756\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.43264\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.46830\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.43343\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.31034\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.60337\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.22663\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.60752\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.47928\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.36255\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.38843\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.54115\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.44863\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.61187\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.42485\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.26517\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.85218\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.33321\n",
      "\tTrain loss: 0.02069, Accuracy: 5119/6768 (75.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.55836\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.42883\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.36726\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.30906\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.54050\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.41947\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.67791\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.36918\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.27045\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.45183\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.27332\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.56517\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.54413\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.18569\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.24592\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.30666\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.34979\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.69343\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.53113\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.34812\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.51808\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.44127\n",
      "\tTrain loss: 0.01957, Accuracy: 5291/6768 (78.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1178/1692 (69.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 610/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.83246\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.22357\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.47856\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.31808\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.33959\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.45184\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.45208\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.40775\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.35815\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.53375\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.15655\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.25467\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.50997\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.49203\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.32865\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.34654\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.36089\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.69024\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.61046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.30180\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.68270\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.30435\n",
      "\tTrain loss: 0.01789, Accuracy: 5414/6768 (79.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 585/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.61258\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.42077\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.58335\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.58006\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.25699\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.23564\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.55074\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.45852\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.24617\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.32089\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.17373\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.44837\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.86099\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.33546\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.34913\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.39917\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.40260\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.49403\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.53407\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.34823\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.57232\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.42260\n",
      "\tTrain loss: 0.02617, Accuracy: 4810/6768 (71.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1080/1692 (63.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.79189\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.68741\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.41600\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.37885\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.32460\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.30297\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.44123\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.60455\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.28674\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.26943\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.27638\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.44440\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.63005\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.55183\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.20531\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.18875\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.67983\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.70251\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.60135\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.30001\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.72170\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.21452\n",
      "\tTrain loss: 0.02191, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.40063\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.21066\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.51608\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.36293\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.37477\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.20572\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.66834\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.30652\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.30073\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.44510\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.20987\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.27398\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.65597\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.50523\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.28949\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.18597\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.30606\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.41839\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.48926\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.30250\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.47674\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.36309\n",
      "\tTrain loss: 0.02091, Accuracy: 5268/6768 (77.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 602/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.69986\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.27440\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.48551\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.35482\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.32220\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.37014\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.32289\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.36548\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.22926\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.33355\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.18377\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.31697\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.42175\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.24505\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.30201\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.23754\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.30869\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.52209\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.36867\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.18968\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.43034\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.48596\n",
      "\tTrain loss: 0.01819, Accuracy: 5386/6768 (79.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1191/1692 (70.00%)\n",
      "\tTest loss: 0.00155, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.62044\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.32636\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.59447\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.15527\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.29231\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.29579\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.31746\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.26795\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.28848\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.60915\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.32458\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.42280\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.46451\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.33874\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.21216\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.38868\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.38927\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.72374\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.34005\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.23903\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.46086\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.32576\n",
      "\tTrain loss: 0.02217, Accuracy: 5132/6768 (75.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1135/1692 (67.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 601/1772 (33.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.53187\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.25414\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.41532\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.36441\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.42630\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.31447\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.34362\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.33430\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.11119\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.46936\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.24612\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.29397\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.56618\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.24541\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.20276\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.20017\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.40051\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.51870\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.29790\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.24019\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.61204\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.24227\n",
      "\tTrain loss: 0.01886, Accuracy: 5289/6768 (78.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1202/1692 (71.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.69774\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.34283\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.35776\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.37140\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.33499\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.28882\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.33687\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.42635\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.29402\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.60294\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.18620\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.23202\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.33564\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.50395\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.23692\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.11319\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.34425\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.51550\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.48401\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.19230\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.52367\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.35466\n",
      "\tTrain loss: 0.02006, Accuracy: 5309/6768 (78.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 601/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.47317\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.13026\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.56029\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.26717\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.32195\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.27619\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.12714\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.38978\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.20647\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.36329\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.26346\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.35425\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.34071\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.29372\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.15109\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.27064\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.31918\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.44130\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.46054\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.45159\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.40133\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.25230\n",
      "\tTrain loss: 0.01604, Accuracy: 5573/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1238/1692 (73.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.50774\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.30892\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.47175\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.19448\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.28998\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.21533\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.46291\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.43022\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.13118\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.21973\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.28379\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.25465\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.33530\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.28419\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.26355\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.13920\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.37675\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.41366\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.51291\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.34819\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.34209\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.27435\n",
      "\tTrain loss: 0.01778, Accuracy: 5425/6768 (80.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 597/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.56546\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.31293\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.39138\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.38423\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.29526\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.26451\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.32888\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.28443\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.12147\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.24570\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.14721\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.23779\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.32713\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.18430\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.23378\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.38148\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.35396\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.74533\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.39642\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.30894\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.70678\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.30413\n",
      "\tTrain loss: 0.02403, Accuracy: 5135/6768 (75.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00175, Accuracy: 565/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.54351\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.39474\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.49936\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.24841\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.26108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.21018\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.26193\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.23745\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.28922\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.35556\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.23149\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.11624\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.42560\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.23130\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.19364\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.32001\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.25661\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.50955\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.58354\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.33209\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.38418\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.46295\n",
      "\tTrain loss: 0.01742, Accuracy: 5416/6768 (80.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1235/1692 (72.00%)\n",
      "\tTest loss: 0.00160, Accuracy: 603/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.81353\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.22742\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.25238\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.18413\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.28744\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.57746\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.31608\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.18109\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.24688\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.41125\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.17563\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.12401\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.26792\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.34060\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.20147\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.21156\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.21228\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.50897\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.45740\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.18357\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.42224\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.09990\n",
      "\tTrain loss: 0.01868, Accuracy: 5426/6768 (80.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1199/1692 (70.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 572/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.45092\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.60331\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.26824\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.33319\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.33223\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.16716\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.55694\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.29050\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.35272\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.31519\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.14147\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.28573\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.24064\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.30624\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.23847\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.22174\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.18082\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.47014\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.20984\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.29052\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.30529\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.10617\n",
      "\tTrain loss: 0.01477, Accuracy: 5664/6768 (83.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1253/1692 (74.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 595/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.47959\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.34974\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.20439\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.33865\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.13232\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.44254\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.25382\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.18912\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.13760\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.35785\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.18741\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.22995\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.18669\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.25958\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.15303\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.30632\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.15459\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.55146\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.17166\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.28305\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.79577\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.09380\n",
      "\tTrain loss: 0.01922, Accuracy: 5386/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1205/1692 (71.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.22415\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.07467\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.44860\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.29913\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.10472\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.23179\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.25627\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.33881\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.41126\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.26731\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.33890\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.27499\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.46437\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.23846\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.36977\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.39295\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.22256\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.31493\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.31073\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.24165\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.55782\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.18050\n",
      "\tTrain loss: 0.01534, Accuracy: 5646/6768 (83.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1272/1692 (75.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.52664\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.34875\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.29118\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.29976\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.19196\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.34421\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.23795\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.27949\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.40526\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.22102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.16329\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.28106\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.49969\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.41523\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.11009\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.14009\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.13337\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.75720\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.33226\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.11261\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.30319\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.18393\n",
      "\tTrain loss: 0.01688, Accuracy: 5537/6768 (81.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00182, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.40257\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.19416\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.22075\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.14843\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.10450\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.15335\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.22112\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.24871\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.38154\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.32783\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.13473\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.27406\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.37253\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.43955\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.16656\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.23639\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.44645\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.42133\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.17829\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.25741\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.46001\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.27736\n",
      "\tTrain loss: 0.01588, Accuracy: 5586/6768 (82.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.47756\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.25548\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.24510\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.23005\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.10349\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.25660\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.12799\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.32043\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.19246\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.40748\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.09652\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.15544\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.40475\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.29898\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.20390\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.13332\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.30714\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.19455\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.18029\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.15916\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.35701\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.18542\n",
      "\tTrain loss: 0.01531, Accuracy: 5638/6768 (83.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1263/1692 (74.00%)\n",
      "\tTest loss: 0.00177, Accuracy: 650/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.36175\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.35822\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.15250\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.15847\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.30040\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.16429\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.12610\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.11178\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.30234\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.19976\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.32951\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.25906\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.23112\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.18787\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.35546\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.26742\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.32391\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.20897\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.16432\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.48750\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.16146\n",
      "\tTrain loss: 0.01948, Accuracy: 5382/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1221/1692 (72.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.20382\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.24088\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.47085\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.32199\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.22537\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.21136\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.15887\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.31820\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.21576\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.17183\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.24717\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.11166\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.47192\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.15348\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.14750\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.16805\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.20425\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.38880\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.22195\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.26222\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.51383\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.26693\n",
      "\tTrain loss: 0.01374, Accuracy: 5714/6768 (84.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1271/1692 (75.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.43359\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.32496\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.22058\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.18745\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.12790\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.39750\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.26901\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.42555\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.22088\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.47663\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.18061\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.26487\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.45434\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.11810\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.16307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.15420\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.21062\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.38434\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.22469\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.12736\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.48986\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.21407\n",
      "\tTrain loss: 0.01570, Accuracy: 5583/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.81950\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.20309\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.22260\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.46659\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.34659\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.26755\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.11494\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.21040\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.18856\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.14672\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.14445\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.18813\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.19302\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.11931\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.23594\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.19331\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.16493\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.38615\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.11773\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.09826\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.18818\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.19176\n",
      "\tTrain loss: 0.02131, Accuracy: 5298/6768 (78.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1176/1692 (69.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.32069\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.19076\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.29143\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.13439\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.12203\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.18773\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.34523\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.39093\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.14307\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.21321\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.28422\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.18704\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.26087\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.23283\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.15054\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.05922\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.16744\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.27240\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.45545\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.09502\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.35998\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.36310\n",
      "\tTrain loss: 0.01858, Accuracy: 5467/6768 (80.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1229/1692 (72.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.36727\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.51850\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.27211\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.12445\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.15850\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.17196\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.31891\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.24976\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.11482\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.12301\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.11290\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.07819\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.20291\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.14439\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.21522\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.21517\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.28443\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.38498\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.21176\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.31196\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.40561\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.41614\n",
      "\tTrain loss: 0.01643, Accuracy: 5590/6768 (82.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 597/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.43394\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.17768\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.52182\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.38734\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.07533\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.19511\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.03379\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.14579\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.19635\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.26756\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.09518\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.08847\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.25620\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.07652\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.06217\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.14271\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.22961\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.21239\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.39794\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.06595\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.52095\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.08412\n",
      "\tTrain loss: 0.01407, Accuracy: 5765/6768 (85.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1291/1692 (76.00%)\n",
      "\tTest loss: 0.00190, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.27218\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.17332\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.38184\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.28861\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.14820\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.18964\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.23026\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.22038\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.21328\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.12073\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.05027\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.12482\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.54795\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.09219\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.23995\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.17905\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.23760\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.34877\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.12310\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.27828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.36930\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.17927\n",
      "\tTrain loss: 0.01279, Accuracy: 5774/6768 (85.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1314/1692 (77.00%)\n",
      "\tTest loss: 0.00183, Accuracy: 571/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.31602\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.19372\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.27814\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.11019\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.12721\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.21112\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.13681\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.11068\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.22389\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.28824\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.09551\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.14025\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.44347\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.08739\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.21035\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.16980\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.23119\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.12981\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.23528\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.25223\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.22406\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.33308\n",
      "\tTrain loss: 0.02322, Accuracy: 5220/6768 (77.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1174/1692 (69.00%)\n",
      "\tTest loss: 0.00203, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.26104\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.45997\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.37994\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.22195\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.16835\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.14740\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.35765\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.35007\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.10294\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.50979\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.18085\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.20328\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.17137\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.25351\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.07298\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.09950\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.21765\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.37585\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.37847\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.10299\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.39079\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.18290\n",
      "\tTrain loss: 0.01556, Accuracy: 5611/6768 (82.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1259/1692 (74.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.55750\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.09281\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.28527\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.34819\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.29200\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.22515\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.11385\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.30842\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.10338\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.14360\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.10704\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.19350\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.18281\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.09077\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.15814\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.16955\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.24425\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.27344\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.13830\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.10510\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.39969\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.05896\n",
      "\tTrain loss: 0.01446, Accuracy: 5714/6768 (84.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1293/1692 (76.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.39094\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.11784\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.35574\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.08157\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.16651\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.13597\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.13571\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.15181\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.09383\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.10255\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.19950\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.60061\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.16314\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.17113\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.20455\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.19647\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.16946\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.27419\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.09720\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.31319\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.28944\n",
      "\tTrain loss: 0.01358, Accuracy: 5770/6768 (85.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1269/1692 (75.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 595/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.29932\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.35209\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.69390\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.20774\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.20175\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.14214\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.09321\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.17377\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.21433\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.13534\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.03962\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.20326\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.31602\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.38441\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.17099\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.05570\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.12162\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.08725\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.37845\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.16019\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.28102\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.06904\n",
      "\tTrain loss: 0.01451, Accuracy: 5744/6768 (84.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1299/1692 (76.00%)\n",
      "\tTest loss: 0.00197, Accuracy: 609/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.33686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.08569\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.20585\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.07160\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.17010\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.05262\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.06711\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.11412\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.11041\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.11576\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.08075\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.13594\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.42850\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.57510\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.32780\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.12096\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.25036\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.37125\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.12069\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.13889\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.17065\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.12440\n",
      "\tTrain loss: 0.01596, Accuracy: 5630/6768 (83.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1286/1692 (76.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 602/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.61859\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.24106\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.31940\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.13019\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.04339\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.18178\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.14673\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.32827\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.16769\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.27304\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.16421\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.22708\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.24355\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.18540\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.10382\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.10757\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.04444\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.27742\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.11345\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.06272\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.13956\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.06538\n",
      "\tTrain loss: 0.01575, Accuracy: 5661/6768 (83.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1282/1692 (75.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.42566\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.13983\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.57543\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.04828\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.16553\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.16302\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.14735\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.05926\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.16783\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.50271\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.12068\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.17862\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.30442\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.18031\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.08022\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.13434\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.22556\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.36274\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.06742\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.26213\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.31189\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.05938\n",
      "\tTrain loss: 0.01405, Accuracy: 5729/6768 (84.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1285/1692 (75.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 606/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.22367\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.07082\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.17655\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.06150\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.17366\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.26325\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.28436\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.10432\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.13779\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.13202\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.07570\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.31024\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.32918\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.07120\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.07234\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.04758\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.21553\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.32905\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.40940\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.12850\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.54032\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.19647\n",
      "\tTrain loss: 0.01346, Accuracy: 5787/6768 (85.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1311/1692 (77.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.31644\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.02814\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.17616\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.17207\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.10682\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.40567\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.21361\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.11960\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.16625\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.20022\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.09255\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.23235\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.25653\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.05326\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.11535\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.08011\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.19124\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.21429\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.26952\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.12440\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.55246\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.05236\n",
      "\tTrain loss: 0.01673, Accuracy: 5594/6768 (82.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1255/1692 (74.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 564/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.23758\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.26266\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.10210\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.07879\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.04234\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.21099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.33816\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.08914\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.05518\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.18578\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.12460\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.17977\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.26461\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.10348\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.11335\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.09485\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.24564\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.60554\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.12285\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.07699\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.15206\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.24306\n",
      "\tTrain loss: 0.01198, Accuracy: 5889/6768 (87.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1298/1692 (76.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 559/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.30712\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.07458\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.07814\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.13938\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.30816\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.38750\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.05773\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.04657\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.12524\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.13513\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.03234\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.17642\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.36084\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.36642\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.18683\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.14807\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.05316\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.12708\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.23390\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.11860\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.12653\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.05277\n",
      "\tTrain loss: 0.01282, Accuracy: 5842/6768 (86.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1304/1692 (77.00%)\n",
      "\tTest loss: 0.00204, Accuracy: 601/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.20126\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.20912\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.34569\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.14187\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.18578\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.16332\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.30488\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.26736\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.12165\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.21936\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.03278\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.07442\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.26117\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.13896\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.17863\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.09440\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.19842\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.14763\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.09369\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.17275\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.22715\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.03885\n",
      "\tTrain loss: 0.01297, Accuracy: 5822/6768 (86.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1315/1692 (77.00%)\n",
      "\tTest loss: 0.00199, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.20985\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.16428\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.10664\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.11032\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.13422\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.06289\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.05247\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.08412\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.08102\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.20161\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.27770\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.05902\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.25693\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.17032\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.09931\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.12531\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.09418\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.47837\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.29988\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.17883\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.25641\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.09122\n",
      "\tTrain loss: 0.01588, Accuracy: 5630/6768 (83.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1281/1692 (75.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 579/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.17161\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.06703\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.24708\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.23893\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.20092\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.17623\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.06860\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.05314\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.03019\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.05450\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.03691\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.09036\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.07764\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.08040\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.07979\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.17058\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.14607\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.18075\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.22887\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.05239\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.33211\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.04866\n",
      "\tTrain loss: 0.01090, Accuracy: 5944/6768 (87.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1355/1692 (80.00%)\n",
      "\tTest loss: 0.00197, Accuracy: 586/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.42589\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.20732\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.16953\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.07057\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.13614\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.23806\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.13639\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.03233\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.12889\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.10162\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.04028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.23571\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.26460\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.07654\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.06433\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.10053\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.24012\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.42229\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.07568\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.22194\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.14892\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.18692\n",
      "\tTrain loss: 0.02038, Accuracy: 5421/6768 (80.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00219, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.24715\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.16405\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.11583\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.12989\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.12276\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.20660\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.22984\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.12476\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.08500\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.11355\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.17901\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.09396\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.33339\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.11391\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.06017\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.08860\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.42940\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.05547\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.36080\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.10428\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.19567\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.05181\n",
      "\tTrain loss: 0.01643, Accuracy: 5662/6768 (83.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1269/1692 (75.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 596/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.51007\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.11784\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.24070\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.06004\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.09764\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.17350\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.23742\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.17614\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.15327\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.25350\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.06167\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.25739\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.31711\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.33733\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.07193\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.07498\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.21380\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.29851\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.23565\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.12507\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.33148\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.42104\n",
      "\tTrain loss: 0.01161, Accuracy: 5862/6768 (86.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1318/1692 (77.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 587/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.12856\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.13260\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.20113\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.16195\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.15019\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.06536\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.06447\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.15324\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.26106\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.09313\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.06887\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.10627\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.18353\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.06195\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.13852\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.17617\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.06733\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.08316\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.03027\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.09774\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.38445\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.12795\n",
      "\tTrain loss: 0.01307, Accuracy: 5804/6768 (85.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1318/1692 (77.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.13840\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.22184\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.28573\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.23157\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.16161\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.21667\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.14822\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.16735\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.14218\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.07094\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.03140\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.21392\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.12832\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.28658\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.20209\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.18703\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.13110\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.06990\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.30468\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.04726\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.22701\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.11344\n",
      "\tTrain loss: 0.01288, Accuracy: 5837/6768 (86.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1313/1692 (77.00%)\n",
      "\tTest loss: 0.00207, Accuracy: 584/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.32853\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.40384\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.13883\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.21468\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.31457\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.26779\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.07220\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.13560\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.07427\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.10005\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.03927\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.09906\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.11889\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.17441\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.09132\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.16633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.10084\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.17133\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.13793\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.17109\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.37053\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.23876\n",
      "\tTrain loss: 0.01413, Accuracy: 5794/6768 (85.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1305/1692 (77.00%)\n",
      "\tTest loss: 0.00212, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.21229\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.15890\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.34802\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.21227\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.19285\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.24211\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.27141\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.06592\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.29319\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.12836\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.02874\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.14259\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.18610\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.21407\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.27272\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.04438\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.09044\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.14020\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.03780\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.09245\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.17911\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.02955\n",
      "\tTrain loss: 0.01385, Accuracy: 5775/6768 (85.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1295/1692 (76.00%)\n",
      "\tTest loss: 0.00205, Accuracy: 621/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.50072\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.08290\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.12432\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.06592\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.13479\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.11452\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.31427\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.03902\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.15976\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.04626\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.13277\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.20116\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.25712\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.22132\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.28647\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.12595\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.19527\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.43035\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.33677\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.08341\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.39854\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.19070\n",
      "\tTrain loss: 0.01177, Accuracy: 5868/6768 (86.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1353/1692 (79.00%)\n",
      "\tTest loss: 0.00207, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.13311\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.11307\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.35133\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.07988\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.10811\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.04594\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.05967\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.18354\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.05749\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.09132\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.11687\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.14506\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.29117\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.03692\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.10877\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.36134\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.13928\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.20849\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.06332\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.12294\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.20278\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.20460\n",
      "\tTrain loss: 0.01097, Accuracy: 5994/6768 (88.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1356/1692 (80.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 575/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.65473\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.23414\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.09157\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.18521\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.20503\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.43093\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.09991\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.06967\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.12933\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.17855\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.18063\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.11817\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.15209\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.15403\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.08582\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.13188\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.09916\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.36809\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.39314\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.13047\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.45715\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.26853\n",
      "\tTrain loss: 0.01197, Accuracy: 5878/6768 (86.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1326/1692 (78.00%)\n",
      "\tTest loss: 0.00209, Accuracy: 584/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.55907\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.12163\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.28048\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.04147\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.23431\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.12137\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.06975\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.08957\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.06920\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.02436\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.04333\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.05172\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.27706\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.13842\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.33631\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.02836\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.19214\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.07413\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.08021\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.13085\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.34137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.08854\n",
      "\tTrain loss: 0.01396, Accuracy: 5707/6768 (84.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1262/1692 (74.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.33387\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.04496\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.23930\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.04557\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.11069\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.03257\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.05360\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.07350\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.16929\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.06643\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.08172\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.06152\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.16793\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.06604\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.38620\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.04005\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.19508\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.27139\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.08133\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.03756\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.20493\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.18322\n",
      "\tTrain loss: 0.01563, Accuracy: 5769/6768 (85.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1289/1692 (76.00%)\n",
      "\tTest loss: 0.00215, Accuracy: 580/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.21637\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.04071\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.30459\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.16543\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.18221\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.17404\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.04218\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.05074\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.04931\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.28772\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.20196\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.07981\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.08120\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.13389\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.19490\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.07423\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.10286\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.06367\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.03255\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.05111\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.16915\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.05527\n",
      "\tTrain loss: 0.01347, Accuracy: 5802/6768 (85.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1306/1692 (77.00%)\n",
      "\tTest loss: 0.00206, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.14069\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.08910\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.14829\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.09362\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.03098\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.26005\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.13956\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.03271\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.18557\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.16631\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.23773\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.11319\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.05538\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.21190\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.13496\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.08823\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.07812\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.18448\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.07262\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.08563\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.23610\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.04015\n",
      "\tTrain loss: 0.01297, Accuracy: 5837/6768 (86.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1324/1692 (78.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 592/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.25513\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.22047\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.15630\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.08545\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.19550\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.06047\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.13211\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.14764\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.14903\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.11001\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.02714\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.08506\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.19706\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.21216\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.04566\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.13471\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.13311\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.37908\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.27396\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.30023\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.12118\n",
      "\tTrain loss: 0.01229, Accuracy: 5937/6768 (87.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00228, Accuracy: 585/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.18718\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.25665\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.45362\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.09680\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.61041\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.07918\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.33221\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.16599\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.14945\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.16338\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.14046\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.11158\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.25057\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.09782\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.14210\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.02858\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.20129\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.17276\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.06407\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.16286\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.08569\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.09351\n",
      "\tTrain loss: 0.01151, Accuracy: 5964/6768 (88.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00216, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.13606\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.15872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.22040\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.05980\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.08897\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.02962\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.15988\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.09882\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.05009\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.27451\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.19678\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.05899\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.12777\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.09799\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.27889\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.19358\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.15415\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.20287\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.13915\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.15571\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.08062\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.15970\n",
      "\tTrain loss: 0.01245, Accuracy: 5997/6768 (88.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1346/1692 (79.00%)\n",
      "\tTest loss: 0.00210, Accuracy: 556/1772 (31.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.8014184397163121\n",
      "Best test accuracy:\n",
      "0.36681715575620766\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAEICAYAAACd/8f0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABwRklEQVR4nO2dd3gc1dX/P3d3tavei61myR33bjCmQwKEGppNEiCQ8EJ6TyAJSUj4Je/75k2FFBICCQEcQjWE3psN7r3Jtmz13rWSVrv398edXa2kVbW6z+d59OzuzJ2Zu6OZ/c4599xzlNYaQRAEQRDGNrbR7oAgCIIgCH0jgi0IgiAI4wARbEEQBEEYB4hgC4IgCMI4QARbEARBEMYBItiCIAiCMA4QwRYEQRCEcYAI9kmKUipfKXX+aPdDEITOKKXeUkrVKKVco90XYWwhgi0IgjBGUErlAGcAGrhsBI/rGKljCYNHBFsIoJRyKaV+o5Qqtv5+43/KV0olK6WeV0rVKqWqlVLvKqVs1rrvKqWKlFINSqkDSqnzRvebCMK45QZgI/AQcKN/oVIqSyn1lFKqQilVpZS6N2jd55VS+6z7b69Saom1XCulpge1e0gp9TPr/dlKqULr3i0FHlRKJVj3eIVl4T+vlMoM2j5RKfWg9dtQo5R6xlq+Wyl1aVC7MKVUpVJq8XCdpJMVEWwhmO8DpwKLgIXACuAH1rpvAoVACpAG3AlopdQs4EvAcq11DPBxIH9Eey0IE4cbgEesv48rpdKUUnbgeeAYkANkAOsAlFLXAD+2tovFWOVV/TzWJCARmALcitGDB63P2YAbuDeo/cNAJDAXSAV+bS3/B/DpoHYXAyVa62397IfQT8QNIgTzKeDLWutyAKXUT4A/Az8EPMBkYIrWOg9412rjBVzAHKVUhdY6fzQ6LgjjHaXUaoxYPq61rlRKHQaux1jc6cC3tdbtVvP3rNfPAf+jtd5kfc4bwCF9wI+01q3WZzfwZFB/7gHetN5PBi4CkrTWNVaTt63XfwI/VErFaq3rgc9gxF0YYsTCFoJJxzzF+zlmLQP4X8yPwStKqSNKqe8BWOL9NcxTfrlSap1SKh1BEAbKjcArWutK6/Oj1rIs4FiQWAeTBRwe5PEqtNYt/g9KqUil1J+VUseUUvXAO0C8ZeFnAdVBYh1Aa10MvA9cpZSKxwj7I4Psk9ALIthCMMWYJ3w/2dYytNYNWutvaq2nYtxu3/CPVWutH9Va+60DDfz3yHZbEMY3SqkI4FrgLKVUqTWu/HXM0FQZkN1DYFgBMK2H3TZjXNh+JnVZ37VU4zeBWcBKrXUscKa/e9ZxEi1BDsXfMW7xa4ANWuuiHtoJJ4AI9slNmFIq3P8HPAb8QCmVopRKBu7CuLtQSl2ilJqulFJAHeAFfEqpWUqpc63gtBaMW803Ol9HEMYtV2DuqTmYGJJFwCmYoacrgBLgF0qpKOt+Pd3a7q/At5RSS5VhulLK/9C9HbheKWVXSl0InNVHH2Iw92+tUioR+JF/hda6BHgR+IMVnBamlDozaNtngCXAVzFj2sIwIIJ9cvMC5gb1/4UDm4GdwC5gK/Azq+0M4DWgEdgA/EFr/SZm/PoXQCVQiglGuWPkvoIgTAhuBB7UWh/XWpf6/zBBX2uBS4HpwHFM8Od1AFrrfwP3YNznDRjhTLT2+VVru1pMfMozffThN0AE5l7eCLzUZf1nMLEs+4FyzFAYVj/849+5wFP9/9rCQFBad/WKCIIgCMLAUErdBczUWn+6z8bCoJAocUEQBOGEsFzot2CscGGYEJe4IAiCMGiUUp/HBKW9qLV+Z7T7M5ERl7ggCIIgjAPEwhYEQRCEccCYG8NOTk7WOTk5o90NQRjzbNmypVJrnTLa/egNuZ8FoX/0534ec4Kdk5PD5s2bR7sbgjDmUUod67vV6CL3syD0j/7cz+ISFwRBEIRxgAi2IAgAKKUutMqj5vlzxXdZP0Up9bpSaqdS6q3g0ouCIAw/ItiCIGAVeLgPU7hhDrBWKTWnS7NfAv/QWi8A7gZ+PrK9FISTmzE3hi2MfzweD4WFhbS0tPTdWOiT8PBwMjMzCQsLG87DrADytNZHAJRS64DLgb1BbeYA37Dev0nfqS5DItfH0DJC14cwBhDBFoacwsJCYmJiyMnJwdQKEQaL1pqqqioKCwvJzc0dzkNlYJJf+CkEVnZpswP4JPBb4EogRimVpLWuCm6klLoVuBUgOzu724Hk+hg6RvD6EMYA4hIXhpyWlhaSkpLkx3gIUEqRlJQ0VqzRb2HKP27DVH4qwlSY6oTW+n6t9TKt9bKUlO6zVOT6GDrG2PUhDDNiYQvDgvwYDx0jdC6LgKygz5nWsgBa62KMhY1SKhq4SmtdO5iDyfUxdMi5PHkYdxZ2i8fLj9fvoaqxdbS7IggTiU3ADKVUrlLKCawB1gc3UEolK6X8vxl3AH8b4T4Kwqiw8UgVHq9vwNvVt3j48fo9NLW2D0k/xp1g7yqq49GPjnPJ799je0HtaHdHGINUVVWxaNEiFi1axKRJk8jIyAh8bmtr63XbzZs385WvfGWEejp20Fq3A18CXgb2AY9rrfcope5WSl1mNTsbOKCUOgikYeowjzvk+hAGwrbjNay5fyOPfXR8QNsVVDdz1R8+4J8bj7H1eM2Q9GXcucSX5yTy1O2ruO2fW7j2Txv4zZpFXDx/8mh3SxhDJCUlsX37dgB+/OMfEx0dzbe+9a3A+vb2dhyO0Jf+smXLWLZs2Uh0c8yhtX4BeKHLsruC3j8BPDHS/Rpq5PoQBsIre8sAeH5nCTecltOvbd4+WME3H99Oa7uPf9y8glXTk4ekL+POwgaYlxHH819ezYLMOL746FYe+XDMZ2gURpmbbrqJ2267jZUrV/Kd73yHjz76iNNOO43FixezatUqDhw4AMBbb73FJZdcApgf85tvvpmzzz6bqVOn8rvf/W40v4IwjMj1MXG5/Z9beGJL4aC3f80S7E351ZTX9x3c9/MX93Hj3z4iMcrJ019YNWRiDePQwvYTH+nk4VtWcvsjW/j+07t5+0AFd18+j0lx4aPdNSGInzy3h73F9UO6zznpsfzo0rkD3q6wsJAPPvgAu91OfX097777Lg6Hg9dee40777yTJ598sts2+/fv580336ShoYFZs2Zx++23y3zXIUSuD2E4aWv38dKeUiKcdq5eOvDEfPmVTRwqb2Ttimwe++g4L+8p5TO9WNkHyxr489tHuGpJJvdcOY/wMPsJ9L4741awASKcdv56wzL++t5Rfv3qQS699z2e+eLpZMRHjHbXhDHINddcg91ubqC6ujpuvPFGDh06hFIKj8cTcptPfOITuFwuXC4XqamplJWVkZkpGTknInJ9jE/avT6OVTczLSW627qSOjdaQ21z6P9fX7y2z1jXXzh7Gh8dreI/u0p6Fex/by7AYVPccfHsIRdrGOeCDeCw27jtrGmcNTOFa/+0gVse2sQTt68i2jXuv9qEYDCWznARFRUVeP/DH/6Qc845h6effpr8/HzOPvvskNu4XK7Ae7vdTnv70ER7Cga5PoQT5fmdJXztX9v5zXWLuGJxRqd1hTVuAGqbew8m7InX9pUxe1IMWYmRfGL+ZO59M4+KhlZSYlzd2nq8Pp7aWsR5p6SSHN19/VAwLsewQ3HK5Fju/dQSDpU38u1/70BrPdpdEsYwdXV1ZGSYm/uhhx4a3c4IYw65PsYPh8obAPjukzvZ0WXmUFFAsDtb2DsLazn3l2+RV97Y437L6lvYlF/D+aekAXDeKWn4NHx4tCpk+zf2l1PV1MZ1y7NCrh8KJoxgA5w1M4VvfWwWL+4u5fmdJaPdHWEM853vfIc77riDxYsXi1UkdEOuj/FDQbWblBgXKTEubnrwIx56/yit7SYBX2GtEeyaLhb2WwcqOFLZxHee2IHXp3n0w+P87Pm9eH0dht66jwrw+nRg7HtOeizhYTa2Hqvt1geP18eD7x8lNcbFmTO6Z/cbKtRYs0SXLVumT6TgfbvXx1V//ICCGjevfv1MkobJNSH0zL59+zjllFNGuxsTilDnVCm1RWs9pucYhbqf5foYek7mc3rlH94nIszO3ZfP4/tP7+LDo9WcMSOZh29ZyTce385TW4uwKci752JsNpMV7r8e3syb+yto8/pYmBUfsMw/fWo2P718Hu0+zer/foNZk2L5x80rAse69k8baPX6ePaLpweWNbe188VHtvLmgQp+evncXse4e6M/9/OEsrDBjGn/z9ULaWjxcN39G3ljf5m4xwVBECYQu4vqAlZ0QXUzWQmRTE+NZt2tp/LZ03P44HAVLR5vwCXu0ybrmJ89xfVcMDeN809JZUdBLZ9bnct/nTmVf248zk+e28vTW4soq2/lM6dO6XTcJVMS2FtcR4vHHLuysZXr//Ihbx+s4J4r5w1arPvLhBNsgFmTYrj/M8to9/q4+aHN/Ow/+0a7S4IgCCc1lY2tPL6pgDue2sV7hyp7beMX41C8treMS37/Hv/eXEhzWzuVjW1kJ0UCJq/6ytwkvD7N3pJ6imrdhNmNVe0fx65r9lBY42Zueiy/XbOYJ247jR9cMofvXjibT63M5qEP8vnOkztJjwvn3NmpnY69JDsej1ezq6iOIxWNXHHf++wvrecPn1rKp1Z2FvfhoF+h1EqpCzEl9ezAX7XWv+iy3gX8A1gKVAHXaa3zg9ZnY+rq/lhr/cuh6XrvnDM7ldOnJ/PT5/fywHtHSY+P4JbVUn5OEARhNPjKY9v44LAJ2Kpv8bB6RueEIj6f5suPbmPDkSr++t4RfnXtIuZlxHVqU9fs4c6ndwGw7XgtK3ITAchM6JjKOz/TbLOjoJbSuhZmpsWwt6SemuY2cohiT0kdAHPT44hyOViWY/ZhsynuuXI+16/M5m/v5XPO7BTsts6FVZZMSQDgo6PVvLCrhOY2L/+69TQWZsUPxSnqkz4tbKWUHbgPuAhTwH6tUmpOl2a3ADVa6+nAr4H/7rL+V8CLJ97dgeF02PjxZXP5+Nw0fvafvXzxka28vk9c5IIgCCNJu9fH1uM1fGplNouy4ql3d58X/dAH+Ww4UsUNp02hzu3hU3/9MOB69vPT/+ylqqmNqclR7C6q43hVMwBZiZGBNulx4SRGOXl9XzntPs28jFigw8LeU2QS9cxNjw3Z17npcfzftQu5ZEF6t3XJ0S6mJEXyx7cOs6e4np9dMW/ExBr65xJfAeRprY9orduAdcDlXdpcDvzdev8EcJ6yar4ppa4AjgJ7hqTHA8RuU/x2zWI+uyqXjUequOXvm/nnRkllKgiCMBT4fJrvPrGTnYW1PbbZX9pAi8fHqVOTSIgM6zbN6nhVM//90n7OnZ3KTy6byy+vWUid28NbByoCbbYcq+aJLYX815lT+cSCyRwqb+CQNS0rO0iwlVLMy4hjwxFjzfutdH+k+J7iOtJiXYOeK700O4HG1nbOnpXCRfMmDWofg6U/gp0BFAR9LrSWhWxjVf2pA5KsmrnfBX5y4l0dPOFhdu66dA4f3nkeZ89K4af/2cee4rrR7JIgCMKEoLjOzb82F/DQ+/k9ttlmVatanB1PfKSTWnfnaVaPfnScdp/mnivnoZTiNEvY/7PLTM/1+TR3P7+PtFgXXzp3OvMy4vBpeGVvKRFhdpKinJ32tyAjLjBFq0OwLQu7uJ656Z1d7QPhnNmpxEWEcfdl80a8FvlwB539GPi11rrn2emAUupWpdRmpdTmioqK3pqeEA67jf+7ZiEJkWF86dFtnaIGu6K1pryhRdzn45BzzjmHl19+udOy3/zmN9x+++0h25999tn4px5dfPHF1NbWdmvz4x//mF/+svfwi2eeeYa9e/cGPt9111289tprA+y9MNzI9dE7Owpqew366kpJnSmI8dbBik7zmIPZdryWlBgXGfERxEWEURdkYXt9mqe3FXL2zBQmx5mxaIfdxoXzJvP6vjLcbV6e21nMjoJavv3x2UQ6Hcy3RHjb8VqyEiO6CWfw2PfsSTEoBXXNbbjbvByuaOzRHd4fLl2YztYfXhAIdBtJ+iPYRUBw6pZMa1nINkopBxCHCT5bCfyPUiof+Bpwp1LqS10PoLW+X2u9TGu9LCVl+CadAyRFu/j92iUUVDfz9XXb8XW5wHw+zZ1P72LxT19lxT2v89Lu0mHtjzD0rF27lnXr1nVatm7dOtauXdvnti+88ALx8fGDOm7XH+S7776b888/f1D7EoYPuT56pqTOzRV/eD9Q3aqptZ31O4q7GS7tXl9gWbGVnKS6qY0dPbjFtxXUsjgrHqUUcRFh1Le0B8T9vbxKyupbuapLcY5LF0ymuc3LL17cxw+e2c28jFg+aaUenWyNU0Nnd7gff+BZcrSLSKeDuIgwapo9HCxrwKdhzuTBCzbQLRhtpOiPYG8CZiilcpVSTmANsL5Lm/XAjdb7q4E3tOEMrXWO1joH+A3w/7TW9w5N1wfPitxE7rp0Dq/vL+dXrx7stO6lPaU8+uFxVk1LwqZgX8nQVhIShp+rr76a//znP7S1Gbdbfn4+xcXFPPbYYyxbtoy5c+fyox/9KOS2OTk5VFaaKSf33HMPM2fOZPXq1YHyigB/+ctfWL58OQsXLuSqq66iubmZDz74gPXr1/Ptb3+bRYsWcfjwYW666SaeeMKUj3799ddZvHgx8+fP5+abb6a1tTVwvB/96EcsWbKE+fPns3///uE8NQJyffTG/tIGtCaQsvOZ7UV85bFtbDjckY6zxePlqj9+wDcf3wFAkSXYNgVv7i/vts+apjaOVjaxONtEWMdHmmpm/sCzJ7cUEhcRxnmndJ5CtSI3keRoJ3/fcIzMhEj+9OmlgcQn/nFqgMyE7oLtDzzLsKLHEyKd1DS3Bca8Z6TFDOb0jDp9TuvSWrdbVvHLmGldf9Na71FK3Q1s1lqvBx4AHlZK5QHVGFEf03zm1CnsLa7n3jfzmJsey0XzJ+P1aX716kGmp0bz+7VLOPN/3qTAmngvDJIXvwelu4Z2n5Pmw0W/6HF1YmIiK1as4MUXX+Tyyy9n3bp1XHvttdx5550kJibi9Xo577zz2LlzJwsWLAi5jy1btrBu3Tq2b99Oe3s7S5YsYenSpQB88pOf5POf/zwAP/jBD3jggQf48pe/zGWXXcYll1zC1Vdf3WlfLS0t3HTTTbz++uvMnDmTG264gT/+8Y987WtfAyA5OZmtW7fyhz/8gV/+8pf89a9/HYKTNE6Q62NMXR+HLUE7ZkVfHy5vAmD9juJAXef/98I+dhTWUWcJbkltC3ERYcxMi+aN/eV882OzOu1zu5VFbEl2PNAh2LVuD2EOGy/vKeXaZVm4HJ2rWznsNr5xwSwOlNbzvYtOIcLZef38jFjeOVgR0sJWSnHrmVOJDQ8LHLO22cPhikYcNsWUUXBnDwX9GsPWWr+gtZ6ptZ6mtb7HWnaXJdZorVu01tdoradrrVdorY+E2MeIzcHuD0opfnL5XBZlxfPNf+/gwyNVPPLhMfLKG/nGBTOx2xSZCREcr24e7a4KgyDY7el3dz7++OMsWbKExYsXs2fPnk7uya68++67XHnllURGRhIbG8tll10WWLd7927OOOMM5s+fzyOPPMKePb1PgDhw4AC5ubnMnDkTgBtvvJF33nknsP6Tn/wkAEuXLiU/P3+wX1kYAHJ9hOZQmV+wjVAfrTSfX9hVQmu7l5d2l/CPDcdIjHJSUOOmrd1Hca2b9PgIzpmdyp7iesrqWzrtc+vxGuw2FXBTx0UYEa1zezhS0Uhru48zuszJ9nP9ymx+cvm8bmINBMaxs0IINsBtZ03j+pXZgLGwa91t5JU3kpMcRZh9fOYMO6lrULocdv706aVceu97XHf/RsCMbVw414TqZydG8vbB4QuCOynoxdIZTi6//HK+/vWvs3XrVpqbm0lMTOSXv/wlmzZtIiEhgZtuuomWlpa+dxSCm266iWeeeYaFCxfy0EMP8dZbb51QX/0lGk/K8oxyffTJUF0fHx6p4ucv7ueRz60kqofyw3kVRqALqt14fZqjlU0kR7uobGzloffzufeNPBZmxnH9ymy+++QuCmqaKa5rIT0unDNnpPA/Lx3go6PVXLqwYw7z2wcrWJAZR6TTHDMuwow91za30e4149hpseED/j7nzk7jh5fM4cyZocU+mPjIMA6UNtDc6mXmOHWHwwRNTToQJsWF88wXT+dX1y7k/65ZyJ8/0zFOkpUYSXlDa7fJ+8LYJzo6mnPOOYebb76ZtWvXUl9fT1RUFHFxcZSVlfHii73n8TnzzDN55plncLvdNDQ08NxzzwXWNTQ0MHnyZDweD4888khgeUxMDA0NDd32NWvWLPLz88nLywPg4Ycf5qyzzhqibyoMhpPx+njnUAXbC2r56Gh1yPVaa/LKGwkPs9Hm9VFY00xBjZurl2aSFOXk5y/uJ8xh4w+fXhoQvSMVTQELOzfZ1BMP9koW17rZWVjHx+Z0zFf2u8Tr3B7KG8xYfWrswOdEOx02blmd282VHor4CCdVTa0cq25memr0gI81VjjpBRsgIz6CTy7J5KqlmZ3cK1mJJmChUMaxxyVr165lx44drF27loULF7J48WJmz57N9ddfz+mnn97rtkuWLOG6665j4cKFXHTRRSxfvjyw7qc//SkrV67k9NNPZ/bs2YHla9as4X//939ZvHgxhw8fDiwPDw/nwQcf5JprrmH+/PnYbDZuu+22of/CwoA42a6P/EojpO/nhc7jXdHYSp3bw+rpZqbOe3mVeH2aGanRXLYoHZuC369dTEZ8BFOTjejtLjJj2enxEUS5HCRHOwPZxwBe21cGwMfmpgWWxVsu8dpmD+UNLSjFoJOY9JeEyDBaPD68Pj2uBXvCldccSjbnV3P1nzbw4GeXc86s1L43EICTu9TfcCHlNYXe6M85vei377KvpJ45k2N54atndFv/weFKrv/Lh/zik/P53lO7uGBOGq/uLeOpL6xibnosBdXuTmK39KevMjUlik35Nfx2zSIuX5TBJ//wPi6HncduPRWAT//1Q0rq3Lz+zbMD23m8PmZ8/0W+ccFMSupaeGVPKVt+eMHQnIgeeHjjMX74zG4AnvvS6sB4+ljipCyvOZT4re1CCTwTBGEco7XmWFUTTrvNFMJoauvWxh8hvnpGMmF2FbDEpyZH4XLYu1mmuclRbDteC0B6vPFGZidGBlzidc0eNh6p4mNzO6fvDLPbiHY5qG32UNHQQkrM8FrXYCxsP9NSo4b9eMOFCHYvpES7cDpsnaZ2vXOwIvCkJggTCaXUhUqpA0qpPKXU90Ksz1ZKvamU2qaU2qmUung0+ikMnPKGVprbvFw834inP892MHnljUS7HGTER5CVEElzm5f4yDDiI53d2gJMTYmi3Up+EhDspCiK60z0+JsHTPGNj81J67ZtXEQYte42yhtaBxVwNlDirUC3jPiIQPDbeEQEuxdsNkVWQgQF1hOj1pr/98I+Ht54jOoQT6hCB2NtqGU8MxLnsp9V+X4APK61XozJtfCHwR5Pro+hoz/n8milmaZ1+aIMopx2PjjcfRz7UHkj01KjUUoF0m76A8lCkWuNY9sUpFlWcnZiJFpDYU0zG49UERcRxsLM+G7b+tOTlte3kjoCFrY/0G3aOB6/BhHsPslKjKSgxgj2pvwa9peaKM+DZd2jPQVDeHg4VVVV8qM8BGitqaqqIjx82K2Q/lTl04A/p2McUDyYA8n1cWJ4vL7A+/5eH37Bnp4azYrcRD7I67Cwj1Q08rvXD7GzsI7pKUbQcpKMUPcm2FNTzLq02HAc1rxmf0KS49XNbDlWw5Ls+MCsm2DiI8Oobm6jsrF1UBHiAyLvNZLbTRER//frN40VsOUhePp2qC3os/lwM359AyNEVkJkYJzmHxvycTlstLb7OFTWwKlTk9hRUEuE0z6u5/YNNZmZmRQWFjKchVxOJsLDw8nMzOy74YkRqirfyi5tfgy8opT6MhAFDCoRtlwfg8ft8VLd2EZanAuHzYhkf66P/Eozfp0eH8GirATePFBBi8dLeJid2/+5lYPlDcyZHMs1y8x+/NnDcpN6EWxLzP3u8ODtdhXWcai8kcsXda8pDUaw9x2up92nSY0ZxodRreFfnyFl/rUsyrqWc2YPoFZFeyvcuxRarMqOMZPg/NApa3vF5wPb0NjGIth9kJUYQZ3bw5ZjNby0u5QbV+Xw+OYCDlgW9hcf3UpuchQP39L1t+3kJSwsjNzc3NHuhjD0rAUe0lr/n1LqNEw64nlaa19wI6XUrcCtANnZ2d12ItfH4PnyY9t4bkcJ/7r1VFZOTeq2/q5ndzMvPY5rl2d1Wn60sonspEjsNkV2Usd01anJURytbOJzq3P5/ic6RkByki3BTulZsLOTIrEpU4jDT2qMC5fDxjPbTX2opVMSQ24bF+EMlLscVpd4Yxl4mrE3V/LMF3ufqteNqsNGrC/+Jex7Dvath/PugoGU1CzbC099Hq5+EFJmDuz4IRCXeB9kWYnlr/rjB9htihtOm8LMtBgOljVSWNNMYY1b5mkLE4H+VOW7BXgcQGu9AQgHuqWZGsnqeycTre3eQHGN+pbuGc9K6tz8Y8Mxnt3R9d8G+VVNATe33wouqG6mtL6FNq+PKV0s6dOmJnPbWdM4u5fprC6HnU+fOoWL508OLFNKkZ0YyeGKJuw2xcKs0NOn/OlJYXBJU/pNzTHz2jQIb07FPvOafSrMuQyq8qDiQO/bBFO4BR68CJqroPMz7aARC7sPVk1L5opF6cxNj+PCeZPISoxkZloML+4u4cMjJmNQUa0bn0+HHKsRhHFCoCofRqjXANd3aXMcOA94SCl1Ckawxa89Qmw8Uk1jqxFqf+GNYPylgI9UNHVa7vNp8quaA+Lrn65aUNNMeJjJEta1GEaE0873LppNX9x9+bxuy6YkRXKovJE5k2N7jMiOD5pmNawu8doTEewDoGyQNAOiUuA/3zKWdmof52XvetixDvJeg9jJcMOzkJAz8OOHQCzsPoiLDOM3axbz+TOnBi70mWnR1DZ7eGGXCWRoa/dRJVHjwjhGa90O+Kvy7cNEg+9RSt2tlPJXtvgm8Hml1A7gMeAmLZFjI8Yre0oJsxujoD6EYL9oCXZJXQvNbR0WuH+ald/CTol2ER5m43hVM8erjbhPSRy6ucn+38mlUxJ6bBMfZGEP6zzsgIXdfRpbn1Tsh4RcCAs349dZK2D/c71vs+EP8PhnoHQnLPss3PzykIk1iIU9KGZZAWZvHCgn0mmnuc1LUa17RBIACMJwobV+AXihy7K7gt7vBQY4ECgMBT6f5tW9ZZw7O5WX95RR39JZsCsaWtmUX83MtGgOljVytLKJuelx7Cmu4xcvmhraM9JMhLTfbX28uhmnw4bDpkiPHzord4ol2Et6E2zLwo4NdwSs/GGhNt+8ttaZIDLHAH6jy/dDSpA1fcql8MoPoPY4xHePzWDL3+HlO+CUy8yYtX3o5VUs7EHgL36uNYHKXkUyji0IwjCxt6Se8oZWPj53EjEuB/XuzmPYL+8pRWv4wtnTAeMWL6p1c+V9H7CzsI47L57NsiAB9Qv2sepmMhIiAtOyhoJV05OZnxHH6dO6B8X5ibUs7NThTprit7ABmoLmnns9cPBl474+9oH5MQ+mvQ2qD0NKUG3vKavMa6j67fXF8OJ3YNq5cNUDwyLWIBb2oEiOdpIQGUZNs4crl2Tw1LYiims7BFtrzWv7yjl3dip2GdcWBOEE2Zxv4mVOnZpEbERYtzHsF3aVkJscxcctA+JIRRPuNi9tXh/P3no6p0yO7dQ+MyGSDYerCLPbAkFoQ8XMtBie+/Lqnhvse560dhOrmDYcAWc71kFzNZz2BSPYzhhoazDj2HEZps3eZ+HJWzq2yVgGF9wNOZYDqfoI+NohNSg/e5J5GKIqr/sx3/oF+Lxwya/BEToz3FAgFvYgUEoxMy2GKKed06YmEeNyUBQk2FuO1fD5f2wORHQKgiCcCFuP1zIpNpz0+Ahiwh2dXOIldW42HKnisoXpRDjtZMRHcLSykY1Hq0iMcjJ7UvccEdmJkTS1eTlQ2tAt4GxYaSyHf9/E5N1/BgYZcNZcDfedCkff7b5uz9Pw9H/Baz8Cdy3UF0LGYmu7IAu7aAs4IuC29+DS30JDKfz9Ethwn7G2/RHiwRZ2eBxEpULloc7HrDwE2/4Jy24e0vHqUIiFPUhuO2saJXUtOKxkBMFTu/KsJPr+DGmCIAgnwtbjNSyZEg+YKVHBQWfPbi9Ga7hysbEep6ZEcaSyieqmNlbkJKJCzBv2W9VtXt+QBpz1yda/g8+D02OSkQxqDvauJ4ygHn4dcoOqjh14CZ76L4jNNEK9Y52ZTpWxDI6+09klXrwdJs3v+Jt/DTx9G7x8p5l/HZUMKEjuMnc6abpZH8zrd4MjHM789sC/ywARC3uQnDM7letXmsCDjISIThb20SoTeRnsJhcEQRgIdz27m6e3FVLe0EJhjZsl2WYMuqtL/JltRSzOjifHyjw2NTmKfSX1FNa4WZEbOnFJdpBVnT1SFra3HTY/CIC9tZavnjeDy3rIhEbhFnj/t93HlgF2PGZeyy0ruLUR/v1ZeOw6I6ifew3ComDzA2Z9plWx0j+1y+czUdyTF3bs0xkF1/wdVn3FbPfBvcZaDuvI4gZA0jSoCrKwj31gEqqc/lWIHv6cAyLYQ0BGfEQncT5mFYovrm0ZrS4JgjCO8Xh9PPLhce75zz42HDZTkhb7BTs8jAYrccre4nr2lzbwScu6BpP/2+M1QrdyamjB9ieEgu5zsIeNgy9CfRFEJKLctXz9gpnMTbcSq7S3GYvYawXTvfX/4NW7OkTXT8UBKN4KdieU7zXLtj0Me56Cs++EW980c59zz4TKg2Z92jzT3i/Y1UegrbGzYINJH3rB3XDal8DT1DlC3E/yDLMfd60R/pfugJh0WPXlITlFfSGCPQRkJJj0pf6kBvl+C7tOLGxBEAZOQXUzXp+msrGNn7+wH6fdxrwMEzgWG+EIuMSf31mMw6a4ZEGHpTrVKnARG+5g9qTY7jvHJEZJCaqwNSJs+yfEZsCcy8Fd03ndrn+bsefdTxgxPPK2cTO/dGfnqOwdj4Gyw9KbzPSq1gY4vhHisuHs73ZM25p+nnm1OSAu0yQ+8c/FLtluXrsKNpi0ox/7GVzwUzjti93XBwLPDpu+lmw3+cWdI3MOZQx7CPAnvy+qcTMjNbpDsMUlLgjCIPD/hiRGOSmtb2Fxdjwuh5mvHBcRRkNrO16f5khFE7nJUSREdUQm+6torchN7HWWil+oR6Q+tM8HxzfAnCvM+HBLbeeiGHmvmdctD5nsYj4PXPcwPPc1ePBiWLgW7GGw9WEjxlPPho/uNxZ34SaTPjSY6VZdmrhMsNkhMqnDwi7ZYSzuUBY0GNE+/Suh1yXNMK9Vecb6T54F868d1CkZDCLYQ0CGJdjFtW5iIxy0eHwkR7sob2ilrd2H0yGODEEQ+o8/veidF5/Ct/69IzB+DcYlDtDQ4qGkvoVJcZ0jrdPjIliYFc+lC3sYH7a4emkm5fWtQ9zzHqjKM4U0MpebV+0zU63C44wb/PAb4Iw2ot7aaNzMMz4ON66Hd/8Ptpixb6aebaxfu5UpLe8142bPXNH5eIm5JmAsfor5HJXSWbDT5g5u+lVCjnmgyHsNCj6E8340ZJW4+oMI9hCQmWBVv6l1B7L2nDYtied2FFNW3xJI1ScIgtAf8quaiA13cNWSDKoaW7lw3qTAOn/SkXp3O6V1bmaldQ52stkUz/ajMtXaFSGydXXF0wIFG41Q9kZjOUQm9yxeRZvNa+ZyM6UKjFs8PM6MSbfUwkX/a6K0y3bB8s+bfaXMgk/eDxf/r3GFu6x61j6vmZa17Z/mc9byEF9wnbGkwQh25SETxFayA+Ze0fd3D4XDaR4Cdj9hPs+/enD7GSRi+g0BKdEuwuyKY5VNAVfWKivLj7jFBUEYKPmVzeSmRKOU4r/OmtapmlZsuLGzqppaKW9oZVJcRE+7OXF2PwH/uByOf9hzm5Y6+O1C2HBvz20KN4Er1li9EfFmmbvWvOa9ZqzW+Veb9J/Q8eonPK5DrMG4uVNmQV2BGetOm9/9mEnTIN4qQBeVbOZh1+Sbh4NQ49f9JXmG8RBkrwqdonQYEcEeAmw2xWnTknl+Zwl55Y047bZAGkAJPBMEYaAcrWwit4fobX9pyrzyRrTuXI96yKkwecjZ+S/zWn3EZPV67quw73mzrGQneJrNHOueasEUboKMpcZqjrDc+/7As7zXzFzpyEQ4+3tw6hdgSj9S1qda9bvTF/ft3o5KNn3c9W/zOefMvvffE/7AswXXDH4fg0QEe4j4zKlTKK1v4YkthWQnRZJpTZuQqV2CIAyEFo+X4jp3YF51V/wu8YNlDQDdxrCHlOqj5nXP09DWBP+8Gt76uXFFv/Ezs65kh3mtyutwdwfT1gRle4w7HCA83ry21JqsZUVbO4LEUmbBhT/vXy5uf9rQzBDu8K5EWcMGm/5qBD55et/b9MSUVRCdZgLoRhgR7CHi3NmpZMSb6V05SVFEOO0kRIaJS1wQhAFxrKoZrc186lD4BftAmcmoOGkoC2h4283cYn82r6rDxh3troZ/32QKYlz/OJz1PZNtzF1jkpBEJhnX9PZHu++zeJtxIfuFNdjCrsoDtBHRgTLJqsXdNUI8FH7Bbiw78ajuUy6Fbx4wHoERRgR7iLDbVCDzWY7lykrvklBFEAShL45WmjiYngTb7xI/WGos7CF1iZfvhY1/MElMfD6oOWqmVEUkwKFXYOo5MONjkL3StC/cbFziGctg9iWw+0lTxjKYQn/AmZVxLHgMu67Q+lIZDJjcs2Htv2DmRX23jTTFRlA2mHfVwI/VlRDpXkcCEewh5LrlWSRFOVmWY54gjWCLS1wQhP7jF+yeXOJRTjs2BaX1LYSH2QICPiSU7TGvxdvMdKn2FuOmnneVEbuP32PEKmOpido+8pbJKDZ5ASxcY9zcR9/p2J+33bjP0+Z3WKRhEcYad9eYY4BJqDJQbDaYdWH/plVFWYKdexbEpA38WGMEEewhJDnaxeYfnM+F8yYDkB4X3qeF3dbu48z/eZNntxeNRBcFQRjj5Fc2kRztDMy37opSKuAWnxwXEbK4R0hKd8GTnzNpQINpqTPjzADllmCXbDfub4DEaXDeXfC51838ZTC5tyfNNy5w7YVJC4yIQ0dKUIAdj5rc22d/r/Mxw+ONuNcVmfnX4XH9+w6DJWayccmf9qXhPc4wI4I9xATfPOnxETS0tncqhdeVY1VNHK9u5v28yh7bCIJw8rC/tJ6cpNDWtR+/mA9o/Hrn4yZKunhr5+WPXAvPWkLmt7CbKiD/PfM+aZoR1IwlnbfLPtWMbYOxsCMSwBVnIskBPG548+dGKGd/ovO2EQmWhV1orOvhdjE7nKYoyIzzh/c4w4wI9jDijxT3FwOpa/ZQ2KXk5hHL/XXQCiARBOHkZX9pPTsK6/jY3N7dtnEBC3sAgu3PoX18Y+fl5fvg0KvGfV22pyP95u6njOs6poeMaVlWdrHwOJNMRCmTYcwv2DvWQUOxyQbWVZAj4q0x7CKI7T0jm9CBCPYw4k/Wv6vI1H79yXN7uOGBjzq18acgNHMqe5jDKAjCScE/NhzD5bBx7bKsXtvFRphpT/2e0qU1FFvTrwqCfoNa6qG1zqQJPfSKiaJecJ0Zr64+DAm5PY8RZ1nR2ZMWdAhy4tSOqWCFmyAqtXPNaj8RCUaw64sGF3B2kiKCPYxkJ0YSFxHGrqJaADYcqaKwxt1JmI9WGsu6sbWd4joJUBOEk43yhhae2VZEWb15vWxhOvGRvScC8bvE+21hVx8xwhwWaXJg+3+D6oNiZzb+wbxmLu0ojJE4ted9xmWYbF+zgqK0E3NNFS2vB8p2d0y96kpEgnk4aCyH2Mz+fQdBcokPJ0opFmTGsbOwjqJaNyWWIDe2thNj3XBHKpqICLPj9ng5WNYQKCQiCMLJwd/ey+dPbx8mzK7weDU3nJbT5zaBMez+piX1u8MXroHNfzMCnjTNuKTBuL7z3zXvU+fC5EVmildSL4INcPOLnT8nTjVBaNVHoXw/rLw19Hbh8dBUbt6Lhd1vxMIeZuZnxHGgtKFTUFl1U0eU5tHKJs6aaSb1H7IyFwmCcPKQV95Ielw458xK5YpF6czP7DtiOi5ygEFnxdtNIYylN5nPBVZu8LoC8+rP3R2ZDNGpkL7IfE6c1r/9+0nINa95r4K3FdJ6sbD9DGZK10mKCPYwsyAzjnaf5pGNxwLLKhuNYNc1e6hqamNxdjwpMS4OlErgmSBMdBpb2/n+07uosR7cj1Y2siAznvtvWMZv1vQv45c/6KzXMey2ZnhsrRmzLtlucm+nzTdBYv7As/oiM169cI35nDbHjEfnnAG2sO6R4X3hd6Hve87aX0+CHR/0ZcQl3l/6JdhKqQuVUgeUUnlKqe+FWO9SSv3LWv+hUirHWr5CKbXd+tuhlLpyiPs/5pmfGQ/AjsI6EqynYr+FfcQav56aEs2stBgOlYuFLYwe/bjPfx10Px9UStWOQjfHPe8dquSRD4/z2r4y2r0+jlc3k5vS+zSuAA1l8NpPuGJBCj+5bC4pMa6e25buggMvwLrroWibSf9ps5na0QELu8jMUZ6y2lTTSrcEOm0O3Fk08KpWMZNM2cvjG8HmMNW5QiEW9qDoU7CVUnbgPuAiYA6wVik1p0uzW4AarfV04NfAf1vLdwPLtNaLgAuBPyulTqpx8/S4cJKiTADJ+aeYqRpVjSZ1nz9CPDc5ihlp0Rwqa8Tnk0hxYeTpz32utf661nqRdT//HnhqxDs6AThgpRTdU1xPYY0bj1cztYesZt147Ufw3q/IaD7IjatyOq/z+eCDezvmT/sTn7hrTRS4382dtQIqDpiEKf550GHhcNu7cNZ3Ovbn6OVhoCf8U7vQkDyr5ypa/gIgXctmCr3SHwt7BZCntT6itW4D1gGXd2lzOfB36/0TwHlKKaW1btZat1vLw4GTTo2UUoExqY/NNUXoqwKusCbsNkV2YiQz02Jwe7wUSe5xYXToz30ezFrgsRHp2QTjQFk9AHuK6wJpSKf2x8Iu22vmNoOp7RyM1wNP3wqvfB/e+7VZVn3EpA+96q9GIHOs6VUZSwFt0o/WFXa4pBNyTAazE8XvFu8pQhw6LGyJEB8Q/RHsDKAg6HOhtSxkG0ug64AkAKXUSqXUHmAXcFuQgAdQSt2qlNqslNpcUVEx8G8xxlmek0h4mI1TpyYS6bR3colnJ0bidNiYmWaeMvcU149mV4WTl/7c5wAopaYAucAbPayf0PfzibLfsrD3FteTV26GxXKT+2Flvn63cTMDNAUJttbw9G0mi1n0JJMIBUylrfhsmHsFfDffRIVDx7h04SaoLx76KO2EHPPqT2MaCv8YtkSID4hhDzrTWn+otZ4LLAfuUEp1i5LQWt+vtV6mtV6WkpIy3F0acT53Ri6vfv0sYsLDSIxydnKJ+yvyzM+IJyEyjOd2Fo9mVwWhP6wBntBae0OtnOj384nQ4vGSX9lERnwETW1e3jxQTnyk+V3olbK9cPBFOP0r5nOwhb35b7D7CTj3B3Dq7SaQzF1rXOJ+azc401hEAiRNN9nN2luG3sr1H7NXwfZb2CLYA6E/gl0EBKfdybSWhWxjjVHHAVXBDbTW+4BGoBc/ycTE5bCTlWjSlCZFu6hqasPn0+RXdQi202Hj8kUZvLqnjNrmth73dbSyiYqG1h7XC8Ig6c997mcN4g4fFHnljfg0XLnYCNXGI1U9ltHsxKGXzevyz5ugLr+FXbrb1K+edh6s/qaJBAdjZVcf7bCqu5KxrCPwbKit3FkXwaJPQfZpPbcJjzNZ0AYa1HaS0x/B3gTMUErlKqWcmJt1fZc264EbrfdXA29orbW1jQMCbrTZQP6Q9HyckhTlpKqxjdL6Flo8vk4367XLsmjz+nh2e89W9q3/2MzPX9g3El0VTi76c5+jlJoNJAAbRrh/EwK/O/yShZMJsyt8uue6153Ie91MyYqdbEpFNlv20Ef3m+CwK/9sIsBTrQxl+e9Ca33P86j9talh6KdVxabDFX/ofTzcZoev74YlN/bcRuhGn4JtjTl/CXgZ2Ac8rrXeo5S6Wyl1mdXsASBJKZUHfAPwTwlZDexQSm0Hnga+oLU+qctSJUU5qW5q6wg2CbpZ56THMi8jlsc3F/S0OaV1LRTWSGCaMLT08z4HI+TrtCS+HxQHSutxOmxMT4lmZloMANNS+hi/bm2A4xtg+nnmc2SSqaYFJvFJ0nSItoYe4rJMuUr/POieUosGz68ercAvh6t/tayFAP2aYqW1fgF4ocuyu4LetwDXhNjuYeDhE+zjhCIx2gi2v0pX1/mX1yzN4kfr97C3uJ456bGd1rW2e2lobaeiUVziwtDT131uff7xSPZprHO8qpmbHvyIhz+3sl9phQ+UNTIjNRqH3ca89Dj2FNf3bWEffQd87TDdKg0ZldzhEq8v6ez2VgpSTzEBZdCzSzxtPthdHfsTxgXyeDPCJEe5aPP62FVYS0SYnbSYzjF4ly5Mx2FTPLu9+/BhbbOpq11eL0VCBGEssKOwliOVTewoqO21XX2Lh/zKJvaX1DPLsqznWdM9e7SwK/OgtsAEhzmjIWulWR6V0uESry/uXp4y9RTzquwmSjwUDqepYR2bPvy1qIUh46RKYjIW8EeDbjlWQ05yFDab6rb+zJkprN9RzHcvnN1pvX86WFObl6bWdqJc8u8ThNGk3AoALahu7rFNca2bi3/3buCB+5TJxnN2zdJMkqOczJoU030jnxceuABaak0O8OnndyQhiUwyFnZro6nA1U2wrcCz+Gywh/Xc+fN/bMprCuMG+cUfYZKizU13uKKJi+dPCtnm8kXpvLG/nI/yqzl1alJgeXDRkIqGVhFsQRhlyhuMt6unuBKtNd99cidt7T5+8cn5AFy8YDIA4WF2Lpo/OfSOy/aAuxqmXwBVh2DR9R3ropKh3Q1VeeZzTA8Wdk/ucD85q3tfL4w55Bd/hEmK6kj319PY1QVz0oh02nlmWxGRTjt2m2JuelwnwS5vaCWnv+kMBUEYFirqLQu7JrSF/dhHBbx7qJKfXTGPNSt6cE+H4tgH5vWSX0N8Vud1kdaYc+lO89qThT3QSlvCmEcEe4RJjO5IkNBTdqNIp4OPzUlj3aYC1m0qIDHKydYfXkBNc7Bgyzi2IIw2fpd4KAu7xePll68cYNW0JD61cgBiDXD8A4jL7i7W0BEkVtKDYEelwJnfgdmfGNgxhTGPCPYIkxQVLNg9W8hfOGc6EU47NU0eXtpTSkOLp5tLXBCE0aUiINjNaK1RQQFcT24tpLqpja+dP7PT8j7RGo5tgGnnhF4fsLB3mdeugq0UnPv9/h9PGDdIlPgIEx5mJ8ppB3oX7JlpMfz8kwu4yBrnLqlrobqpjdhwBw6bCjzZC4IwepQ3tOCwKVo8vkCdewCfT/PAe0dZkBnH8pyEXvYQgqrD0FQOU1aFXu+3sMt2mxSfYX1PJxMmBiLYo0BStIu4iLBAfezeSLfmdhbXuqluaiM52kVKjIvyehFsQRhN2tp91DR7mJthpmcVBo1jv3mgnCMVTdyyOndg1jUYdzhAdh+C3dbYPeBMmNCIYI8CqTEupqVE9etGnhxn5mmX1LVQ09xGQpSTlBiXJE8RhFHGfw8uyY4HoCBoHPuhD/KZHBfOxT1FgffGsQ+M2zt5Ruj1zuiOpCdd3eHChEYEexT4yeVz+dkV8/vVNi02HKWgpNZNVWMbiVFOUmNclNe30Nru5Y6ndnGsqmmYeywIQlf8CYwWZxuXt9/CLqhu5t1DlaxZnk2YfRA/scfehymn9ZzQRKkOKzt2EA8EwrhFBHsUmJse1y3taE+E2W2kxrgCFnZipJOUmHAqGlr5IK+Kxz46znM7+leS0+P14fNJCmhBGAr8AWe5SVEkRjkpqDYW9r83F6AUXLNsEDm6awug9jhM6WOOdKSVn0HKU55UiGCPAybHRRjBbvIEXOLVzW28sb8cgN1F/ctW9PHfvMNf3j0ynF0VhJMGf+BnaqyLzIQICmuaaff6eHxzIWfNTAnEnwyIY++b15zTe28XsLDFJX4yIYI9DkiPD+dQeQNtXh9Jlktca3h+p7Gs95TU9bkPrTVHK5sC5f0EQTgxyhtaUcpM1cxKiKSwxs1r+8oorW9hzfIQ86d7oq3ZRIYD5L8H4fGQOrf3baKs6lwSdHZSIYI9DpgUG0GZFRWeYAk2QE2zh0mx4RRUu6mz8hT3RIvHh9aScEUQhoqKhhaSopw47DYyEyI4VtXElx7dRnZiJOedktb/HW24F+5bARUHrfHrVX2XnYwUC/tkRAR7HJAe31HRKzEqjJSYjvSmnzsjF+jbym5sbQck4YognCi/evUg2wtqKa9vJcWqtjd7cgw+DZctTOfZL54+sGCzoi2mfOazX4TqIzClD3c4WFW27BAnY9gnE5LpbBwwOa5jLCwh0klqrPmRmBQbzhWLM/jZf/axt7ieVdN6rmvb3CaCLQgnSkmdm9+9foi3D1bg8+mAt+vyhRksz0kkMyFy4Dst2wNhUVD4kfnc1/g1wNKbTLnN8LiBH08Yt4iFPQ6YHGRhJ0W5SIl2oRScOTOZ5GgXk2LD2V3UPwu7ptlDW7tvWPsrCBOV7cdrAdhRUMvekvqAt8tmU4MTa3ct1BXA6V8x5TBdsTBpQd/buaIha/nAjyeMa8TCHgekB1vYUWE4HTbuXbuERVbChrnpsewp7j1SvLnNG3hf1dTayWoXBKF/bC+oxWm3ERvhoLKxLWBhD5qyPeY1YynMuhgaSsFmP/GOChMSsbDHASkxLhw2RZhdEW3VwP7EgslkWNNG5mbEcbiiEXeQKHelybKwAUlrKgiDZFtBLXPSY7l5tYkdGZRg+7yw83Fob+sQ7LR5MHkBzPzYEPZWmGiIhT0OsNsUabHheLy+kOlM56bH4tNwoKyBRVnxIffR1Noh5jKOLQgDp93rY1dhHdctz+LTp05hd1Edq2f0HDfSI3mvw1OfB3eNEeyIRIiZNPQdFiYcItjjhMlx4YFx6K5kJhhLu7TODT0JdlvHtieSh7yioZXkaOfACxoIwjjnYFkjbo+XxdnxxIaH8YdPLR3cjvzFPTb91eQFT5vbcxpSQQhCXOLjhK+eP4NvXDAz5Dp/4EtvJTeDXeKDtbALa5o59eevs+FI1aC2F4TxzPaCWoAevVj95vhGsIVB5UEo3gqT+ldXQBBEsMcJZ8xI4WNzQ7vNkqJc2FTvQuwPOoty2gct2EU1brw+TWG1u+/GgjDB2F5QQ2KUk+zEQUSD+/G0mHnXyz5rXOFgLGxB6Aci2BMAu02RHN17jezG1nbC7Ir0+IhBZzurc5tsavUtvWdVE4SJyLbjtSzMjDux4aCS7eBtg6lnw5LPmGUi2EI/EcGeIKTEuHoV4ubWdiKdDlNLe5AWdn1Le6fXkaaqsZUPDleOyrFPBpRSFyqlDiil8pRS3+uhzbVKqb1KqT1KqUdHuo+jRU1TG4fKG1mWk3hiOzq+wbxmrYQzvgmX/R4mLzrh/gknByLYE4TUGFevwWRNbV6iXY4+2/VGwMJ2j46F/ciHx7nhgY9o90ril6FGKWUH7gMuAuYAa5VSc7q0mQHcAZyutZ4LfG2k+zlabMqvBmBFbhfBLt8PP8/uKN7RF8c3QtIMU20rPA6W3CABZ0K/EcGeIKTGhPfqEm9qbSfSaQ9Y2FoPvC62X6gbRsnCrnd7aPdpmnqZby4MmhVAntb6iNa6DVgHXN6lzeeB+7TWNQBa6/IR7uOo8dHRapwOGwsyu6QCLdkOrXVQ8GHPG2sNBZtg6z+MhZ196rD2VZi4yLSuCUJKjIvKxla8Po3d1v2JvanNS5TLuMRbPD4aW9uJCQ8b0DFGewzb7TFC3djaTlzEwPou9EkGUBD0uRBY2aXNTACl1PuAHfix1vqlrjtSSt0K3AqQnZ09LJ0dCZ7cUsgbB8r5/ZrFbMqvZlFWPC5HlyxkdYXmtfJg6J3sXQ9v/j+o2Gc+K7vJaCYIg0AEe4KQGuvCp6G6qa1TNS8/Ta3tRLnsgXUVDa0DFmy/UI+WS7zFY1zhTT3MRxeGHQcwAzgbyATeUUrN11rXBjfSWt8P3A+wbNmygbtyxgh/e/8oe4rrOWtmCruL67n9rGndG9WbmvRUHuq+rmQHPPFZSJ4Jl90LuWeaBCmOE0xnKpy0iEt8gpAamIsdOvCsqbWdKKeDlOhwq11n93l+ZRP/3His12OMtku8xbKwR+v4E5wiICvoc6a1LJhCYL3W2qO1PgocxAj4hKOkzh3Iz/+jZ/fg9WmWdx2/Bqi3TlFXC9vTAk/9l6lbfdN/TER4whQRa+GEEMGeIPSVPKXZcomnxnZY2ME8tuk4P3hmd6/u7nq3P0p8dF3iYmEPC5uAGUqpXKWUE1gDrO/S5hmMdY1SKhnjIj8ygn0cMd7Yb4bnv3HBTNweLzYFS6ckdG/oF+zqI+ANui/e/aVxg1/2e4g8wchyQbAQwZ4gpMYYy7mih8Azf9DZpLhw7DbVrRynP2CtqKbnpCh+oR4tC9df3KSnFK3C4NFatwNfAl4G9gGPa633KKXuVkpdZjV7GahSSu0F3gS+rbWekGnvXt9XTlZiBF86ZzoLM+NYkp0QKLzTiboiUxLT1w7VR80yjxs++guccpkU8xCGFBHsCUJgbLqHKVtNbe1EuxzEhodx7uxUntxa2Kkutt+V3ptg1wVc4p5BRZmfKMFBZ8LQo7V+QWs9U2s9TWt9j7XsLq31euu91lp/Q2s9R2s9X2u9bnR7PDy427y8n1fJebPTsNkUD39uJQ/cGKL2tMcN7mozNg0dbvG9z0JLLSz/3Ij1WTg5EMGeIISH2YkJd1Be330Mu93ro8XjI9JpLITrV2RT2djGq3vLAm3K/BZ2bS8WttuDTYFPM+RTq17YVcIHeb0nRWkRl7gwAryfV0lru4/zT0kDIDY8jLjIEAGa/oCz3LPMq1+wtzwEiVM7hFwQhggR7AlEaowr5Bh2syV0US4zJeXMmSlkxEfw6EcdQWZlltD3JNger4+mNi+TYo3rfagjxf/35QP89vUQkbZBBCxsCToThpGdRXUoBctzQ4xZB+Mfv06dDTGTTaR4+T4z13rpTZIQRRhyRLAnEKkx4SHTjvot0ihrDM5uU6xZnsX7eVUcq2qiua09MC7dk0vcvz4zwRQ+GOrAs9rmNvKrmnpt47ewG9tEsIXho7jWTWqMq/uc667UWYIdmwHJM6BsN7x8J9idsOhTw99R4aRDBHsCkWJZ2C/uKuGnz+8NjDM3tfot7I6gmQvnmcpfW4/XdMqQVtiDhe0fv/bX3h7KwDOfT1Pn9lBW39qruzsQdCYWtjCMFNe6yYiP6Luh38KOTYfkWVC6Ew6/AZ/4P5N6VBCGmH4Jdl9FAZRSLqXUv6z1HyqlcqzlFyiltiildlmv5w5x/4UgUmNcFNW6+dJj23jgvaOBeaTNlkUa5eywGKYkRWFTcLSiKeAOz4iP6NHCru8i2EPpEm9oacdnxbD1ZmWPdOKUptZ2CqqbR+RYwtihqNZNem+C/cHvoXS3EeyIRAiL6Ki4ddH/mPzggjAM9CnY/SkKANwC1GitpwO/Bv7bWl4JXKq1ng/cCDw8VB0XupMa68Lr08yeFEOYXfH0NmMB+KOq/UFnAE6HjazESI5UNlFmudGXTEmgsrE14HoOpsPCNi7xobSwa91tgfdHK0MLdrvXR5tV9GOkosT//PZhLr/v/VGJiBdGB59PU1Lb0rOF3VAGr/wAXr7DuMTjMszyRZ+C2z+Alf81cp0VTjr6Y2H3pyjA5cDfrfdPAOcppZTWepvW2gqlZA8QoZSSVD/DxFkzU7lsYTr/uHkFZ89K5bkdxXh9mmbLJd51HmlOUhRHK5sCkeVLsuMB4xK87808/vbe0UBb/5h1wMIewjHs2uaOfR2tCC3YLUFT0EZKsCsaW6luaqNBotJPGiqbWmnz+shI6EGwizab16PvmIIfsZZgO5xS11oYdvoj2KGKAmT01MZKwFAHJHVpcxWwVWvdLSpKKXWrUmqzUmpzRUVFf/sudGHWpBh+t3YxSdEurlycQXlDKxsOV9FkucQjXZ2DaHKTjWCX1bfgctiYMzkWgINljfz+jUM8s70jM+VwWtg1zUEWdg8ucXfQNLKREuxm65ihpsoJE5PiWvO/To/rSbC3mAIejggz1zq260+hIAwfIxJ0ppSai3GTh/QXaa3v11ov01ovS0lJGYkuTXjOnZ1KjMvB09uKAkFnXS3sqSlRNLd52VVUR1pseMCqeHhjPi0eX2BsGzrSkibHOHE5bIMew9Za09DFOvc/DKTFunp0iQe76f3fZ7jxPySU9VK2VJhY+GM4ehzDLtxsLOlFa83n2PQR6pkg9E+w+1MUINBGKeUA4oAq63Mm8DRwg9a6n1XehRMlPMzOBXPSePNAeSBIK9LZ3cIG2Hq8lrRYF5NiTdrS9/NMtsmKBlOuE4yohtkVEWF2YsLDBu0Sf2l3KSv/3+udtve7xJdkJ/Qo2P452LHhjhGzsP3HLBML+6Sh2JolEdIl7vNB8TbIXAYrbwdHOKTNG+EeCicz/RHs/hQFWI8JKgO4GnhDa62VUvHAf4Dvaa3fH6I+C/3k1KlJVDe1saOwFugcdAYdgt3W7iM1NhyH3RZIjBLjcuDTUNVkrMv6Fg+x4WEopYiNcFAfwiX+u9cP8fMX9/Xap/2lDTS3eTu5mf0u8UVZ8dQ2e6hpauu2nd/CTo5xjdi0rmaxsE86imrdVgrfEHnDqw5Baz1kLIOUmfDtPJj58ZHvpHDS0qdg97MowANAklIqD/gG4J/69SVgOnCXUmq79Zc65N9CCIm/HOA7ByuICLNjt3XOvJQeF4HTYS4Bf3lOf3TsdcuNU8U/R7vO7SEuwqRnjA0PC+kSf31fGa/sKeu2PBh/JrbgQLPaZg8xLgcz0qKB0OPYfvd0crQLt8cbsPyHkw7BFgt7ItPU2s7t/9xCfmWTNaUrHBUqS1mhFXCWsdS8umIkm5kwooR4jOyO1voF4IUuy+4Ket8CXBNiu58BPzvBPgqDJCcpkuRoF5WNrSRHO7utt9kUuUlRHChrIM2yrGdNiqGk3s1F8yfz1/eOWkVB4qh3e4ixBDsm3EFDSzt1bg9bjlVz7myTc7mysY2a5ja01qF/8Ogo6xks2HVuD/FRYeQkGYv/aEUTS7I7p4X0u6dTos2DRWNre+ABYrhwW8F6PdUYFyYG247X8uLuUpKinb0nTSnaAs4YSJ45sh0UBAvJdDaBUUqx0rKyo0KVBqTDLZ5m1cm+8+JTWP/F1UyOMwLudwfXB1vYEWYM+/evH+LmhzZTa4l0RWMrzW3eXqdBVVjiVxtkodc0txEf4SQrMRKHTXGksrHbdn6XuL8q2UgkTxGX+MmB/3p7dnsxx6ubew44K9oMGYvBJj+bwuggV94EZ3mOsVS7jl/7yU2xBNuqpx3htJMQ5QwIo98dXN/SHhjXiw13UO9u56U9pQCU1rdQ39IeKNdZVtezRep3ide5O7vE4yPDCLPbmJoSxYHShm7b+S1sv6dgJALP3OISPyk4Ys39b2gxOfVDCra7Bkp2QvaqEe6dIHQggj3B8Y9jR7tCFzKYlRYDdMyv9hNmt5EU5ewksLFBY9iVja0UWlNgSutaqAyqw92TRerz6UC7uqC517XNbcRHGiGeMzmWvVZK1WDcbeZhwP8gMSKC7fHPw26VbGcTmMMVjcxNjyUr0Qi1PzkQH94PD18JWptEKWiYds7odVQ46RHBnuDMnhRLTLijRwv7kgWTeeoLq8hOiuy2LjU2nPL6FrTWnVziMV0iaMvrW6kMqhJW2oNFWuv24PHqwPvg5fHWvuekx1Jc19ItUrzDwrYEe5gjxdvafbT7NMnRTtq8Pmqahy6zmzC2OFLRxLSUaK5dagItAxb25r+ZYh6Fm+DIW+CM7gg4E4RRoF9BZ8L4xW5TfPfC2SGDzgAcdlu3AC8//vraNc0e2n2ahMiOMWyAxdnxbDteS1l9S6cx8p5cyMHBW/6gM3+lrnhr36dY2db2ldSzanpHxaOWLoI93GPYfnd4TlIUlY1tlNW3kBgV+hwK45cWj5fiOjfXpGRy4+k5OB02FmfFQ00+VFhTFLc/CkffhpzVYB/eQEdB6A2xsE8CPn3qFC6cN3nA26XFuiirb2Hb8RoA5mfEAx0W9mUL00mIDKO0vsMlbrepHgXbHyGuVIeFXd/iQWsCLnG/YO8t6ewWb/F4sSkCojncLvFmj9l/jhWUJ+PYE5P8qia0hqkp0cSGh/FfZ03DYbfBgZdMg6xTYcc6qD4CU8UdLowuIthCj6TFhlPR0MpH+dU4bIpFWfEALMyMZ1FWPJ9YMJm02HDK6lupbGzFpsxUstIegs78c7qzEyMDY9h+S9vvEk+OdpEW6+om2O42LxFh9kB61WEX7ICFHdmp78LEwh9wNtV6MAtw8EUzfevs70G7VXJ26tkj2zlB6IIIttAjqTEufBpe2VPG3Iw4IqzUplNTonnmi6eTGhNOWmw45Q0tVDS0khTtIj0+IlCu0+P1dQrW8gewzUiNCVjY/le/SxxCB565PV4inPaA632kXOJTksTCnoi0eLxorTlSYaZ05QYLdks95L8PMy+E3DMhJh1iJkPKrFHqrSAYRLCFHkm1kqkcrWxi2ZTQ49xpsa5AlLixjsMpq2uhtd3Lql+8wf3vHAm0rWhoJcppJyM+PGBZ+9OS+l3iYNzieeWNtLZ3FPlwe7yEh9lxOmw47bZBl7w8VtUU+JHuDb+FHR8ZRkJkGGWSPGXC0Nru5aLfvsuXH9vGkYomJsWGd85TcORN8Hlg1kVgs8Pl98Ilv5asZsKoI0FnQo/405UCvQh2OJWNrZTVm2xqabEuKhpb2XKshoqGVh58P59bVufisNsob2ghNTacOCvxitenqWsOYWGnx9Lu0xwqa2ReRhxgLKKIMGPhR4c7Bm1h3/HULjxeH/++rff5tM1tHQVT/G5/YWLw9NYijlY2cbSyCZfDxtKu13bJDrA5IHO5+Tz9vJHvpCCEQCxsoUf86UoBlub0LNg+DQfLGkiJNhW/vD7NczuKATPF64395YBxiadEu4iLdKI1NLR4qPVb2BGdXeLQOfDM3eYNuOSjXPZBT+vKr2zqcdpZMP6o9IgwB6mx4eISnyB4fZo/v3OEeRmxrJ6eTGu7j6kpXcava/IhLksiwoUxhwi20CP+JCVTkiJJjQkP2cYv6q3tPpJjXIHPz+8sYV5GLGmxLh796DgAlQ2tpMS6AuJc2+wJzG8Ozgs+JSkKl8PGwaCMZ26Pl3CHZWG7wmgcRE1sj9dHaX0LVY3dq4F1xe8Sj3TaSY529mub8Y5S6kKl1AGlVJ5S6nsh1t+klKoIKuTzudHo54nwyp5SjlY2cftZ0/nlNQvJTY5i1bTkzo1q8iEhZzS6Jwi9IoIt9EiY3UZGfASnTU3qsY0/BzlgucSNYDe0tHPmjBSuW5bF2wcrKKhupryhldQYV8D9Xev2UOf2EBPuMFNpLOw2xYy0aA6UBQu2j3CnX7DtNLaGTmRS29zGDX/7iAfeOxqwkv2U1rXg00aM/UFlwXh9mv/sLEFr3Umwk6KcgTKjExWllB24D7gImAOsVUrNCdH0X1rrRdbfX0e0k0PAox8dJzsxkgvnTWJSXDhvfutsLp7fZcqjCLYwRhHBFnpl3a2ncucnTulx/aQgt3lKjItJcR2fT5+ezJoV2YTZbdz59C4aW9tJCRbs5jYrLWl31+PM1BgOlXUEh7V6vESEmcs12uWgqQcL+5W9ZbxzsIKfPr+Xj/36nYDLHQikUgVCCvAb+8v54qNb2Xq8NiDoEU47SdEuWjy+wLj2iZBX3tApmG4MsQLI01of0Vq3AeuAy0e5T0POsapmFmXFdys1G6ClHpqrRLCFMYkIttArWYmRxIb3PJaXFO3C/9uXHO0i2frsD+ZJj4/guxfO5t1DlQCkxoQTF2EiwuvcHo5VNzM5rnuxhZmTYiitbwkUCXF3CjoLo7optIv6nYMVpMS4+N3axRyvbg4cF6CwpjnwPpSL+6hVtamioSVgYUeE2QPJWk7ULV7R0MqFv3mXhzccO6H9DBMZQEHQ50JrWVeuUkrtVEo9oZTKCrUjpdStSqnNSqnNFRUVw9HXQaG1pqy+pZNXqBs1+eZVBFsYg4hgCyeE3aYCY93J0S7sNkVabDjLchIItwT2s6tyWG2lGQ12iVc2trG3uJ4FViR4MP6iJIcst3hw0NmKnASKat3d5mp7fZr38io5c0YKF8+bRKTTzqb86sD6otreLexjVc3WujaaPe047TYcVhEU//IT4aOj1bT7NDsK605oP6PIc0CO1noB8Crw91CNtNb3a62Xaa2XpaSkjGgHe6PO7aG13dcpmLIbItjCGEYEWzhh/G5xf57v/716IXddMjew3mZT/Orahdy0KoelUxICAWabjlbT2u5jgZVBLZgZadEAgXFs/zxsgEsWpBNmVzy9rbDTNruK6qht9nDmzORAjvSPjnYIdmGNO+AKDWUt+wW7pqmt0wNCkvW9qk9wHPujo1UA7C0ek4JdBARbzJnWsgBa6yqttf8k/BUYV5Uw/LMDgodtAKgvgd8ugsItHYKdmDuifROE/iCCLZwwqbHhnfJ8r56RzKxJMd3a/PiyuUS5HITZbUS7HLx/2LirQ1nYGfERRDntgUjx4HnYCVFOzpmVyjPbi2n3+gLbvH2gAqXgjBnGqluek8iBsoaAW72oxs2MVPMgEMpaPlbdFFjX3OYl0i/Y1veqPEGX+IfWw8PRyqaQQW+jzCZghlIqVynlBNYA64MbKKWCo7MuA/aNYP9OGP9c+m4W9oEXoOYo7HjMCHZEAoR3vyYFYbQRwRZOmNmTYpiaEt1zIE8I4iLCaGhpJzbcwZQQpT2VUsycFMPBskY8Xh8erw5Y2ACfXJJBRUMr7x+uCix751AF8zPiAg8Oy3MS0Bq2HjPFSwprm5mZFkN4mI2qxs7Wclu7jyIrKK26qS2QChUgyap01tO4eX+obW7jQFkD8zJi8Wk6RcCPBbTW7cCXgJcxQvy41nqPUupupdRlVrOvKKX2KKV2AF8Bbhqd3g6OMivH/aSugp33mnk9+LIRbnGHC2MUEWzhhPnKeTNY/6XTB7SN3y2+IDMe1UPKx1lpMRwsawhKYtIh2OfMTiUuIownthi3eHlDC9sLajlzRseY6aLseBw2xab8arw+TUltC5kJESRFubq5xItq3fistOfVlkvcb2FHOh2Eh9lOSLA359egNdx4Wg5At/H3sYDW+gWt9Uyt9TSt9T3Wsru01uut93doredqrRdqrc/RWu8f3R4PDL9LPDU46Ky9DY68DZFJUHccjm0QwRbGLCLYwgkTZrcR6RxYllt/4NmCzJ5djzPSYqhqagsEi/nnYQO4HHauWpLJi7tKKK5188jG4/i05qqlmYE2kU4HczPi2JRfTVl9C+0+TUZChEmE0kV8j1UZd3hchIlAb25rJzKs4zslRbkCJUSD0VrzweHKbnO+u/JRfjVOu41LFqQT43Kwt2RMjmNPaMrqW0iIDMPlsMO/b4L3fg0FG8HTBOf+wDRqd4tgC2MWEWxhVOiPYPsjxTfnG5d2sIUNcPPqHDTw57cP88iHxzlnVmrnqkuYiPIdBXVsL6gFIDMhkqRoV7cocX/A2aKs+ICFHfyAkBTtDGlhv7avnOv/8iGX3/s+e3oJJvvwaDULs0zFs1PSu1cjE4YfM6Ur3IxT73kaXvsJvPn/wBYG86+B9MWmoQi2MEYRwRZGBf9c7PmZ8T22WTolgfAwGy/sKgG6C3ZmQiSXLJjM3zcco7KxlZtW5XTbx5oV2SgFdz69CzDBbIlR3VONHqtqJtJpZ9akmI6gs6DjJUaFFuwXd5UQ43JQ3dzGVX/8gIqG7lZ4i8fL3uI6lk5JBEyu9P2lDfh8ultbYfgoq281EeIHXjIL4rLg+AbIPhVcMTDzIrNcBFsYo4hgC6PCitwEzpiRTHrXKTZBRDjtnDEjhQ1HqqzP3S/XW8+cCsD01GjOmJHcbf20lGi+c+HsQDnPzIQIkiyXeHCt7uPVTWQnRpIU5aSt3UdlY2tgDBsIKfJt7T5e21fGx+dN4r7rl9Di8bHDsuSDOVDagMerWWh5E+ZMjqW5zcux6uZubYXho7S+hbSYcDj4IiTPhE89Ds5oOMWKqVvyGVh4fUeVLkEYY4hgC6PClYszefiWlT0GnPn52Jw0/Loa3sXCBpibHsd3LpzFjy+d2+O+Prsqh9OmJpGZEEF4mJ3kKBdt7T4ag0p05lc1MyUpkgQrwrym2ROIEgczx7yrG33DkSrqW9q5cO4k5qbHohTsDuEW31lkls23BHuhNe/8ofeP9vrdhaHD4zUPYVlR7ZD/Psy8EFJPgW8dhBWfN41i0+HKP4IzqvedCcIoIfWwhTHNeaekYVPg06EFG+ALZ0/vdR82m+JvNy0PzMcOTjUaEx6Gz6c5Xt3MubNTA3OugW4Wtj+f+PbjtcRFhvHS7lIinXZWz0gmPMxObnIUe0KMTe8urCMhMoyMeJOCddakGG5ZncsD7x1lXkYc1ywLmeFTGEIqG1vRGha2bgWfB2ZZ7m8RZ2EcIYItjGkSo5ysyE1k45HqbmPYAyHCae82r7qqqZXq5jYe3nCMtnYfU5IiA2Jutum4PfzLj1Q08Zm/fYTXp3HYFB+fNynwIDEvPY4t1pzvYHYW1TG/y/S1Oy6azb6Ser7/zG4WZ8czPTWm23bCidHU2h6oBldqzcGeVvueSYySuWKUeycIA0dc4sKY58K5k4DONbNPBH8K1W3Ha7n2Txt4bW8Z1y7L5KJ5k0mK6pijG9nJJW4E+/mdJXh9mjXLs5iSFMmnVmYH2sxNj6Wo1k1NUHBai8fLwbIG5mfEduqDw27jd2sX43LY+MlzezuNpwsnzmt7y1h09yus+sUbnP6LNwIzDZKrNkPumWAXW0UYf8hVK4x5Pn3qFOZlxJEe372q12DwW8v3vpmHTSle/cZZgfzSYfYOKziiU5S4EfJntxcR7XLw0yvmEWbv/Lw7N92MUe8prme1FQC3r6Qer08zPyO+Wz+So11844KZ/OS5vbyyt4yPWw8mf333CACfO2PqUHzdkw6vT/OLl/aTlRjJzafn8osX9/O/rxwgjkacDQWQfstod1EQBoVY2MKYx2G3sSwnccj25xfs2mYPVy/L7FQMItrlwGkJcXDQmX9su6SuhVXTkrqJNRgLGzoHnu2yAs56mm/+mVOnMDMtmp8+v5fWdi8tHi/3vZkX0rUu9I8XdpWQV97INy6YyadPncJ3L5xFW7uPRY5802DyotHsniAMGhFs4aQjPMxOjMuB3aa4/axpndYppQKCHtklcYqfM2aGLhmZEOUkIz6iU+DZzsI6kqKcTO5h+prDbuOHl8yhsMbNvzYVsH5HMTXNHm6wUpgKA8Pr0/zu9UPMSI3m4nmmVsmnVk5h2ZQEVkda5b7TF41eBwXhBBCXuHBSMntyDDPSYshK7F54JCHKSWl9SyfB9ucTb/H4OGtGzzWe56bHBjKeeX2aDYerWJTVc750gNXTk1mRm8jv38gjKcrJrLQYTp06dB6Fk4lX9pRyqLyR369djM0qRmOzKR66eQX2J+6DyhwTdCYI4xARbOGk5F+3ntbjOr/7OyLM0WW5C4ddkR2iupifJVMSeGVvGftK6imta6Go1s2dF5/Sa1+UUnzzgplcd/9GKhpauefKeX3OTxe6o7XmT28fZkpSJBfPn9xpXbTLARW7OtKPCsI4RFziwkmJzaYCFlhXQrnEAa5dlsVtXVzoXVmzPIsYl4PfvnaIRz48RnK0kwvmpPXZn5VTkzhzZgpxEWFcsSijn99CCGbjkWp2FNbx+TOmdi/12lwNtcfEHS6Ma8TCFoQu9CTYXz1/Rp/bxkc6+ezqXH73+iFsCm47axpOR/+ei3+/ZjG17jaiXHJbDoY/vX2Y5GgnVy/NhNYGU+Bj4Vqwh0HJdtNIAs6EcYxY2ILQBb9gRzgHl6jlltW5xIQ70MDaFdl9tvcTFxnGlCTJvDUYDpU18PbBCm48Lccksnn/d7D+y/DGT02D4u3mdfLCUeujIJwo8igvCF04ZXIsseGOTlnPBkJcRBg/uWwux6qaQwa1CUPPPzYcw+mwcf3KbPD5YOc6Uzbz/d+CzwtbHoLUuRApwXzC+KVfFrZS6kKl1AGlVJ5S6nsh1ruUUv+y1n+olMqxlicppd5USjUqpe4d4r4LwrBw/imp7PjRx4h0Dv559pNLMvn6BTOHsFdCT9S3eHhyayGXLkgnKdoFxz+A2uPwif8zIr3hXmNZf+rfo91VQTgh+vxFUkrZgfuAC4BCYJNSar3Wem9Qs1uAGq31dKXUGuC/geuAFuCHwDzrTxDGPBKhPb54akshzW1eblw1xSzY/pgpmzn/aph+Hhx9FxZcC7bB56IXhLFAfyzsFUCe1vqI1roNWAdc3qXN5cDfrfdPAOcppZTWuklr/R5GuAVBEIYUrTUPbzzGoqx4FmTGQ1sz7H0G5lxhKnHFZcKitSLWwoSgP4KdARQEfS60loVso7VuB+qApKHooCAIQk8cKm/kcEWTiQwHOPQytDUai1oQJhhjIkpcKXWrUmqzUmpzRUXFaHdHEIRxwqt7ywA4/xRrrvu+5yEyCXJWj2KvBGF46I9gFwFZQZ8zrWUh2yilHEAcUNXfTmit79daL9NaL0tJ6TntoyAIQjCv7StjQWacKeDS3goHX4ZZF4sLXJiQ9EewNwEzlFK5SiknsAZY36XNeuBG6/3VwBtaCvwKgjCMlDe0sL2gtsO6PvI2tDXAKZeNbscEYZjoU7CtMekvAS8D+4DHtdZ7lFJ3K6X8d8YDQJJSKg/4BhCY+qWUygd+BdyklCpUSs0Z4u8gCMIQ0Nf0zaB2VymltFJq2Uj2rytv7i9H62B3+HpwxsDUs0azW4IwbPRroqnW+gXghS7L7gp63wJc08O2OSfQP0EQRoB+Tt9EKRUDfBX4cOR7aThc0ci/NhXw8p5SMuIjOKXubdi10Qj2zI+DwzVaXROEYUUynQmCAEHTNwGUUv7pm3u7tPspJs/Ct0e2ewatNbc9vIVjVc1MTYni9tNiUY9fbfKFx2bA8ltGo1uCMCKIYAuCAKGnb64MbqCUWgJkaa3/o5TqUbCVUrcCtwJkZ/c/l3p/eC+vkkPljfzfNQu5amkmfHg/aC98/j1Ik9E2YWIzJqZ1CYIwtlFK2TCxKN/sq+1wzvr423tHmRnl5tKsZrNg1+OQNk/EWjgpEMEWBAH6nr4Zg0kv/JYVSHoqsH4kA8+OVDTy5oEK/hJzP877z4RdT0DhJpgfMnxGECYc4hIXBAGCpm9ihHoNcL1/pda6Dkj2f1ZKvQV8S2u9eaQ6+OD7+STaW8iu3wK+dnjyFkCZnOGCcBIgFrYgCP2dvjlq1Da38cSWQr6eexzla4fL7oWIBDOFKy5ztLsnCCOCWNiCIAB9T9/ssvzskeiTn3WbCnB7vFwavhMiEmHR9ZLRTDjpEMEWBGFM4/H6+PsH+ayeGk980Zsw4+NGqKOkvpBwciEucUEQxjTP7yympK6Fr82uBXcNzLpwtLskCKOCCLYgCGOW1nYv//fKQeZMimFp2RNgC4Np5412twRhVBCXuCAIY5aHNxyjsMbNK2cXoDY+Bed8H8JjR7tbgjAqiIUtCMKYpM7t4fdv5LEm183MLT+B3DPhjD7ztgjChEUEWxCEMck7Byuoc7fxff0XsDvhyvslKlw4qRGXuCAIY5LdxXVc6fiQmNKN8IlfQezk0e6SIIwqItiCIIxJ8gpK+J+wR2DSIlh602h3RxBGHRFsQRDGHFprFhf/iySq4OJ/iStcEJAxbEEQxiBFpaV8Rq+nIPVsyFo+2t0RhDGBCLYgCGOO5nfuJU41417VY9ltQTjpEJe4IAhjhhd3lbDUu4MpBx/kFd9yzpp32mh3SRDGDCLYgiCMCZrb2ql4/Muk2l+l2J7OvxM+x8ccMnYtCH7EJS4IwpjgaEERN9hf5Snvas5p+n8kZs0Z7S4JwphCBFsQhDFB5ZHtALhnXkErThZlx49qfwRhrCEucUEQxgTu4j0AXHvxBcw+M5Z5GXGj3CNBGFuIYAuCMCYIqzpAMxFEJk5haZIa7e4IwphDXOKCIIwJEhoPUxaeA0rEWhBCIYItCMKo0+Lxku09RlPsjNHuiiCMWUSwBUEYdY4XFJCs6lGpp4x2VwRhzCKCLQjCqFNxdDsAsVPmjW5HBGEMI4ItCMKo01JoIsRTpy0a3Y4IwhhGBFsQhFHHUX2ARiJxJWSNdlcEYcwigi0IwqiitSauIY9SV65EiAtCL4hgC4IAgFLqQqXUAaVUnlLqeyHW36aU2qWU2q6Uek8pNSS5Qw+W1DHNewRv6tyh2J0gTFhEsAVBQCllB+4DLgLmAGtDCPKjWuv5WutFwP8AvxqKY2/a+DYxys3k+ecOxe4EYcIigi0IAsAKIE9rfURr3QasAy4PbqC1rg/6GAXoEz2o1pq6A28DEDvrzBPdnSBMaCQ1qSAIABlAQdDnQmBl10ZKqS8C3wCcQEiTWCl1K3ArQHZ2dq8H3V/awNTmnTREZxATlzG4ngvCSYJY2IIg9But9X1a62nAd4Ef9NDmfq31Mq31spSUlF739/yOIlbYDhCWe/ow9FYQJhbjz8KuOgyv/ACUDWwO688OKPPe4QS707xXNrNO2UDZrR1o0Nq8+rdXNvOnfeBxQ0sd+NpNxGpEIkSlWOu9Zj0aIhLMMZsqTLuoVIhMAmckVOZB5QGIy4KU2RA7GRzhUFdgtslcBmFRUF8IlYegKg9cMaa9I9z02WYHWxjYwyA8HqJTTb+rj5h18VNMP9w14IyGsPC+z53XY86DbZw+p/m80NYI4VLFaRgoAoLnVGVay3piHfDHEz1oY9F+klQ9TFt9orsShAnP+BNsb5sRPp/XiKr/FW3et7eAt90s016zTPvMe6wpI8oSLO3tvn9lA1esEX3tM4IYql1fOGOgraGHldbDhc/T//2FxxnBbrWGEe0us732mc8RiZCxFNIXG5HX2jxIOMLNtsc3wt5nzPfKWmEeQhzhEDMZHC44/AbUHoMpp0NiLtQVmrZp88xDSHM1HH3b7GfSAshZDS215gEm+1TzUHHkTfNQMGWV6V/tcbPvqBTTl7YmKNsNDaVmv8kzzP+naAvsfhI8zZA4zRw/IQfis81fXBbUF8N/vgFle2DWxTD/KvOAZHOY/7ktzDy4tNaZvvq85n/pcEFYJEQmmr4WfGiuodQ5ZpnWZr3DZa6rxjKYsto8kL3/ayjcbPqUvghmfhxi0s1Dmrsa3LUd10xbozlm1gqzXz9ejzkPlYfM/87nNf8fR7jpf2RSx4NlZJJ5cBudqU2bgBlKqVyMUK8Brg9uoJSaobU+ZH38BHCIE+QnC2uNI36KWNiC0BdK677jRpRSFwK/BezAX7XWv+iy3gX8A1gKVAHXaa3zrXV3ALcAXuArWuuXezvWsmXL9ObNmwf+TQaDtkReW6KubEakgn8wfV5jcfvXh0WY5e5asywqxbw2lZtlrQ1GbGLTjaBVHjIi4HEb4fG2wvEPod0NCbmQPBOSphtxrys0YuLzWQ8jHvOD31wFFfsBZQQZDZUHrR/9ZLNt7XGz38oDob+rMxrmXmneF20138nTZB5IAJJnQeJUOPaBEb2IBGhvNSLqJybdiHHxVmPp251GKD1NZr3fsxG8TVdsDiNMjWUdy5QNpp5t9l99GGryoaGk+7axGTD7Etj1byOYg0bRZ7yULcxcF+lLzLltKu/ntgoSppjrprXBXAODIXsV3Pxir02UUlu01ssGd4CQ+7sY+A3mPv+b1voepdTdwGat9Xql1G+B8wEPUAN8SWu9p7d99nk/P3WreVj81iGZgy2c1PTnfu7Twg6a7nEBJhBlk1JqvdZ6b1CzW4AarfV0pdQa4L+B66xpIWuAuUA68JpSaqbWgzFZhwGlwO6g19Ngs3e2mPz4hduP3xoMJiLBWFxdmXp2iAOlGME8UbztHT98WhvxdNeYBwtnZPf2Hje0NkJ0Ssf27S3gijaiU5NvHhpcMeYhRCmz35ZacMWZh5WS7eZhJftUI9qlO02bhClG9JsqzL7DIsx3dLigocxY9PYw8yATldylXy3G4q09Zh5k2ttg0VrTjwvuNsMILbWmj45w83DT1mTWRyab/5v2meO3NRqr22aHzOWmfcV+I6hKmWO1NZp+RCRA3qvmgWTZLZA62/Sn8hAcfNlsE51iHjrC48zDhs9rvDI+D+S/Z/Ztd5qHpKhk86CRPNPyCNisc+yGpkrTLzAPas1Vph8oiMs88WthgGitXwBe6LLsrqD3Xx3yg6bM6riuBEHolT4tbKXUacCPtdYftz7fAaC1/nlQm5etNhuUUg6gFEgBvhfcNrhdT8cbUQtbEMYxQ21hDwdyPwtC/+jP/dyf6KNQ0z26zr8ItNFatwN1QFI/t0UpdatSarNSanNFRUU/uiQIgiAIJxdjIlx4INNABEEQBOFkpD+C3Z/pHoE2lks8DhN8NtCpIoIgCIIghKA/gh2Y7qGUcmKCyNZ3abMeuNF6fzXwhjaD4+uBNUoplzVdZAbw0dB0XRAEQRBOHvqMEtdatyulvgS8TMd0jz3B0z2AB4CHlVJ5QDVG1LHaPQ7sBdqBL46ZCHFBEARBGEf0K3FKP6Z7tADX9LDtPcA9J9BHQRAEQTjpGRNBZ4IgCIIg9I4ItiAIgiCMA/qVmnQkUUpVAMf60TQZqBzm7gwU6VP/GIt9grHZr976NEVrPabnQfbzfh5v5300GYv9kj71j7761Of9POYEu78opTaPtSxP0qf+MRb7BGOzX2OxT0PNWPyOY7FPMDb7JX3qH0PRJ3GJC4IgCMI4QARbEARBEMYB41mw7x/tDoRA+tQ/xmKfYGz2ayz2aagZi99xLPYJxma/pE/944T7NG7HsAVBEAThZGI8W9iCIAiCcNIggi0IgiAI44BxJ9hKqQuVUgeUUnlKqe+NUh+ylFJvKqX2KqX2KKW+ai1PVEq9qpQ6ZL0mjELf7EqpbUqp563PuUqpD63z9S+rgMtI9yleKfWEUmq/UmqfUuq00T5XSqmvW/+73Uqpx5RS4aNxrpRSf1NKlSuldgctC3lulOF3Vv92KqWWDHf/hhu5n/vs25i6n+Ve7rUfw34vjyvBVkrZgfuAi4A5wFql1JxR6Eo78E2t9RzgVOCLVj++B7yutZ4BvG59Hmm+CuwL+vzfwK+11tOBGuCWUejTb4GXtNazgYVW/0btXCmlMoCvAMu01vMwRW3WMDrn6iHgwi7Lejo3F2Eq3s0AbgX+OAL9Gzbkfu4XY+1+lnu5Zx5iuO9lrfW4+QNOA14O+nwHcMcY6NezwAXAAWCytWwycGCE+5FpXRTnAs8DCpNZxxHq/I1Qn+KAo1gBjkHLR+1cARlAAZCIKYDzPPDx0TpXQA6wu69zA/wZWBuq3Xj8k/u5z36MqftZ7uV+9WdY7+VxZWHT8c/xU2gtGzWUUjnAYuBDIE1rXWKtKgXSRrg7vwG+A/isz0lArda63fo8GucrF6gAHrRce39VSkUxiudKa10E/BI4DpQAdcAWRv9c+enp3Iy56/8EGXPfR+7nXpF7eeAM6b083gR7TKGUigaeBL6mta4PXqfNY9OIzZlTSl0ClGutt4zUMfuJA1gC/FFrvRhooovLbBTOVQJwOeYHKB2Iorsra0ww0ufmZEbu5z6Re/kEGIpzM94EuwjICvqcaS0bcZRSYZib+xGt9VPW4jKl1GRr/WSgfAS7dDpwmVIqH1iHcaP9FohXSvnrno/G+SoECrXWH1qfn8Dc9KN5rs4HjmqtK7TWHuApzPkb7XPlp6dzM2au/yFizHwfuZ/7hdzLA2dI7+XxJtibgBlWBKATE1ywfqQ7oZRSwAPAPq31r4JWrQdutN7fiBkLGxG01ndorTO11jmY8/KG1vpTwJvA1aPRJ6tfpUCBUmqWteg8YC+jeK4w7rNTlVKR1v/S36dRPVdB9HRu1gM3WBGmpwJ1Qe628Yjczz0wFu9nuZcHxdDeyyMVHDCEg/oXAweBw8D3R6kPqzGujZ3AduvvYswY0+vAIeA1IHGU+nc28Lz1firwEZAH/BtwjUJ/FgGbrfP1DJAw2ucK+AmwH9gNPAy4RuNcAY9hxt48GAvmlp7ODSbo6D7r2t+FiYwd8etriL+/3M9992/M3M9yL/faj2G/lyU1qSAIgiCMA8abS1wQBEEQTkpEsAVBEARhHCCCLQiCIAjjABFsQRAEQRgHiGALgiAIwjhABFsQBEEQxgEi2IIgCIIwDvj/KK6GabG/RB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(lstm, lstm_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95517be3",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e8676c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.61701\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.36332\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.42349\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.47050\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.35153\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.43636\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.50179\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.34796\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.37872\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.50042\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.42771\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.46220\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.38592\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.42766\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.45746\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.37038\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.41778\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.38464\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.46940\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.42666\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.35407\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.35328\n",
      "\tTrain loss: 0.04293, Accuracy: 2060/6768 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 523/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 481/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.35607\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.38127\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.41767\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.34812\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.43826\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.35984\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.39115\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.36827\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.36243\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.43825\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.31810\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.37840\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.36802\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.46610\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.41845\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.38451\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.35336\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.36812\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.43365\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.39292\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.47538\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.35438\n",
      "\tTrain loss: 0.04255, Accuracy: 2277/6768 (33.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 544/1692 (32.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 497/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.44336\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.38399\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.35776\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.29525\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.38842\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.34309\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.41853\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.31803\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.32054\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.38609\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.40184\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.35653\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.43352\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.36629\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.34726\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.35097\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.31901\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.45615\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.38366\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.40131\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.41669\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.33918\n",
      "\tTrain loss: 0.04217, Accuracy: 2369/6768 (35.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 610/1692 (36.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 512/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.38251\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.41705\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.39798\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.32417\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.40191\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.33531\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.36344\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.41602\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.32426\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.40772\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.35302\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.36333\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.39286\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.32556\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.36691\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.37549\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.34740\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.32977\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.44440\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.29875\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.33190\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.27462\n",
      "\tTrain loss: 0.04191, Accuracy: 2386/6768 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 589/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.38390\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.39917\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.48786\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.32384\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.35998\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.31975\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.50512\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.37706\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.26499\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.39476\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.34279\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.35881\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.34838\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.38175\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.27775\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.37041\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.27168\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.36220\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.44350\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.33654\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.39998\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.26908\n",
      "\tTrain loss: 0.04152, Accuracy: 2396/6768 (35.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 600/1692 (35.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.30605\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.44900\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.34953\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.25320\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.33739\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.22100\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.38611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.30357\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.22695\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.36559\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.29208\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.37809\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.36657\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.36338\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.32481\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.30098\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.35228\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.27706\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.42663\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.40146\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.34128\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.14436\n",
      "\tTrain loss: 0.04196, Accuracy: 2315/6768 (34.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 567/1692 (33.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 518/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.23721\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.42858\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.35478\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.26608\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.36782\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.28486\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.38923\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.30880\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.29521\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.36910\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.24288\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.43255\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.35873\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.31453\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.28569\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.31426\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.22183\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.34849\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.39057\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.23270\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.38930\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.17134\n",
      "\tTrain loss: 0.04122, Accuracy: 2545/6768 (37.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 617/1692 (36.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.21144\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.48593\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.32225\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.15085\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.33419\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.21560\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.50618\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.24168\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.11074\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.27660\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.19824\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.33630\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.39132\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.33524\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.24361\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.27110\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.35851\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.28072\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.40057\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.21859\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.23955\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.05071\n",
      "\tTrain loss: 0.04050, Accuracy: 2618/6768 (38.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 628/1692 (37.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.14194\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.39781\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.34079\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.22564\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.30729\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.13557\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.38695\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.25140\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.25045\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.34283\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.20468\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.33880\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.37424\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.32574\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.16900\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.22040\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.24608\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.32150\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.46026\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.29993\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.27880\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.08559\n",
      "\tTrain loss: 0.04087, Accuracy: 2572/6768 (38.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 626/1692 (36.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.10363\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.44555\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.25481\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.20611\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19657\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.18912\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.32386\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.28116\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.11670\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.37604\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.05072\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.41703\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.21380\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.16191\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.21457\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.25797\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.15348\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.26643\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.40545\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.24101\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.30425\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 0.91792\n",
      "\tTrain loss: 0.04124, Accuracy: 2644/6768 (39.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 645/1692 (38.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.08698\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.50665\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.19285\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.37177\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.21557\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.11972\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.36648\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.13913\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.10748\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.36187\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.00834\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.26553\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.26908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.27460\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.24953\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.22284\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.15224\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.17479\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.45608\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.10564\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.27319\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 0.95179\n",
      "\tTrain loss: 0.04083, Accuracy: 2821/6768 (41.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 686/1692 (40.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.14520\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.36199\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.22491\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.20451\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.31700\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.04437\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.33470\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.33126\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.01559\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.30485\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.96594\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.34943\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.24718\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.35698\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.28531\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.23237\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.16110\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.13965\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.26347\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.14968\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.30558\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.00005\n",
      "\tTrain loss: 0.03995, Accuracy: 2947/6768 (43.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 710/1692 (41.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 544/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.06824\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.39048\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.21162\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.17864\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.20839\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.98197\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.28502\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.16758\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.14110\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.27164\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 1.02601\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.20536\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.16393\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.13981\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.12980\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.10573\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.13168\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.26642\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.25728\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.18745\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.32376\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 0.92997\n",
      "\tTrain loss: 0.03930, Accuracy: 2984/6768 (44.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00088, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.07145\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.24098\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.17851\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.15451\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.25088\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.02774\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.20629\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.96144\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.05352\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.24378\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.93168\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.24410\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.18111\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.21239\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.19631\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.18991\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.07550\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.10356\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.26326\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.07638\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.20547\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.95784\n",
      "\tTrain loss: 0.04005, Accuracy: 3044/6768 (44.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 722/1692 (42.00%)\n",
      "\tTest loss: 0.00092, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.08145\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.19725\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.09449\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.21943\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.20613\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.98010\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.16307\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.10630\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.04007\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.10581\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.88999\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.25178\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.13394\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.11625\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.16552\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.98126\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.03610\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.23920\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.25438\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.93874\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.12897\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.91432\n",
      "\tTrain loss: 0.03839, Accuracy: 3139/6768 (46.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 725/1692 (42.00%)\n",
      "\tTest loss: 0.00091, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.03154\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.18819\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.08983\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.05366\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.25169\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.88989\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.16705\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 1.30554\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 0.94720\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.27791\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.88245\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.13716\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 1.12658\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.19586\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.91658\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.83348\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.04726\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.13159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.09841\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.92782\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.09265\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.91158\n",
      "\tTrain loss: 0.03860, Accuracy: 3247/6768 (47.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 763/1692 (45.00%)\n",
      "\tTest loss: 0.00093, Accuracy: 588/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.98963\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.13631\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.15564\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.15446\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.08535\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.87435\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.05345\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.94566\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.92126\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.26801\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.88075\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.08949\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.92635\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.04041\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.05949\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.89761\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.96845\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.13117\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.23037\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.01480\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 1.10264\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.82997\n",
      "\tTrain loss: 0.03947, Accuracy: 3151/6768 (46.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 745/1692 (44.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 538/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.93233\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.20690\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.22527\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.10327\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.16608\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 1.04666\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.12613\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.05680\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.98746\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.22716\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.94790\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.96913\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.04680\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.89210\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.98534\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.01130\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.13381\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.13722\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.90968\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.19276\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.89734\n",
      "\tTrain loss: 0.03988, Accuracy: 3327/6768 (49.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 785/1692 (46.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 556/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 1.05248\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.31615\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.04061\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.03080\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.98843\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.83730\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.15629\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 1.08260\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.78083\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.15556\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.69679\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.17706\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 1.05404\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.89546\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.90014\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.85148\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 1.01758\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.98668\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.28708\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.75097\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.89166\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.72530\n",
      "\tTrain loss: 0.03505, Accuracy: 3709/6768 (54.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 850/1692 (50.00%)\n",
      "\tTest loss: 0.00096, Accuracy: 559/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.87143\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.08718\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.10330\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 1.12156\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.91507\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.70937\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.90463\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 1.13799\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.92111\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.14936\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.77698\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.97891\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.85094\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.83842\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.82589\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.80843\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.06107\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.11223\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.07942\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.71509\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 1.01905\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.80205\n",
      "\tTrain loss: 0.03659, Accuracy: 3551/6768 (52.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 807/1692 (47.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.96693\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.04323\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.05353\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.01508\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.79557\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.73198\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.09569\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.84275\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.80543\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 1.08690\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.72046\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.90893\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.82185\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.99944\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.79976\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.85938\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.85970\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.08597\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 1.15470\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.85128\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.92623\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.90877\n",
      "\tTrain loss: 0.03499, Accuracy: 3638/6768 (53.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00075, Accuracy: 835/1692 (49.00%)\n",
      "\tTest loss: 0.00103, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.76583\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.20750\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.11462\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.80605\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.81190\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.52988\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.05602\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.98731\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.59503\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.20581\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.70271\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.80338\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.80062\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.84470\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.70864\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.81523\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.80675\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.02464\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.95899\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.80704\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.99169\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.63980\n",
      "\tTrain loss: 0.03980, Accuracy: 3317/6768 (49.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 776/1692 (45.00%)\n",
      "\tTest loss: 0.00112, Accuracy: 563/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.82460\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 1.04626\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 1.00029\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.85602\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.76637\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.48569\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.68889\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.80053\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.64188\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 1.06810\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.54091\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.87966\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.68422\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.81862\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.55446\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.59651\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.83184\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.92637\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.70611\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.61176\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 1.07569\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.67941\n",
      "\tTrain loss: 0.03552, Accuracy: 3711/6768 (54.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 863/1692 (51.00%)\n",
      "\tTest loss: 0.00106, Accuracy: 589/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.77557\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 1.16489\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.96115\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.81533\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.79194\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.54835\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.78392\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.78646\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.72509\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 1.02417\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.47049\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.85714\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.66332\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.78149\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.50255\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.59616\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.68634\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.78829\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.88381\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.68007\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.72983\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.56677\n",
      "\tTrain loss: 0.03851, Accuracy: 3650/6768 (53.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 836/1692 (49.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.81138\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.00258\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.83112\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.79863\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.61192\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.50919\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.88401\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.79939\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.63083\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.87728\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.60381\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.87149\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.71987\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.75619\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.59050\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.72488\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.68871\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.94121\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.91205\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.57276\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.85755\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.59549\n",
      "\tTrain loss: 0.03811, Accuracy: 3609/6768 (53.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 812/1692 (47.00%)\n",
      "\tTest loss: 0.00120, Accuracy: 571/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.76815\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.69848\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.76283\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.83649\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.80812\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.60127\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.84301\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.99774\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.58304\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 1.00284\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.47473\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.64194\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.63398\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.49813\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.47786\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.68685\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.93271\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.79328\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.57022\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.02986\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.73203\n",
      "\tTrain loss: 0.04126, Accuracy: 3578/6768 (52.00%)\n",
      "\tValidation loss: 0.00093, Accuracy: 795/1692 (46.00%)\n",
      "\tTest loss: 0.00131, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.63567\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.93057\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.77963\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.79181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.83851\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.49384\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.64829\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.99598\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.63213\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.63769\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.56403\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.82808\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.63535\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.73235\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.51843\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.58460\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.71169\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.93018\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.80573\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.67717\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.92638\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.47542\n",
      "\tTrain loss: 0.03802, Accuracy: 3773/6768 (55.00%)\n",
      "\tValidation loss: 0.00087, Accuracy: 855/1692 (50.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.55596\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.85588\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.85318\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.58816\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.53279\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.43704\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.83527\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.63433\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.51109\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.78508\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.54066\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.96426\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.68633\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.87062\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.38681\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.72429\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.62026\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.91036\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.85082\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.79475\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.60198\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.37718\n",
      "\tTrain loss: 0.03268, Accuracy: 4242/6768 (62.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 941/1692 (55.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 627/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.68444\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.69853\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.74627\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.74776\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.75744\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.30212\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.69930\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.71198\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.37089\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.79582\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.55347\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.60408\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.47544\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.59117\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.28767\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.49856\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.55297\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.80461\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.65186\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.47311\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.79298\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.37691\n",
      "\tTrain loss: 0.04425, Accuracy: 3583/6768 (52.00%)\n",
      "\tValidation loss: 0.00099, Accuracy: 810/1692 (47.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.54507\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.49534\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.79002\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.65074\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.59621\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.30983\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.61499\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.75642\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.55276\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.83541\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.34871\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.71331\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.62159\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.54359\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.39783\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.63142\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.56108\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.67286\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.58430\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.51162\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.92081\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.43942\n",
      "\tTrain loss: 0.03383, Accuracy: 3998/6768 (59.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 910/1692 (53.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 609/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.56692\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.65930\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.76464\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.49101\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.59738\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.28007\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.64867\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.63042\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.46374\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.69798\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.47831\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.72154\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.52570\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.69798\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.40524\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.43713\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.81934\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.50433\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.70336\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.38931\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.74011\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.36178\n",
      "\tTrain loss: 0.03701, Accuracy: 3964/6768 (58.00%)\n",
      "\tValidation loss: 0.00088, Accuracy: 872/1692 (51.00%)\n",
      "\tTest loss: 0.00129, Accuracy: 617/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.47163\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.74519\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.56168\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.57079\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.54025\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.24277\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.72634\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.82799\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.58737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.57453\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.43561\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.66068\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.64873\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.66504\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.54117\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.52813\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.70086\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.62547\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.51774\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.44197\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.73059\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.44593\n",
      "\tTrain loss: 0.02802, Accuracy: 4632/6768 (68.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 659/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.60942\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.74723\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.55135\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.61180\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.35638\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.26990\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.54567\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.74816\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.53564\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.71763\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.49109\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.30890\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.52550\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.42531\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.37901\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.41286\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.78473\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.85066\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.46063\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.42327\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.65408\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.45291\n",
      "\tTrain loss: 0.02691, Accuracy: 4503/6768 (66.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 995/1692 (58.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 645/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.63063\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.58364\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.59067\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.50587\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.56751\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.25987\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.62205\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.45597\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.38207\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.62982\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.30256\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.40987\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.38010\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.74832\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.34486\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.48428\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.80324\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.46319\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.55016\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.43819\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.66046\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.24589\n",
      "\tTrain loss: 0.02952, Accuracy: 4469/6768 (66.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 998/1692 (58.00%)\n",
      "\tTest loss: 0.00130, Accuracy: 610/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.47173\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.52461\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.66405\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.22539\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.61219\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.33025\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.49517\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.54298\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.47445\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.53523\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.36152\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.54322\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.45464\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.45224\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.45782\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.41497\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.47235\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.58226\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.81663\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.36027\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.56360\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.33370\n",
      "\tTrain loss: 0.03086, Accuracy: 4469/6768 (66.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 963/1692 (56.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 605/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.67900\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.56139\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.55898\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.65326\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.47516\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.20997\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.62984\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.50687\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.42018\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.52463\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.27178\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.76618\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.46367\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.64655\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.27153\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.35636\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.48582\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.50287\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.54248\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.38790\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.53047\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.30322\n",
      "\tTrain loss: 0.03392, Accuracy: 4307/6768 (63.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 938/1692 (55.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 631/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.61198\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.78651\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.58415\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.53449\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.53836\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.19739\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.37886\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.32150\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.48813\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.53818\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.48275\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.68137\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.48620\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.62141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.23404\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.26330\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.49805\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.44846\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.38167\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.44365\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.47922\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.27569\n",
      "\tTrain loss: 0.03011, Accuracy: 4682/6768 (69.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1031/1692 (60.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 639/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.46819\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.65417\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.41532\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.52571\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.43501\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.14242\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.40725\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.27571\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.40092\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.62420\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.31668\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.50290\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.49474\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.42123\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.31171\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.33165\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.70877\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.72470\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.71112\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.46996\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.63980\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.39061\n",
      "\tTrain loss: 0.02884, Accuracy: 4697/6768 (69.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1011/1692 (59.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 652/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.58013\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.48912\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.47551\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.53338\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.43798\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.20049\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.55608\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.52524\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.29502\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.45393\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.28957\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.82577\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.54835\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.53319\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.42106\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.35093\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.53285\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.46442\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.23191\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.47995\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.67982\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.20546\n",
      "\tTrain loss: 0.02405, Accuracy: 5056/6768 (74.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1110/1692 (65.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 671/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.62522\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.56948\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.37457\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.40427\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.44103\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.21122\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.52359\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.43669\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.55504\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.60885\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.21388\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.40557\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.30229\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.39941\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.47640\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.36965\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.40424\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.61850\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.31094\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.24028\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.67720\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.19374\n",
      "\tTrain loss: 0.03266, Accuracy: 4389/6768 (64.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 941/1692 (55.00%)\n",
      "\tTest loss: 0.00141, Accuracy: 622/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.54714\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.30052\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.59961\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.22366\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.25369\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.22519\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.56110\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.48522\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.37769\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.44456\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.24859\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.39611\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.49521\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.35853\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.40793\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.17388\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.51516\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.60369\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.57927\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.22845\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.57769\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.22752\n",
      "\tTrain loss: 0.03023, Accuracy: 4603/6768 (68.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1017/1692 (60.00%)\n",
      "\tTest loss: 0.00151, Accuracy: 628/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.61017\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.24658\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.47419\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.21332\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.31360\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.19461\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.41696\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.24000\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.36977\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.45675\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.31026\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.41003\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.25355\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.37911\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.29835\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.22392\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.42431\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.37960\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.26656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.41025\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.61278\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.13002\n",
      "\tTrain loss: 0.02951, Accuracy: 4769/6768 (70.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1067/1692 (63.00%)\n",
      "\tTest loss: 0.00148, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.49730\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.36515\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.51583\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.20608\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.35003\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.16427\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.20534\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.20710\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.24971\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.37799\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.21118\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.53090\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.41827\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.43883\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.38743\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.25343\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.45691\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.37919\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.38237\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.30628\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.62170\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.31503\n",
      "\tTrain loss: 0.02025, Accuracy: 5307/6768 (78.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1177/1692 (69.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 679/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.49448\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.23148\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.42418\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.35975\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.34535\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.08709\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.39774\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.32201\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.44704\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.61448\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.35025\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.81718\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.24776\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.52935\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.42824\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.29316\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.31151\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.55436\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.27107\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.35096\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.43246\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.23738\n",
      "\tTrain loss: 0.02257, Accuracy: 5186/6768 (76.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1138/1692 (67.00%)\n",
      "\tTest loss: 0.00146, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.47433\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.28139\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.27709\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.26071\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.43196\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.14943\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.54060\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.32063\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.30755\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.35787\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.19096\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.33679\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.29090\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.28387\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.19148\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.33773\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.43869\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.50697\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.30362\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.33704\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.55144\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.29054\n",
      "\tTrain loss: 0.02594, Accuracy: 4905/6768 (72.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 1071/1692 (63.00%)\n",
      "\tTest loss: 0.00147, Accuracy: 680/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.36987\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.16494\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.27365\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.22289\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.18188\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.31061\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.63858\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.30051\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.24997\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.24790\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.23151\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.48354\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.24666\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.17637\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.45535\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.23463\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.37278\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.17692\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.21850\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.24370\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.24291\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.13744\n",
      "\tTrain loss: 0.02437, Accuracy: 5115/6768 (75.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 1131/1692 (66.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.54604\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.39527\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.42118\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.25945\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.32131\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.13462\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.52301\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.32991\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.18292\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.53795\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.13823\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.63418\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.24953\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.26597\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.25683\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.19526\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.28309\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.24316\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.35212\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.25344\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.38902\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.27491\n",
      "\tTrain loss: 0.02590, Accuracy: 5062/6768 (74.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1128/1692 (66.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 652/1772 (36.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.49111\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.33719\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.44923\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.21714\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.18721\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.28798\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.30099\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.28233\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.35417\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.52675\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.20985\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.23257\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.26942\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.24132\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.26652\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.27869\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.37263\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.48486\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.38099\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.52421\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.26240\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.22509\n",
      "\tTrain loss: 0.02327, Accuracy: 5222/6768 (77.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1157/1692 (68.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 716/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.36848\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.51711\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.43573\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.17518\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.33112\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.12920\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.32497\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.30976\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.40840\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.45794\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.33074\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.55281\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.26726\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.31960\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.11314\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.37404\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.28833\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.45809\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.34196\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.43001\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.30115\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.12489\n",
      "\tTrain loss: 0.02046, Accuracy: 5338/6768 (78.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00156, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.67580\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.53579\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.23554\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.18487\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.17524\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.14343\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.48185\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.48711\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.21890\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.20092\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.09611\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.47697\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.17522\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.21211\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.26425\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.09689\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.61616\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.26045\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.20759\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.19698\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.51277\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.09258\n",
      "\tTrain loss: 0.02695, Accuracy: 4975/6768 (73.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 1084/1692 (64.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 650/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.34998\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.28429\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.21978\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.33612\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.11497\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.15460\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.33403\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.29763\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.31932\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.33548\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.07697\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.34268\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.15633\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.15992\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.11955\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.30594\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.42261\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.36906\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.37428\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.22906\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.52805\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.09384\n",
      "\tTrain loss: 0.02489, Accuracy: 5144/6768 (76.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 1140/1692 (67.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.35641\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.08362\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.30248\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.28866\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.30886\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.03582\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.28537\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.13164\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.18695\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.32681\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.16201\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.38633\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.49226\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.16001\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.12971\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.21995\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.33319\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.31930\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.31086\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.27626\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.55583\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.24190\n",
      "\tTrain loss: 0.02198, Accuracy: 5267/6768 (77.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1156/1692 (68.00%)\n",
      "\tTest loss: 0.00156, Accuracy: 676/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.42010\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.18767\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.54213\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.23852\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.25943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.14419\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.28716\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.24268\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.14989\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.45854\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.11746\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.38390\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.10836\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.37021\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.16248\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.08541\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.23621\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.24174\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.24896\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.31438\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.19035\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.05220\n",
      "\tTrain loss: 0.02349, Accuracy: 5186/6768 (76.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1155/1692 (68.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 675/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.28316\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.39453\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.64057\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.31453\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.16882\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.09461\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.28204\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.19623\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.24235\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.29966\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.19731\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.56269\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.31982\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.15104\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.31329\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.17904\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.28087\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.36724\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.35107\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.36368\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.48935\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.16568\n",
      "\tTrain loss: 0.02252, Accuracy: 5278/6768 (77.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 1158/1692 (68.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.42767\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.35170\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.34622\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.24222\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.22986\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.15293\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.19760\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.29755\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.30620\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.27191\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.19130\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.19146\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.17133\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.28039\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.22718\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.16189\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.16490\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.42224\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.15385\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.15350\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.32240\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.11456\n",
      "\tTrain loss: 0.02030, Accuracy: 5374/6768 (79.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1194/1692 (70.00%)\n",
      "\tTest loss: 0.00160, Accuracy: 683/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.51704\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.29141\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.47302\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.13123\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.28884\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.06556\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.13960\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.37039\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.29005\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.10135\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.16701\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.31495\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.11541\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.30102\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.10316\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.12313\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.42446\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.35223\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.15725\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.16092\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.24367\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.17304\n",
      "\tTrain loss: 0.02356, Accuracy: 5214/6768 (77.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 659/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.24810\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.13425\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.29688\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.26810\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.25061\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.07474\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.16891\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.25310\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.06060\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.26604\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.08523\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.39074\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.16474\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.13274\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.19185\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.06211\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.21008\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.24608\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.25782\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.13670\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.37092\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.17409\n",
      "\tTrain loss: 0.02596, Accuracy: 5087/6768 (75.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1122/1692 (66.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 647/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.31360\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.26302\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.26407\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.41891\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.20950\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.18714\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.39567\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.22395\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.30771\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.35984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.10440\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.30265\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.25610\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.16828\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.19582\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.25274\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.11726\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.52153\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.41395\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.16201\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.35060\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.15403\n",
      "\tTrain loss: 0.01755, Accuracy: 5573/6768 (82.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1231/1692 (72.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 725/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.59962\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.25045\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.42651\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.17540\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.07306\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.24279\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.12497\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.24171\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.05552\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.30417\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.13506\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.18949\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.14445\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.24090\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.28232\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.14988\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.08547\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.22429\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.26304\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.17793\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.66511\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.07937\n",
      "\tTrain loss: 0.03097, Accuracy: 4891/6768 (72.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1092/1692 (64.00%)\n",
      "\tTest loss: 0.00192, Accuracy: 636/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.37577\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.32631\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.16734\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.25934\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.20062\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.03813\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.14620\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.35642\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.12276\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.07039\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.13919\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.17736\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.26677\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.19948\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.19807\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.08118\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.27649\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.37160\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.24072\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.10813\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.31774\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.13979\n",
      "\tTrain loss: 0.02396, Accuracy: 5288/6768 (78.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 1179/1692 (69.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 697/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.43332\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.15467\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.16618\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.27389\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.40610\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.05842\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.18565\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.13348\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.16147\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.20327\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.06579\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.26480\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.36871\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.19210\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.22467\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.11857\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.16637\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.43303\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.16661\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.16594\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.43756\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.13081\n",
      "\tTrain loss: 0.01414, Accuracy: 5732/6768 (84.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1278/1692 (75.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 726/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.44529\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.14333\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.11893\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.16772\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.03637\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.04100\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.39680\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.14128\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.23967\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.18913\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.07267\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.30294\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.26844\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.22575\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.13312\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.29156\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.30564\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.18640\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.19677\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.07122\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.25499\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.14923\n",
      "\tTrain loss: 0.02103, Accuracy: 5404/6768 (79.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1200/1692 (70.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 693/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.21468\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.16960\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.21707\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.33022\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.13195\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.03349\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.21598\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.10932\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.06750\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.19602\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.16847\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.13379\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.13480\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.17969\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.25175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.25464\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.15972\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.20260\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.18449\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.21797\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.37644\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.45870\n",
      "\tTrain loss: 0.02949, Accuracy: 4986/6768 (73.00%)\n",
      "\tValidation loss: 0.00085, Accuracy: 1104/1692 (65.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 688/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.49565\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.28470\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.26413\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.11708\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.26862\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.04511\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.05870\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.06477\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.10817\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.25356\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.09361\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.16512\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.22376\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.20473\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.22373\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.06111\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.15997\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.15948\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.18483\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.27013\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.32868\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.37054\n",
      "\tTrain loss: 0.02247, Accuracy: 5370/6768 (79.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1196/1692 (70.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 710/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.24581\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.03373\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.26207\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.20168\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.07669\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.08629\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.08383\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.13099\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.16416\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.10344\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.04452\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.23261\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.20714\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.34639\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.09051\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.06740\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.19459\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.30436\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.12213\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.09884\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.30091\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.10085\n",
      "\tTrain loss: 0.01941, Accuracy: 5539/6768 (81.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1248/1692 (73.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 715/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.45011\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.25678\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.25579\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.09600\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.09056\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.08959\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.11069\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.02865\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.26212\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.18191\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.06149\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.35868\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.18044\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.52612\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.12278\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.05314\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.15807\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.10763\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.17334\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.32146\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.25491\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.12648\n",
      "\tTrain loss: 0.02821, Accuracy: 5070/6768 (74.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 1135/1692 (67.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 653/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.15951\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.36507\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.30427\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.15073\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.18957\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.08277\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.21947\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.11267\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.27110\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.13277\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.12693\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.16667\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.27745\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.30116\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.18194\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.15197\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.07562\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.26382\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.26168\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.36145\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.42603\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.19649\n",
      "\tTrain loss: 0.02228, Accuracy: 5318/6768 (78.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1168/1692 (69.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 679/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.34164\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.07069\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.22684\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.28212\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.06480\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.09946\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.11267\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.08831\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.18278\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.24342\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.18796\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.28479\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.10021\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.09269\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.17821\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.08823\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.17709\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.13918\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.24871\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.15702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.33769\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.18267\n",
      "\tTrain loss: 0.02267, Accuracy: 5362/6768 (79.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 681/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.44891\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.06868\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.13287\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.03928\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.25318\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.09885\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.16951\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.14742\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.18647\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.20458\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.06457\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.25776\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.23090\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.21309\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.08501\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.26281\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.33248\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.16618\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.48295\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.23289\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.08941\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.20612\n",
      "\tTrain loss: 0.02452, Accuracy: 5262/6768 (77.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 656/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.32235\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.07789\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.18210\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.14409\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.08509\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.04826\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.07476\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.06382\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.10432\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.12004\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.15654\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.48645\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.25825\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.09318\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.14887\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.11274\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.09949\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.38233\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.17267\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.18128\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.18551\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.09087\n",
      "\tTrain loss: 0.01919, Accuracy: 5487/6768 (81.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1227/1692 (72.00%)\n",
      "\tTest loss: 0.00179, Accuracy: 693/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.52532\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.49185\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.31389\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.07354\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.14602\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.07760\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.06140\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.11588\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.21187\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.14299\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.09957\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.21357\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.12561\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.11285\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.15188\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.04168\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.40214\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.20381\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.34278\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.03231\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.09819\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.21406\n",
      "\tTrain loss: 0.01799, Accuracy: 5569/6768 (82.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 713/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.31018\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.21007\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.18734\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.08395\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.20356\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.05774\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.14044\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.20814\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.32353\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.20916\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.06368\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.19125\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.13225\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.07468\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.13225\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.15840\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.26711\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.14805\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.26092\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.11946\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.21772\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.03559\n",
      "\tTrain loss: 0.01965, Accuracy: 5584/6768 (82.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1247/1692 (73.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 698/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.19365\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.42478\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.06455\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.16507\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.10962\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.34039\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.12233\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.06215\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.30077\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.15598\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.16636\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.23620\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.06547\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.10302\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.12002\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.13883\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.23538\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.18770\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.17632\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.15011\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.33609\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.14928\n",
      "\tTrain loss: 0.02339, Accuracy: 5345/6768 (78.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 694/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.20763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.25428\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.36649\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.06713\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.13598\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.04189\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.18772\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.07470\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.17468\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.15037\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.14853\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.24698\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.08473\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.14528\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.10657\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.06212\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.09727\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.11223\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.19095\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.13864\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.25657\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.04044\n",
      "\tTrain loss: 0.02742, Accuracy: 5097/6768 (75.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 658/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.40212\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.19527\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.12367\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.15835\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.12638\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.02795\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.11793\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.12108\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.05332\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.28669\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.05907\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.19144\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.07978\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.02648\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.05666\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.08108\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.24828\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.09283\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.03724\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.19452\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.34131\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.08846\n",
      "\tTrain loss: 0.02374, Accuracy: 5301/6768 (78.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 677/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.32997\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.22968\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.23877\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.16708\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.34300\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.03611\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.08069\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.06001\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.06872\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.19521\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.09410\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.14911\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.15565\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.14448\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.28831\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.36630\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.11270\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.16968\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.24131\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.11315\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.05523\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.15346\n",
      "\tTrain loss: 0.02344, Accuracy: 5405/6768 (79.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 1205/1692 (71.00%)\n",
      "\tTest loss: 0.00201, Accuracy: 686/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.21670\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.14941\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.23437\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.24730\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.14449\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.10814\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.04068\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.20395\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.16571\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.09762\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.18912\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.14394\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.34266\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.07568\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.49496\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.07097\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.28104\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.09186\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.08487\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.19807\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.08575\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.05670\n",
      "\tTrain loss: 0.02024, Accuracy: 5508/6768 (81.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1241/1692 (73.00%)\n",
      "\tTest loss: 0.00188, Accuracy: 725/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.34222\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.24352\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.13666\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.09034\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.07656\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.06574\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.30688\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.11350\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.16863\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.07005\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.04933\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.13868\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.17738\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.24805\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.09933\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.19212\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.08792\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.12495\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.10461\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.28886\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.16130\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.05532\n",
      "\tTrain loss: 0.01942, Accuracy: 5680/6768 (83.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 1297/1692 (76.00%)\n",
      "\tTest loss: 0.00195, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.41181\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.06538\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.24119\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.13271\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.39677\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.14775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.11322\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.13698\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.17017\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.14120\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.26387\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.15409\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.10995\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.25015\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.04191\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.09340\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.26263\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.09968\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.14198\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.17475\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.17814\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.16434\n",
      "\tTrain loss: 0.03465, Accuracy: 5029/6768 (74.00%)\n",
      "\tValidation loss: 0.00098, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00228, Accuracy: 649/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.22838\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.04004\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.16486\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.09376\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.10253\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.06815\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.13355\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.04698\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.06529\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.25360\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.02319\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.31921\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.06214\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.09658\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.06931\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.20925\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.09573\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.10856\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.22073\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.09114\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.09991\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.17669\n",
      "\tTrain loss: 0.03246, Accuracy: 4934/6768 (72.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1078/1692 (63.00%)\n",
      "\tTest loss: 0.00216, Accuracy: 639/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.20361\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.25050\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.20733\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.12868\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.07152\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.04346\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.05959\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.06516\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.02959\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.27511\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.09658\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.19017\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.03628\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.17003\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.17424\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.02673\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.14609\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.08105\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.12083\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.51017\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.04942\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.18669\n",
      "\tTrain loss: 0.02204, Accuracy: 5320/6768 (78.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 1186/1692 (70.00%)\n",
      "\tTest loss: 0.00199, Accuracy: 717/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.15083\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.12673\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.19309\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.17935\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.09022\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.05309\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.19200\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.09695\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.07125\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.09030\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.14365\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.07842\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.17610\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.07258\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.10915\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.12049\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.13332\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.08406\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.06995\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.26339\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.41961\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.18433\n",
      "\tTrain loss: 0.01557, Accuracy: 5723/6768 (84.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1259/1692 (74.00%)\n",
      "\tTest loss: 0.00193, Accuracy: 698/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.34734\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.06895\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.25249\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.12603\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.01619\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.04786\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.28648\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.16585\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.08011\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.15783\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.08239\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.22929\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.02503\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.25445\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.10289\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.19277\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.04656\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.09701\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.22544\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.07671\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.52862\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.08680\n",
      "\tTrain loss: 0.01951, Accuracy: 5604/6768 (82.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 1252/1692 (73.00%)\n",
      "\tTest loss: 0.00200, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.25462\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.11301\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.06207\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.19457\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.16752\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.15405\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.09669\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.06362\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.06705\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.17563\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.08747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.24614\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.22558\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.28303\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.15432\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.09444\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.43383\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.23886\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.10719\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.10676\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.16356\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.04025\n",
      "\tTrain loss: 0.02082, Accuracy: 5486/6768 (81.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 1234/1692 (72.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 699/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.38097\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.20545\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.37770\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.02224\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.17001\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.08220\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.08368\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.13493\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.03928\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.19413\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.13261\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.36559\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.08618\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.14511\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.07484\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.07229\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.03729\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.07811\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.09792\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.12064\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.23280\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.06682\n",
      "\tTrain loss: 0.03370, Accuracy: 4983/6768 (73.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1102/1692 (65.00%)\n",
      "\tTest loss: 0.00234, Accuracy: 641/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.11135\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.38136\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.06973\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.01631\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.03555\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.11827\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.05507\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.03089\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.06358\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.09535\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.05967\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.14100\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.20472\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.13960\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.13051\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.15036\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.07073\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.31627\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.08118\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.20135\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.07001\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.03547\n",
      "\tTrain loss: 0.01905, Accuracy: 5597/6768 (82.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1238/1692 (73.00%)\n",
      "\tTest loss: 0.00198, Accuracy: 695/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.26876\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.11099\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.19814\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.07152\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.23101\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.04292\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.16894\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.18332\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.06791\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.20702\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.05227\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.33191\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.16223\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.09196\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.15575\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.20423\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.25908\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.07028\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.19305\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.06850\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.06353\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.14391\n",
      "\tTrain loss: 0.01915, Accuracy: 5551/6768 (82.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 1224/1692 (72.00%)\n",
      "\tTest loss: 0.00196, Accuracy: 686/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.18653\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.06194\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.20303\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.06980\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.05542\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.11412\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.15509\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.06140\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.11450\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.14027\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.16406\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.14523\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.05514\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.05863\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.06081\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.31002\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.15341\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.15205\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.04884\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.10115\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.07293\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.02216\n",
      "\tTrain loss: 0.02923, Accuracy: 5155/6768 (76.00%)\n",
      "\tValidation loss: 0.00084, Accuracy: 1160/1692 (68.00%)\n",
      "\tTest loss: 0.00211, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.24354\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.22065\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.04012\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.18526\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.02377\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.01039\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.14151\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.02821\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.07334\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.09943\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.03911\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.29628\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.05734\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.24604\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.03015\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.03194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.17545\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.33594\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.06727\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.09223\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.18005\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.17874\n",
      "\tTrain loss: 0.02943, Accuracy: 5238/6768 (77.00%)\n",
      "\tValidation loss: 0.00086, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00220, Accuracy: 668/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.20638\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.10900\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.03591\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.24564\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.11289\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.06945\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.08106\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.12439\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.08698\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.21501\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.03253\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.36828\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.10063\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.15242\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.14127\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.02400\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.16113\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.34771\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.48712\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.10445\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.19009\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.20280\n",
      "\tTrain loss: 0.03477, Accuracy: 4932/6768 (72.00%)\n",
      "\tValidation loss: 0.00093, Accuracy: 1097/1692 (64.00%)\n",
      "\tTest loss: 0.00232, Accuracy: 642/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.39164\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.33135\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.19351\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.05032\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.07561\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.02355\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.15243\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.07277\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.22460\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.15384\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.04201\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.21532\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.07735\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.07180\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.11418\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.06566\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.24422\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.10339\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.39322\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.15571\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.10366\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.03677\n",
      "\tTrain loss: 0.02516, Accuracy: 5230/6768 (77.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 1180/1692 (69.00%)\n",
      "\tTest loss: 0.00212, Accuracy: 643/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.22973\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.14955\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.13855\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.07726\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.03133\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.02025\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.14430\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.02900\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.17131\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.08519\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.19587\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.15657\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.21487\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.10221\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.07116\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.15680\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.28385\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.21123\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.13671\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.03021\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.28243\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.02720\n",
      "\tTrain loss: 0.03107, Accuracy: 5090/6768 (75.00%)\n",
      "\tValidation loss: 0.00087, Accuracy: 1141/1692 (67.00%)\n",
      "\tTest loss: 0.00224, Accuracy: 646/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.18522\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.17678\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.02695\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.03917\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.03737\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.04128\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.06245\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.06241\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.05514\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.08463\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.02863\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.17542\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.28609\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.03128\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.11834\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.04513\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.11432\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.20927\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.58272\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.17194\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.19575\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.20542\n",
      "\tTrain loss: 0.02625, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1146/1692 (67.00%)\n",
      "\tTest loss: 0.00214, Accuracy: 637/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.12998\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.15309\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.12944\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.03521\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.03610\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.03495\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.09067\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.05924\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.08013\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.06959\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.05093\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.14524\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.28417\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.24802\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.06978\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.06494\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.08945\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.14904\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.32613\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.15183\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.15701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.08422\n",
      "\tTrain loss: 0.02059, Accuracy: 5673/6768 (83.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 1292/1692 (76.00%)\n",
      "\tTest loss: 0.00194, Accuracy: 720/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.40080\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.16652\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.26865\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.21446\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.02222\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.05673\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.10948\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.06328\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.12273\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.02470\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.06199\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.09973\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.15439\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.05700\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.06105\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.09578\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.05264\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.31589\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.07734\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.05210\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.15244\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.02939\n",
      "\tTrain loss: 0.02742, Accuracy: 5373/6768 (79.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 1209/1692 (71.00%)\n",
      "\tTest loss: 0.00231, Accuracy: 671/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.25911\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.14174\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.33817\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.13484\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.03682\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.09428\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.38131\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.19065\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.03379\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.25651\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.25597\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.17344\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.08929\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.18392\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.07810\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.24765\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.15438\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.11829\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.03181\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.02804\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.24721\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.17542\n",
      "\tTrain loss: 0.02839, Accuracy: 5257/6768 (77.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 1198/1692 (70.00%)\n",
      "\tTest loss: 0.00210, Accuracy: 689/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.16417\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.26279\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.12543\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.08199\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.01124\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.02499\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.03614\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.06375\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.08970\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.03635\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.02836\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.33286\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.08825\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.14452\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.15710\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.07698\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.21015\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.09728\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.10955\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.12267\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.18218\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.04756\n",
      "\tTrain loss: 0.03302, Accuracy: 4978/6768 (73.00%)\n",
      "\tValidation loss: 0.00095, Accuracy: 1106/1692 (65.00%)\n",
      "\tTest loss: 0.00231, Accuracy: 662/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.21840\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.04765\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.09792\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.23917\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.09578\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.12135\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.05302\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.12484\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.03962\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.15023\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.09466\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.20928\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.03638\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.25606\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.34955\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.15201\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.17520\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.11388\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.05491\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.21396\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.09308\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.00525\n",
      "\tTrain loss: 0.02462, Accuracy: 5389/6768 (79.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00213, Accuracy: 701/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.24113\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.08602\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.09809\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.01909\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.18482\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.12117\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.13199\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.01072\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.06135\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.08257\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.04884\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.13554\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.15769\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.20626\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.07694\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.02311\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.08656\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.16341\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.04103\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.31265\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.09627\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.06980\n",
      "\tTrain loss: 0.02287, Accuracy: 5585/6768 (82.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 1255/1692 (74.00%)\n",
      "\tTest loss: 0.00217, Accuracy: 687/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.23229\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.07206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.07209\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.16742\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.05213\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.02984\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.15736\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.19429\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.02379\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.13899\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.11065\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.09170\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.30002\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.07556\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.09697\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.13692\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.07852\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.20128\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.27405\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.02512\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.13930\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.10156\n",
      "\tTrain loss: 0.02479, Accuracy: 5395/6768 (79.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 1213/1692 (71.00%)\n",
      "\tTest loss: 0.00218, Accuracy: 682/1772 (38.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.766548463356974\n",
      "Best test accuracy:\n",
      "0.40970654627539504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(gru, gru_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c17b8f",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e0ecb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.49864\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.40551\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.44457\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.40890\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.39938\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.37963\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.47968\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.40555\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.41232\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.36486\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.36251\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.41141\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.36253\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.33095\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.36439\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.33433\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.40658\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.39382\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.37206\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.30125\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.33634\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.34129\n",
      "\tTrain loss: 0.04218, Accuracy: 2356/6768 (34.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 587/1692 (34.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 643/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.41466\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.37307\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.39320\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.25860\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.36447\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.27328\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.30506\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.31515\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.38097\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.17155\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.27859\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.38077\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.29818\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.31172\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.34862\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.53471\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.30241\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.32657\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.28341\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.29400\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.20426\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.44962\n",
      "\tTrain loss: 0.03954, Accuracy: 2741/6768 (40.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 708/1692 (41.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 695/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.28813\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.36809\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.36935\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.38356\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.31248\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.16117\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.29125\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.27015\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.26146\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.22909\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.24904\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.35677\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.18551\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.32477\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.32721\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.34516\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.35768\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.21659\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.22667\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.24111\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.19481\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.26794\n",
      "\tTrain loss: 0.03904, Accuracy: 2784/6768 (41.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 725/1692 (42.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 702/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.17482\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.48224\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.33283\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.24303\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.29936\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.13991\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.30127\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.31286\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.24373\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.21122\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.03841\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.34805\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.10717\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.39343\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.34640\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.36846\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.24737\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.22423\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.07324\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.30275\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.19040\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.20695\n",
      "\tTrain loss: 0.03772, Accuracy: 2962/6768 (43.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 757/1692 (44.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 754/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.25991\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.29404\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.31042\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.18315\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.12655\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 0.94480\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.18930\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.26074\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.13537\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.18711\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.06127\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.23336\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.17291\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.22262\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.16650\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.18229\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.24947\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.33756\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.18917\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.20590\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.11121\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.14786\n",
      "\tTrain loss: 0.03787, Accuracy: 2941/6768 (43.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 723/1692 (42.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 718/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.11318\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.31881\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.28174\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.05723\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.08658\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.02258\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.32673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.12449\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.27307\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.14980\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.00185\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.31892\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.18385\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.36910\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.15572\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.18151\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.14790\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.18423\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.11119\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.18973\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.13559\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.24057\n",
      "\tTrain loss: 0.03720, Accuracy: 3044/6768 (44.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 733/1692 (43.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 746/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.16706\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.37391\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.32506\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.24107\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.10651\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 0.94815\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.26242\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.23797\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.35947\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.06026\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 0.90510\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.18445\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.28866\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.23406\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.28049\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.27451\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.24254\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.26831\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.07690\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.18749\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.03730\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.18934\n",
      "\tTrain loss: 0.03814, Accuracy: 2971/6768 (43.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 721/1692 (42.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 726/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.10640\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.22647\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.30183\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.09690\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.26219\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.99512\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.09612\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.13944\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.13289\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.14945\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 0.99069\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.21066\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.19276\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.14558\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.15190\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.32511\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.03228\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.23357\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.12722\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.12070\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.12541\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.36832\n",
      "\tTrain loss: 0.03824, Accuracy: 2982/6768 (44.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 766/1692 (45.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 723/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.12974\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.38158\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.16153\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.09703\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.13066\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.95884\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.31069\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.09721\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.07978\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 0.93158\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 0.91486\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.16062\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.06726\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.38460\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.08903\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.18996\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.13804\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.13513\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.10466\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.02084\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.05148\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.22873\n",
      "\tTrain loss: 0.03688, Accuracy: 3237/6768 (47.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 778/1692 (45.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 761/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.25195\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.28595\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.11806\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.01866\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.16074\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 0.92616\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.36348\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.05497\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.25543\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.06285\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.01584\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.11028\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 0.99969\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.12912\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.20747\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.24980\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 0.95524\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.29532\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.24971\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.11343\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.13635\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.01723\n",
      "\tTrain loss: 0.03622, Accuracy: 3148/6768 (46.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 775/1692 (45.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 733/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.01916\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.02722\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.21373\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.02449\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.12358\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.85520\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.26561\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 0.98866\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 0.99339\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.12180\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.85099\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.08762\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 0.89810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.15413\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.13358\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 0.98148\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.00653\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.12844\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.00253\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.05326\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.22223\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.13012\n",
      "\tTrain loss: 0.03667, Accuracy: 3224/6768 (47.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 778/1692 (45.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 754/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.95151\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.08932\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.11606\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.04754\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.05184\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.87808\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.25447\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 0.97731\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 0.99773\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.16805\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.84070\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.06747\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.92904\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.14976\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.10615\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.17987\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.94871\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.08376\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.11224\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.02374\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.10768\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.17201\n",
      "\tTrain loss: 0.03535, Accuracy: 3385/6768 (50.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 820/1692 (48.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 764/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.96552\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.07395\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.09056\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.04288\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.13090\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.86041\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.02541\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 0.96050\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 0.96936\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.08258\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.93939\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.20992\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.96338\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.08902\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.04245\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.00454\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.01553\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.02863\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.03456\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 0.97050\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 1.01639\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.14538\n",
      "\tTrain loss: 0.03471, Accuracy: 3332/6768 (49.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 796/1692 (47.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 739/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.06171\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.27035\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.20416\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.09126\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.00675\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.95914\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.27613\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.13302\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.02788\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 0.94760\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.82254\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.00234\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.87743\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.25525\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.07994\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.00726\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.91288\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.07119\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.90116\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.07917\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.00057\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.86239\n",
      "\tTrain loss: 0.03599, Accuracy: 3225/6768 (47.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 749/1692 (44.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 705/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.12026\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.29311\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.24491\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.02107\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.06691\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.78058\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.17905\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.94571\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.13781\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.08754\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.91211\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.03892\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.78656\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 0.99032\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.09333\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.95532\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.97507\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.07618\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.98784\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.98667\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.91629\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.07305\n",
      "\tTrain loss: 0.03397, Accuracy: 3444/6768 (50.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 836/1692 (49.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.13978\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.20731\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.14262\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.98248\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.39372\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.77601\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.28808\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.89397\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.06991\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.02629\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.84045\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.03922\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.88845\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.04788\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.88654\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.00496\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.98763\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.17201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.04019\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 1.04064\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.01300\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.19921\n",
      "\tTrain loss: 0.03440, Accuracy: 3434/6768 (50.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 814/1692 (48.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 755/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.97556\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.16594\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.19479\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.05348\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.11111\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.94333\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.26465\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.01941\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.92753\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.12757\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.76294\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.86494\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.90187\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.04731\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.18177\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.09588\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.94826\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.02430\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.93938\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 1.04186\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.90654\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.03675\n",
      "\tTrain loss: 0.03170, Accuracy: 3720/6768 (54.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 876/1692 (51.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 790/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 1.07729\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.28555\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.02630\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.85421\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.99135\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.91543\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.13906\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.87782\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.96768\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.80103\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.94993\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 1.00288\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.90900\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.07518\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 1.11911\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.89920\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.80089\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.14802\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.99209\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.93819\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.93319\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.95114\n",
      "\tTrain loss: 0.03553, Accuracy: 3370/6768 (49.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 784/1692 (46.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 741/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.97064\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.15111\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.09847\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.96476\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.15177\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.84148\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.25707\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.92051\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.23308\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.98055\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.89382\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.87805\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.85043\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 1.16103\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 1.12157\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.97441\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.91562\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 1.01550\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.93490\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.86228\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.80265\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.88310\n",
      "\tTrain loss: 0.03505, Accuracy: 3516/6768 (51.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 812/1692 (47.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 732/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.94343\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.99040\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.25103\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 1.04377\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 1.07220\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.78872\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.35974\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.95527\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 1.01043\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.06044\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.72470\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.95647\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.81341\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.11699\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 1.07149\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.89569\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.83729\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 0.97225\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.02561\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.95896\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.80432\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.01934\n",
      "\tTrain loss: 0.03553, Accuracy: 3388/6768 (50.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 790/1692 (46.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 715/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.88616\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.17039\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.03096\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.04004\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.02048\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.79448\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.11992\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.84877\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.98229\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.99618\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.91914\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 1.02690\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.92106\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.96973\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.92127\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.99921\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.88276\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.04882\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.87101\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 1.00979\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.85985\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.91849\n",
      "\tTrain loss: 0.03291, Accuracy: 3669/6768 (54.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00067, Accuracy: 848/1692 (50.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 739/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.90474\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.12256\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.11362\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.77910\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.90196\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.77739\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.17875\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.81110\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.94125\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.86188\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.81695\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.97868\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.99204\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.92869\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 1.01017\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 1.02391\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.71723\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.16076\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.83013\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.80998\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.78960\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.89001\n",
      "\tTrain loss: 0.03253, Accuracy: 3660/6768 (54.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 840/1692 (49.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 718/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.86817\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.97275\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.99150\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.92157\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 1.01793\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.77086\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.06700\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.77599\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.86174\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.99284\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.90151\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.94072\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.73895\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.88027\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 1.03887\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.98292\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.91655\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.15966\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.96313\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.98409\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.84634\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 1.03291\n",
      "\tTrain loss: 0.03108, Accuracy: 3837/6768 (56.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 887/1692 (52.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.93446\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.99427\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.90261\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.85933\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 1.09159\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.72276\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 1.12958\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.82904\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.90473\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.81450\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.67812\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.83022\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.88632\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.98905\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 1.02577\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.93535\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.89193\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.90823\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.85932\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 1.07084\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.80683\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.89494\n",
      "\tTrain loss: 0.02922, Accuracy: 3881/6768 (57.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 941/1692 (55.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 772/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.85831\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.15238\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.78934\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.80887\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 1.19464\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.72659\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 1.15703\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.71636\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.74831\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.88504\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 1.05711\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 1.09191\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.75774\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 1.02583\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.92699\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.76803\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.83788\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.17492\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.77286\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.89241\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.74384\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.88147\n",
      "\tTrain loss: 0.02882, Accuracy: 4053/6768 (59.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 947/1692 (55.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 811/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.88479\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.98435\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.96569\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.79201\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 1.22807\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.76145\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 1.05248\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.75493\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.84036\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.83833\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.67861\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 1.01776\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.78936\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.91411\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.94740\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 1.12054\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.89399\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.90053\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.70969\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.79047\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 1.12979\n",
      "\tTrain loss: 0.03337, Accuracy: 3709/6768 (54.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 848/1692 (50.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 727/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.92388\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.00028\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.79846\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.87617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.02849\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.85603\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 1.09110\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.69125\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 1.09768\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.95935\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.86164\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.81447\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.80690\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.76298\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.95031\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 1.05481\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.82064\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.96609\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.96014\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.87372\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.87782\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.79629\n",
      "\tTrain loss: 0.02859, Accuracy: 4216/6768 (62.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 972/1692 (57.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 813/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 1.16577\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 1.09405\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.90860\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.99374\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 1.12768\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.86026\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.99867\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.93688\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.77680\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 1.05186\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.89995\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.89472\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.79924\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.93567\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.95087\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.77591\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.86845\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.14465\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.90148\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.84098\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.93582\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.95012\n",
      "\tTrain loss: 0.03012, Accuracy: 3952/6768 (58.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 926/1692 (54.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 796/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.90146\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.99296\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.81113\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.96762\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 1.05178\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.67340\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 1.13105\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.94266\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.77912\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.82368\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.72963\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.71449\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.74843\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.95255\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.89651\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.77715\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.72495\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.80541\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.84623\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.98139\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.69591\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.76351\n",
      "\tTrain loss: 0.03013, Accuracy: 3847/6768 (56.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 865/1692 (51.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.78651\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.76611\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.95249\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.76617\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.77980\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.73388\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.94364\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.71971\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.79883\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.96154\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.75817\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.87326\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.74356\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.76869\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.96597\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.91980\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 1.00106\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.95304\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.66315\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.84026\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.76387\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.92635\n",
      "\tTrain loss: 0.03021, Accuracy: 3840/6768 (56.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 890/1692 (52.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.83244\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.92556\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 1.00005\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.90920\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 1.04096\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.66055\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 1.10633\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.69586\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.87363\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.84024\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.68576\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.79931\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.89907\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.87294\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 1.06926\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 1.08208\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.80474\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.95235\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.64000\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.80559\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.74921\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.99657\n",
      "\tTrain loss: 0.03212, Accuracy: 3776/6768 (55.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 866/1692 (51.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 714/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.78286\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.89318\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.90679\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.67233\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.70489\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.62689\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 1.05670\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.61195\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.83429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 1.00360\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.72072\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.96471\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.87484\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 1.07141\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.99738\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.94800\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.86335\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.90340\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.69541\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.95848\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.60712\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.82327\n",
      "\tTrain loss: 0.02865, Accuracy: 4006/6768 (59.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 905/1692 (53.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 781/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.84016\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 1.11100\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.87697\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.70019\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.81489\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.62813\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.91795\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.81452\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.82415\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 1.00666\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.90706\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.86799\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.60422\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.85585\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.86050\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.97431\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.73885\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.79492\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.84379\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.88515\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.67428\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.89409\n",
      "\tTrain loss: 0.02933, Accuracy: 3873/6768 (57.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 891/1692 (52.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 759/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.98019\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.97283\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 1.04196\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.70664\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.96789\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.72550\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 1.16122\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.77260\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.66412\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.89399\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.67495\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.79810\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.75612\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 1.06417\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.93707\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.83087\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.72029\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.96637\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.65194\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 1.14639\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.68642\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.78659\n",
      "\tTrain loss: 0.02560, Accuracy: 4398/6768 (64.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1030/1692 (60.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 823/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.84165\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.98058\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.84118\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.77857\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.88410\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.69884\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.91193\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.77479\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.72972\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 1.07307\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.66109\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 1.12367\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.78790\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 1.02946\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 1.00668\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.67737\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.81510\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 1.09426\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.71315\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.69850\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.67195\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.74152\n",
      "\tTrain loss: 0.02555, Accuracy: 4314/6768 (63.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 968/1692 (57.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 797/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.77175\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 1.02970\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.72278\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.89290\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.87111\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.55499\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 1.05569\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.80885\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 1.01387\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.86855\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.64703\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.78762\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.87563\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.88813\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 1.06718\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.89540\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.94463\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.82768\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.76762\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.95712\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.65949\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.89184\n",
      "\tTrain loss: 0.02553, Accuracy: 4353/6768 (64.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 989/1692 (58.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.88538\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.94206\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.91810\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.71391\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 1.10889\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.53769\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 1.05411\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.67264\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.89184\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.74711\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.80094\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.74978\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.81284\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.62746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.96062\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.87095\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.80149\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.81921\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.75736\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.94520\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.67533\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 1.02517\n",
      "\tTrain loss: 0.02443, Accuracy: 4526/6768 (66.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1038/1692 (61.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 845/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.87711\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.71371\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.89904\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.68173\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.89597\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.75507\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 1.05474\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.84401\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 1.02189\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.83775\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.68117\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.80510\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.69983\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.80110\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.99453\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.74022\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.73947\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.93045\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.72160\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.87272\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.77159\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.72783\n",
      "\tTrain loss: 0.02443, Accuracy: 4530/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1041/1692 (61.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 821/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.74544\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.92655\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.74605\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.72048\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.87675\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.65044\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 1.04964\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.80668\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.78670\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.85298\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.75784\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.72138\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.64031\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.97356\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 1.02688\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 1.00745\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 1.02191\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 1.02115\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.71093\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.63304\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.64286\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.78718\n",
      "\tTrain loss: 0.02650, Accuracy: 4346/6768 (64.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 973/1692 (57.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 822/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.78562\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.81253\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.72451\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.79476\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.98625\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.69497\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.85577\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.77196\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.65451\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.81801\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.97549\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.74171\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.70829\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.94427\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.89738\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 1.04001\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.77184\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.81491\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.66416\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.85475\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.69691\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.87688\n",
      "\tTrain loss: 0.02442, Accuracy: 4498/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1030/1692 (60.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 803/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.88842\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.82886\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.86100\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.75795\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.69208\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.71063\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.87675\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.78071\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 1.04919\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.72625\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.60794\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.80752\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.92467\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.91787\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.97860\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.89283\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.68831\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.82999\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.77769\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.60687\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.70636\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.60892\n",
      "\tTrain loss: 0.02452, Accuracy: 4423/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 983/1692 (58.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 758/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.77749\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.87141\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.80168\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.95740\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.76968\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.65457\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.86057\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.65116\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.61363\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.66351\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.70696\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.80928\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.79679\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.74239\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.96003\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 1.06501\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.77084\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.85506\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.52609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.85037\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.70030\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.94166\n",
      "\tTrain loss: 0.02373, Accuracy: 4494/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1025/1692 (60.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 818/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.66637\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.98332\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 1.05562\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.79360\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.80682\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.74846\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 1.02581\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.75416\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.73441\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.60565\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.67072\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.99098\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.64188\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.80391\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.74454\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.77426\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.79888\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.92649\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.68579\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.62819\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.87061\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.82428\n",
      "\tTrain loss: 0.02529, Accuracy: 4323/6768 (63.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 969/1692 (57.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 786/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.69007\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 1.06691\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.78863\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.62080\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.91354\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.58054\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 1.14153\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.79776\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.76404\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.75543\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.83534\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.72405\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.78366\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.92401\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.87788\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.82078\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.65767\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.96288\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.72959\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.76719\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.56871\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.73187\n",
      "\tTrain loss: 0.02315, Accuracy: 4567/6768 (67.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1033/1692 (61.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 825/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.66453\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.77338\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.72513\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.73365\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.92849\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.74892\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 1.21988\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.65908\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.91423\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 1.06304\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.68566\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.51669\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.58386\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.83130\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.93821\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.65224\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.87620\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.90081\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.71853\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.85332\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.69579\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.70945\n",
      "\tTrain loss: 0.02515, Accuracy: 4333/6768 (64.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 985/1692 (58.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 809/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.82282\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.87403\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.83826\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.67296\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.76220\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.60300\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.85661\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.73444\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.77214\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.71933\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.64193\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.77854\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.55670\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.87678\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.91472\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.87752\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.79762\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.78836\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.95268\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.95959\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.70758\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.82909\n",
      "\tTrain loss: 0.02523, Accuracy: 4480/6768 (66.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1006/1692 (59.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 801/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.83597\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 1.24225\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.97472\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.66711\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.86806\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.66514\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 1.05243\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.70561\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.87508\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.95023\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.51718\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.79135\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.75122\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.83376\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.76467\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.69169\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.74256\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 1.05531\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.69947\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.87056\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.63966\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.81792\n",
      "\tTrain loss: 0.02226, Accuracy: 4620/6768 (68.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1043/1692 (61.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 811/1772 (45.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.72708\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 1.09959\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.91748\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.88851\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.87204\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.48751\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 1.08203\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.58289\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.87619\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.81430\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.72869\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.61343\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.65206\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.75410\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.84271\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.75022\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.95535\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.74471\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.56083\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.58081\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.75852\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.97746\n",
      "\tTrain loss: 0.02125, Accuracy: 4773/6768 (70.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1087/1692 (64.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 849/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.78889\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.88060\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.82533\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.75482\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.84232\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.64662\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 1.07109\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.63131\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.75079\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.58654\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.70435\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.72764\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.87886\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.70367\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 1.00535\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.87152\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.66435\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.72098\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.54610\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.62778\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.61832\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.78479\n",
      "\tTrain loss: 0.02237, Accuracy: 4580/6768 (67.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1044/1692 (61.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 830/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.78297\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.95985\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.62580\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.69982\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.71243\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.49207\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 1.11915\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.69959\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.65128\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.89456\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.53254\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.57909\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.65404\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.93114\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.87477\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.82362\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.72680\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.87650\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.80378\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.62826\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.52609\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.71043\n",
      "\tTrain loss: 0.02428, Accuracy: 4402/6768 (65.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1001/1692 (59.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 780/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.86066\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 1.02680\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.91582\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.57870\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.69131\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.57009\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.81070\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.78796\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.80576\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.57129\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.73995\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.59597\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.68112\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.83704\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.87186\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.88860\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.58094\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.78559\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.65941\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.90806\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.71078\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.67380\n",
      "\tTrain loss: 0.02365, Accuracy: 4539/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1042/1692 (61.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 768/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.81003\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 1.05110\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.72436\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.61492\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.71163\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.78199\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 1.04000\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.78683\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.71756\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.61018\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.76611\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.73912\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.60853\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.92635\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.86830\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.99252\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.57610\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.96548\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.57016\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.99647\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.62091\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.93368\n",
      "\tTrain loss: 0.02024, Accuracy: 4869/6768 (71.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1118/1692 (66.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 852/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.84018\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.86766\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.83851\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.80881\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.62680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.53693\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.81706\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.68776\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.51781\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.71006\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.65500\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.65201\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.50005\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.78009\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.67137\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.91219\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.61037\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.73302\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.62061\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.78525\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.62458\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.84086\n",
      "\tTrain loss: 0.01944, Accuracy: 4960/6768 (73.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1126/1692 (66.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 839/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.74490\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.74438\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.87663\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.73910\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.87573\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.63350\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.95295\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.50901\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.62700\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.73546\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.54188\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.71213\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.62808\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.63423\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.82681\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.75271\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.65955\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.88296\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.86693\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.81452\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.63648\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.86867\n",
      "\tTrain loss: 0.01695, Accuracy: 5394/6768 (79.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1220/1692 (72.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 883/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.69360\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.69716\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.56957\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.67235\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.72656\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.54257\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.91779\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.65242\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.84081\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.68524\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.54010\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 1.07653\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.62797\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.76007\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.86080\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.81541\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.83062\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.73283\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.73970\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.66810\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.61896\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.75847\n",
      "\tTrain loss: 0.02087, Accuracy: 4845/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1116/1692 (65.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 826/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.93391\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.61476\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.78330\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.68674\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.84583\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.48379\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.82935\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.64919\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.69750\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.69579\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.58365\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.83075\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.52656\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.73801\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.99495\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 1.03067\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.71400\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.77111\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.64781\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.73552\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.59714\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.77073\n",
      "\tTrain loss: 0.02173, Accuracy: 4691/6768 (69.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1050/1692 (62.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 804/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.76087\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.79990\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.62323\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.61413\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.76757\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.58500\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.86083\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.80298\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.72066\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.72469\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.46962\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.69839\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.81424\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.75684\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.84358\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 1.02869\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.44145\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.75866\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.82553\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.66954\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.72811\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.90918\n",
      "\tTrain loss: 0.01909, Accuracy: 5033/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1163/1692 (68.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 846/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.58217\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.69326\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.89431\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.70918\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.66637\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.42279\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.83193\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.46548\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.65715\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.88434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.64230\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.76311\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.69928\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.95781\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.94417\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.71256\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.65008\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.95099\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.61008\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.80122\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.59324\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.67743\n",
      "\tTrain loss: 0.01814, Accuracy: 5139/6768 (75.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1168/1692 (69.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 882/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.91697\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.59593\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.82512\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.74795\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.65237\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.55787\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.77186\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.69599\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.85115\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.61533\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.66430\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.59607\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.70250\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.59431\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.85823\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.65227\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.84220\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.87889\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.65425\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.94317\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.64962\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.77591\n",
      "\tTrain loss: 0.01992, Accuracy: 4940/6768 (72.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1126/1692 (66.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 843/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.74215\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.75764\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.85710\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.64604\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.80055\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.54588\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 1.03048\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.54509\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.71305\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.60818\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.75840\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.61074\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.82534\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.76420\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.58275\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.75001\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.78792\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.60450\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.47383\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.61882\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.56370\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 1.06428\n",
      "\tTrain loss: 0.02484, Accuracy: 4323/6768 (63.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 976/1692 (57.00%)\n",
      "\tTest loss: 0.00085, Accuracy: 794/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.80599\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.82425\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.93721\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.64167\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.60484\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.67306\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.65251\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.58764\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.94318\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.83513\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.69186\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.71409\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.69334\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.65248\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.75030\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.67384\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.77598\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.71945\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.53541\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.72708\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.56411\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.53698\n",
      "\tTrain loss: 0.01922, Accuracy: 5028/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1144/1692 (67.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 859/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.78238\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.81930\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.81892\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.55350\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.82685\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.49717\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.88535\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.68545\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.66931\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.60616\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.54174\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.64110\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.68168\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.72027\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.89816\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.78821\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.73198\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.70214\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.55496\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.76186\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.58473\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.70727\n",
      "\tTrain loss: 0.01830, Accuracy: 5009/6768 (74.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1149/1692 (67.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 860/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.85049\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 1.00250\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.66043\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.53396\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.58851\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.55192\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.93130\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.68130\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.76474\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.98402\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.45096\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.70544\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.70421\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.56112\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.56656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.72195\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.77637\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.81190\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.50451\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.69141\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.57460\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.73740\n",
      "\tTrain loss: 0.01954, Accuracy: 4970/6768 (73.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1150/1692 (67.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 885/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.72499\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.72398\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.80241\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.59234\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.84794\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.66138\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.76197\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.46911\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.48175\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.55250\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.72412\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.52716\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.53055\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.59678\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.66953\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.61125\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.85247\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.65799\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.44803\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.86060\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.59835\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.67473\n",
      "\tTrain loss: 0.01924, Accuracy: 5046/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1132/1692 (66.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 889/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.78185\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.65273\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.75254\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.75577\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 1.04244\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.56245\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.75954\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.55230\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.63865\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.72442\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.70180\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.69026\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.57852\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.62339\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.55732\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.83498\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.71017\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.65939\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.48433\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.75431\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.69007\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.64161\n",
      "\tTrain loss: 0.01869, Accuracy: 5028/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1131/1692 (66.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 875/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.80713\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.90795\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.77545\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.61702\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.80574\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.54804\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.69210\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.62772\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.69946\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.52049\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.48242\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.57779\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.48236\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.85247\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.59668\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.71051\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.91062\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.86401\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.49302\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.74570\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.56196\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.77878\n",
      "\tTrain loss: 0.01812, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1173/1692 (69.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 910/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.65751\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.91512\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.70239\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.38242\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.63434\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.70170\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.77121\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.59081\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.52342\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.80547\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.73524\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.55272\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.59111\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.62811\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.81307\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.61798\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.76080\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.72101\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.45140\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.85130\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.43950\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.80982\n",
      "\tTrain loss: 0.01851, Accuracy: 4972/6768 (73.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1131/1692 (66.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 887/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.71485\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.69430\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.68285\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.57465\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.71222\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.52807\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.68686\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.54966\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.87310\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.51177\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.62027\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.73537\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.64973\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.83558\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.77157\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.57939\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.75520\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.70885\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.51852\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.60043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.65277\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.81286\n",
      "\tTrain loss: 0.01862, Accuracy: 5046/6768 (74.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1150/1692 (67.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 860/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.78173\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.89391\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.86553\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.52450\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.50372\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.63870\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.79091\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.77585\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.86239\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.55092\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.53768\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.56610\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.51170\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.77524\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.68483\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.90953\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.65343\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.85654\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.54335\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.55431\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.58229\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.97387\n",
      "\tTrain loss: 0.01820, Accuracy: 5107/6768 (75.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1175/1692 (69.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 877/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.79614\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.77648\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.71066\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.67088\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.63357\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.53082\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.88394\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.51187\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.66673\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.52914\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.50613\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.66004\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.56781\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.59830\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.94619\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.82682\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.71218\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.65578\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.70789\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.81550\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.45060\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.61197\n",
      "\tTrain loss: 0.01695, Accuracy: 5286/6768 (78.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1202/1692 (71.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 893/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.73762\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.60452\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.74320\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.63992\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.95368\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.56365\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.67571\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.72666\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.58407\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.39841\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.49638\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.70080\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.65178\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.64532\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.71476\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.75852\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.73135\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.67806\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.62463\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.84896\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.62569\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.69580\n",
      "\tTrain loss: 0.02268, Accuracy: 4622/6768 (68.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1046/1692 (61.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 794/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.59738\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.64002\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.65089\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.55237\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.74631\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.62049\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.65805\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.44923\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.91065\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.63537\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.67306\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.57473\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.60385\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.83648\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.86984\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.93758\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.69941\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.58697\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.59418\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.70359\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.60362\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.70876\n",
      "\tTrain loss: 0.01908, Accuracy: 4879/6768 (72.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1115/1692 (65.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 863/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.82826\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.64582\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.76031\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.46702\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.61060\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.60412\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.77191\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.57922\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.58264\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.75083\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.48603\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.55222\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.52331\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.93565\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.77894\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.89894\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.62545\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.74338\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.65999\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.67643\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.55539\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.80425\n",
      "\tTrain loss: 0.02130, Accuracy: 4761/6768 (70.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1068/1692 (63.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 817/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.79357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.81349\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.68421\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.51605\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.77499\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.55417\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.81777\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.52179\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.54596\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.63065\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.49528\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.75639\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.50142\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.67036\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.73264\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.66939\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.85974\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.98458\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.79790\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.77684\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.80725\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.57598\n",
      "\tTrain loss: 0.01675, Accuracy: 5271/6768 (77.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1203/1692 (71.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 882/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.85278\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.95135\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.68930\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.80232\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.73895\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.63177\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.80905\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.71171\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.73731\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.64702\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.45987\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.82747\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.58053\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.63879\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.69239\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.54821\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.70302\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.59538\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.52698\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.93161\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.60186\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.62331\n",
      "\tTrain loss: 0.01880, Accuracy: 4984/6768 (73.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1157/1692 (68.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 843/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.87751\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.54916\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.77103\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.45038\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.65844\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.61757\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.71163\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.59634\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.80295\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.58163\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.66492\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.48682\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.39696\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.58002\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.83177\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.74469\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.78466\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.77090\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.45441\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.68863\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.45590\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.52463\n",
      "\tTrain loss: 0.01887, Accuracy: 5074/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1163/1692 (68.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 855/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.60992\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.76220\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.68022\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.49974\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.60279\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.35058\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.94099\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.44995\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.45632\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.51631\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.61814\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.71038\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.56710\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.58935\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.69789\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.51616\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.76512\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.53433\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.57518\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.60741\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.52953\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.74093\n",
      "\tTrain loss: 0.01912, Accuracy: 4870/6768 (71.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1121/1692 (66.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 826/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.63739\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.64734\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.65784\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.64066\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.61973\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.47263\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.75853\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.67539\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.54097\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.49228\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.70009\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.55647\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.78632\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.67855\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.65982\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.64643\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.62924\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.69262\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.51930\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.60967\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.48087\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.73868\n",
      "\tTrain loss: 0.02089, Accuracy: 4785/6768 (70.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1085/1692 (64.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 827/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.81801\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.65319\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.73134\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.75787\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.52165\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.59514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.83059\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.81096\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.69472\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.45217\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.43249\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.58109\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.76206\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.66286\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.65942\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.80635\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.72137\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.52184\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.56557\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.68007\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.52803\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.76286\n",
      "\tTrain loss: 0.01680, Accuracy: 5313/6768 (78.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1245/1692 (73.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 897/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.76845\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.59025\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.65030\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.54970\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.66036\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.41313\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.92024\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.52713\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.72703\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.58090\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.54676\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.50313\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.61312\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.53774\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.78078\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.77585\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.84419\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.94947\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.59609\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.95229\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.68360\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.59357\n",
      "\tTrain loss: 0.01631, Accuracy: 5566/6768 (82.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1284/1692 (75.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 881/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.94880\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 1.07298\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.69014\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.59547\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.63311\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.55474\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.67865\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.63341\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.79204\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.76536\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.55458\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.49687\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.48306\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.68561\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.67935\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.91555\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.59697\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.52143\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.60886\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.89106\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.43789\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.87801\n",
      "\tTrain loss: 0.02226, Accuracy: 4632/6768 (68.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1060/1692 (62.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 811/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.77088\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.65128\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.98772\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.83893\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.57295\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.46740\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.74333\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.59049\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.65174\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.59618\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.60438\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.54647\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.46705\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.67013\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 1.15249\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.64423\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.61138\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.71031\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.46369\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.52116\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.77486\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.65538\n",
      "\tTrain loss: 0.01657, Accuracy: 5344/6768 (78.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1250/1692 (73.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 900/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.62532\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.61188\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.61246\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.67293\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.71215\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.48215\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.70206\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.46142\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.76521\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.64401\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.41168\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.42479\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.42090\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.65904\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.67578\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.40773\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.56912\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.80425\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.40830\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.66685\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.57742\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.57535\n",
      "\tTrain loss: 0.01865, Accuracy: 5171/6768 (76.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1189/1692 (70.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 875/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.60736\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.59408\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.82230\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.69382\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.50155\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.40627\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.84485\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.62458\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.59653\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.74336\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.52147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.62521\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.52715\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.59514\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.63036\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.63974\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.74842\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.57475\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.55781\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.65771\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.45178\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.82667\n",
      "\tTrain loss: 0.01719, Accuracy: 5135/6768 (75.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1194/1692 (70.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 866/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.64547\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.56102\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.70753\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.62686\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.52757\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.40481\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.86980\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.60303\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.70921\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.61801\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.64980\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.68214\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.57608\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.97546\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.80742\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.67788\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.61657\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.64428\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.58913\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.68943\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.53341\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.81758\n",
      "\tTrain loss: 0.02090, Accuracy: 4692/6768 (69.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1066/1692 (63.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 809/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.51148\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.82808\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.76959\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.68557\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.83209\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.48549\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.82061\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.75778\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.55664\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.60593\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.46770\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.64374\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.69662\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.73747\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.63250\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.76740\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.58881\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.51252\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.65834\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.44505\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.48491\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.63996\n",
      "\tTrain loss: 0.01804, Accuracy: 5102/6768 (75.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1172/1692 (69.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 878/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.60166\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.64640\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.64586\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.62420\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.85099\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.42292\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.70496\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.54324\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.49594\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.64977\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.58190\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.65959\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.45358\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.81140\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.86495\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.64560\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.83050\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.64497\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.67278\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.45441\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.73905\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.49432\n",
      "\tTrain loss: 0.01469, Accuracy: 5644/6768 (83.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1310/1692 (77.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 915/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.81565\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.57710\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.63418\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.58618\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.61908\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.49029\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.78092\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.43146\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.62321\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.55624\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.58164\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.55682\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.56742\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.58167\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.83282\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.67495\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.88927\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.72232\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.48007\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.63176\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.65703\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.65438\n",
      "\tTrain loss: 0.01629, Accuracy: 5455/6768 (80.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1259/1692 (74.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 906/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.77348\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.61947\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.54411\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.55423\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.60822\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.72207\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.84985\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.48859\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.57372\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.68168\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.45749\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.39215\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.51854\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.96177\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.73302\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.59060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.59791\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.71018\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.43872\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.55844\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.44314\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.65524\n",
      "\tTrain loss: 0.01346, Accuracy: 5631/6768 (83.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1297/1692 (76.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 908/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.78812\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.75624\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.75501\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.64999\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.39068\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.66628\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.88592\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.62999\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.52930\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.62687\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.54413\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.61735\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.46937\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.88192\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.77171\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.78538\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.57898\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.75603\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.72613\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.59923\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.43227\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.83609\n",
      "\tTrain loss: 0.01613, Accuracy: 5297/6768 (78.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1232/1692 (72.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 908/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.69186\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.76666\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.69429\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.45221\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.55834\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.53741\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.79129\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.68038\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.81339\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.81498\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.47265\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.70390\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.47428\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 1.20731\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.87146\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.50477\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.56987\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.65730\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.46653\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.67478\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.57937\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.62659\n",
      "\tTrain loss: 0.01540, Accuracy: 5424/6768 (80.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1249/1692 (73.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 889/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.56703\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.52975\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.83243\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.58300\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.59327\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.56213\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 1.01660\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.45276\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.43860\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.79830\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.57959\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.35069\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.53554\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.58103\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.65414\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.57231\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.53225\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.58709\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.67282\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.41842\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.54560\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.70350\n",
      "\tTrain loss: 0.02477, Accuracy: 4574/6768 (67.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1042/1692 (61.00%)\n",
      "\tTest loss: 0.00094, Accuracy: 817/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.67655\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.83503\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.73317\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.51702\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.49029\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.42939\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.52193\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.52844\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.55760\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.68143\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.47561\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.87490\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.39636\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.54413\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.73403\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.68939\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.87652\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.69750\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.34441\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.52710\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.47468\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.52686\n",
      "\tTrain loss: 0.02226, Accuracy: 4758/6768 (70.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1075/1692 (63.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 832/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.76359\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.92634\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.57821\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.51514\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.59460\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.44373\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.68908\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.34804\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.61258\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.51803\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.64628\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.49336\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.55880\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.76092\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.53042\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.66791\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.78252\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.72792\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.40259\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.51825\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.39133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.55980\n",
      "\tTrain loss: 0.01831, Accuracy: 5091/6768 (75.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1153/1692 (68.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 893/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.69143\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.67982\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.87276\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.52484\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.61191\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.36141\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.84887\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.46436\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.66293\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.62413\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.42230\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.71498\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.50154\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.78952\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.70626\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.48634\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.65584\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.67801\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.58577\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.53656\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.54549\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.84493\n",
      "\tTrain loss: 0.01598, Accuracy: 5361/6768 (79.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1222/1692 (72.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 921/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.61257\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.40866\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.56736\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.44486\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.57612\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.50098\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.74445\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.27847\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.67628\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.63733\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.47312\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.51358\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.76501\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.55606\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.40784\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.74621\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.71016\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.64831\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.58609\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.57878\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.52229\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.49464\n",
      "\tTrain loss: 0.01562, Accuracy: 5338/6768 (78.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1211/1692 (71.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 913/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.54025\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.77687\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.76513\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.44832\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.53056\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.30749\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.80417\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.64040\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.64292\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.55194\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.37273\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.56187\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.64931\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.76262\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.71272\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.57942\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.92787\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.64046\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.40311\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.67905\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.33956\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.48071\n",
      "\tTrain loss: 0.01768, Accuracy: 5228/6768 (77.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 895/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.88708\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.49582\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.68325\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.39018\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.40321\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.42545\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.65725\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.41065\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.56449\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.52057\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.47403\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.37021\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.64456\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.67813\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.67689\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.97197\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.61203\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.80832\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.57290\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.72204\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.53308\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.51711\n",
      "\tTrain loss: 0.01641, Accuracy: 5274/6768 (77.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1203/1692 (71.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 899/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.44892\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.90238\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.57212\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.64431\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.51168\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.42387\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.43371\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.51392\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.61887\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.66548\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.60068\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.64464\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.50760\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.73249\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.64457\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.56529\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.57468\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.57114\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.34355\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.76683\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.52796\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.51143\n",
      "\tTrain loss: 0.01286, Accuracy: 5825/6768 (86.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1358/1692 (80.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 976/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.69968\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.66016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.71970\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.43859\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.54962\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.39492\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.65530\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.40770\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.78219\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.76529\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.51370\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.69761\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.52208\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.49714\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.45634\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.57951\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.52331\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.70564\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.60386\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.50141\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.35857\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.67307\n",
      "\tTrain loss: 0.01300, Accuracy: 5794/6768 (85.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1328/1692 (78.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 963/1772 (54.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.8026004728132388\n",
      "Best test accuracy:\n",
      "0.5507900677200903\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_lstm, cnn_lstm_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9c135",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24ac8c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.30810\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.38942\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.40191\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.42637\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.45949\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.42487\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.34933\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.38494\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.34921\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.27871\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.37201\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.36708\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.46088\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.42834\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.41323\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.40169\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.40193\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.42263\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.37804\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.43850\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.37211\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.40141\n",
      "\tTrain loss: 0.04305, Accuracy: 1899/6768 (28.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 454/1692 (26.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 499/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.40006\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.34243\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.37825\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.38519\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.38880\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.39583\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.38126\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.31863\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.36078\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.31027\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.26415\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.37484\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.30360\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.42563\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.37630\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.38155\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.35939\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.34885\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.29888\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.41475\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.36891\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.32837\n",
      "\tTrain loss: 0.04179, Accuracy: 2243/6768 (33.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 528/1692 (31.00%)\n",
      "\tTest loss: 0.00076, Accuracy: 570/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.31598\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.36975\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.40900\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.28844\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.39833\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.28084\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.31406\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.36035\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.33570\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.23348\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.26180\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.29719\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.26257\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.25427\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.28350\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.29623\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.40183\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.33440\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.29997\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.25473\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.18829\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.18812\n",
      "\tTrain loss: 0.03977, Accuracy: 2621/6768 (38.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 612/1692 (36.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 644/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.24744\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.31735\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.31790\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.36223\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.24936\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.09687\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.25325\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.34376\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.38037\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.15130\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.12165\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.23237\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.14624\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.29933\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.36524\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.32661\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.08658\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.21865\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.20849\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.26203\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.13776\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.30613\n",
      "\tTrain loss: 0.03985, Accuracy: 2553/6768 (37.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 611/1692 (36.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 620/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.18598\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.41273\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.29300\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.26616\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.23553\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.00383\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.26630\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.24056\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.26054\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.18309\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.03574\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.15014\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.19733\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.22244\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.28626\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.26542\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.09729\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.15940\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.09470\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.25493\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.13267\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.25412\n",
      "\tTrain loss: 0.03877, Accuracy: 2678/6768 (39.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 665/1692 (39.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 683/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.10745\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.26677\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.28667\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.19910\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.22664\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.04138\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.30697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.06708\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.19705\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.14270\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 0.99974\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.13341\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.02061\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.18812\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.16971\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.26012\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.11025\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.24283\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.04785\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.16840\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.16466\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.19039\n",
      "\tTrain loss: 0.03838, Accuracy: 2819/6768 (41.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 669/1692 (39.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 653/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.07458\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.35626\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.25925\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.26838\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.24894\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.08779\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.35694\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.10938\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.24780\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.04070\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.00766\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.13077\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.10994\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.33733\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.23005\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.20005\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.00826\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.18496\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.08186\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.11799\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.00805\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.15628\n",
      "\tTrain loss: 0.03753, Accuracy: 2982/6768 (44.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 698/1692 (41.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 702/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.02744\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.33126\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.24350\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.09826\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.19140\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.04348\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.36548\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.06505\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.31926\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.20410\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 0.99372\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.04646\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.04210\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.26284\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.28405\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.13284\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.04488\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.19818\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.04887\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.14553\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.08317\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.10397\n",
      "\tTrain loss: 0.03657, Accuracy: 3091/6768 (45.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 736/1692 (43.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 713/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.00566\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.49391\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.23619\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.08976\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.23350\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.88224\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.27503\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.08598\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.06787\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.08725\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 0.94236\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.29247\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.08172\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.14333\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.08971\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.20513\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.05441\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.17595\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.01311\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.03541\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.15135\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.15230\n",
      "\tTrain loss: 0.03839, Accuracy: 2906/6768 (42.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 679/1692 (40.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 657/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 0.97450\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.22088\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.21097\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.15571\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.24457\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 0.97452\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.34216\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.01929\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.10920\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.05010\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 0.96389\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.06898\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.10844\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.12785\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.12302\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.01789\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.00921\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.16744\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 0.98326\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.04268\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.13975\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.18800\n",
      "\tTrain loss: 0.03900, Accuracy: 2958/6768 (43.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 689/1692 (40.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 655/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.01839\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.32437\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.28894\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.06453\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.19929\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.97951\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.21681\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.20545\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.12444\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 0.88479\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.92224\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.07390\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 0.90112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.20169\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.08081\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.07630\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 0.92159\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.04989\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.19983\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.12436\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 0.92720\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.19327\n",
      "\tTrain loss: 0.03781, Accuracy: 2957/6768 (43.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 671/1692 (39.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 660/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.86421\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.40741\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.17484\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 0.91721\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.23580\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.82233\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.16706\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 0.88925\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.09329\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.05173\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.90999\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.01422\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.90213\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.04845\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.10661\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.19305\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 0.99549\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.16525\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.87329\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.04173\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.06636\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.15918\n",
      "\tTrain loss: 0.03601, Accuracy: 3096/6768 (45.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 724/1692 (42.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 696/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.98416\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.22955\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.13434\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.05093\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.17499\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.86628\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.49693\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.06652\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.02840\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.05463\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.85645\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.00852\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.98102\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.10365\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.11105\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.02318\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 0.94407\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.11240\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.08696\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.03068\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.99581\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.16885\n",
      "\tTrain loss: 0.03639, Accuracy: 3140/6768 (46.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 719/1692 (42.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 721/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.88374\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.34534\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.11591\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.01924\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.22727\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.92173\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.29424\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 0.84546\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.05427\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.03789\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.97077\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.85074\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.02757\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 0.98484\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.05806\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.19168\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.90222\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.12866\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.91833\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.17848\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.07018\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.97520\n",
      "\tTrain loss: 0.03452, Accuracy: 3335/6768 (49.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 777/1692 (45.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 748/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.89945\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.12488\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 0.88788\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.96826\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.18864\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.76959\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.25596\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 0.87551\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.16573\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.94786\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 1.01463\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 0.93236\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.81357\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.09722\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.10969\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.95700\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.00567\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.07047\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.96453\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.99921\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.90385\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.12318\n",
      "\tTrain loss: 0.03518, Accuracy: 3270/6768 (48.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 767/1692 (45.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 712/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.92042\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.25342\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 0.97855\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.94921\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.13898\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.83645\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.41374\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.90495\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.13879\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.86845\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 1.01173\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.92950\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.84089\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.05222\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.99152\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.03977\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.85073\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.26362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.02016\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.96722\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.87513\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.00073\n",
      "\tTrain loss: 0.03373, Accuracy: 3431/6768 (50.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 817/1692 (48.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 731/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.87886\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 0.97886\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.20843\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.87754\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.06944\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.87504\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.46406\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.83778\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 1.29386\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.07235\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.80795\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.94284\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.83476\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 1.10271\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.87546\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.14423\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.90421\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.01217\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.94793\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.92565\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.84998\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.00670\n",
      "\tTrain loss: 0.03430, Accuracy: 3417/6768 (50.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 798/1692 (47.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 727/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.94137\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.99151\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.03093\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.97727\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.03295\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.87140\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.35197\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.96412\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.12372\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.93711\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.84468\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.83256\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.87509\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.18780\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 1.00817\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.93901\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.76820\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.08091\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.86296\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.01855\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.80505\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 1.24752\n",
      "\tTrain loss: 0.03316, Accuracy: 3473/6768 (51.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 789/1692 (46.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 735/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.93116\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.28167\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 1.16072\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.15426\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.10992\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.94863\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 1.21426\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.94789\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.96785\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.87868\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.76384\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.04257\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.88935\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.85061\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.94947\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 1.10809\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.89673\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 1.09282\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.04500\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.89744\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.93209\n",
      "\tTrain loss: 0.03303, Accuracy: 3552/6768 (52.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 814/1692 (48.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 749/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.96174\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.14969\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.15702\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.96875\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.90052\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.85409\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.15901\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.81390\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.85740\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.89306\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.84094\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.85345\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.69318\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.17620\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.95592\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 1.01556\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.80848\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.06668\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.81667\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.05714\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.94980\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.05175\n",
      "\tTrain loss: 0.03378, Accuracy: 3467/6768 (51.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 791/1692 (46.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 743/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.92054\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.05571\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.78998\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.04273\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.98330\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.79629\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.25468\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.79914\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 1.00408\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.85425\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.83033\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.87617\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 1.03550\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.96762\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.85461\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.74389\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.84942\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.87693\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.97649\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.91005\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.85704\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.78247\n",
      "\tTrain loss: 0.03538, Accuracy: 3344/6768 (49.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00074, Accuracy: 743/1692 (43.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 708/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 1.00682\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.31546\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.06369\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.92438\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.73833\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.83275\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.22065\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.81826\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 1.07760\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.88927\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.83277\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.77676\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.93023\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.94104\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.92254\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.77217\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.78163\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.12481\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.86323\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.94302\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.99831\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 1.14696\n",
      "\tTrain loss: 0.03257, Accuracy: 3561/6768 (52.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 809/1692 (47.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 738/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.88300\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.92934\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 1.21125\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 1.05031\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 1.13796\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.69000\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 1.12918\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.92805\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 1.04957\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.94399\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.75242\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.92781\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.80875\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 1.07034\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.91593\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.74681\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.72053\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 1.03275\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.73615\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.81231\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.99693\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.99634\n",
      "\tTrain loss: 0.03288, Accuracy: 3553/6768 (52.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 819/1692 (48.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 746/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.88850\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 1.25747\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.94033\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.85282\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.83970\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.72161\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 1.06897\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.67159\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 1.00880\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.98409\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.77609\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 1.06995\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.86450\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.83987\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.98854\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.81290\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.76883\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 1.11179\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.67757\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.88598\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.85077\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.83599\n",
      "\tTrain loss: 0.03066, Accuracy: 3774/6768 (55.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 880/1692 (52.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 772/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.93130\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.14601\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 1.09377\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 1.03577\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.90749\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.77657\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 1.17204\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.75845\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.99261\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.84791\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.84101\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.78858\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.83150\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.91566\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.84690\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.72899\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.84185\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.99446\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.78122\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 1.02813\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.77810\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.87509\n",
      "\tTrain loss: 0.03232, Accuracy: 3574/6768 (52.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 820/1692 (48.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 749/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.75971\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.91094\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.91632\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.92668\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 1.00241\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.75786\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 1.11339\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.79405\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 1.02651\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.87443\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.69982\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.83275\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.70794\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.81203\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.88399\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.95824\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.84192\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.01031\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.95888\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.86836\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.69446\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.99512\n",
      "\tTrain loss: 0.03066, Accuracy: 3759/6768 (55.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 861/1692 (50.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 763/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.85389\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.03260\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 1.19443\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.93430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.91063\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.69302\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 1.08521\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.79473\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 1.08662\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.78199\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.80261\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.76133\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.72044\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.84748\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.75628\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.85760\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.82155\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.75133\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.78322\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.76865\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.99389\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.89058\n",
      "\tTrain loss: 0.02962, Accuracy: 3894/6768 (57.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 892/1692 (52.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 796/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.80762\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.99378\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 1.03696\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 1.22231\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 1.04941\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.84564\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 1.21781\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.79127\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 1.01459\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.84425\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.67942\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.87654\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.86243\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.83724\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.79033\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.95364\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.86324\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.25669\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.94605\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.83201\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.76754\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.76469\n",
      "\tTrain loss: 0.02986, Accuracy: 3828/6768 (56.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 874/1692 (51.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 774/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.95226\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 1.11274\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.88044\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.78411\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.87531\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.69926\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 1.00553\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.76465\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.91218\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.77480\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.72430\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.81668\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.74539\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.80877\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.84009\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 1.06427\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.81223\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 1.09295\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.70853\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 1.11566\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.78277\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.99071\n",
      "\tTrain loss: 0.03119, Accuracy: 3666/6768 (54.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 829/1692 (48.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 740/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.74947\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.83517\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.79496\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.88479\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.81929\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.72146\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 1.19602\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.68416\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 1.14884\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.83434\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.75558\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.88047\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.79028\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.90881\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.71398\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.82347\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.84721\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.90062\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.71161\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.87899\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.87182\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.89984\n",
      "\tTrain loss: 0.02762, Accuracy: 4115/6768 (60.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 932/1692 (55.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 853/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.99630\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.89064\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 1.02570\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.68878\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.89898\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.57776\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 1.04367\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.81844\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.93112\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.64525\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.69252\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.86935\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.80996\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 1.00833\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.73841\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.93321\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.71151\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 1.01820\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.79318\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.86552\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.63923\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.88102\n",
      "\tTrain loss: 0.02747, Accuracy: 4090/6768 (60.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 930/1692 (54.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.88159\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 1.11261\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.82188\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.71408\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 1.03801\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.76495\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.88231\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.75457\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.86639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.78132\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.60769\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.89328\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.60096\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.85151\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.86374\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.86883\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.68666\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 1.00174\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.65950\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.71175\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.78531\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.89870\n",
      "\tTrain loss: 0.02870, Accuracy: 3927/6768 (58.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 880/1692 (52.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 746/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.88742\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.82981\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.93634\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.88469\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.75976\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.79244\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 1.03358\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.67108\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 1.06453\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.71756\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.73778\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.77548\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.70085\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.69784\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.69844\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 1.03065\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.81367\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 1.04898\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.81358\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.84972\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.62005\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.74600\n",
      "\tTrain loss: 0.02909, Accuracy: 3943/6768 (58.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 876/1692 (51.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 746/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.83579\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 1.10219\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.86066\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.57295\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.93208\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.54013\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 1.01571\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.72662\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 1.05597\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.77442\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.92498\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.97367\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.70744\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.64958\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 1.06468\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.83659\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.77890\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.80755\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.73066\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.82957\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.81507\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 1.01057\n",
      "\tTrain loss: 0.02713, Accuracy: 4129/6768 (61.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 946/1692 (55.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 793/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.81948\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 1.04863\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.79037\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.77442\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.63036\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.76924\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 1.14488\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.66369\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.91200\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.80248\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.64906\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.79669\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.58045\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.78331\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.88946\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.91502\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.64331\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.90356\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.69104\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.69457\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.73283\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 1.05306\n",
      "\tTrain loss: 0.02637, Accuracy: 4187/6768 (61.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 967/1692 (57.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.80495\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.90414\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.81831\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.71954\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.79610\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.83462\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 1.12972\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.81874\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.77540\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.79688\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.69870\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.74066\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.71204\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.81657\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.98174\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.93761\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.74258\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.83872\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.69678\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.77589\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.80888\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.70822\n",
      "\tTrain loss: 0.02860, Accuracy: 3970/6768 (58.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 895/1692 (52.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.77115\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.73399\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.75215\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.64844\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.77135\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.64709\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.96489\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.72462\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.81018\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.90242\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.67588\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.78576\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.90073\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.83527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.75056\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.95050\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.59209\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.90919\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.69850\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.88679\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.76459\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 1.03851\n",
      "\tTrain loss: 0.02730, Accuracy: 4201/6768 (62.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 953/1692 (56.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 799/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.91737\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.97616\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.75884\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.62812\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.59846\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.58349\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 1.07945\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.72960\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.88524\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.70444\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.66202\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.84554\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.67318\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.90292\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.74440\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.68496\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.71282\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.97424\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.75921\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.75803\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.76589\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.82915\n",
      "\tTrain loss: 0.02729, Accuracy: 4113/6768 (60.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 948/1692 (56.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 783/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.74545\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 1.18512\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.88747\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.71146\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.62738\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.71186\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 1.16340\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.79351\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.87313\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.83678\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.67401\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.59743\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.69639\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.76129\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.90231\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.66483\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.62869\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.76944\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.88829\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.83293\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.78157\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.76092\n",
      "\tTrain loss: 0.02342, Accuracy: 4490/6768 (66.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1044/1692 (61.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 841/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.84285\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.89362\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.81203\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.88887\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.98017\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.73339\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.91494\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.69369\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.74127\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.87095\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.64706\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.80938\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.81512\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.91782\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.73375\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.79875\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.70442\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 1.00648\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.54644\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.75356\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.79346\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 1.01469\n",
      "\tTrain loss: 0.02279, Accuracy: 4546/6768 (67.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1035/1692 (61.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 833/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 1.04201\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.74904\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.80961\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.77164\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.75365\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.63814\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.95974\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.80070\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.83388\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.78885\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.71644\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.75302\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.60421\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.65564\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.71590\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.75115\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.69304\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.91569\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.64369\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.88409\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.83359\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 1.01402\n",
      "\tTrain loss: 0.02235, Accuracy: 4676/6768 (69.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1047/1692 (61.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.85783\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.70348\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.85811\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.72249\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.83423\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.55679\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.87920\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.94875\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.93289\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.72811\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.69403\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.71187\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.68171\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.67842\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.87633\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.57856\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.68100\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 1.08002\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.54077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.87517\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.71029\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.74293\n",
      "\tTrain loss: 0.02266, Accuracy: 4642/6768 (68.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1058/1692 (62.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 842/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.69240\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.86613\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.77048\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.60521\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.76179\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.62793\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.78238\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.77521\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.81904\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.66716\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.66395\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.82007\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.69561\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.85085\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.67864\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.79643\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.65398\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.89722\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.75396\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.73696\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.57715\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.81384\n",
      "\tTrain loss: 0.02471, Accuracy: 4447/6768 (65.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1001/1692 (59.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 817/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.94409\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.86000\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.65820\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.61628\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.71803\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.66972\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 1.00874\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.69365\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.86664\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.87686\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.71232\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.65190\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.77347\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.78938\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.81091\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.79564\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.83549\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.94592\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.58497\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.82847\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.82571\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.93676\n",
      "\tTrain loss: 0.02319, Accuracy: 4548/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1018/1692 (60.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 813/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.81099\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.75326\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.85619\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.72874\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.77421\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.52916\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.85516\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.54200\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.73450\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.81933\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.70991\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.58780\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.62148\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.60358\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.64531\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.95521\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.65498\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.87570\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.88883\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.80100\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.64469\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.80874\n",
      "\tTrain loss: 0.02352, Accuracy: 4559/6768 (67.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1036/1692 (61.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 807/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.90339\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.56990\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.70322\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.82662\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.85318\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.76807\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.86767\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.55237\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.96924\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.87378\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.61913\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.82117\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.51977\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.93602\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.63782\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.62520\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.60854\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.96631\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.56761\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.72815\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.54961\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.75441\n",
      "\tTrain loss: 0.02253, Accuracy: 4754/6768 (70.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1068/1692 (63.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 808/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.80990\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 1.03065\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.65706\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.48902\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.72212\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 1.09055\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 1.09577\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.67135\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.69576\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.95005\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.68703\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.76065\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.68167\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.88674\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.64003\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.68615\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.67810\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.76759\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.49497\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.64763\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.74800\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.65877\n",
      "\tTrain loss: 0.02073, Accuracy: 5093/6768 (75.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1134/1692 (67.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 917/1772 (51.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.71878\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.74401\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.77567\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.66791\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.56582\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.76255\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.94671\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.65989\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.70627\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.74037\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.77618\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.50600\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.60490\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.61270\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.63021\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.88319\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.70036\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.91782\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.62098\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.65232\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.63583\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.62523\n",
      "\tTrain loss: 0.02143, Accuracy: 5112/6768 (75.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1154/1692 (68.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 905/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.85751\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.89394\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.68706\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.60978\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.86360\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.49437\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.93252\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.67821\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.89530\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.67396\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.75256\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.82772\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.76333\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.71256\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.66205\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.85692\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.57638\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.81954\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.57022\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.67585\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.77956\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.83029\n",
      "\tTrain loss: 0.02204, Accuracy: 4895/6768 (72.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1098/1692 (64.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 880/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.59174\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.83602\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.54784\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.64388\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.55410\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.61029\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.88872\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.60320\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.79510\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.80529\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.54347\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.65960\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.68733\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.76417\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.78457\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.67350\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.79171\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.71866\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.76108\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.67107\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.67894\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.73065\n",
      "\tTrain loss: 0.01998, Accuracy: 4989/6768 (73.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1128/1692 (66.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 868/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.81064\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.70210\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.87706\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.55071\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.85519\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.53332\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 1.09914\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.64217\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.73599\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.74590\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.55242\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.89675\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.67814\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.80831\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.79531\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.79487\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.48414\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 1.26523\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.50844\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.68789\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.63637\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.76687\n",
      "\tTrain loss: 0.02354, Accuracy: 4728/6768 (69.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1057/1692 (62.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.67497\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.86854\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.81350\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.69205\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.66924\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.60784\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.85763\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.70829\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.82181\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.60273\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.61753\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.79009\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.60277\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.43607\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.76217\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.91059\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.55224\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.67226\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.59952\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.65515\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.59851\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.74394\n",
      "\tTrain loss: 0.01971, Accuracy: 5206/6768 (76.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1169/1692 (69.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 938/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.54726\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.57731\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.87650\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.58332\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.55439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.59280\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.88598\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.61372\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.55363\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.55408\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.52993\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.64243\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.73216\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.61730\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.90785\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.73269\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.63133\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.85273\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.50455\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.75212\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.55726\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.63031\n",
      "\tTrain loss: 0.02039, Accuracy: 4978/6768 (73.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1125/1692 (66.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 856/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.66078\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.90145\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.78914\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.65750\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.45311\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.65267\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 1.04929\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.72510\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.86994\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.57695\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.48179\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.61931\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.68230\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.58893\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.73231\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 1.06935\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.71437\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.64286\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.58951\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 1.07926\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.75214\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.76435\n",
      "\tTrain loss: 0.02173, Accuracy: 4804/6768 (70.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1071/1692 (63.00%)\n",
      "\tTest loss: 0.00073, Accuracy: 829/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.77203\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.58488\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.93202\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.55969\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.58125\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.49724\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 1.00438\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.84860\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.57565\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.66122\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.59259\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.70093\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.50367\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.80009\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.53292\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.82106\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.79587\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.93857\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.48315\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.92012\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.63117\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.75096\n",
      "\tTrain loss: 0.02530, Accuracy: 4456/6768 (65.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 1005/1692 (59.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.74409\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.77221\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 1.01539\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.86851\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.69775\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.46395\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 1.09964\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.54507\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.63339\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.78909\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.57474\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.88182\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.84667\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.74555\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.61726\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.56164\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.65133\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.93278\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.58032\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.50379\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.53781\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.82507\n",
      "\tTrain loss: 0.01718, Accuracy: 5345/6768 (78.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1205/1692 (71.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 952/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.88671\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.61077\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.62022\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.68991\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.77239\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.58489\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.93355\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.60930\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.77658\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.79205\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.29314\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.93175\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.53567\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.74894\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.91504\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.65091\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.44359\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.60469\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.63822\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.70585\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.64093\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.43130\n",
      "\tTrain loss: 0.01846, Accuracy: 5159/6768 (76.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1171/1692 (69.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 908/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.63937\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.69208\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.66256\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.71806\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.78377\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.65427\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 1.03601\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.45096\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.96142\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.44869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.46045\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.59266\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.50689\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.59069\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.71238\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.90775\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.51489\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.59727\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.96812\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.69066\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.91883\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.61676\n",
      "\tTrain loss: 0.01941, Accuracy: 5078/6768 (75.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1152/1692 (68.00%)\n",
      "\tTest loss: 0.00072, Accuracy: 870/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.64751\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.80132\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.68403\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.54299\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.62187\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.40648\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.86049\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.44496\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.79582\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.69118\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.32327\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.41998\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.47244\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.73627\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.80407\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.47656\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.72028\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.73785\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.61635\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.66369\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.46055\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.72778\n",
      "\tTrain loss: 0.01751, Accuracy: 5277/6768 (77.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1177/1692 (69.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 885/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.88270\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.72045\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.79340\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.50332\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.71122\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.50662\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.86432\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.65092\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.43360\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.53132\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.41881\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.55479\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.67403\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.85946\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.76454\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.74002\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.49717\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.95407\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.59946\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.80324\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.34198\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.65438\n",
      "\tTrain loss: 0.01572, Accuracy: 5715/6768 (84.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1286/1692 (76.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 972/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.99818\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.77754\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.64977\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.53408\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.59023\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.69778\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.65665\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.28825\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.94887\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.58471\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.41610\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.54270\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.49298\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.53407\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.64311\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.97434\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.49550\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.95613\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.69609\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.67801\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.35490\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.72802\n",
      "\tTrain loss: 0.01591, Accuracy: 5475/6768 (80.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1238/1692 (73.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 914/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.59819\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.78128\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.83260\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.49906\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.63638\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.50738\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.78740\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.37259\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.56207\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.67881\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.39360\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.70219\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.55031\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.80399\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.75638\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.69175\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.34238\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.74695\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.59883\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.72391\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.51417\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.57453\n",
      "\tTrain loss: 0.01650, Accuracy: 5300/6768 (78.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1183/1692 (69.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 904/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.71288\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.61182\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.88754\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.48593\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.62595\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.57530\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.68681\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.60451\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.50333\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.89760\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.31538\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.52955\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.51758\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.58986\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.66955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.51811\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.50821\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.72420\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.65150\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.68170\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.42768\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.44208\n",
      "\tTrain loss: 0.01632, Accuracy: 5548/6768 (81.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1268/1692 (74.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 969/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.77202\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.78333\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.64081\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.60276\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.62891\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.35494\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.72015\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.49347\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.46158\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.60788\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.46754\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.60052\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.53120\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.36616\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.48243\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.64459\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.41539\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.88593\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.24974\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.73100\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.38400\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.72967\n",
      "\tTrain loss: 0.01435, Accuracy: 5712/6768 (84.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1293/1692 (76.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 941/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.71777\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.54261\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.75463\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.56807\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.64687\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.47585\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.84954\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.37156\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.53552\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.58815\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.48552\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.42570\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.43977\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.86800\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.42244\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.53403\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.48455\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.77572\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.66547\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.85546\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.36469\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.52791\n",
      "\tTrain loss: 0.01439, Accuracy: 5590/6768 (82.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1252/1692 (73.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.66251\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.73488\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.60001\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.38435\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.40415\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.43580\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.61086\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.48947\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.65645\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.59032\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.26376\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.43674\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.41361\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.90727\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.66284\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.66440\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.45143\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.74328\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.41248\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.53974\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.41914\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.66079\n",
      "\tTrain loss: 0.01601, Accuracy: 5750/6768 (84.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1328/1692 (78.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 997/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.57370\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.66637\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.49057\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.47278\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.63279\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.32940\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.59050\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.37267\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.75166\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.62134\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.48172\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.43909\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.67560\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.59145\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.53642\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.51688\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.45795\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.59948\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.53182\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.65407\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.44164\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.44579\n",
      "\tTrain loss: 0.01348, Accuracy: 5676/6768 (83.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1284/1692 (75.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 969/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.46525\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.74773\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 1.02761\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.49440\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.50870\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.39154\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.63923\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.62984\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.73194\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.49714\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.34662\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.55371\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.38330\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.66252\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.50604\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.66097\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.60044\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.66383\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.53294\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.58314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.49883\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.39411\n",
      "\tTrain loss: 0.01302, Accuracy: 5791/6768 (85.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1313/1692 (77.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 970/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.72756\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.67079\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.60645\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.44854\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.46386\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.64191\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.79349\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.53050\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.48346\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.49185\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.35699\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.61440\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.39409\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.47993\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.53111\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.53763\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.58552\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.54399\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.47318\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.54951\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.55709\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.52316\n",
      "\tTrain loss: 0.01284, Accuracy: 5800/6768 (85.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1304/1692 (77.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 972/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.78628\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.61589\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.49286\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.64534\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.46537\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.57484\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.94748\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.35047\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.73270\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.74751\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.39375\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.69754\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.40935\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.45210\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.56240\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.34223\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.71928\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.57885\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.45358\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.67113\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.39533\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.66354\n",
      "\tTrain loss: 0.01180, Accuracy: 5993/6768 (88.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1386/1692 (81.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 993/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.56463\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.67798\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.69764\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.32191\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.64397\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.34558\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.93690\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.58373\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.66834\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.44975\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.37959\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.62904\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.36614\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.65822\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.49376\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.53599\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.53262\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.65885\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.33391\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.58223\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.53827\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.66090\n",
      "\tTrain loss: 0.01391, Accuracy: 5663/6768 (83.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1276/1692 (75.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 937/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.74009\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.67567\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.69668\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.33915\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.58048\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.26469\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.68540\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.29619\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.51415\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.52601\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.56210\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.46883\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.48120\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.72018\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.36195\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.70281\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.35983\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.54933\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.45043\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.97626\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.41204\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.53977\n",
      "\tTrain loss: 0.00974, Accuracy: 6267/6768 (92.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1462/1692 (86.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1050/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.37844\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.62122\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.59755\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.51415\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.43622\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.40287\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.76074\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.41187\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.57460\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.76360\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.41883\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.38745\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.42754\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.64739\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.65389\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.67727\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.53531\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.62306\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.45215\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.79876\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.48377\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.65488\n",
      "\tTrain loss: 0.01026, Accuracy: 6143/6768 (90.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1397/1692 (82.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.58475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.56459\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.77443\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.43454\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.52888\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.36110\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.74841\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.43800\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 1.02172\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.57211\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.40662\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.57630\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.43714\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.34675\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.73988\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.57523\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.55643\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.66512\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.59298\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.59086\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.22234\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.46787\n",
      "\tTrain loss: 0.00954, Accuracy: 6223/6768 (91.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1437/1692 (84.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.64161\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.58496\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.60043\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.59894\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.40077\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.43495\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.64655\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.53136\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.31876\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.50948\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.42628\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.47006\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.50277\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.56036\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.59970\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.70978\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.51793\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.58401\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.59655\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.87121\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.62111\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.53009\n",
      "\tTrain loss: 0.01156, Accuracy: 5912/6768 (87.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1341/1692 (79.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 960/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.55100\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.61454\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.77715\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.49313\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.36868\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.46733\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.66312\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.32398\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.57282\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.58239\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.33306\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.43982\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.26957\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.48988\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.50317\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.40494\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.63100\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.62446\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.53412\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.38374\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.71337\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.43232\n",
      "\tTrain loss: 0.00995, Accuracy: 6070/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1397/1692 (82.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 994/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.66336\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.41621\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.80013\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.77255\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.33195\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.27950\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.76200\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.42672\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.41358\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.49392\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.45351\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.37830\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.35357\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.38572\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.44785\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.62993\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.51850\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.44538\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.67008\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.86669\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.42216\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.40023\n",
      "\tTrain loss: 0.01007, Accuracy: 6068/6768 (89.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1407/1692 (83.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1036/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.82558\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.72084\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.75238\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.47823\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.64210\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.75490\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.82687\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.28359\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.56073\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.61279\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.43794\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.48036\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.56046\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.38763\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.45444\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.49871\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.61038\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.85866\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.38013\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.38463\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.46207\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.46602\n",
      "\tTrain loss: 0.00877, Accuracy: 6227/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1460/1692 (86.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1026/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.51229\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.69364\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.84266\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.37619\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.71925\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.38027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.84799\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.40171\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.61391\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.37206\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.26794\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.52663\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.32604\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.65026\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.49883\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.49532\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.50966\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.63123\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.40266\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.41799\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.49180\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.62919\n",
      "\tTrain loss: 0.01158, Accuracy: 5814/6768 (85.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1309/1692 (77.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 949/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.63792\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.59061\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.59263\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.37056\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.17984\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.38658\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.60170\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.46846\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.53715\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.50639\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.37924\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.58756\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.49878\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.31493\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.42803\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.49361\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.52842\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.74236\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.52215\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.50115\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.28650\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.63824\n",
      "\tTrain loss: 0.00869, Accuracy: 6205/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1432/1692 (84.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1039/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.74298\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.51738\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.49370\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.50804\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.38991\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.55740\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.61222\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.28301\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.71591\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.76511\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.37227\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.42384\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.41377\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.66871\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.38420\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.76314\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.30151\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.47718\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.48165\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.55276\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.49600\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.35967\n",
      "\tTrain loss: 0.00895, Accuracy: 6137/6768 (90.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1421/1692 (83.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1029/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.80386\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.59454\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.62466\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.45876\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.52446\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.32640\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.86141\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.43295\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.61838\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.37020\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.29261\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.32791\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.36416\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.34756\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.43388\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.42738\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.61977\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.63040\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.40171\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.57834\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.50035\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.76099\n",
      "\tTrain loss: 0.00934, Accuracy: 6147/6768 (90.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1430/1692 (84.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1022/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.70024\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.50550\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.53226\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.27077\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.74921\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.37767\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 1.05591\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.34919\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.60884\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.37839\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.31393\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.33972\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.32937\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.50466\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.41398\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.64508\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.46086\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.38439\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.47252\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.29517\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.28585\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.40623\n",
      "\tTrain loss: 0.00851, Accuracy: 6238/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1467/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.49868\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.66431\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.57301\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.38937\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.46792\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.39921\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.73693\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.51597\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.58778\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.40430\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.19355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.51173\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.39483\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.43453\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.55527\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.61491\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.44686\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.64334\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.58459\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.55845\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.54568\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.49555\n",
      "\tTrain loss: 0.00872, Accuracy: 6237/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1463/1692 (86.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1060/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.33173\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.43461\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.54026\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.33721\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.34354\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.34659\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.79570\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.25792\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.48148\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.43707\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.38146\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.42409\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.28379\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.47555\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.52222\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.65844\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.48793\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.55125\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.68690\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.54250\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.28889\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.43675\n",
      "\tTrain loss: 0.00810, Accuracy: 6217/6768 (91.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1456/1692 (86.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.72923\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.40036\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.68976\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.31076\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.39128\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.45526\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.65333\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.54354\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.41710\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.32423\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.50265\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.57191\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.36778\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.53513\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.64307\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.53600\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.30250\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.74664\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.69975\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.70563\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.33638\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.57151\n",
      "\tTrain loss: 0.00969, Accuracy: 6089/6768 (89.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1424/1692 (84.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1026/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.39256\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.83294\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.63525\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.48317\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.39330\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.30774\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.78909\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.38441\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.41988\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.43368\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.42446\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.47892\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.66238\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.69234\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.52581\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.57371\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.44231\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.58726\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.42322\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.54247\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.28489\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.63639\n",
      "\tTrain loss: 0.00822, Accuracy: 6305/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1488/1692 (87.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.66736\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.47044\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.57495\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.43946\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.60727\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.46133\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.56395\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.42332\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.45662\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.36997\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.37170\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.68758\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.38460\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.41221\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.15963\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.35649\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.38282\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.46964\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.54662\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.54588\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.49641\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.23377\n",
      "\tTrain loss: 0.00947, Accuracy: 6073/6768 (89.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1426/1692 (84.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1014/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.71055\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.35602\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.71817\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.41423\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.48105\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.18766\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.50091\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.32342\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.70191\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.97704\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.51760\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.32374\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.41636\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.65336\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.28474\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.70604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.41999\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.49625\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.40895\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.71667\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.52125\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.48973\n",
      "\tTrain loss: 0.00774, Accuracy: 6303/6768 (93.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1492/1692 (88.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1071/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.46432\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.77792\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.71418\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.24278\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.28242\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.44766\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.59836\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.37385\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.60032\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.64291\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.31987\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.46487\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.34588\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.39184\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.33003\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.52088\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.32987\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.82120\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.40582\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.59786\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.24531\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.55035\n",
      "\tTrain loss: 0.00906, Accuracy: 6136/6768 (90.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1443/1692 (85.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1036/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.45999\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.48779\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.68270\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.37472\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.73674\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.45015\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 1.31760\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.30403\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.40429\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.66487\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.18823\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.35702\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.31081\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.45885\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.57848\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.75549\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.45808\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.52904\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.36315\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.75332\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.63436\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.46204\n",
      "\tTrain loss: 0.00629, Accuracy: 6388/6768 (94.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1509/1692 (89.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1096/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.64619\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.76788\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.47093\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.59680\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.35592\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.76951\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.51440\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.44812\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.44006\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.29271\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.20189\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.37578\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.57189\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.39124\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.42323\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.51113\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.58441\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.53501\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.45657\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.40481\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.27963\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.35676\n",
      "\tTrain loss: 0.00732, Accuracy: 6343/6768 (93.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1495/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.46702\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.82369\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.61369\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.30088\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.50524\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.22618\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.39965\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.59021\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.30681\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.35201\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.55648\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.36765\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.43146\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.37172\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.52836\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.59458\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.41371\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.51868\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.48753\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.62193\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.46053\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.38667\n",
      "\tTrain loss: 0.00808, Accuracy: 6296/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1477/1692 (87.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1063/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.39532\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.53698\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.53804\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.58914\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.43365\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.23271\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.50993\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.20864\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.60712\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.49696\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.28675\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.48165\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.49985\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.35494\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.51925\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.63108\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.42571\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.64438\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.34584\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.49770\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.62014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.25263\n",
      "\tTrain loss: 0.00867, Accuracy: 6200/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1443/1692 (85.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.67518\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.63536\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.76043\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.57765\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.26728\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.25569\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.63481\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.41124\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.30443\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.38823\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.37421\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.72723\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.65834\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.72492\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.56928\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.52353\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.58738\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.84958\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.37302\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.67957\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.27856\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.33873\n",
      "\tTrain loss: 0.00703, Accuracy: 6381/6768 (94.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.71564\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.52465\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.97587\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.31472\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.35379\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.32775\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.52804\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.26949\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.36018\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.53200\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.26185\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.47892\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.40146\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.51369\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.43309\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.31025\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.47857\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.43353\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.43813\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.46574\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.25010\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.55471\n",
      "\tTrain loss: 0.00747, Accuracy: 6361/6768 (93.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1480/1692 (87.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1060/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.58660\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.52057\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.70124\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.31137\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.53781\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.72326\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.56047\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.57824\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.68698\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.27820\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.43487\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.38304\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.50674\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.74370\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.40236\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.85059\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.40389\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.50057\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.43957\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.75603\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.23257\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.46397\n",
      "\tTrain loss: 0.00667, Accuracy: 6324/6768 (93.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1493/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1071/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.48854\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.64842\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.42934\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.40092\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.47061\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.47753\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.58463\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.42567\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.36424\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.61210\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.27478\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.46244\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.32938\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.49203\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.69877\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.34659\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.46257\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.75815\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.50585\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.39039\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.33005\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.37374\n",
      "\tTrain loss: 0.00677, Accuracy: 6337/6768 (93.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1062/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.72690\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.38420\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.47019\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.37995\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.33914\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.31556\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 1.01301\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.22038\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.48650\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.48439\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.26824\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.31622\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.28984\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.53606\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.43792\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.48734\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.48632\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.61805\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.34204\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.35258\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.18698\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.44141\n",
      "\tTrain loss: 0.00678, Accuracy: 6338/6768 (93.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1488/1692 (87.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.73906\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.54478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.60527\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.37460\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.19153\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.43208\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.51617\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.45983\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.43626\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.30464\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.29892\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.43778\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.43090\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.30579\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.17748\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.66340\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.33469\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.51742\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.45369\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.38645\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.50043\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.63293\n",
      "\tTrain loss: 0.00875, Accuracy: 6132/6768 (90.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1427/1692 (84.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1013/1772 (57.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.8918439716312057\n",
      "Best test accuracy:\n",
      "0.618510158013544\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_gru, cnn_gru_optimizer, data_loaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc453a",
   "metadata": {},
   "source": [
    "# GAN Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd14438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_WGAN = np.load('./GAN/WGAN_generate_X.npy')\n",
    "fake_label = np.load('./GAN/generate_label.npy')\n",
    "\n",
    "fake_WGAN = np.swapaxes(fake_WGAN, 1,2)\n",
    "fake_WGAN = np.swapaxes(fake_WGAN,2,3)\n",
    "\n",
    "#Add 8460/4 fake data\n",
    "x_train_plus_WGAN = np.vstack((x_train, fake_WGAN[0:fake_WGAN.shape[0]//4]))\n",
    "y_train_plus_WGAN = np.vstack((y_train, fake_label[0:fake_WGAN.shape[0]//4]))\n",
    "p = np.random.permutation(x_train.shape[0])\n",
    "x_train_plus_WGAN, y_train_plus_WGAN = x_train_plus_WGAN[p], y_train_plus_WGAN[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e77b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_wgan = dataloader_setup(x_train_plus_WGAN, y_train_plus_WGAN, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3d10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ACGAN = np.load('./GAN/ACGAN_generate_X.npy')\n",
    "fake_label = np.load('./GAN/generate_label.npy')\n",
    "\n",
    "fake_ACGAN = np.swapaxes(fake_ACGAN, 1,2)\n",
    "fake_ACGAN = np.swapaxes(fake_ACGAN,2,3)\n",
    "\n",
    "\n",
    "#Add 8460/4 fake data\n",
    "x_train_plus_ACGAN = np.vstack((x_train, fake_WGAN[0:fake_ACGAN.shape[0]//4]))\n",
    "y_train_plus_ACGAN = np.vstack((y_train, fake_label[0:fake_ACGAN.shape[0]//4]))\n",
    "p = np.random.permutation(x_train.shape[0])\n",
    "x_train_plus_ACGAN, y_train_plus_ACGAN = x_train_plus_ACGAN[p], y_train_plus_ACGAN[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db00a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_acgan = dataloader_setup(x_train_plus_ACGAN, y_train_plus_ACGAN, x_valid, y_valid, x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28593b3e",
   "metadata": {},
   "source": [
    "# ACGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11427be",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51d52747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.67252\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.63236\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.41847\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.36450\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.55478\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.44000\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.61453\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.48255\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.35901\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.40006\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.45883\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.57594\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.39963\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.34614\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.42274\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.41666\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.30865\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.68926\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.33011\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.46536\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.52405\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.36381\n",
      "\tTrain loss: 0.03916, Accuracy: 2938/6768 (43.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 717/1692 (42.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 685/1772 (38.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.20810\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.42363\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.27636\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.35600\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.38005\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.28749\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.41301\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.30204\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.41767\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.47485\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.31675\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.47966\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.14214\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.29963\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.29920\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.17735\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.17828\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.32381\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.31435\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.21535\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.44326\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.43142\n",
      "\tTrain loss: 0.03602, Accuracy: 3421/6768 (50.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 849/1692 (50.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 792/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.13516\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.25853\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.06754\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.34958\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.44878\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.15919\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.35022\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.22545\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.42640\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.32558\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.28518\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.29827\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.24463\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.31954\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.36450\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.18889\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.18952\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.37647\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.35390\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.15921\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.22158\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.18962\n",
      "\tTrain loss: 0.03328, Accuracy: 3974/6768 (58.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 959/1692 (56.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 901/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.33970\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.10858\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.13070\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.35162\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.23565\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.13493\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.19853\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.02777\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.21611\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.38031\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.19194\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.11213\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.11955\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.09089\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.28432\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.01796\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.00857\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.40666\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.37161\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.19179\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.13539\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.28200\n",
      "\tTrain loss: 0.03095, Accuracy: 4104/6768 (60.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 969/1692 (57.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 945/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.04392\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.10307\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 0.96310\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.21025\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.39318\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.13390\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 0.96778\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.18534\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.30657\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.20317\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.18359\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.26948\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 0.99825\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.18716\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.06380\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.17254\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.03003\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.29320\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.26007\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.03027\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.16172\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.10009\n",
      "\tTrain loss: 0.02883, Accuracy: 4457/6768 (65.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1051/1692 (62.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 982/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 0.94335\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.11072\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 0.90711\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.07290\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.23731\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.05949\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 0.98691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.19579\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.17866\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.33782\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 0.99225\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 0.98126\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 0.95149\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.05098\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 0.94507\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.17356\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 0.97712\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.21734\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.11138\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.13649\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.11503\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.00362\n",
      "\tTrain loss: 0.02807, Accuracy: 4536/6768 (67.00%)\n",
      "\tValidation loss: 0.00055, Accuracy: 1080/1692 (63.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 961/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.04454\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.30456\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 0.87089\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.06002\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.21363\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 0.86175\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 0.93247\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.10689\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.04951\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.02032\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 0.99243\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 0.95073\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.06486\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.18158\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.13710\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.38840\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 0.98840\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.19718\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.13049\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.21252\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.14985\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.01914\n",
      "\tTrain loss: 0.02591, Accuracy: 4792/6768 (70.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1143/1692 (67.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1043/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.02876\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.17688\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.03005\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.02536\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.03667\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 0.91082\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.08044\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.14670\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.20261\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 0.91518\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 0.87392\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 0.99716\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 0.87828\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.06191\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 0.90788\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 0.97472\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.28255\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.13440\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.20924\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.01744\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.33698\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.16767\n",
      "\tTrain loss: 0.02461, Accuracy: 4859/6768 (71.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1136/1692 (67.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1031/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 0.88202\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 0.93477\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.00749\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.02814\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.23273\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 0.78523\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 0.99745\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.08818\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.08090\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.24008\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.04896\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 0.98792\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 0.98883\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 0.90547\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 0.86554\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 0.94433\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.09646\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.07878\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.15804\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 0.86534\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.10907\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 0.97424\n",
      "\tTrain loss: 0.02296, Accuracy: 5079/6768 (75.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1167/1692 (68.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1063/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 0.99560\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 0.83551\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 0.91672\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.18657\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.12527\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.19380\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 0.88625\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.11937\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 0.89436\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 0.82668\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 0.99342\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.04413\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 0.89611\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.04529\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.00910\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 0.96483\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 0.84595\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.30161\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 0.97998\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 0.85448\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 0.92085\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 0.97706\n",
      "\tTrain loss: 0.02146, Accuracy: 5162/6768 (76.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1195/1692 (70.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1044/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 0.98854\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.02364\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 0.80785\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 0.88289\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.01043\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 0.95448\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 0.90655\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.04741\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 0.97284\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 0.89015\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 0.88311\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 0.95923\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.00210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 0.96951\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.01929\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 0.95956\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.01296\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 0.96476\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 0.92568\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 0.80413\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.09492\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.10338\n",
      "\tTrain loss: 0.02041, Accuracy: 5304/6768 (78.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1229/1692 (72.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1154/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.92238\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.03421\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 0.78679\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 0.89613\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.15524\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 0.93273\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 0.78044\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.13447\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 0.73483\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 0.98469\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 0.80734\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.01184\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.82700\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 0.87803\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 0.90338\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 0.90347\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.08937\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 0.95907\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.75016\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.07867\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.13230\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 0.72604\n",
      "\tTrain loss: 0.01953, Accuracy: 5335/6768 (78.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1242/1692 (73.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1117/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.90102\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 0.91340\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 0.88924\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 0.93365\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.12061\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 0.84828\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 0.86421\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.00661\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.01046\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 0.85608\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.61959\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 0.78478\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.75094\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.03956\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 0.88488\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 0.83632\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 0.85036\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 0.89632\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 0.71860\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 0.87124\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.93524\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 0.78736\n",
      "\tTrain loss: 0.01779, Accuracy: 5528/6768 (81.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1291/1692 (76.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1112/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.74114\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 0.99148\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 0.83497\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 0.91819\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 0.91978\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.80600\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 0.80140\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.02135\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 0.94032\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 0.74515\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.73057\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 0.81033\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.84204\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 0.80507\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 0.80910\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 0.96196\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.86533\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 0.92305\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.95185\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 0.89707\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.38492\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 0.89259\n",
      "\tTrain loss: 0.01714, Accuracy: 5555/6768 (82.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1258/1692 (74.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.94517\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 0.66478\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 0.76929\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 0.94763\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 0.82288\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.89985\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 0.83822\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.01360\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.16525\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.87923\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.87703\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 0.68431\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.78674\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.11136\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 0.81999\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.87629\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.64288\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 0.88253\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.71911\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.83616\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.11554\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 0.81518\n",
      "\tTrain loss: 0.01645, Accuracy: 5633/6768 (83.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1311/1692 (77.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.81556\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 0.97823\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 0.46091\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 0.81267\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.09760\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.81892\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 0.87179\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.93487\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.05373\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.89388\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.73556\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 0.83493\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.71853\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.75740\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.71927\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.80263\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.65819\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.00470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.88164\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.62776\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.07496\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 0.71344\n",
      "\tTrain loss: 0.01530, Accuracy: 5686/6768 (84.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1305/1692 (77.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1147/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.86102\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.01583\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.83190\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 0.94665\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.71410\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.89153\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 0.89715\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.06498\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 0.82325\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.96848\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.58783\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.78599\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.88843\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.69902\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 0.73640\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.76992\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.75301\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 0.76867\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.04197\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.60013\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.90630\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.70257\n",
      "\tTrain loss: 0.01492, Accuracy: 5802/6768 (85.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1161/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.85266\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.69241\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 0.60328\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 0.85225\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.85918\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.74240\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 0.83974\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.96473\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.80686\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.88839\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.77815\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.81770\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.93133\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.81596\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.86833\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.97608\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.64465\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 0.91062\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.82463\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.63141\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.25858\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.73507\n",
      "\tTrain loss: 0.01354, Accuracy: 5886/6768 (86.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1350/1692 (79.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1168/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.74725\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.13619\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.57692\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.64878\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.71949\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.65023\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.89461\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.92213\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.88998\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.70017\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.70380\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 0.55751\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.56261\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.74845\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.83081\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.86923\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.61646\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.66008\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.86939\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.68911\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.89675\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.66648\n",
      "\tTrain loss: 0.01258, Accuracy: 6014/6768 (88.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1398/1692 (82.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1154/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.66729\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.67721\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.65925\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.80168\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.79001\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.61260\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 1.03423\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.83396\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.63926\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.93157\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.60864\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.60494\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.58167\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.72929\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.72810\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.79630\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.67958\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 0.93664\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.85047\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.70224\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.98147\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.60572\n",
      "\tTrain loss: 0.01187, Accuracy: 6074/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1407/1692 (83.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1199/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.46711\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.80093\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.56233\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.64062\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.85500\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.66152\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.81935\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.73061\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.83866\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.56519\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.57466\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.65589\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.74174\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.58128\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.70017\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.80298\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.59345\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.97466\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.55415\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.81548\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.64243\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.70127\n",
      "\tTrain loss: 0.01090, Accuracy: 6155/6768 (90.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00027, Accuracy: 1457/1692 (86.00%)\n",
      "\tTest loss: 0.00050, Accuracy: 1206/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.57689\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.92753\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.75921\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.65511\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.93168\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.73935\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.74002\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.99493\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.74570\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.62005\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.70924\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.63223\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.70448\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.75846\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.74806\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.70646\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.71836\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.61048\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.73376\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.50316\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.71148\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.96154\n",
      "\tTrain loss: 0.01011, Accuracy: 6200/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1461/1692 (86.00%)\n",
      "\tTest loss: 0.00050, Accuracy: 1167/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.47410\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.79940\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.60372\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 1.19365\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.74136\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.65574\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.58836\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.74378\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.75972\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.55829\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.46103\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.48088\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.67092\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.75148\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.54313\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.58686\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.69863\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.54232\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.75586\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.73648\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.79347\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.59015\n",
      "\tTrain loss: 0.00985, Accuracy: 6225/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1450/1692 (85.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1139/1772 (64.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.58281\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.60133\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.71737\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.75471\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.73678\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.49074\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.77705\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.97206\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.56724\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.69513\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.54429\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.59013\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.69965\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.65866\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.77972\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.67057\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.78834\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.80544\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.78644\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.68471\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.85270\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.81729\n",
      "\tTrain loss: 0.00924, Accuracy: 6265/6768 (92.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1477/1692 (87.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1152/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.70005\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.51746\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.53759\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.59134\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.77472\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.79900\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.62332\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.75250\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.66317\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.67784\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.59785\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.51133\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.75812\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.61174\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.70226\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.47786\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.70716\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.55864\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.70636\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.57638\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.86812\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.55670\n",
      "\tTrain loss: 0.00849, Accuracy: 6360/6768 (93.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1499/1692 (88.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1167/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.47913\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.71157\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.69592\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.82597\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.61204\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.65664\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.65613\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.77292\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.51820\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.55017\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.71196\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.47956\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.53789\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.40836\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.70611\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.57619\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.66563\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.67595\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.54444\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.69180\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.95228\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.50257\n",
      "\tTrain loss: 0.00841, Accuracy: 6321/6768 (93.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1200/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.63279\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.60283\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.75720\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.69625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.66996\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.53269\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.48914\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 1.02919\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.46856\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.79253\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.60451\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.47738\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.61358\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.51969\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.62604\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.51831\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.41185\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.68107\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.62474\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.56954\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.59854\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.91265\n",
      "\tTrain loss: 0.00782, Accuracy: 6375/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1505/1692 (88.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1161/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.26101\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.38050\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.61543\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.90173\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.65510\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.58875\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.72329\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.61947\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.48446\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.55901\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.48935\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.60374\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.67329\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.54827\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.74384\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.68319\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.63396\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.48413\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.56972\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.40214\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.65258\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.64050\n",
      "\tTrain loss: 0.00708, Accuracy: 6437/6768 (95.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1524/1692 (90.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1183/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.53112\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 1.00961\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.39983\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.73808\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.73367\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.56562\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.50496\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.64583\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.69344\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.44378\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.56944\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.62597\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.90236\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.47384\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.46647\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.62689\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.64058\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.74365\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.53978\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.63711\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.52278\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.67438\n",
      "\tTrain loss: 0.00663, Accuracy: 6463/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1535/1692 (90.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1175/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.51387\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.67526\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.41130\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.70696\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.71222\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.60246\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.48462\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.78244\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.57933\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.45404\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.48179\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.50818\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.34093\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.47015\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.57829\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.47466\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.65595\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.53090\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.76381\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.42581\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.86269\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.75152\n",
      "\tTrain loss: 0.00668, Accuracy: 6449/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1529/1692 (90.00%)\n",
      "\tTest loss: 0.00052, Accuracy: 1163/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.47503\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.57814\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.40656\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.48130\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.74289\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.39281\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.60262\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.68068\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.52899\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.35402\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.58848\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.72593\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.44907\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.63459\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.49832\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.41015\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.53583\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.41719\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.54889\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.54514\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.71465\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.68629\n",
      "\tTrain loss: 0.00612, Accuracy: 6495/6768 (95.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1538/1692 (90.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1156/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.55708\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.53661\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.46288\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.50511\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.60533\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.59419\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.70819\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.60524\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.45591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.45447\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.61851\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.75184\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.46654\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.32597\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.64145\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.63923\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.59658\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.46346\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.79484\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.33075\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.58253\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.72950\n",
      "\tTrain loss: 0.00566, Accuracy: 6514/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1549/1692 (91.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1154/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.52271\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.63720\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.64035\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.67184\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.62633\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.66623\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.43495\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.66484\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.33179\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.57894\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.49296\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.51671\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.54563\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.84624\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.88785\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.57214\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.68764\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.62581\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.64454\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.54712\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.42125\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.42017\n",
      "\tTrain loss: 0.00556, Accuracy: 6531/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1546/1692 (91.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1192/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.43582\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.45587\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.51328\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.76576\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.50867\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.57644\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.55001\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.74403\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.33392\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.48001\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.46900\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.36743\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.48512\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.37914\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.46374\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.52064\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.78894\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.31071\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.68961\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.35864\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.84564\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.89290\n",
      "\tTrain loss: 0.00542, Accuracy: 6518/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1560/1692 (92.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1162/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.55608\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.37561\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.42812\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.64741\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.44788\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.58803\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.59992\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.54448\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.57157\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.32555\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.64701\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.70366\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.41523\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.38392\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.71995\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.46598\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.56980\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.43133\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.43327\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.40273\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.66151\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.49673\n",
      "\tTrain loss: 0.00482, Accuracy: 6597/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1573/1692 (92.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1166/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.40597\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.65745\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.48541\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.64911\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.72072\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.48861\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.54904\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.49547\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.35307\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.69748\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.46446\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.46752\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.67606\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.49682\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.43177\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.31734\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.77411\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.27169\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.58226\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.55995\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.68227\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.51336\n",
      "\tTrain loss: 0.00447, Accuracy: 6625/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1584/1692 (93.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1206/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.39636\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.72688\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.43005\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.85699\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.55454\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.46169\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.59992\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.72184\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.37047\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.28743\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.43937\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.49609\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.75629\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.57130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.65402\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.39872\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.49961\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.47481\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.56149\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.64131\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.50371\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.78949\n",
      "\tTrain loss: 0.00407, Accuracy: 6646/6768 (98.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1592/1692 (94.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1196/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.46648\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.45358\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.57844\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.71242\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.67886\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.32146\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.33813\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.72461\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.39965\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.45595\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.44033\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.58353\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.57258\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.44966\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.57180\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.41380\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.57961\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.38787\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.50987\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.35312\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.89748\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.59368\n",
      "\tTrain loss: 0.00423, Accuracy: 6612/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1595/1692 (94.00%)\n",
      "\tTest loss: 0.00051, Accuracy: 1209/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.35239\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.49454\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.25097\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.50449\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.98113\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.55227\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.46069\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.53071\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.46114\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.31747\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.34425\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.58853\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.75612\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.60545\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.49856\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.40795\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.67335\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.44019\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.48573\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.54236\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.51997\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.53492\n",
      "\tTrain loss: 0.00399, Accuracy: 6638/6768 (98.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1594/1692 (94.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1188/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.29529\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.59780\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.51303\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.76554\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.74011\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.53764\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.44107\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.59743\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.38043\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.63915\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.36748\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.34727\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.68335\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.87035\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.40487\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.30086\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.48832\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.47341\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.44941\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.44561\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.68201\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.50196\n",
      "\tTrain loss: 0.00415, Accuracy: 6592/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1588/1692 (93.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1184/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.46943\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.54061\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.45972\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.57531\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.47660\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.61019\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.35030\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.77886\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.31015\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.38392\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.61324\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.52859\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.67624\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.34252\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.62580\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.59905\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.38258\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.37890\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.46032\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.36067\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.81079\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.55308\n",
      "\tTrain loss: 0.00360, Accuracy: 6643/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1600/1692 (94.00%)\n",
      "\tTest loss: 0.00053, Accuracy: 1223/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.69512\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.64061\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.45076\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.46602\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.61832\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.47115\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.44961\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.50759\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.57722\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.35674\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.39445\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.70484\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.42520\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.45560\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.42076\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.48317\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.44237\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.64678\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.63346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.49240\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.55276\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.39459\n",
      "\tTrain loss: 0.00344, Accuracy: 6682/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1609/1692 (95.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1209/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.32967\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.49416\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.31253\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.40739\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.69183\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.45798\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.55455\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.54045\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.43394\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.55266\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.43017\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.63369\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.53423\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.57358\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.51978\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.23031\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.45135\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.48176\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.30611\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.46031\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.30580\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.49878\n",
      "\tTrain loss: 0.00307, Accuracy: 6675/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1613/1692 (95.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1205/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.29508\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.43655\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.39468\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.66176\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.34992\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.39255\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.48799\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.35382\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.50340\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.40931\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.43924\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.34930\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.55643\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.56791\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.30405\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.58622\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.53291\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.27450\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.66773\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.62217\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.52216\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.54798\n",
      "\tTrain loss: 0.00301, Accuracy: 6671/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1621/1692 (95.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1186/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.35581\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.44326\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.26508\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.31218\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.70196\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.60371\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.26097\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.55888\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.42168\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.69563\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.44931\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.50216\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.74764\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.50287\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.40321\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.37542\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.59040\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.47497\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.62352\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.40111\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.56505\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.46021\n",
      "\tTrain loss: 0.00290, Accuracy: 6694/6768 (98.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1625/1692 (96.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1199/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.37035\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.50382\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.39860\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.49397\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.28744\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.29512\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.34829\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.55175\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.28998\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.44036\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.29740\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.52716\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.40576\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.42728\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.37866\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.45983\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.53588\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.47761\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.37651\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.53217\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.71562\n",
      "\tTrain loss: 0.00294, Accuracy: 6692/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1616/1692 (95.00%)\n",
      "\tTest loss: 0.00055, Accuracy: 1170/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.41015\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.36525\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.43932\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.59511\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.61319\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.45537\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.30799\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.40236\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.41670\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.36309\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.56037\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.32125\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.50586\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.41388\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.39608\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.37987\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.76688\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.51694\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.35409\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.34733\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.66380\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.52949\n",
      "\tTrain loss: 0.00255, Accuracy: 6706/6768 (99.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1629/1692 (96.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00056, Accuracy: 1221/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.23216\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.22729\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.36004\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.48304\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.42773\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.37026\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.67010\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.55223\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.61737\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.47009\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 1.03567\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.72387\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.58446\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.50913\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.28747\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.27864\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.63844\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.60822\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.38474\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.26771\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.76579\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.59667\n",
      "\tTrain loss: 0.00244, Accuracy: 6722/6768 (99.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1631/1692 (96.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1196/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.58885\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.49879\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.32289\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.59789\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.45361\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.39136\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.42745\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.58342\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.47640\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.46153\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.53947\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.56427\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.43627\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.42784\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.29931\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.21139\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.33804\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.37105\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.37088\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.42999\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.45194\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.72812\n",
      "\tTrain loss: 0.00238, Accuracy: 6721/6768 (99.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1638/1692 (96.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1182/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.30219\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.49028\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.52039\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.50316\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.42945\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.45218\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.33575\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.45230\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.44198\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.47439\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.35391\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.38223\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.47946\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.30304\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.34069\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.51792\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.40985\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.36685\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.37625\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.19894\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.50007\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.42509\n",
      "\tTrain loss: 0.00221, Accuracy: 6718/6768 (99.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1634/1692 (96.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1213/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.32259\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.51706\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.45675\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.58649\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.44002\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.30777\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.46595\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.55550\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.38160\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.39802\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.42320\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.40842\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.55045\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.42745\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.33465\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.30670\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.52383\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.51305\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.32622\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.68124\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.36786\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.42210\n",
      "\tTrain loss: 0.00202, Accuracy: 6721/6768 (99.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1636/1692 (96.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1191/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.41547\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.60389\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.26267\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.58541\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.33280\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.39272\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.43772\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.60696\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.33621\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.25881\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.35900\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.36262\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.31233\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.50380\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.43007\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.25817\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.59798\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.50901\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.37697\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.35128\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.65494\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.16419\n",
      "\tTrain loss: 0.00196, Accuracy: 6735/6768 (99.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1203/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.37557\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.33514\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.27869\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.45082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.45157\n",
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.42141\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.46560\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.56967\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.69147\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.40441\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.60360\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.24638\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.54320\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.58463\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.32010\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.31064\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.56808\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.56951\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.63337\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.30058\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.45723\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.59123\n",
      "\tTrain loss: 0.00198, Accuracy: 6732/6768 (99.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1644/1692 (97.00%)\n",
      "\tTest loss: 0.00054, Accuracy: 1223/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.35380\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.39379\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.31860\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.48172\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.46535\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.61898\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.36505\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.34876\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.32114\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.18139\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.41692\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.44403\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.49528\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.49901\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.31453\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.35293\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.41405\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.35491\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.38660\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.38469\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.41635\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.45421\n",
      "\tTrain loss: 0.00201, Accuracy: 6733/6768 (99.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1637/1692 (96.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1187/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.38292\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.51759\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.62405\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.63569\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.38527\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.34377\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.43341\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.30720\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.17129\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.51595\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.52484\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.34677\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.47086\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.40509\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.36397\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.43251\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.60051\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.24332\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.50439\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.37983\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.60755\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.41247\n",
      "\tTrain loss: 0.00171, Accuracy: 6745/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1651/1692 (97.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1194/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.41623\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.62295\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.41735\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.40614\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.29887\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.45630\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.56262\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.70118\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.53717\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.40558\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.54547\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.33174\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.31767\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.38143\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.49037\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.38001\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.41078\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.34728\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.39513\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.39436\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.47596\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.29617\n",
      "\tTrain loss: 0.00210, Accuracy: 6708/6768 (99.00%)\n",
      "\tValidation loss: 0.00008, Accuracy: 1623/1692 (95.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1173/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.20954\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.35865\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.46740\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.64788\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.23559\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.39289\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.36243\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.43160\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.33131\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.51376\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.49490\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.26242\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.20043\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.48082\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.44236\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.25618\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.64343\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.50560\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.58678\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.52309\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.59367\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.91391\n",
      "\tTrain loss: 0.00165, Accuracy: 6738/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1650/1692 (97.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1216/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.21833\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.44768\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.39978\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.40822\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.42837\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.59824\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.55964\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.35442\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.43419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.30697\n",
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.37639\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.35330\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.64957\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.31171\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.59590\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.27812\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.48025\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.28810\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.32559\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.53639\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.36305\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.35677\n",
      "\tTrain loss: 0.00181, Accuracy: 6743/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1645/1692 (97.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1206/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.56162\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.44916\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.37296\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.30387\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.57495\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.40886\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.42944\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.52005\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.44841\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.52055\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.44832\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.47138\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.46888\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.35954\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.31328\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.28337\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.41908\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.41816\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.61492\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.38845\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.32209\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.65001\n",
      "\tTrain loss: 0.00184, Accuracy: 6725/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1650/1692 (97.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1225/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.35771\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.38985\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.32021\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.46108\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.64294\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.23712\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.36397\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.45651\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.35191\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.20791\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.38620\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.29427\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.65830\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.55597\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.46872\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.46105\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.57310\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.39301\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.41544\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.36219\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.37413\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.72130\n",
      "\tTrain loss: 0.00166, Accuracy: 6733/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1646/1692 (97.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1184/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.23129\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.53634\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.72717\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.24325\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.34708\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.49347\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.55137\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.60292\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.34745\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.21423\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.55200\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.45172\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.57169\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.38860\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.35130\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.52994\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.52833\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.35221\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.54219\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.22123\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.29515\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.38764\n",
      "\tTrain loss: 0.00167, Accuracy: 6728/6768 (99.00%)\n",
      "\tValidation loss: 0.00007, Accuracy: 1638/1692 (96.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1204/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.29344\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.45057\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.23716\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.68885\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.46187\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.49503\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.66486\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.38819\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.37978\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.62470\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.44011\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.34188\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.34391\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.26632\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.63464\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.28609\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.60879\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.26882\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.37314\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.19704\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.36331\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.49880\n",
      "\tTrain loss: 0.00136, Accuracy: 6753/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1665/1692 (98.00%)\n",
      "\tTest loss: 0.00056, Accuracy: 1215/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.29582\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.20059\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.44385\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.39726\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.42946\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.22320\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.51370\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.44812\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.29073\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.70715\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.58313\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.60917\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.33434\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.49616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.38146\n",
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.28081\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.27235\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.34203\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.27841\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.30994\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.68797\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.49870\n",
      "\tTrain loss: 0.00155, Accuracy: 6744/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1658/1692 (97.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1216/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.41407\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.53752\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.27176\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.81611\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.41234\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.46804\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.40034\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.58117\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.40095\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.41284\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.51442\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.51539\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.76340\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.44703\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.45972\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.29325\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.39006\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.63747\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.29611\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.25479\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.52209\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.50593\n",
      "\tTrain loss: 0.00142, Accuracy: 6747/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1658/1692 (97.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1173/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.21465\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.48609\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.48626\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.39077\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.28423\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.31533\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.53093\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.33342\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.44376\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.42786\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.32797\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.57620\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.43622\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.39738\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.21926\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.27107\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.34009\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.52277\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.34086\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.58383\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.49676\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.34982\n",
      "\tTrain loss: 0.00137, Accuracy: 6753/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1662/1692 (98.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1189/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.26288\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.44066\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.54015\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.54124\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.52050\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.56752\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.45721\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.33899\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.24902\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.47939\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.26727\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.25069\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.65535\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.29890\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.31526\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.27677\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.29416\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.33550\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.64108\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.61382\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.47613\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.36403\n",
      "\tTrain loss: 0.00138, Accuracy: 6750/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1659/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1202/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.28178\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.41918\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.34802\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.28768\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.37939\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.37888\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.67161\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.51829\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.24752\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.44106\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.35048\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.16459\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.53296\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.43640\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.42805\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.31625\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.19280\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.59462\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.46854\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.21178\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.26120\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.28190\n",
      "\tTrain loss: 0.00123, Accuracy: 6758/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1665/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1209/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.49242\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.57230\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.25048\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.59557\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.48526\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.37174\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.71280\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.62419\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.32972\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.30675\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.26133\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.32140\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.48414\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.22465\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.37567\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.37539\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.29126\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.26597\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.35673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.52032\n",
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.57053\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.54855\n",
      "\tTrain loss: 0.00124, Accuracy: 6750/6768 (99.00%)\n",
      "\tValidation loss: 0.00006, Accuracy: 1661/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1187/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.33019\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.38828\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.25743\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.20944\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.31759\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.53579\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.38069\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.42281\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.48958\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.37553\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.42595\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.28889\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.28866\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.42358\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.43573\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.23640\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.31426\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.33203\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.39113\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.24344\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.43038\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.44486\n",
      "\tTrain loss: 0.00113, Accuracy: 6750/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1664/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1180/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.52959\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.58867\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.13393\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.50477\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.48408\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.23538\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.57486\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.33337\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.23165\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.68545\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.55893\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.63895\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.67831\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.39817\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.35298\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.39928\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.45114\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.34336\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.24893\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.25259\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.29278\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.42708\n",
      "\tTrain loss: 0.00111, Accuracy: 6755/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1660/1692 (98.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1206/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.29309\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.36645\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.40590\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.39472\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.41995\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.28828\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.27862\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.54808\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.37425\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.44837\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.29907\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.29372\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.48572\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.35163\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.42478\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.36845\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.34886\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.73982\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.59690\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.12450\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.28795\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.47301\n",
      "\tTrain loss: 0.00106, Accuracy: 6756/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1667/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1226/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.35028\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.25956\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.40545\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.57116\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.36350\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.43070\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.43430\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.46152\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.52750\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.14154\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.35074\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.48151\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.40845\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.43603\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.46039\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.24531\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.59886\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.43314\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.44074\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.35887\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.43471\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.24293\n",
      "\tTrain loss: 0.00110, Accuracy: 6754/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1668/1692 (98.00%)\n",
      "\tTest loss: 0.00057, Accuracy: 1234/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.35623\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.26655\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.17036\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.33439\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.29156\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.42780\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.43404\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.41056\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.30104\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.50500\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.29953\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.31828\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.21350\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.31786\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.44408\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.24491\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.27950\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.34481\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.33350\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.46325\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.42807\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.48283\n",
      "\tTrain loss: 0.00109, Accuracy: 6757/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1668/1692 (98.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00061, Accuracy: 1160/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.33791\n",
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.33916\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.47814\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.41228\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.47363\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.24358\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.64390\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.24201\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.19074\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.48156\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.44937\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.35121\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.43613\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.41174\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.20162\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.32551\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.38506\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.43147\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.36009\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.20453\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.32761\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.55924\n",
      "\tTrain loss: 0.00114, Accuracy: 6753/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1660/1692 (98.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1189/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.27409\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.31618\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.16495\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.35505\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.65534\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.42982\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.38444\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.36705\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.40809\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.45860\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.34252\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.49820\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.39128\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.28983\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.29405\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.47099\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.45280\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.22887\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.52921\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.21461\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.30245\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.47801\n",
      "\tTrain loss: 0.00106, Accuracy: 6756/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1665/1692 (98.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1193/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.44605\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.26211\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.30756\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.37372\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.23028\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.24964\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.30845\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.46871\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.22024\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.30664\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.29924\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.23540\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.42903\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.44763\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.44494\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.28414\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.46687\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.41758\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.28092\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.29520\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.35873\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.42163\n",
      "\tTrain loss: 0.00100, Accuracy: 6756/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1666/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1182/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.66244\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.68057\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.18891\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.29802\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.37346\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.45987\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.45183\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.30391\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.48981\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.31953\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.22419\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.47161\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.42701\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.60282\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.48909\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.34342\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.22317\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.46157\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.39431\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.42631\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.39312\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.37916\n",
      "\tTrain loss: 0.00104, Accuracy: 6755/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1664/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1185/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.22160\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.20635\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.25968\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.65257\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.41671\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.29396\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.23871\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.36466\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.31893\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.45812\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.26757\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.32622\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.28572\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.30950\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.43553\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.45562\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.39881\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.63969\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.32069\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.38877\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.26234\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.20542\n",
      "\tTrain loss: 0.00096, Accuracy: 6756/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1669/1692 (98.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1189/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.39028\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.23015\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.37209\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.13509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.36106\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.52662\n",
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.20271\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.33864\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.24469\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.21577\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.41554\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.26859\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.44656\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.26167\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.22940\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.22834\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.46695\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.41683\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.35956\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.19499\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.38214\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.23584\n",
      "\tTrain loss: 0.00090, Accuracy: 6757/6768 (99.00%)\n",
      "\tValidation loss: 0.00005, Accuracy: 1665/1692 (98.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1177/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.60650\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.26781\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.28146\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.36654\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.57530\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.25228\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.41694\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.44539\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.32012\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.35220\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.38668\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.20362\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.23209\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.28509\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.34698\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.43727\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.42138\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.34265\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.51314\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.23101\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.27755\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.24925\n",
      "\tTrain loss: 0.00089, Accuracy: 6753/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1666/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1240/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.20003\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.27981\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.33686\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.22669\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.26837\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.39131\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.18427\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.39897\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.26907\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.21545\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.27709\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.39196\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.38303\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.28259\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.18397\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.31387\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.34584\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.18651\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.19987\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.08556\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.11885\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.40416\n",
      "\tTrain loss: 0.00082, Accuracy: 6762/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1673/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1207/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.16396\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.43541\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.22874\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.42375\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.47881\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.35002\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.35596\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.42503\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.19732\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.19552\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.23933\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.38365\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.45382\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.43926\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.17132\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.39795\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.47196\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.23876\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.41838\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.26981\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.22902\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.32483\n",
      "\tTrain loss: 0.00084, Accuracy: 6762/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1670/1692 (98.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1206/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.25492\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.24811\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.55390\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.27159\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.62119\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.52957\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.26643\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.13870\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.09086\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.30756\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.32511\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.32336\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.55727\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.35717\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.16520\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.33550\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.26053\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.49163\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.32861\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.64596\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.46057\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.31727\n",
      "\tTrain loss: 0.00077, Accuracy: 6759/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1224/1772 (69.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.41284\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.44081\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.18418\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.40671\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.25293\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.20269\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.38474\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.64483\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.30526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.24757\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.45979\n",
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.41049\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.49093\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.38188\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.36680\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.40977\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.36253\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.38548\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.54154\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.18666\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.54761\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.38758\n",
      "\tTrain loss: 0.00081, Accuracy: 6757/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1669/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1193/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.31904\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.32713\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.11754\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.47276\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.61230\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.17215\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.20196\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.42376\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.41109\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.27822\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.33242\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.23097\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.40171\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.20815\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.25680\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.31082\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.32898\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.58996\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.23433\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.25823\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.44738\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.27013\n",
      "\tTrain loss: 0.00086, Accuracy: 6760/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1216/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.16054\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.59627\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.14860\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.41546\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.25166\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.22822\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.37214\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.37982\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.28754\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.26674\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.18910\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.48626\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.26457\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.33848\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.27117\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.21550\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.39894\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.17071\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.18287\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.40973\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.24583\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.50487\n",
      "\tTrain loss: 0.00081, Accuracy: 6761/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1673/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1205/1772 (68.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.45448\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.38648\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.50062\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.42113\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.30223\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.29601\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.33084\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.46793\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.31810\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.22624\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.19944\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.27725\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.31526\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.22090\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.24047\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.24922\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.30443\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.23947\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.58139\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.09457\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.53876\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.32259\n",
      "\tTrain loss: 0.00083, Accuracy: 6760/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1672/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1184/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.30764\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.60790\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.33371\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.34804\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.31505\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.16627\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.35831\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.41223\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.08955\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.38905\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.21850\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.37741\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.28441\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.34831\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.27409\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.19286\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.55683\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.27494\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.40634\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.16751\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.36739\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.21345\n",
      "\tTrain loss: 0.00073, Accuracy: 6764/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1675/1692 (98.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1196/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.61468\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.15104\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.37733\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.28115\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.38876\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.36553\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.52542\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.24224\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.15946\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.44508\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.19488\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.15740\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.60487\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.50255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.31085\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.44244\n",
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.38167\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.29348\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.34943\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.28749\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.30269\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.27833\n",
      "\tTrain loss: 0.00076, Accuracy: 6763/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1674/1692 (98.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1169/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.36565\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.29924\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.19242\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.34739\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.29663\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.26652\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.40007\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.47028\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.47640\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.13271\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.31658\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.28109\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.33720\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.15826\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.37240\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.21726\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.70706\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.44035\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.46401\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.24862\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.56901\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.30768\n",
      "\tTrain loss: 0.00070, Accuracy: 6764/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1681/1692 (99.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1186/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.22688\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.45982\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.33873\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.33754\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.54479\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.60418\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.26237\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.23774\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.46474\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.33515\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.39008\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.22221\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.36887\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.45882\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.24989\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.46897\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.52722\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.20052\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.42343\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.21744\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.31691\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.45467\n",
      "\tTrain loss: 0.00064, Accuracy: 6764/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1675/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1201/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.19325\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.36473\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.27997\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.37246\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.33959\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.23337\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.20872\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.49641\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.55616\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.45848\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.61926\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.43181\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.40286\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.27871\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.36558\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.32031\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.31778\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.45919\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.12282\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.21435\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.29414\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.89979\n",
      "\tTrain loss: 0.00071, Accuracy: 6763/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1680/1692 (99.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1188/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.41375\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.23161\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.21758\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.58288\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.58353\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.55260\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.51356\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.48702\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.18846\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.25337\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.35189\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.33119\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.29413\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.37762\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.36566\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.27918\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.28681\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.52943\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.35755\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.28040\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.42328\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.74961\n",
      "\tTrain loss: 0.00076, Accuracy: 6762/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1674/1692 (98.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1167/1772 (65.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.28655\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.42364\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.37439\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.26259\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.30124\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.36132\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.14888\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.64278\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.72386\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.58844\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.44665\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.40856\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.29172\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.32372\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.35438\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.26286\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.20238\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.31274\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.38813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.26145\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.32195\n",
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.21380\n",
      "\tTrain loss: 0.00071, Accuracy: 6763/6768 (99.00%)\n",
      "\tValidation loss: 0.00004, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1179/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.25541\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.34533\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.29423\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.47995\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.18933\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.14345\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.21113\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.35933\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.23397\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.37832\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.21351\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.36447\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.36516\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.31307\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.36802\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.27691\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.44784\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.45572\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.24623\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.29467\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.67552\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.30107\n",
      "\tTrain loss: 0.00062, Accuracy: 6764/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1196/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.19286\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.31273\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.26524\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.38588\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.36191\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.26760\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.24268\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.56249\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.13680\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.14540\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.14026\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.13388\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.59883\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.30440\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.41827\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.26815\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.22936\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.09475\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.36642\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.26031\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.23686\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.45899\n",
      "\tTrain loss: 0.00055, Accuracy: 6766/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1687/1692 (99.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1182/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.28156\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.42893\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.30590\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.60135\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.39010\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.24837\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.56457\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.55526\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.22798\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.23705\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.24803\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.55155\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.35981\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.32226\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.59453\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.17413\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.46578\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.46155\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.58411\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.19122\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.16106\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.21210\n",
      "\tTrain loss: 0.00057, Accuracy: 6767/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1677/1692 (99.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1184/1772 (66.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.22654\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.28202\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.13199\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.25168\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.29494\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.50817\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.33708\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.57499\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.22203\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.26323\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.27513\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.28524\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.18041\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.59333\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.24761\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.27453\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.30357\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.38099\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.67146\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.22027\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.28012\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.42931\n",
      "\tTrain loss: 0.00063, Accuracy: 6760/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1671/1692 (98.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1204/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.20846\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.31751\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.28555\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.18363\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.40938\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.53375\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.40705\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.52423\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.34509\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.40633\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.23479\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.20600\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.71392\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.25650\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.28174\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.35149\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.36031\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.27333\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.19358\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.06164\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.54018\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.35748\n",
      "\tTrain loss: 0.00064, Accuracy: 6763/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1681/1692 (99.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00062, Accuracy: 1196/1772 (67.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.28581\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.51233\n",
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.36353\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.47584\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.21980\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.18243\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.50416\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.33813\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.26461\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.31495\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.36840\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.10327\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.53572\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.20257\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.16533\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.32269\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.25004\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.27387\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.25435\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.59984\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.33777\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.24545\n",
      "\tTrain loss: 0.00058, Accuracy: 6764/6768 (99.00%)\n",
      "\tValidation loss: 0.00003, Accuracy: 1681/1692 (99.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1203/1772 (67.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9970449172576832\n",
      "Best test accuracy:\n",
      "0.6997742663656885\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn, cnn_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b39d0",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faba7c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.46685\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.41770\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.42353\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.46140\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.43312\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.48316\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.41534\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.40484\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.42347\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.34464\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.41453\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.36902\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.40778\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.44674\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.42980\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.35856\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.37350\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.34862\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.42199\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.35376\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.43411\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.37950\n",
      "\tTrain loss: 0.04317, Accuracy: 1978/6768 (29.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 444/1692 (26.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 476/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.39506\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.36062\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.34715\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.36272\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.39989\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.36236\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.33557\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.39680\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.43451\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.33718\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.40668\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.45187\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.35199\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.36793\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.42031\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.29594\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.36841\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.35265\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.48402\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.33112\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.35228\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.37106\n",
      "\tTrain loss: 0.04259, Accuracy: 2179/6768 (32.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 514/1692 (30.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 469/1772 (26.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.38791\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.34403\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.38098\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.40698\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.38558\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.35016\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.34879\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.44207\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.40806\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.33478\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.38602\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.42046\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.32098\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.41032\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.37585\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.35837\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.42847\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.34918\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.41520\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.33276\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.40827\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.38450\n",
      "\tTrain loss: 0.04233, Accuracy: 2303/6768 (34.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 510/1692 (30.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 512/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.39636\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.34581\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.35632\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.37999\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.39532\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.32057\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.32583\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.32722\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.33619\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.31321\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.32252\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.40016\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.30104\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.41385\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.37870\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.36697\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.40226\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.34495\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.39932\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.32877\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.32271\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.27712\n",
      "\tTrain loss: 0.04165, Accuracy: 2458/6768 (36.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 560/1692 (33.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 508/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.41803\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.37156\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.29673\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.43995\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.39082\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.33183\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.34098\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.42568\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.39211\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.27110\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.29352\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.40298\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.22308\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.34944\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.31826\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.25395\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.39855\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.29956\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.45344\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.27387\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.26263\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.26165\n",
      "\tTrain loss: 0.04115, Accuracy: 2489/6768 (36.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 540/1692 (31.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 521/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.42056\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.33599\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.35695\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.48160\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.29954\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.35940\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.23800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.29398\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.31143\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.28089\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.32428\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.35203\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.23310\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.34560\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.26537\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.24333\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.43557\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.32991\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.30943\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.29492\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.24718\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.25722\n",
      "\tTrain loss: 0.03998, Accuracy: 2831/6768 (41.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 595/1692 (35.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 517/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.31178\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.28228\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.29938\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.42433\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.30300\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.36527\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.28999\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.29563\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.29748\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.27252\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.27374\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.46811\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.18140\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.26870\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.30647\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.32961\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.27417\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.33397\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.30796\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.18920\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.15492\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.36902\n",
      "\tTrain loss: 0.03896, Accuracy: 2937/6768 (43.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 621/1692 (36.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 518/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.36249\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.25868\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.30321\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.39172\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.37719\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.37545\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.23402\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.29957\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.16668\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.19158\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.28155\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.44293\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.12486\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.29659\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.15008\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.32389\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.25353\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.33714\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.29584\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.17020\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.22289\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.30545\n",
      "\tTrain loss: 0.03783, Accuracy: 3163/6768 (46.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 657/1692 (38.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 537/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.40817\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.25469\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.27727\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.40110\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.36718\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.18257\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.25407\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.28528\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.30520\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.13218\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.18153\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.35994\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.23643\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.15447\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.13587\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.22322\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.19544\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.40887\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.23883\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.11080\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.08814\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.37998\n",
      "\tTrain loss: 0.03665, Accuracy: 3371/6768 (49.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 704/1692 (41.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 513/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.30528\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.32791\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.28050\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.48717\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.19645\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.34165\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.18768\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.14813\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.24704\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.16105\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.10590\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.38097\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.11735\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.16130\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.23757\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.24037\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.24952\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.28408\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.29118\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.10629\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.08874\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.20625\n",
      "\tTrain loss: 0.03551, Accuracy: 3492/6768 (51.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 717/1692 (42.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 496/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.26314\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.31788\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.24562\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.36313\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.21814\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.27917\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.12265\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.17051\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.21164\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 0.94385\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.05658\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.37091\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.07956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.20822\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.13422\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.15082\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.24351\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.13611\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.18309\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.07318\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 0.98831\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.22036\n",
      "\tTrain loss: 0.03462, Accuracy: 3574/6768 (52.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 749/1692 (44.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.28735\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.23424\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.32686\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.30783\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.18248\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.22115\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.16833\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.22665\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.21381\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 0.93458\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.06912\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.43410\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.02255\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.12877\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.22513\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.11655\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.31570\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.25836\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.21819\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.02001\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.07147\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.11301\n",
      "\tTrain loss: 0.03360, Accuracy: 3694/6768 (54.00%)\n",
      "\tValidation loss: 0.00071, Accuracy: 792/1692 (46.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 514/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.21943\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.25897\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 1.25712\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.26439\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.13469\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.22223\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.01753\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.21801\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.13082\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 0.99995\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.98551\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.42183\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.04265\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.10918\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.16191\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.13179\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.17636\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.29353\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.13868\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.13283\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.94036\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.17576\n",
      "\tTrain loss: 0.03320, Accuracy: 3752/6768 (55.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 797/1692 (47.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 504/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.20887\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.25426\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.11404\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.43791\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.13180\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.15307\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.03499\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.13902\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.12776\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.00096\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 1.07432\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.24908\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.93435\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.08533\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.22176\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.11470\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.20850\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.12310\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 0.98349\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.08170\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 0.98522\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.15897\n",
      "\tTrain loss: 0.03135, Accuracy: 3998/6768 (59.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 813/1692 (48.00%)\n",
      "\tTest loss: 0.00086, Accuracy: 536/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.17307\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.19032\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.14840\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.25436\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.06281\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 1.14033\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.07632\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.12766\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.32916\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.87104\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.91940\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.29533\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.01403\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 0.99184\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.12441\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 0.98240\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.03123\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.20028\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.25325\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.99703\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.82392\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.11813\n",
      "\tTrain loss: 0.02990, Accuracy: 4178/6768 (61.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 842/1692 (49.00%)\n",
      "\tTest loss: 0.00089, Accuracy: 499/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.00579\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.04824\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 1.04077\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.28920\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 0.99076\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 1.28417\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.05726\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 1.20233\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.17841\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.93568\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.79311\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.35051\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 1.08821\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.89007\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 1.11010\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.01397\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.08363\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.02791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.03359\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.95279\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.82528\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.02223\n",
      "\tTrain loss: 0.02913, Accuracy: 4291/6768 (63.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 877/1692 (51.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 490/1772 (27.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 1.07165\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.26566\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.10425\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.15306\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.05032\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 1.12056\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.00483\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 0.96868\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 1.20941\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.84340\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.85598\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.28534\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.95220\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.95190\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.19673\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.01003\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 1.00610\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 0.99386\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.95448\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.88860\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.79933\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 0.90200\n",
      "\tTrain loss: 0.02838, Accuracy: 4333/6768 (64.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 879/1692 (51.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 537/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 1.05055\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.03546\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.17492\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.22727\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.04338\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 1.01515\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 0.86724\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.07274\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.24834\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.89948\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.78941\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 1.29210\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.83206\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.95293\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 1.01677\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.93140\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 0.97805\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 0.98521\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.96515\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.86188\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.79953\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.97159\n",
      "\tTrain loss: 0.02690, Accuracy: 4549/6768 (67.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 924/1692 (54.00%)\n",
      "\tTest loss: 0.00091, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 1.07998\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.12686\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.92905\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.15287\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.05995\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 1.19324\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.91350\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.93645\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.00178\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.68591\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.77580\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.32860\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.71312\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.80452\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.84224\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.96548\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.88771\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 1.13106\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.00599\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.87104\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.85287\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.94519\n",
      "\tTrain loss: 0.02592, Accuracy: 4560/6768 (67.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 916/1692 (54.00%)\n",
      "\tTest loss: 0.00095, Accuracy: 505/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.91732\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.00960\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 1.15168\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 1.28696\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.99334\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 1.02542\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.78990\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.95250\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 1.01324\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.75339\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.68563\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 1.07591\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.90051\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.86507\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.85262\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.89979\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.25919\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.00984\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.83034\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.82396\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.72362\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 0.89076\n",
      "\tTrain loss: 0.02497, Accuracy: 4637/6768 (68.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 928/1692 (54.00%)\n",
      "\tTest loss: 0.00097, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.93897\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.11738\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 1.24150\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.17925\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.08615\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 1.03137\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 1.01068\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 1.00720\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.84819\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.79467\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.76604\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 1.02221\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.89615\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 1.00658\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 1.12812\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.87756\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.85616\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.91083\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.84494\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 1.10470\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.65735\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 1.07136\n",
      "\tTrain loss: 0.02459, Accuracy: 4672/6768 (69.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00060, Accuracy: 967/1692 (57.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.95282\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.88022\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 1.07534\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 1.01219\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 1.04362\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.93421\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.80372\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 1.04289\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 1.09971\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.71398\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.94917\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 1.02565\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.81801\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.72128\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 1.05688\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.85081\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.92528\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.97436\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.78443\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.69612\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.69127\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.88609\n",
      "\tTrain loss: 0.02340, Accuracy: 4848/6768 (71.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 972/1692 (57.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 521/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.95151\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.83218\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.89407\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.88479\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 1.01608\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.99408\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.77921\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.80177\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.96189\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.60586\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.68009\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 1.08408\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.68563\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.66017\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.93758\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.66448\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.87978\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.92449\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.88516\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.85046\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.73068\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.86898\n",
      "\tTrain loss: 0.02248, Accuracy: 4943/6768 (73.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1014/1692 (59.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.87873\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.94728\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.76486\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.85311\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.83332\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 1.00894\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.75545\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.95471\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 1.01973\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.68131\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.74442\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.64965\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.79115\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.75238\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.84486\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.71461\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.73579\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.99223\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.74873\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.84270\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.59281\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.77746\n",
      "\tTrain loss: 0.02210, Accuracy: 4983/6768 (73.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1023/1692 (60.00%)\n",
      "\tTest loss: 0.00100, Accuracy: 547/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 1.06733\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.98609\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.84788\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 1.02129\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.89389\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.83972\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.65295\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.95052\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.75034\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.63096\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.66664\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.94899\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.70500\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.79513\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.80359\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.79978\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.77051\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.79147\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.82118\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.65014\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.62334\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.82405\n",
      "\tTrain loss: 0.02065, Accuracy: 5128/6768 (75.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1062/1692 (62.00%)\n",
      "\tTest loss: 0.00102, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.97541\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.94863\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.83798\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.83628\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.92379\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.75606\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.69211\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.88018\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.85012\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.59183\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.63318\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.93005\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.68251\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.66676\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.87203\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.79383\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.85452\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.92096\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.76921\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.58133\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.64240\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.72724\n",
      "\tTrain loss: 0.02045, Accuracy: 5082/6768 (75.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1072/1692 (63.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 527/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 1.09030\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.85729\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 1.01583\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.82282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.67591\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.89770\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.66228\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.89977\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 1.01130\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.83673\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.65089\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.86164\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.65495\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.57070\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.85887\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.65601\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.69457\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.78657\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.72674\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.70142\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.59374\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.73038\n",
      "\tTrain loss: 0.01875, Accuracy: 5254/6768 (77.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1096/1692 (64.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 520/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.77176\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.94720\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.94119\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 1.02966\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.88189\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.89964\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.59420\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.82308\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.69152\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.48260\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.54658\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.84843\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.62473\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.63422\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.67231\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.78759\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.80809\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.60082\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.83599\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.99840\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.60377\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.48125\n",
      "\tTrain loss: 0.01799, Accuracy: 5361/6768 (79.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1121/1692 (66.00%)\n",
      "\tTest loss: 0.00109, Accuracy: 518/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.82216\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.97528\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.86426\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 1.00978\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.62273\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.99382\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.67010\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.82058\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.83150\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.71281\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.68898\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.88465\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.84145\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.79700\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 1.01961\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 1.02052\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.44536\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.61812\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.62763\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.77866\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.70968\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.68389\n",
      "\tTrain loss: 0.01747, Accuracy: 5407/6768 (79.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1138/1692 (67.00%)\n",
      "\tTest loss: 0.00107, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.88176\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.91989\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.78683\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.83808\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.71151\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.76817\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.69597\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.67034\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.69819\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.42066\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.63720\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.80496\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.70471\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.46883\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.69110\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.58078\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.69800\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.76668\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.71756\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.62620\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.60297\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.73584\n",
      "\tTrain loss: 0.01694, Accuracy: 5409/6768 (79.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1132/1692 (66.00%)\n",
      "\tTest loss: 0.00113, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.81689\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.99613\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.76579\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.92319\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.99597\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.79373\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.67706\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.71971\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.62659\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.69566\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.65200\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.63918\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.56306\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.57796\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.58489\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.73195\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.63801\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.85206\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.52557\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.69058\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.45273\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.58304\n",
      "\tTrain loss: 0.01576, Accuracy: 5510/6768 (81.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1151/1692 (68.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 529/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.80173\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.83157\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.66301\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.71013\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.62723\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.72866\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.55356\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.99871\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.57909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.43067\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.62193\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.48562\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.71514\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.51970\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.84467\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.57926\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.70908\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.70319\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.49240\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.82006\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.71087\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.53974\n",
      "\tTrain loss: 0.01551, Accuracy: 5579/6768 (82.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1155/1692 (68.00%)\n",
      "\tTest loss: 0.00113, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.93283\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.70224\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.67094\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.92616\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.72236\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.64635\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.52243\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.67987\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.64112\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.46934\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.57087\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.55960\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.64990\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.59035\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.67025\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.49130\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.74395\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.73742\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.49801\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.63648\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.47391\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.63381\n",
      "\tTrain loss: 0.01505, Accuracy: 5562/6768 (82.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1161/1692 (68.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.91550\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.61444\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.69064\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.74940\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.60015\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.84866\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.50551\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.64694\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.64907\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.53553\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.55443\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.57641\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.62419\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.39881\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.75109\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.30910\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.69700\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.64389\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.67121\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.57394\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.49882\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.75240\n",
      "\tTrain loss: 0.01438, Accuracy: 5680/6768 (83.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1193/1692 (70.00%)\n",
      "\tTest loss: 0.00122, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.58998\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.73336\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.94172\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.71066\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.90630\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.66758\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.84869\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.55198\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.68690\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.37007\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.52680\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.47807\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.66980\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.58223\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.62351\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.60819\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.40865\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.97907\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.60653\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.67988\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.56575\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.41670\n",
      "\tTrain loss: 0.01381, Accuracy: 5656/6768 (83.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1172/1692 (69.00%)\n",
      "\tTest loss: 0.00125, Accuracy: 528/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.86681\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.65812\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.75198\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.83641\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.68374\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.90269\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.42764\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.69050\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.56739\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.37060\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.56093\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.78442\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.63660\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.52396\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.60622\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.39378\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.53778\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.64042\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.56172\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.62908\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.61152\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.58547\n",
      "\tTrain loss: 0.01243, Accuracy: 5828/6768 (86.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1228/1692 (72.00%)\n",
      "\tTest loss: 0.00126, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.90218\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.75303\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.54476\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.94328\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.68201\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.64118\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.61444\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.56247\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.51020\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.42053\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.37516\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.54142\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.56069\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.54525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.53080\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.40865\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.47374\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.57509\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.64411\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.73199\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.40531\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.40116\n",
      "\tTrain loss: 0.01315, Accuracy: 5758/6768 (85.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1234/1692 (72.00%)\n",
      "\tTest loss: 0.00128, Accuracy: 547/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.63314\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.91704\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.73685\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.53284\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.60596\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.77344\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.65155\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.72242\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.79212\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.23336\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.48962\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.83876\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.48489\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.43520\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.58768\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.42303\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.60724\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.44096\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.36632\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.60193\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.54630\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.52180\n",
      "\tTrain loss: 0.01191, Accuracy: 5841/6768 (86.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1249/1692 (73.00%)\n",
      "\tTest loss: 0.00130, Accuracy: 510/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.59256\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.88360\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.56951\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.70163\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.71705\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.48033\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.53141\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.61360\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.56118\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.34371\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.56526\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.40959\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.52155\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.39271\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.69181\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.66050\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.43313\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.99321\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.37800\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.52197\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.47387\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.44487\n",
      "\tTrain loss: 0.01096, Accuracy: 5945/6768 (87.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1249/1692 (73.00%)\n",
      "\tTest loss: 0.00127, Accuracy: 516/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.53088\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.85130\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.50683\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.57008\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.68160\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.52956\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.31302\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.54561\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.47379\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.56664\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.37633\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.24913\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.57917\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.48298\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.45668\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.38813\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.47540\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.47382\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.50569\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.39361\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.43394\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.39952\n",
      "\tTrain loss: 0.01109, Accuracy: 5928/6768 (87.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1265/1692 (74.00%)\n",
      "\tTest loss: 0.00131, Accuracy: 524/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.79269\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.93497\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.55423\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.75264\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.53344\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.54421\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.34884\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.68001\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.60621\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.31299\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.26960\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.37620\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.55213\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.40850\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.68469\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.39310\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.36929\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.72746\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.45974\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.55700\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.46014\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.46292\n",
      "\tTrain loss: 0.01014, Accuracy: 6004/6768 (88.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1308/1692 (77.00%)\n",
      "\tTest loss: 0.00135, Accuracy: 525/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.54741\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.79028\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.61236\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.51851\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.65597\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.59547\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.41124\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.50452\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.43452\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.37304\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.37582\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.40921\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.60865\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.65812\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.70842\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.37344\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.38868\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.42224\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.42525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.61229\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.47000\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.45237\n",
      "\tTrain loss: 0.00930, Accuracy: 6085/6768 (89.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1330/1692 (78.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.60670\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.66740\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.70261\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.56409\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.61971\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.61856\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.40088\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.65670\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.54381\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.52973\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.34437\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.51707\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.63094\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.26939\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.55755\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.29039\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.51211\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.73130\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.45458\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.66285\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.35006\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.36542\n",
      "\tTrain loss: 0.00964, Accuracy: 6049/6768 (89.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1307/1692 (77.00%)\n",
      "\tTest loss: 0.00137, Accuracy: 516/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.57081\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.72620\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.61219\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.45124\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.75646\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.49789\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.25282\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.72298\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.55828\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.51570\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.63928\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.49496\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.46773\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.33491\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.34420\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.41253\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.20410\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.42663\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.58049\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.39084\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.38949\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.23837\n",
      "\tTrain loss: 0.00913, Accuracy: 6082/6768 (89.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1309/1692 (77.00%)\n",
      "\tTest loss: 0.00139, Accuracy: 541/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.59648\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.83881\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.42156\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.81626\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.58812\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.41996\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.27348\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.51129\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.34147\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.50789\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.29685\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.44724\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.55152\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.44210\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.25147\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.57243\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.46118\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.48142\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.33446\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.49889\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.29238\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.46819\n",
      "\tTrain loss: 0.01076, Accuracy: 5920/6768 (87.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1290/1692 (76.00%)\n",
      "\tTest loss: 0.00139, Accuracy: 533/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.63162\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.69846\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.61578\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.49643\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.59844\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.63352\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.40806\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.50320\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.49854\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.23171\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.35903\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.48338\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.58712\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.69811\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.51492\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.28723\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.55089\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.60607\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.34886\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.55108\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.29470\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.27038\n",
      "\tTrain loss: 0.00776, Accuracy: 6237/6768 (92.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1359/1692 (80.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 572/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.24463\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.80553\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.41358\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.40587\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.73107\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.39316\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.25007\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.63653\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.44381\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.37288\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.19945\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.40983\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.71326\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.37203\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.38703\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.35796\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.37355\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.48208\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.41070\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.38548\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.23979\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.45076\n",
      "\tTrain loss: 0.00911, Accuracy: 6076/6768 (89.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1309/1692 (77.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 537/1772 (30.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.60893\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.63021\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.42958\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.47107\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.40629\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.52535\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.41942\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.48824\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.46746\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.41318\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.46267\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.25732\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.59129\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.30781\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.39128\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.49334\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.35951\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.60531\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.39845\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.47515\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.38165\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.25196\n",
      "\tTrain loss: 0.00715, Accuracy: 6270/6768 (92.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1387/1692 (81.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 565/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.41487\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.44574\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.34849\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.46936\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.42819\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.52583\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.15897\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.69902\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.30809\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.15009\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.34339\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.14066\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.34174\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.26666\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.80189\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.32173\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.34169\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.49763\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.23748\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.42562\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.45865\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.26507\n",
      "\tTrain loss: 0.00708, Accuracy: 6268/6768 (92.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1359/1692 (80.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.65351\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.59076\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.58472\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.66832\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.46104\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.30486\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.25728\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.48572\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.29233\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.42886\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.28468\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.35138\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.47879\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.39473\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.32058\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.24863\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.28433\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.63054\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.26973\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.36166\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.37818\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.28966\n",
      "\tTrain loss: 0.00755, Accuracy: 6220/6768 (91.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1369/1692 (80.00%)\n",
      "\tTest loss: 0.00146, Accuracy: 560/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.58742\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.70824\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.54326\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.48064\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.73184\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.39242\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.22894\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.61918\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.27920\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.31244\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.17082\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.32405\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.43116\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.16835\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.45789\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.48275\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.44236\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.51812\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.29719\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.45342\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.27685\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.09219\n",
      "\tTrain loss: 0.00647, Accuracy: 6300/6768 (93.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1406/1692 (83.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.32046\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.69709\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.37620\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.39664\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.29991\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.58275\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.27306\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.53825\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.30310\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.27515\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.31829\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.31866\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.50443\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.38316\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.48307\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.39071\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.24362\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.41466\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.44024\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.37480\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.57768\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.25406\n",
      "\tTrain loss: 0.00738, Accuracy: 6221/6768 (91.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1375/1692 (81.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.37293\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.66256\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.44375\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.38997\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.31264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.40945\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.35129\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.47226\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.29286\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.34910\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.31280\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.56521\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.45571\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.19468\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.34897\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.34446\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.18683\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.45970\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.34575\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.53980\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.26976\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.43427\n",
      "\tTrain loss: 0.00648, Accuracy: 6315/6768 (93.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1372/1692 (81.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 535/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.37505\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.46398\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.52602\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.34863\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.30366\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.31458\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.19899\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.73819\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.35687\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.12257\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.27351\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.49857\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.53262\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.42655\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.61781\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.29555\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.47840\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.69235\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.29659\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.34245\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.32890\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.18146\n",
      "\tTrain loss: 0.00614, Accuracy: 6341/6768 (93.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1415/1692 (83.00%)\n",
      "\tTest loss: 0.00143, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.35970\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.54159\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.16265\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.34902\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.57842\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.21518\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.32631\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.45723\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.45030\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.15299\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.24618\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.19720\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.64755\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.13166\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.41190\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.24354\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.24308\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.42422\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.17131\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.51939\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.30863\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.34201\n",
      "\tTrain loss: 0.00599, Accuracy: 6337/6768 (93.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1402/1692 (82.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.24380\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.53083\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.41330\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.27080\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.41338\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.42645\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.26483\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.46162\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.44865\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.25860\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.45973\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.40032\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.30492\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.28298\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.22648\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.38594\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.31108\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.20672\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.16002\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.30893\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.28963\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.21173\n",
      "\tTrain loss: 0.00576, Accuracy: 6359/6768 (93.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1422/1692 (84.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.61701\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.51495\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.54492\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.45052\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.25453\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.47936\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.14601\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.23566\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.29721\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.15802\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.19176\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.22003\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.66280\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.19562\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.36226\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.35421\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.27941\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.55796\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.30219\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.41638\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.21513\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.26600\n",
      "\tTrain loss: 0.00477, Accuracy: 6421/6768 (94.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1444/1692 (85.00%)\n",
      "\tTest loss: 0.00155, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.52818\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.66002\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.47638\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.29789\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.20062\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.29540\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.19521\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.41647\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.40825\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.52871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.23564\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.34603\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.37159\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.16845\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.32817\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.31515\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.11689\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.40837\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.25662\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.15017\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.24801\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.17099\n",
      "\tTrain loss: 0.00471, Accuracy: 6451/6768 (95.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00155, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.56200\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.33782\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.45494\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.41597\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.38597\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.48937\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.29241\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.39763\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.45211\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.37668\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.24990\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.27008\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.40655\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.46676\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.53295\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.23995\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.14759\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.59407\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.30232\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.29264\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.29514\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.20515\n",
      "\tTrain loss: 0.00529, Accuracy: 6386/6768 (94.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1419/1692 (83.00%)\n",
      "\tTest loss: 0.00156, Accuracy: 575/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.21143\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.41930\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.33311\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.33960\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.20175\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.22338\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.33676\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.45433\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.19840\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.20264\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.18075\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.39073\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.54899\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.15206\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.46193\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.15915\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.06383\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.48841\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.16496\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.27844\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.19346\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.15427\n",
      "\tTrain loss: 0.00501, Accuracy: 6395/6768 (94.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1444/1692 (85.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.36853\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.51737\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.57462\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.19102\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.33605\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.23525\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.21001\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.44124\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.21626\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.25825\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.20335\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.25520\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.34872\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.24735\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.13976\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.35009\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.23278\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.56815\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.26409\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.35464\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.48127\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.26585\n",
      "\tTrain loss: 0.00469, Accuracy: 6431/6768 (95.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1441/1692 (85.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 545/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.44504\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.39358\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.39751\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.31457\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.23642\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.25876\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.31380\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.37566\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.20819\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.33344\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.08499\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.25422\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.32250\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.25436\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.24815\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.27099\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.45915\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.44596\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.37943\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.43940\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.21185\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.18272\n",
      "\tTrain loss: 0.00492, Accuracy: 6395/6768 (94.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.38528\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.68368\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.32735\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.35028\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.38262\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.15236\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.20687\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.37476\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.17049\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.14772\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.29433\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.21849\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.33093\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.08826\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.47626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.13122\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.25063\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.24280\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.39430\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.23008\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.14360\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.32720\n",
      "\tTrain loss: 0.00612, Accuracy: 6307/6768 (93.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1411/1692 (83.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.56919\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.55795\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.15577\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.26384\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.38876\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.45546\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.19737\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.41681\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.37004\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.19934\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.33987\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.33554\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.29313\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.18140\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.44365\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.21021\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.27296\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.28979\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.19691\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.10435\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.28886\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.29571\n",
      "\tTrain loss: 0.00399, Accuracy: 6495/6768 (95.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 539/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.56243\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.59606\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.25652\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.45749\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.15584\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.41876\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.30474\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.23491\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.30642\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.12684\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.30255\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.23175\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.26397\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.13545\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.27369\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.35227\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.09500\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.40048\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.26286\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.23497\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.16264\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.11964\n",
      "\tTrain loss: 0.00406, Accuracy: 6475/6768 (95.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1470/1692 (86.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 522/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.52294\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.42644\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.38668\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.16329\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.18020\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.43188\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.14181\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.22956\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.10941\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.28109\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.17152\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.29278\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.75689\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.20350\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.22794\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.18318\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.18833\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.23460\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.11899\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.32318\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.05943\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.24415\n",
      "\tTrain loss: 0.00430, Accuracy: 6453/6768 (95.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1455/1692 (85.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.42561\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.35140\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.23251\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.46234\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.39647\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.42376\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.13631\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.45095\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.19154\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.09266\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.13825\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.15538\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.35677\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.17694\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.49645\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.13002\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.13993\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.42889\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.30273\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.32100\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.30858\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.21120\n",
      "\tTrain loss: 0.00457, Accuracy: 6445/6768 (95.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00166, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.41114\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.45984\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.27443\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.22579\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.11681\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.36258\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.20956\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.26825\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.15273\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.24839\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.11910\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.28672\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.33028\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.11642\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.27740\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.16713\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.23767\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.32919\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.41652\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.19401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.09087\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.12494\n",
      "\tTrain loss: 0.00371, Accuracy: 6516/6768 (96.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 524/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.26110\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.60624\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.23801\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.35324\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.18831\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.22460\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.07835\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.09426\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.27052\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.06055\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.31008\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.22031\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.34644\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.21229\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.27516\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.23062\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.16204\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.38216\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.11248\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.18351\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.17297\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.15248\n",
      "\tTrain loss: 0.00368, Accuracy: 6517/6768 (96.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 544/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.11198\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.57091\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.15608\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.33127\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.43930\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.21865\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.14171\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.25993\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.25273\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.27140\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.13784\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.11999\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.38396\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.11604\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.13137\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.24518\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.47076\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.20508\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.39877\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.29498\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.22472\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.12769\n",
      "\tTrain loss: 0.00394, Accuracy: 6492/6768 (95.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1487/1692 (87.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 549/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.52329\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.50409\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.27392\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.32085\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.50958\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.16818\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.28795\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.20940\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.26534\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.07317\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.11165\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.33366\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.25531\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.18804\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.39483\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.23326\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.16497\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.25223\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.12741\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.12659\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.20732\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.21728\n",
      "\tTrain loss: 0.00434, Accuracy: 6453/6768 (95.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1462/1692 (86.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 570/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.39586\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.53093\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.17756\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.31190\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.19519\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.22789\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.14182\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.33040\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.22912\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.17599\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.11031\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.25311\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.48475\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.32603\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.27281\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.18031\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.11905\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.21623\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.16136\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.24311\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.09391\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.12693\n",
      "\tTrain loss: 0.00281, Accuracy: 6579/6768 (97.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1509/1692 (89.00%)\n",
      "\tTest loss: 0.00170, Accuracy: 558/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.44086\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.29536\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.23906\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.27760\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.21872\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.19927\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.23805\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.26670\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.16906\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.12380\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.12608\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.47752\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.32684\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.12768\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.28780\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.12315\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.25094\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.45106\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.20067\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.45825\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.37708\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.12815\n",
      "\tTrain loss: 0.00344, Accuracy: 6541/6768 (96.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00167, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.13039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.36007\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.30657\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.20086\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.40090\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.31515\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.30105\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.20171\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.08153\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.26309\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.31040\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.15894\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.28112\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.15503\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.23208\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.12676\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.05601\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.40338\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.06105\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.42326\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.12336\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.29388\n",
      "\tTrain loss: 0.00284, Accuracy: 6569/6768 (97.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 539/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.43061\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.48586\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.27649\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.22683\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.30965\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.33186\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.10896\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.37692\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.15027\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.25804\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.13231\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.46749\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.36378\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.12271\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.23749\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.11856\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.42230\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.29787\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.16635\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.17867\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.41098\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.05630\n",
      "\tTrain loss: 0.00307, Accuracy: 6556/6768 (96.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1510/1692 (89.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 529/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.23115\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.34137\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.37744\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.28104\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.18210\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.26776\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.13255\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.23024\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.15483\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.36331\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.28481\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.18144\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.61081\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.29602\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.19581\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.07351\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.32949\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.20646\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.32315\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.31588\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.17893\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.11515\n",
      "\tTrain loss: 0.00364, Accuracy: 6500/6768 (96.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1488/1692 (87.00%)\n",
      "\tTest loss: 0.00177, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.21861\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.46709\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.38169\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.24074\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.15681\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.28691\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.10863\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.42342\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.22596\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.08019\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.14884\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.41377\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.28132\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.25548\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.18582\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.26555\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.11944\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.46618\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.12699\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.20370\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.49050\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.06577\n",
      "\tTrain loss: 0.00319, Accuracy: 6547/6768 (96.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1499/1692 (88.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 523/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.40123\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.37564\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.27275\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.28609\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.17159\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.34630\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.17694\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.38070\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.23904\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.20685\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.12409\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.31750\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.28565\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.11432\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.30553\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.12121\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.51341\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.22827\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.22321\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.11423\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.10339\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.33875\n",
      "\tTrain loss: 0.00334, Accuracy: 6535/6768 (96.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1501/1692 (88.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.21303\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.38820\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.40582\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.06025\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.14362\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.16889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.05290\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.19770\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.26080\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.21049\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.33280\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.33703\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.29822\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.15364\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.53063\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.36861\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.05566\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.26915\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.21459\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.23185\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.19918\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.37432\n",
      "\tTrain loss: 0.00244, Accuracy: 6604/6768 (97.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1506/1692 (89.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 531/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.70533\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.30510\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.18760\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.30764\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.12077\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.23278\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.15718\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.24906\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.27171\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.21629\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.20753\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.14473\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.27525\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.16600\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.19728\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.28906\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.14791\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.12999\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.24496\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.43127\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.26904\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.10346\n",
      "\tTrain loss: 0.00313, Accuracy: 6530/6768 (96.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00173, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.28321\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.16432\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.30936\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.19495\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.35510\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.12551\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.26330\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.26607\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.12642\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.08190\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.07496\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.14618\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.32612\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.11224\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.62967\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.07712\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.18162\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.05610\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.19713\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.08303\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.27238\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.09010\n",
      "\tTrain loss: 0.00294, Accuracy: 6555/6768 (96.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1504/1692 (88.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.11699\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.29492\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.24568\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.34110\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.18410\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.35044\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.21217\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.19261\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.29167\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.18899\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.03899\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.18181\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.36220\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.06947\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.17220\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.32955\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.20548\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.43647\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.28101\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.17062\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.12179\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.15840\n",
      "\tTrain loss: 0.00277, Accuracy: 6581/6768 (97.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1512/1692 (89.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 554/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.30097\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.15278\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.09752\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.10459\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.29927\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.17067\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.14288\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.57473\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.23306\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.09919\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.08009\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.05514\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.21461\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.14143\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.23969\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.13565\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.18825\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.21577\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.25325\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.40167\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.07155\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.07709\n",
      "\tTrain loss: 0.00262, Accuracy: 6597/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1527/1692 (90.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 551/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.33880\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.36926\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.07976\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.20478\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.13056\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.20491\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.13160\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.34236\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.20294\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.16716\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.39505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.15146\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.42249\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.20456\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.37297\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.11962\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.05584\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.16462\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.04350\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.09881\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.37126\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.13534\n",
      "\tTrain loss: 0.00294, Accuracy: 6556/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1530/1692 (90.00%)\n",
      "\tTest loss: 0.00169, Accuracy: 573/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.21795\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.31451\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.08475\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.15014\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.22845\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.28244\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.16307\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.22900\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.06554\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.06096\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.10142\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.11989\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.34944\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.18700\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.15420\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.12039\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.07808\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.19967\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.12049\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.19103\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.41698\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.22972\n",
      "\tTrain loss: 0.00174, Accuracy: 6647/6768 (98.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1541/1692 (91.00%)\n",
      "\tTest loss: 0.00185, Accuracy: 566/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.22057\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.29023\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.20471\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.14959\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.20707\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.26371\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.04150\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.12163\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.32295\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.17496\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.29290\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.14393\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.24203\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.17929\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.19140\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.07545\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.02747\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.13157\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.08859\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.17676\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.16909\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.13400\n",
      "\tTrain loss: 0.00248, Accuracy: 6592/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1526/1692 (90.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 552/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.16852\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.24845\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.17103\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.06427\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.24701\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.25462\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.11522\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.20315\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.10011\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.31171\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.12535\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.33823\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.29160\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.19690\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.24198\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.18610\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.10073\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.33504\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.40284\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.31326\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.22850\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.03000\n",
      "\tTrain loss: 0.00292, Accuracy: 6553/6768 (96.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1514/1692 (89.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 546/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.43425\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.32845\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.32860\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.14025\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.13809\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.20629\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.28925\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.13157\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.21493\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.39356\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.18494\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.18999\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.22555\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.08790\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.12690\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.15959\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.16603\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.18035\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.05932\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.13603\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.47017\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.47556\n",
      "\tTrain loss: 0.00525, Accuracy: 6396/6768 (94.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1451/1692 (85.00%)\n",
      "\tTest loss: 0.00189, Accuracy: 521/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.41327\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.23971\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.19690\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.20198\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.43062\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.24807\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.13227\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.28312\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.27396\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.13928\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.10464\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.42497\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.28991\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.07434\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.12671\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.14751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.06824\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.15549\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.03017\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.04515\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.14689\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.20882\n",
      "\tTrain loss: 0.00294, Accuracy: 6541/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1513/1692 (89.00%)\n",
      "\tTest loss: 0.00172, Accuracy: 584/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.26114\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.11917\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.08164\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.26579\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.30710\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.13278\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.07766\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.37223\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.18960\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.17883\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.20133\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.11302\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.15454\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.20315\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.24078\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.22886\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.17070\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.25603\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.35536\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.14027\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.13144\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.11411\n",
      "\tTrain loss: 0.00274, Accuracy: 6566/6768 (97.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1523/1692 (90.00%)\n",
      "\tTest loss: 0.00187, Accuracy: 534/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.31669\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.36466\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.21799\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.19890\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.36719\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.37673\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.12381\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.36984\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.22329\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.32315\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.22883\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.12631\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.25789\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.07526\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.09622\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.22583\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.28701\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.11168\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.16695\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.18800\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.04655\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.13686\n",
      "\tTrain loss: 0.00213, Accuracy: 6627/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1537/1692 (90.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 555/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.24221\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.10279\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.18424\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.20946\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.07106\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.24425\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.07938\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.27674\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.10601\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.19090\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.24498\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.07519\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.24416\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.06241\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.18073\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.14130\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.26045\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.21771\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.18559\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.06991\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.10417\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.22871\n",
      "\tTrain loss: 0.00239, Accuracy: 6611/6768 (97.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1536/1692 (90.00%)\n",
      "\tTest loss: 0.00191, Accuracy: 519/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.16357\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.48368\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.17069\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.18370\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.15530\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.12556\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.07908\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.11703\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.07880\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.15240\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.30249\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.40283\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.11237\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.09797\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.14160\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.11573\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.02880\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.23818\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.15710\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.08673\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.30954\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.05183\n",
      "\tTrain loss: 0.00256, Accuracy: 6588/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1532/1692 (90.00%)\n",
      "\tTest loss: 0.00183, Accuracy: 534/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.07558\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.37678\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.06561\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.07634\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.13150\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.27906\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.04779\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.23998\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.10326\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.24123\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.08381\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.12034\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.34651\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.35105\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.11151\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.10803\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.31512\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.09202\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.17119\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.04615\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.32437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.07552\n",
      "\tTrain loss: 0.00226, Accuracy: 6612/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1533/1692 (90.00%)\n",
      "\tTest loss: 0.00190, Accuracy: 532/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.18310\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.08389\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.12844\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.13373\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.26589\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.16045\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.04036\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.41600\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.42017\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.26977\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.21828\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.12039\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.28495\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.02804\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.29244\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.09276\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.08135\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.23191\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.29870\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.03422\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.19202\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.05161\n",
      "\tTrain loss: 0.00248, Accuracy: 6588/6768 (97.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1517/1692 (89.00%)\n",
      "\tTest loss: 0.00180, Accuracy: 575/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.21037\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.36508\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.31073\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.08126\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.04056\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.27182\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.08118\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.17820\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.17580\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.10968\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.13041\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.25736\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.41718\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.05402\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.33691\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.37234\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.17341\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.03456\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.17143\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.10151\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.05944\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.08673\n",
      "\tTrain loss: 0.00199, Accuracy: 6647/6768 (98.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1544/1692 (91.00%)\n",
      "\tTest loss: 0.00178, Accuracy: 543/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.18596\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.53553\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.22642\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.15259\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.09258\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.10793\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.11802\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.19571\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.29335\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.53208\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.16908\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.08209\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.59187\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.29292\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.20587\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.15701\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.09390\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.41022\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.24133\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.07907\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.08699\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.06382\n",
      "\tTrain loss: 0.00203, Accuracy: 6632/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1553/1692 (91.00%)\n",
      "\tTest loss: 0.00184, Accuracy: 550/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.05569\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.10585\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.37722\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.14218\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.13048\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.30201\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.13649\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.25771\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.29445\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.11055\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.04235\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.06633\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.30858\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.11622\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.07524\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.03568\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.21879\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.23714\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.07279\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.19176\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.20874\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.43141\n",
      "\tTrain loss: 0.00231, Accuracy: 6606/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1543/1692 (91.00%)\n",
      "\tTest loss: 0.00186, Accuracy: 541/1772 (30.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.24503\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.50729\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.13768\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.25980\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.23603\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.13275\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.16356\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.17740\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.07369\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.07787\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.25724\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.08945\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.31345\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.10937\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.11381\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.18405\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.15329\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.23130\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.45276\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.10966\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.33972\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.17374\n",
      "\tTrain loss: 0.00169, Accuracy: 6660/6768 (98.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1565/1692 (92.00%)\n",
      "\tTest loss: 0.00184, Accuracy: 525/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.20784\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.51066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.18341\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.35533\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.20499\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.23727\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.02486\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.21573\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.17478\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.17134\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.19355\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.24910\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.37153\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.19125\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.04127\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.06553\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.03888\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.11272\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.09764\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.34368\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.08347\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.05411\n",
      "\tTrain loss: 0.00188, Accuracy: 6643/6768 (98.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1562/1692 (92.00%)\n",
      "\tTest loss: 0.00181, Accuracy: 548/1772 (30.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9249408983451537\n",
      "Best test accuracy:\n",
      "0.3295711060948081\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(lstm, lstm_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8fc3d",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67e55b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.55227\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.43199\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.49540\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.55626\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.34794\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.35239\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.35925\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.41089\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.47074\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.29287\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.39019\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.36379\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.33574\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.37152\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.45336\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.39079\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.40458\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.35431\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.52187\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.32453\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.41842\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.36596\n",
      "\tTrain loss: 0.04286, Accuracy: 2037/6768 (30.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 491/1692 (29.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 506/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.37514\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.41665\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.39615\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.41102\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.38628\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.38812\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.35930\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.38290\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.39362\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.30635\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.37291\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.38029\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.35388\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.39294\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.45536\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.34938\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.36754\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.37998\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.45163\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.29348\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.40841\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.38633\n",
      "\tTrain loss: 0.04244, Accuracy: 2261/6768 (33.00%)\n",
      "\tValidation loss: 0.00081, Accuracy: 527/1692 (31.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 499/1772 (28.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.40337\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.35781\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.37785\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.35346\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.41146\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.36864\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.31783\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.40090\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.39315\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.38406\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.36929\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.38488\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.34219\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.36251\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.37156\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.35803\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.39301\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.32319\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.36313\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.36136\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.38771\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.40000\n",
      "\tTrain loss: 0.04187, Accuracy: 2452/6768 (36.00%)\n",
      "\tValidation loss: 0.00080, Accuracy: 586/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 519/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.35645\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.34364\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.34452\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.45298\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.41468\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.40353\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.29654\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.40001\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.34855\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.33795\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.30563\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.38096\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.33297\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.33853\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.39915\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.33883\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.44332\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.32529\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.33642\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.33787\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.30612\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.43422\n",
      "\tTrain loss: 0.04131, Accuracy: 2517/6768 (37.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 585/1692 (34.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 576/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.36409\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.32888\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.36579\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.44431\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.32476\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.30880\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.27031\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.36374\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.33813\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.28117\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.37287\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.41068\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.30464\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.30487\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.38711\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.25797\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.40604\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.31779\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.42088\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.27356\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.35371\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.38573\n",
      "\tTrain loss: 0.04062, Accuracy: 2706/6768 (39.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 606/1692 (35.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.37951\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.32837\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.29833\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.36693\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.30791\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.29035\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.17126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.34824\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.34704\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.36948\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.29944\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.46816\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.26641\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.36709\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.38401\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.27302\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.35661\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.31250\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.29963\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.25626\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.29964\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.35669\n",
      "\tTrain loss: 0.03985, Accuracy: 2754/6768 (40.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 605/1692 (35.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 567/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.35109\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.25804\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.29065\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.34611\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.32142\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.29261\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.19353\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.38989\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.27126\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.20989\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.36504\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.39843\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.24488\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.27076\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.32696\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.17352\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.28039\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.30456\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.37796\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.30548\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.31330\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.33921\n",
      "\tTrain loss: 0.03896, Accuracy: 2985/6768 (44.00%)\n",
      "\tValidation loss: 0.00077, Accuracy: 657/1692 (38.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 568/1772 (32.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.40249\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.20451\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.28456\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.28312\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.30893\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.25104\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.19121\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.30682\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.22930\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.22812\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.19663\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.38880\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.20484\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.21420\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.24563\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.25379\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.36757\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.34178\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.26238\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.21063\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.32693\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.38618\n",
      "\tTrain loss: 0.03819, Accuracy: 3107/6768 (45.00%)\n",
      "\tValidation loss: 0.00076, Accuracy: 671/1692 (39.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 562/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.36972\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.20526\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.20768\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.26834\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.26438\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.30507\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.19736\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.36941\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.38886\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.15874\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.26000\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.42499\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.21122\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.31769\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.29112\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.15454\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.23386\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.24332\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.29556\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 1.08959\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.25747\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.34027\n",
      "\tTrain loss: 0.03704, Accuracy: 3206/6768 (47.00%)\n",
      "\tValidation loss: 0.00075, Accuracy: 685/1692 (40.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 590/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.35005\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.14028\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.10562\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.27133\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.22701\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.30455\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.17880\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.26470\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.34920\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.14424\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.18963\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.39590\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.19913\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.22800\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.34369\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.08244\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.17110\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.23632\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.22592\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.17729\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.14924\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.31928\n",
      "\tTrain loss: 0.03610, Accuracy: 3365/6768 (49.00%)\n",
      "\tValidation loss: 0.00074, Accuracy: 710/1692 (41.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 588/1772 (33.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.18462\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.24425\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 1.16840\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.32784\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.16063\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.40049\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.06495\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.32679\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.22659\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.16670\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.14691\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.37735\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.09207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.08210\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.23180\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.17843\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.15770\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.12540\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.25411\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.09805\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.09986\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.15964\n",
      "\tTrain loss: 0.03507, Accuracy: 3495/6768 (51.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 725/1692 (42.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 561/1772 (31.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 1.26133\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.08268\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.15780\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.24194\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.13063\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.31166\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.00836\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.19112\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.21798\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 0.98514\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.03351\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.23913\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.05559\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.25187\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.16029\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 0.98059\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.07236\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.14910\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.29207\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.17244\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.10847\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.25720\n",
      "\tTrain loss: 0.03359, Accuracy: 3719/6768 (54.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 828/1692 (48.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 627/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 1.23471\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.14739\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 0.96260\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.16984\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.16834\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.35462\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 0.98694\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.16324\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.11151\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.03757\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.97092\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.57379\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.07824\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.09077\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.22931\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.08171\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.11185\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.12327\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.22147\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.19075\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.96405\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.14945\n",
      "\tTrain loss: 0.03252, Accuracy: 3812/6768 (56.00%)\n",
      "\tValidation loss: 0.00068, Accuracy: 853/1692 (50.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 626/1772 (35.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 1.01520\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.24363\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 1.04802\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.20284\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.08146\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.27421\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.06581\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.25757\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.03100\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 0.99452\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.96522\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.35457\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.99055\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.07700\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.15607\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.02455\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.23576\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.08300\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.25195\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.09164\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 0.89329\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.15144\n",
      "\tTrain loss: 0.03030, Accuracy: 4155/6768 (61.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 953/1692 (56.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 672/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 1.02525\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.20879\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.04901\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.17666\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 0.97302\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 1.15223\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 0.92522\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.05482\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.10431\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 0.98262\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 1.00549\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.27191\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.95791\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.15012\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.02144\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 1.07950\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 0.98129\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 0.95443\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.13561\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 1.00888\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 0.92194\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.15065\n",
      "\tTrain loss: 0.02999, Accuracy: 4127/6768 (60.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 925/1692 (54.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 669/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 1.06664\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.08343\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 0.94887\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.12316\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 0.96552\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 1.16810\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.08350\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 0.99681\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.03629\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 0.85610\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.86558\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.24552\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 1.10314\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.98530\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 0.96681\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 0.92773\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 0.95642\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.02379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.16898\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 1.07664\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 0.81781\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.07072\n",
      "\tTrain loss: 0.02838, Accuracy: 4322/6768 (63.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 940/1692 (55.00%)\n",
      "\tTest loss: 0.00081, Accuracy: 664/1772 (37.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 1.04098\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.09676\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.84089\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.05601\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.84426\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 1.04452\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 0.95576\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.05536\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 1.05750\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 0.88600\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.89061\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.34034\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.80083\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.89807\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.12344\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 0.88580\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.94130\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 0.91376\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.18347\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.93372\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.81695\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.01079\n",
      "\tTrain loss: 0.02640, Accuracy: 4568/6768 (67.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 1030/1692 (60.00%)\n",
      "\tTest loss: 0.00079, Accuracy: 705/1772 (39.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.81053\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.94397\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 0.77618\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.01111\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.74363\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.99879\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.00481\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.11333\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 0.94041\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 0.90637\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 0.85179\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 1.29068\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 0.70240\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.83496\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.97023\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.80691\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.02037\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 0.90714\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.12533\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.02813\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 0.84897\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.92639\n",
      "\tTrain loss: 0.02368, Accuracy: 4829/6768 (71.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1080/1692 (63.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 747/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.69119\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.10928\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.92074\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 0.91868\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 0.81701\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.91453\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.90742\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 0.86174\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.95007\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 0.81772\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.82323\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.22626\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.87505\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 0.96148\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.81277\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 0.81284\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.87154\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.94843\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.97546\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.83829\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.87657\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.96234\n",
      "\tTrain loss: 0.02307, Accuracy: 4900/6768 (72.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1091/1692 (64.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 724/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.84002\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 0.75795\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.84573\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.82478\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.80869\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.78331\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.88810\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 0.92489\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.73371\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.87366\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.68349\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 1.23687\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.85516\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.66971\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.74270\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.73816\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 0.79778\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 0.83910\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.03721\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.89093\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.77257\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.01009\n",
      "\tTrain loss: 0.02151, Accuracy: 5071/6768 (74.00%)\n",
      "\tValidation loss: 0.00050, Accuracy: 1133/1692 (66.00%)\n",
      "\tTest loss: 0.00080, Accuracy: 757/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.69713\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.80129\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.78938\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 0.75598\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.58378\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.67955\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.80561\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 0.83823\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 0.93396\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.91373\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.52215\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 1.04907\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.70047\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.78864\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 0.77701\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 0.66578\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 0.70850\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.80116\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.95232\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 1.03833\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.50284\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.90203\n",
      "\tTrain loss: 0.01976, Accuracy: 5182/6768 (76.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00048, Accuracy: 1146/1692 (67.00%)\n",
      "\tTest loss: 0.00082, Accuracy: 781/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.73910\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 0.78194\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.69621\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.66705\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 0.68525\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.71359\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.82577\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 0.82052\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.80665\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.64846\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.63292\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 0.97816\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 0.70893\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.74840\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.63969\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 0.51940\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 0.77357\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.74943\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 1.09935\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.78265\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.58431\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.98133\n",
      "\tTrain loss: 0.01861, Accuracy: 5325/6768 (78.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1192/1692 (70.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 758/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.68165\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.66944\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.62990\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 0.68256\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.70595\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.62493\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.66244\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 0.65149\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.85789\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.73978\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.70536\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 1.22174\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.65840\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.54882\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.83076\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.60207\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.76791\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.63561\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 1.00054\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.72758\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.55147\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.96840\n",
      "\tTrain loss: 0.01875, Accuracy: 5302/6768 (78.00%)\n",
      "\tValidation loss: 0.00046, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00084, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.87541\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.58734\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.68089\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.65434\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.65429\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.59867\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.59609\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.81364\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 0.74082\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.74550\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.67160\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.88248\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.76600\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.53931\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.61151\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 0.53181\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.50691\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.74628\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.82923\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.95676\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.55049\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.79181\n",
      "\tTrain loss: 0.01843, Accuracy: 5283/6768 (78.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1190/1692 (70.00%)\n",
      "\tTest loss: 0.00083, Accuracy: 785/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.66535\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.57834\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.66780\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 0.74526\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 0.67937\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.63758\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.82143\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 0.74388\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 0.66203\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.65064\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.51103\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.82231\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.68146\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.75485\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.60284\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 0.62602\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.75624\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 0.67184\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.54010\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.79421\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.64475\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 0.67194\n",
      "\tTrain loss: 0.01617, Accuracy: 5534/6768 (81.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1233/1692 (72.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 765/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.51875\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.60472\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.42661\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 0.58015\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.46561\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.51069\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.62699\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.60489\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 0.66124\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.64595\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.41853\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 1.31592\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.85310\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.50945\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.54803\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.48348\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.66384\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.71570\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.65941\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.88925\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.33967\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.68244\n",
      "\tTrain loss: 0.01519, Accuracy: 5594/6768 (82.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1266/1692 (74.00%)\n",
      "\tTest loss: 0.00090, Accuracy: 767/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.64978\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 0.62591\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.57488\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.61600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 0.50050\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.50370\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.50650\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.69379\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.51343\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 0.49265\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.49504\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.97394\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.65533\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.50756\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.45327\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.36423\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.53941\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.45812\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.66660\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.94478\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.33999\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 0.79288\n",
      "\tTrain loss: 0.01534, Accuracy: 5567/6768 (82.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1240/1692 (73.00%)\n",
      "\tTest loss: 0.00089, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.64535\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 0.84059\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.57114\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.73556\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.45855\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.37995\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.71808\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.38647\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.37587\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.54356\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.41819\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.88196\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.34983\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.56353\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.58411\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.32638\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 0.57491\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.66927\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.53427\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.85702\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.31788\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.89764\n",
      "\tTrain loss: 0.01375, Accuracy: 5698/6768 (84.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1298/1692 (76.00%)\n",
      "\tTest loss: 0.00089, Accuracy: 788/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.70954\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.36890\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.52850\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.68356\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.46251\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.42913\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.72198\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.82953\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.61315\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.70370\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.46867\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 0.79392\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.57670\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.49781\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.59567\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.24715\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.69947\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.73567\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.54186\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.77005\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.36660\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.65342\n",
      "\tTrain loss: 0.01396, Accuracy: 5678/6768 (83.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1307/1692 (77.00%)\n",
      "\tTest loss: 0.00094, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.47005\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.54977\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.52202\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.63589\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.41130\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.42406\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.48836\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.52447\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.66882\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.47872\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.57544\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.92449\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.47861\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.57289\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.36385\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.30051\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.50274\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.55165\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.53038\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.54136\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.38464\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 0.65618\n",
      "\tTrain loss: 0.01171, Accuracy: 5858/6768 (86.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1338/1692 (79.00%)\n",
      "\tTest loss: 0.00095, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.58550\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.43751\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.39682\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.58382\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.54217\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.57483\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.63003\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 0.36639\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.43958\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.51565\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.52915\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.75556\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.70262\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.47600\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.53583\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.29803\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.47251\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.52817\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.54404\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.67837\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.40333\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.52386\n",
      "\tTrain loss: 0.01071, Accuracy: 5959/6768 (88.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1335/1692 (78.00%)\n",
      "\tTest loss: 0.00094, Accuracy: 816/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.37535\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.57743\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.49523\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.45747\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.43930\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.37878\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.46331\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.50785\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.48365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.50448\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.32353\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.76983\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.40379\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.46562\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.48761\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.21916\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.35928\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.39549\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.68562\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.52218\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.43020\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.54723\n",
      "\tTrain loss: 0.01181, Accuracy: 5858/6768 (86.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1353/1692 (79.00%)\n",
      "\tTest loss: 0.00098, Accuracy: 760/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.68172\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.53339\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.29622\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.58235\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.39789\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.52572\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.40367\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 0.48820\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.55990\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.47713\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.39801\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.57923\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.50441\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.28577\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.26278\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.28953\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.44266\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.36211\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.64332\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.84159\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.25551\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.40236\n",
      "\tTrain loss: 0.01109, Accuracy: 5897/6768 (87.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1330/1692 (78.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 749/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.46434\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.55488\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.59175\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.47162\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.42159\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.39915\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.49674\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.46600\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.50427\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.47501\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.46320\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.45448\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.43908\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.39174\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.47240\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 0.26808\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.23314\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.42652\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.33007\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.54958\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.33565\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.51848\n",
      "\tTrain loss: 0.01180, Accuracy: 5820/6768 (85.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1345/1692 (79.00%)\n",
      "\tTest loss: 0.00107, Accuracy: 776/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.46561\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.65739\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.49435\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.56763\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 0.25648\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.43141\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.59799\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.62246\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.20077\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.60891\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.46600\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.64093\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.64365\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.35334\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.27635\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.34829\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.53676\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.63405\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.61661\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.81713\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.25133\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.55073\n",
      "\tTrain loss: 0.00908, Accuracy: 6119/6768 (90.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1412/1692 (83.00%)\n",
      "\tTest loss: 0.00099, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.36072\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.51567\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.36164\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.46346\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.30268\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.37755\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.34676\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.44974\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.31298\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.28760\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.23375\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.80447\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.42052\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.42938\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.37337\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.25927\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.36816\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.38387\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.59402\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.48674\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.35204\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.44532\n",
      "\tTrain loss: 0.00923, Accuracy: 6071/6768 (89.00%)\n",
      "\tValidation loss: 0.00031, Accuracy: 1363/1692 (80.00%)\n",
      "\tTest loss: 0.00101, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.31762\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.52647\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.28956\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.67600\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.34607\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.26713\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.61045\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.33556\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.44516\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.31225\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.32415\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.58267\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.45968\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.29849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.36710\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.29631\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.52781\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.34452\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.34075\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.41936\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.25030\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.44508\n",
      "\tTrain loss: 0.00896, Accuracy: 6075/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1407/1692 (83.00%)\n",
      "\tTest loss: 0.00108, Accuracy: 745/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.29876\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.32609\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.25842\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.38375\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.26789\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.35994\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.56027\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.59270\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.33473\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.39657\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.29232\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.82450\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.53362\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.37777\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.42404\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.25279\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.35475\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.43061\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.38923\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.46950\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.24924\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.26779\n",
      "\tTrain loss: 0.00802, Accuracy: 6166/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1428/1692 (84.00%)\n",
      "\tTest loss: 0.00107, Accuracy: 794/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.31661\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 0.29418\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.37927\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.44616\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.37027\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.31849\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.42022\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.27887\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.36623\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.40187\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.33704\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.31680\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.39771\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.23524\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.53181\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.23689\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.47569\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.23726\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.40748\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.62954\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.22927\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.41089\n",
      "\tTrain loss: 0.00891, Accuracy: 6070/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1405/1692 (83.00%)\n",
      "\tTest loss: 0.00114, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.31307\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 0.60014\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.41513\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.56945\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.28557\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.32035\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.35000\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.44390\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.24605\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.30961\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.23307\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.56386\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.23693\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.21041\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.26820\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.21248\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.27573\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.35688\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.55607\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.70466\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.40459\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 0.33748\n",
      "\tTrain loss: 0.00717, Accuracy: 6225/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1440/1692 (85.00%)\n",
      "\tTest loss: 0.00110, Accuracy: 798/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.39120\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.29655\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.30401\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.57137\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.21340\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.19314\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.64294\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.39788\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.31063\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.30776\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.32199\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.40218\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.39180\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.12065\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.31870\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.28695\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.40730\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.33539\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.43560\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.63203\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.36817\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.33192\n",
      "\tTrain loss: 0.00746, Accuracy: 6210/6768 (91.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1428/1692 (84.00%)\n",
      "\tTest loss: 0.00113, Accuracy: 790/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.36317\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 0.44689\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.38322\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.35088\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.31412\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.48144\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.33091\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.29142\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.38737\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.35606\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.38301\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.48305\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.28014\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.38169\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.28056\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.17615\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.30597\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.33631\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.59351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.47431\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.37517\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.32783\n",
      "\tTrain loss: 0.00689, Accuracy: 6242/6768 (92.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1449/1692 (85.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 772/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.59143\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.36633\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.29393\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.66900\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.27605\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.32364\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.26714\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.42200\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.38280\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.31853\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.24533\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.43322\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.56311\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.25395\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.09172\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.15523\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.27515\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.40706\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.50846\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.54515\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.24251\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.25433\n",
      "\tTrain loss: 0.00744, Accuracy: 6181/6768 (91.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1402/1692 (82.00%)\n",
      "\tTest loss: 0.00128, Accuracy: 728/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.31078\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.43982\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.33862\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.51038\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.09775\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.26056\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.49427\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.27387\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.22773\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.29080\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.36248\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.46537\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.21584\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.15295\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.25847\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.18129\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.22143\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.21810\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.51121\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.54124\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.29651\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.35556\n",
      "\tTrain loss: 0.00567, Accuracy: 6362/6768 (94.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1461/1692 (86.00%)\n",
      "\tTest loss: 0.00117, Accuracy: 793/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.39841\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.21901\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.22460\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.39982\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.26434\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.23798\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.40233\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.24336\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.49943\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.10051\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.37819\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.56992\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.37851\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.25263\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.14677\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.11163\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.19157\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.32054\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.50334\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.48689\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.18496\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.28551\n",
      "\tTrain loss: 0.00553, Accuracy: 6370/6768 (94.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1475/1692 (87.00%)\n",
      "\tTest loss: 0.00119, Accuracy: 779/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.11651\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.36976\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.16247\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 0.31490\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.31109\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.17477\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.14829\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.34284\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.16043\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.29506\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.26251\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.46666\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.39598\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.26574\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.24032\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.15819\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.47395\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.28992\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.26522\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.35953\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.32975\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.32376\n",
      "\tTrain loss: 0.00696, Accuracy: 6227/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1439/1692 (85.00%)\n",
      "\tTest loss: 0.00123, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.37368\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.43997\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.26731\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.34350\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.16259\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.27280\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.27877\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.25826\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.28399\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.34713\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.24197\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.25699\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.22855\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.25449\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.21323\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.16939\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.39774\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.42315\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.36933\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.69016\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.24113\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.17784\n",
      "\tTrain loss: 0.00579, Accuracy: 6342/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1444/1692 (85.00%)\n",
      "\tTest loss: 0.00129, Accuracy: 762/1772 (43.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.23067\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.36130\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.35726\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.49047\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.23124\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.19466\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.38037\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.36495\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.17808\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.39773\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.26689\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.31912\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.23373\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.14624\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.13275\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.11089\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.22268\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.21611\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.39062\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.43224\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.26912\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.20058\n",
      "\tTrain loss: 0.00565, Accuracy: 6357/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1462/1692 (86.00%)\n",
      "\tTest loss: 0.00126, Accuracy: 773/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.24351\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.40814\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.21418\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.33941\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.32544\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.22470\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.48922\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.27209\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.31650\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.21357\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.20365\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.37252\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.21606\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.12331\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.22677\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.06879\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.33951\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.23515\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.55165\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.41806\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.24548\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.25767\n",
      "\tTrain loss: 0.00634, Accuracy: 6290/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1445/1692 (85.00%)\n",
      "\tTest loss: 0.00131, Accuracy: 732/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.41822\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.24897\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.30764\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.24304\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.15338\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.12474\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.32607\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.39112\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.18948\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.22928\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.23305\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.35274\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.13786\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.16051\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.16243\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.10382\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.10821\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.30112\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.29507\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.36567\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.23563\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.25155\n",
      "\tTrain loss: 0.00594, Accuracy: 6282/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1456/1692 (86.00%)\n",
      "\tTest loss: 0.00134, Accuracy: 748/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.40733\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.19260\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.16227\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.38575\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.16584\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.18076\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.33569\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.24047\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.18873\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.14305\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.17843\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.51838\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.24832\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.13967\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.08005\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.08075\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.23523\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.30000\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.31123\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.33499\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.14086\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.15959\n",
      "\tTrain loss: 0.00608, Accuracy: 6299/6768 (93.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1447/1692 (85.00%)\n",
      "\tTest loss: 0.00129, Accuracy: 773/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.42483\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.32848\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.13151\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.22113\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.37926\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.20463\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.24712\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.20005\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.17626\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.17802\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.25981\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.39369\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.13353\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.23132\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.09645\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.20565\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.19423\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.21492\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.24948\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.30378\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.15450\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.12865\n",
      "\tTrain loss: 0.00476, Accuracy: 6372/6768 (94.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1496/1692 (88.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 783/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.19025\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.19094\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.23402\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.26055\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.11174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.14110\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.22529\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.27526\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.21143\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.12290\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.15405\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.44728\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.31338\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.22885\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.06396\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.12551\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.22898\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.32275\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.27607\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.37590\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.20490\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.17209\n",
      "\tTrain loss: 0.00560, Accuracy: 6333/6768 (93.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1474/1692 (87.00%)\n",
      "\tTest loss: 0.00127, Accuracy: 786/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.32941\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.57641\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.22636\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.60398\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.07179\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.36578\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.20915\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.26915\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.18359\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.25093\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.05765\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.19297\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.19167\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.09893\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.07128\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.32321\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.18890\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.25579\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.20270\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.40413\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.18219\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.14070\n",
      "\tTrain loss: 0.00366, Accuracy: 6502/6768 (96.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1511/1692 (89.00%)\n",
      "\tTest loss: 0.00133, Accuracy: 822/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.07652\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.23420\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.19163\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.28900\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.07381\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.32065\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.32586\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.53536\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.31813\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.15065\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.33215\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.25087\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.10735\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.26395\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.14941\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.16122\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.14783\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.26481\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.27527\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.27963\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.40586\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.04169\n",
      "\tTrain loss: 0.00318, Accuracy: 6542/6768 (96.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1518/1692 (89.00%)\n",
      "\tTest loss: 0.00131, Accuracy: 809/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.11899\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.30442\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.12132\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.15444\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.28138\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.19607\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.22083\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.23243\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.12723\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.27116\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.14789\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.11198\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.45021\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.07687\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.09977\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.16527\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.24531\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.88127\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.29687\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.37414\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.09342\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.16820\n",
      "\tTrain loss: 0.00392, Accuracy: 6465/6768 (95.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1503/1692 (88.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 803/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.14846\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.22117\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.17577\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.42129\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.21919\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.09518\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.28272\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.18335\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.12999\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.09252\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.16682\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.28883\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.23142\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.36037\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.13928\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.11610\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.25070\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.21962\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.40422\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.22078\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.16932\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.17549\n",
      "\tTrain loss: 0.00410, Accuracy: 6469/6768 (95.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1499/1692 (88.00%)\n",
      "\tTest loss: 0.00132, Accuracy: 813/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.08976\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.36569\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.10095\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.39825\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.29485\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.30985\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.34926\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.37864\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.07027\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.09012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.19233\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.10696\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.39011\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.26421\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.08545\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.26099\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.31897\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.26337\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.30996\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.38454\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.27508\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.09371\n",
      "\tTrain loss: 0.00321, Accuracy: 6528/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1521/1692 (89.00%)\n",
      "\tTest loss: 0.00136, Accuracy: 809/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.20197\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.29108\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.10167\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.17216\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.12618\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.12798\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.21369\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.33012\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.20342\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.23474\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.19934\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.14865\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.35170\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.19026\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.24008\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.08377\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.23261\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.41549\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.35520\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.24945\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.36835\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.07606\n",
      "\tTrain loss: 0.00562, Accuracy: 6316/6768 (93.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00144, Accuracy: 770/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.08077\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.14993\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.08790\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.32695\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.17009\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.06813\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.19179\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.11757\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.19185\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.09869\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.08789\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.27285\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.46148\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.09127\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.38919\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.09714\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.18424\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.28478\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.12722\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.27203\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.18792\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.24644\n",
      "\tTrain loss: 0.00517, Accuracy: 6367/6768 (94.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1474/1692 (87.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 774/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.12186\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.08640\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.14993\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.21398\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.17257\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.21635\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.25253\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.11967\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.46259\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.11051\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.18711\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.24517\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.20728\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.19973\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.11140\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.10722\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.06073\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.14094\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.28330\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.14406\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.09237\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.27664\n",
      "\tTrain loss: 0.00375, Accuracy: 6486/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1508/1692 (89.00%)\n",
      "\tTest loss: 0.00142, Accuracy: 797/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.16442\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.36606\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.18106\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.32951\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.09438\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.35414\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.25913\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.34606\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.14510\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.28229\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.28164\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.56193\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.12273\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.12960\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.35468\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.11161\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.02314\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.16531\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.23711\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.36711\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.29659\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.04994\n",
      "\tTrain loss: 0.00297, Accuracy: 6562/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1546/1692 (91.00%)\n",
      "\tTest loss: 0.00135, Accuracy: 824/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.04678\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.16583\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.14326\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.07045\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.04767\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.10700\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.16292\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.09542\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.13185\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.05215\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.06063\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.05949\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.17638\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.07009\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.25996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.10585\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.11255\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.24969\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.13815\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.28013\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.08212\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.14928\n",
      "\tTrain loss: 0.00237, Accuracy: 6625/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1545/1692 (91.00%)\n",
      "\tTest loss: 0.00139, Accuracy: 826/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.15387\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.25384\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.28115\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.23083\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.27435\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.07412\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.22891\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.24964\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.19415\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.11280\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.02807\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.13811\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.12282\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.27830\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.13296\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.09728\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.11971\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.19848\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.22077\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.37355\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.16352\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.18856\n",
      "\tTrain loss: 0.00402, Accuracy: 6466/6768 (95.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1517/1692 (89.00%)\n",
      "\tTest loss: 0.00143, Accuracy: 818/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.09698\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.10011\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.12052\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.14683\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.06424\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.12562\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.10146\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.26017\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.16006\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.14729\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.17161\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.10718\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.23122\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.09425\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.12944\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.09585\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.17450\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.23774\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.18021\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.21809\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.16617\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.06436\n",
      "\tTrain loss: 0.00282, Accuracy: 6544/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1542/1692 (91.00%)\n",
      "\tTest loss: 0.00143, Accuracy: 803/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.18086\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.24314\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.08316\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.11703\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.09644\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.17228\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.23165\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.15084\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.26522\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.11131\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.24553\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.25288\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.16705\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.21514\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.07360\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.09033\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.15267\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.17872\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.29983\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.17014\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.14935\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.23856\n",
      "\tTrain loss: 0.00320, Accuracy: 6508/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1549/1692 (91.00%)\n",
      "\tTest loss: 0.00147, Accuracy: 794/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.21411\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.38063\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.10522\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.32743\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.20061\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.13534\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.22247\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.19265\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.22796\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.22495\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.17361\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.39960\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.16302\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.12461\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.03225\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.09973\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.10578\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.39041\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.20339\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.15450\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.05162\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.08651\n",
      "\tTrain loss: 0.00253, Accuracy: 6571/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1558/1692 (92.00%)\n",
      "\tTest loss: 0.00145, Accuracy: 812/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.05646\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.29465\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.23033\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.33553\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.09639\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.20325\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.18614\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.47017\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.57716\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.22944\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.15757\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.39473\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.21642\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.16211\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.07193\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.10714\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.06969\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.20609\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.28213\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.37257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.26092\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.05464\n",
      "\tTrain loss: 0.00400, Accuracy: 6450/6768 (95.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1514/1692 (89.00%)\n",
      "\tTest loss: 0.00151, Accuracy: 788/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.07566\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.12164\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.06908\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.20895\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.08815\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.09087\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.20358\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.38749\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.04144\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.20328\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.22404\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.18574\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.10972\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.20521\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.06403\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.07150\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.23434\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.17040\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.09065\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.10735\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.49350\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.21223\n",
      "\tTrain loss: 0.00233, Accuracy: 6604/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1563/1692 (92.00%)\n",
      "\tTest loss: 0.00138, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.37197\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.23592\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.03651\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.28967\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.13177\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.12303\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.31597\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.15046\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.05155\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.05234\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.09422\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.05073\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.09229\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.15004\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.11533\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.06939\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.04978\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.14563\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.20848\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.23129\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.15962\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.02777\n",
      "\tTrain loss: 0.00330, Accuracy: 6502/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1538/1692 (90.00%)\n",
      "\tTest loss: 0.00148, Accuracy: 780/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.12634\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.15504\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.08668\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.14378\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.13850\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.09939\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.21083\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.13558\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.06906\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.14300\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.08083\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.08125\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.18054\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.10117\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.19431\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.07206\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.13883\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.21074\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.35386\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.24068\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.09603\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.03725\n",
      "\tTrain loss: 0.00224, Accuracy: 6614/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1582/1692 (93.00%)\n",
      "\tTest loss: 0.00140, Accuracy: 798/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.15303\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.17978\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.08946\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.08654\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.27115\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.13443\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.17635\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.34294\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.04136\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.18175\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.11719\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.31807\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.03845\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.31969\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.05611\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.15187\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.10633\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.12014\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.24606\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.24326\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.34328\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.17947\n",
      "\tTrain loss: 0.00426, Accuracy: 6452/6768 (95.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 775/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.27303\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.08752\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.26741\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.20674\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.21937\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.14743\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.18515\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.06890\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.19831\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.34621\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.15805\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.14758\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.22127\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.24541\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.12509\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.03333\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.11508\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.12924\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.13387\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.22119\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.09011\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.17064\n",
      "\tTrain loss: 0.00243, Accuracy: 6584/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1560/1692 (92.00%)\n",
      "\tTest loss: 0.00151, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.06918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.11906\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.14493\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.26018\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.12758\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.13792\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.20366\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.31283\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.15033\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.08287\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.27300\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.31101\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.37099\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.21110\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.30378\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.18902\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.06656\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.18480\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.18602\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.21601\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.08524\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.06977\n",
      "\tTrain loss: 0.00267, Accuracy: 6565/6768 (97.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1545/1692 (91.00%)\n",
      "\tTest loss: 0.00149, Accuracy: 792/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.26169\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.13335\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.10306\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.12944\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.07087\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.09458\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.13838\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.11763\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.09829\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.11618\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.13233\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.06511\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.10442\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.12027\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.25111\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.07077\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.10018\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.27948\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.24765\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.19745\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.10649\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.03789\n",
      "\tTrain loss: 0.00228, Accuracy: 6599/6768 (97.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1562/1692 (92.00%)\n",
      "\tTest loss: 0.00155, Accuracy: 784/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.01497\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.18381\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.18297\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.34982\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.09036\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.07528\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.13258\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.07028\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.16602\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.03764\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.19095\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.14954\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.35813\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.10535\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.02990\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.06142\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.20053\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.26502\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.15435\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.19955\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.24174\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.17728\n",
      "\tTrain loss: 0.00394, Accuracy: 6463/6768 (95.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1528/1692 (90.00%)\n",
      "\tTest loss: 0.00163, Accuracy: 740/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.10563\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.40336\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.17323\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.22387\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.15769\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.15716\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.12398\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.10362\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.07650\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.15141\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.04286\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.14290\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.18388\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.12781\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.08778\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.12447\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.06119\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.22245\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.17720\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.11769\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.12680\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.08254\n",
      "\tTrain loss: 0.00173, Accuracy: 6653/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1583/1692 (93.00%)\n",
      "\tTest loss: 0.00153, Accuracy: 806/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.21007\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.21745\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.04483\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.26097\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.05432\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.05202\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.14572\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.06984\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.09338\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.02902\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.09714\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.30166\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.19894\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.07835\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.21510\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.06532\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.07414\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.20605\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.22266\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.27256\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.06426\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.17929\n",
      "\tTrain loss: 0.00267, Accuracy: 6560/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1546/1692 (91.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 789/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.11197\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.37476\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.11297\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.16672\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.07761\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.11773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.12108\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.02729\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.15623\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.02515\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.06843\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.06113\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.11253\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.06541\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.02556\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.09496\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.30066\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.09644\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.18259\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.14072\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.16277\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.09529\n",
      "\tTrain loss: 0.00226, Accuracy: 6591/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1562/1692 (92.00%)\n",
      "\tTest loss: 0.00160, Accuracy: 795/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.06218\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.36183\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.08510\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.19438\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.05374\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.08464\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.19863\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.07454\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.06560\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.05848\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.18918\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.13800\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.11698\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.03460\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.08475\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.11101\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.07931\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.04960\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.14651\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.19514\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.12363\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.31033\n",
      "\tTrain loss: 0.00185, Accuracy: 6626/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1563/1692 (92.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.22915\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.13556\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.13299\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.39148\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.08526\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.12644\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.18857\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.12453\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.08397\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.06844\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.12649\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.05388\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.05821\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.12074\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.03815\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.05458\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.16354\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.21435\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.23399\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.11774\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.27876\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.04388\n",
      "\tTrain loss: 0.00176, Accuracy: 6643/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1571/1692 (92.00%)\n",
      "\tTest loss: 0.00152, Accuracy: 814/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.14911\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.08753\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.07428\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.24349\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.03344\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.30898\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.14935\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.18169\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.13934\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.08661\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.10707\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.13071\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.06747\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.10645\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.13773\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.04719\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.12556\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.23436\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.11776\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.15904\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.01277\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.01856\n",
      "\tTrain loss: 0.00227, Accuracy: 6603/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1554/1692 (91.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 777/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.15119\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.09985\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.08241\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.34539\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.05441\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.02816\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.09818\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.10143\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.15956\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.06045\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.17649\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.18115\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.01656\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.35290\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.16531\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.05454\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.21651\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.15162\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.18578\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.21816\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.21351\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.18398\n",
      "\tTrain loss: 0.00473, Accuracy: 6393/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1483/1692 (87.00%)\n",
      "\tTest loss: 0.00174, Accuracy: 751/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.05333\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.14271\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.21469\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.19175\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.10239\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.17125\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.09078\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.08737\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.13157\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.04242\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.15027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.36249\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.10498\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.06970\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.06289\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.09705\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.10755\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.09131\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.09929\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.22329\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.25445\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.02901\n",
      "\tTrain loss: 0.00159, Accuracy: 6659/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1590/1692 (93.00%)\n",
      "\tTest loss: 0.00154, Accuracy: 823/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.08254\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.07600\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.06979\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.18179\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.08491\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.06365\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.36278\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.05412\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.05645\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.03853\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.02757\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.25645\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.06510\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.07845\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.04895\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.04094\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.45980\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.08807\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.19044\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.12689\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.19200\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.10262\n",
      "\tTrain loss: 0.00253, Accuracy: 6589/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1539/1692 (90.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 808/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.11036\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.15386\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.11705\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.21905\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.09854\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.01391\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.20295\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.20219\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.03148\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.11146\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.02400\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.03326\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.22946\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.05313\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.12440\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.03152\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.03826\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.12945\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.23092\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.34228\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.02521\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.01857\n",
      "\tTrain loss: 0.00273, Accuracy: 6569/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1555/1692 (91.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 824/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.10102\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.18880\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.03980\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.13219\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.12603\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.19663\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.07800\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.13934\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.13486\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.30499\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.16572\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.32459\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.02132\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.05013\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.24743\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.24790\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.21672\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.45808\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.28244\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.14441\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.16482\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.06317\n",
      "\tTrain loss: 0.00203, Accuracy: 6613/6768 (97.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1575/1692 (93.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 776/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.13748\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.20185\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.12501\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.08045\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.18550\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.16414\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.11057\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.16433\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.18950\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.04017\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.02130\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.07196\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.05092\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.19669\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.04298\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.06266\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.19598\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.18282\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.11387\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.22313\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.10010\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.04972\n",
      "\tTrain loss: 0.00143, Accuracy: 6664/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1580/1692 (93.00%)\n",
      "\tTest loss: 0.00153, Accuracy: 841/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.16401\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.10841\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.15785\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.16608\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.05933\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.03460\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.07439\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.06478\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.35308\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.05051\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.14548\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.22915\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.11032\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.04465\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.02706\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.07472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.07270\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.03905\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.08250\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.25235\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.08941\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.05635\n",
      "\tTrain loss: 0.00155, Accuracy: 6672/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1574/1692 (93.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 787/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.09454\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.18980\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.17576\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.12393\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.04040\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.12778\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.19442\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.08308\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.13605\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.02800\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.02354\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.16899\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.11679\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.05170\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.10053\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.06323\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.06379\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.04691\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.08723\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.22793\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.09237\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.07511\n",
      "\tTrain loss: 0.00120, Accuracy: 6683/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1585/1692 (93.00%)\n",
      "\tTest loss: 0.00158, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.04528\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.08201\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.12095\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.08190\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.04944\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.16588\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.28439\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.12315\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.08810\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.05019\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.08469\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.21709\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.07405\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.13815\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.15722\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.13650\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.23175\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.10759\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.33193\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.29917\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.06716\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.09544\n",
      "\tTrain loss: 0.00207, Accuracy: 6599/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1571/1692 (92.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 805/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.07438\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.07926\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.03663\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.30143\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.06348\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.04887\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.04518\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.07713\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.02949\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.02076\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.03280\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.12120\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.23239\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.02685\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.04226\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.12290\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.23706\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.04542\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.22153\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.20634\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.04090\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.15687\n",
      "\tTrain loss: 0.00129, Accuracy: 6679/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1592/1692 (94.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.02982\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.16550\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.02088\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.12113\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.21343\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.10660\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.06017\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.31196\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.39980\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.04316\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.12533\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.05479\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.02111\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.05508\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.17167\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.03377\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.07970\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.05816\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.17716\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.33212\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.10879\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.17235\n",
      "\tTrain loss: 0.00248, Accuracy: 6584/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1554/1692 (91.00%)\n",
      "\tTest loss: 0.00164, Accuracy: 786/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.09510\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.14786\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.20826\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.30631\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.04560\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.13277\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.03687\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.04350\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.17417\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.05607\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.05930\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.09025\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.03960\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.08443\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.07651\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.01864\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.02042\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.04868\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.03906\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.12809\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.03726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.09165\n",
      "\tTrain loss: 0.00193, Accuracy: 6623/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1576/1692 (93.00%)\n",
      "\tTest loss: 0.00165, Accuracy: 790/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.03062\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.08041\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.09922\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.04963\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.09002\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.22682\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.13212\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.13711\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.09167\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.01866\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.03518\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.14481\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.13801\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.07542\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.05711\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.01631\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.03439\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.20446\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.06138\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.08636\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.10423\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.18113\n",
      "\tTrain loss: 0.00355, Accuracy: 6507/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1552/1692 (91.00%)\n",
      "\tTest loss: 0.00176, Accuracy: 769/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.13437\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.15262\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.06919\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.07565\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.13496\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.08423\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.09774\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.09701\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.04731\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.04332\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.03748\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.24966\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.11346\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.05645\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.14516\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.01610\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.07807\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.04083\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.09441\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.16462\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.05097\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.18114\n",
      "\tTrain loss: 0.00132, Accuracy: 6676/6768 (98.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1597/1692 (94.00%)\n",
      "\tTest loss: 0.00159, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.09099\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.13482\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.05419\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.09447\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.05425\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.22135\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.03346\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.24435\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.02116\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.10395\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.13379\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.04064\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.03840\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.13249\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.02635\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.04703\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.14312\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.21598\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.12820\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.21690\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.12348\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.02266\n",
      "\tTrain loss: 0.00130, Accuracy: 6677/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1577/1692 (93.00%)\n",
      "\tTest loss: 0.00157, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.21946\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.07334\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.04911\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.18021\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.03476\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.39956\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.16884\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.03134\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.06681\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.23072\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.06081\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.03514\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.28469\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.11051\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.10724\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.03632\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.03474\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.13689\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.23046\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.36738\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.11200\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.06293\n",
      "\tTrain loss: 0.00113, Accuracy: 6676/6768 (98.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1596/1692 (94.00%)\n",
      "\tTest loss: 0.00162, Accuracy: 804/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.05059\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.42665\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.07508\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.12604\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.07189\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.05999\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.14612\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.08138\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.07708\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.20345\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.21536\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.17785\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.07596\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.11301\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.10513\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.06407\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.11370\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.09363\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.08343\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.10984\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.07607\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.05272\n",
      "\tTrain loss: 0.00240, Accuracy: 6584/6768 (97.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1540/1692 (91.00%)\n",
      "\tTest loss: 0.00167, Accuracy: 803/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.34580\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.09302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.09144\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.21504\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.02763\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.03615\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.11718\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.28481\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.11086\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.07059\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.04575\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.10624\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.02593\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.03791\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.03406\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.02432\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.03355\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.08546\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.09199\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.07477\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.20800\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.03435\n",
      "\tTrain loss: 0.00119, Accuracy: 6689/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1590/1692 (93.00%)\n",
      "\tTest loss: 0.00168, Accuracy: 819/1772 (46.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.943853427895981\n",
      "Best test accuracy:\n",
      "0.47460496613995484\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(gru, gru_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f35d1",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "589650cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.41283\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.35485\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.37245\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.46991\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.41459\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.38192\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.42502\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.39545\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.39187\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.38619\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.37064\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.41690\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.36859\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.37242\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.42283\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.40599\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.39007\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.33127\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.38842\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.40499\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.42001\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.33254\n",
      "\tTrain loss: 0.04313, Accuracy: 1867/6768 (27.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 457/1692 (27.00%)\n",
      "\tTest loss: 0.00077, Accuracy: 517/1772 (29.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.41464\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.39740\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.37182\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.36713\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.34821\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.34128\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.36192\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.39073\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.37436\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.34720\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.35812\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.39035\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.34614\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.34443\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.44041\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.31414\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.34480\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.33627\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.35285\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.28898\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.42373\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.32247\n",
      "\tTrain loss: 0.04109, Accuracy: 2524/6768 (37.00%)\n",
      "\tValidation loss: 0.00078, Accuracy: 619/1692 (36.00%)\n",
      "\tTest loss: 0.00074, Accuracy: 641/1772 (36.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.26359\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.36361\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.33215\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.40281\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.34370\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.20418\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.35753\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.32821\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.45831\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.35525\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.22956\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.30559\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.31826\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.32002\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.51805\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.21199\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.32833\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.34048\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.28341\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.21650\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.28914\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.31258\n",
      "\tTrain loss: 0.03828, Accuracy: 3085/6768 (45.00%)\n",
      "\tValidation loss: 0.00073, Accuracy: 769/1692 (45.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 736/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.19335\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.28071\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.28545\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.37398\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.43738\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.13898\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.18327\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.34278\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.29869\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.22845\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.20575\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.23631\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.24560\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.20124\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.34583\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.22330\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.22797\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.25150\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.27491\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.13065\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.27732\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.35928\n",
      "\tTrain loss: 0.03661, Accuracy: 3127/6768 (46.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 800/1692 (47.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 729/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.17208\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.23368\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.14280\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.27030\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.49662\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.19459\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.14937\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.31922\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.34179\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.27558\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.09301\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.23732\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.11662\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.29039\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.41942\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.26371\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.12301\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.33110\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.22328\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.12103\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.24256\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.27440\n",
      "\tTrain loss: 0.03645, Accuracy: 3219/6768 (47.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 779/1692 (46.00%)\n",
      "\tTest loss: 0.00069, Accuracy: 756/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.12804\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.11868\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.10074\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.28092\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.34900\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.09092\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.18423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.20756\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.16381\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.32989\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.09392\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.22209\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.20200\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.19966\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.32194\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.19398\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.09717\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.38059\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.24769\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.11981\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.24819\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.27988\n",
      "\tTrain loss: 0.03481, Accuracy: 3408/6768 (50.00%)\n",
      "\tValidation loss: 0.00066, Accuracy: 877/1692 (51.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 827/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 1.08794\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.18906\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.04324\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.18389\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.27639\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.25550\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.13968\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.27910\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.24963\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.23514\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.12765\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.14085\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.20348\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.31789\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.17360\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.21870\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.11484\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.40309\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.07144\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.05028\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.22076\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.29340\n",
      "\tTrain loss: 0.03386, Accuracy: 3471/6768 (51.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 883/1692 (52.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 773/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.07199\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.27080\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.00299\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.21440\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.34540\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.31949\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.12040\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.39415\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.23425\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.23953\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.14844\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.28447\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.07959\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.16863\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.20526\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.14583\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.24745\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.32927\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.23162\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.07191\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.18020\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.18543\n",
      "\tTrain loss: 0.03414, Accuracy: 3484/6768 (51.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 877/1692 (51.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 797/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 1.01883\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.10274\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.07449\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.15600\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.31133\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.14650\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.12048\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.21450\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.26645\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.24004\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.03312\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.09394\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 1.11241\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.36531\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.35649\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.21506\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.21868\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.19376\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.02475\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 0.96073\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.28088\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.26631\n",
      "\tTrain loss: 0.03238, Accuracy: 3744/6768 (55.00%)\n",
      "\tValidation loss: 0.00063, Accuracy: 912/1692 (53.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 0.93256\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.19458\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.01992\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.18436\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.29765\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.23311\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 0.98334\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.26229\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.15705\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.23763\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.03216\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.03890\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.12985\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.32206\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.18132\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.24884\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.11956\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.27419\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.04118\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 0.95858\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.24237\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.27192\n",
      "\tTrain loss: 0.03183, Accuracy: 3720/6768 (54.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 924/1692 (54.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 840/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 1.01016\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.17070\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 0.92825\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.26250\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.22038\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.08654\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.06753\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.34746\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.12751\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.20006\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.11475\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.22991\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.03939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.06268\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.22575\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.11452\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.06434\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.22155\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.02360\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 1.00690\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.17587\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.25751\n",
      "\tTrain loss: 0.03144, Accuracy: 3756/6768 (55.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 917/1692 (54.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 831/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.96807\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 1.17554\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 0.91094\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.13466\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.21298\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.17143\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.04730\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.39432\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.17248\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.17947\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.13285\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.14545\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 0.98518\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.26095\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.35498\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.19235\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.00379\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.00561\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 1.14609\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 0.89979\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.04784\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.13735\n",
      "\tTrain loss: 0.03105, Accuracy: 3771/6768 (55.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 922/1692 (54.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 815/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.97497\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.09042\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 0.94658\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.10591\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.00435\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.10609\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.08425\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.20536\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.21172\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.32973\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 0.97669\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.01267\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 1.15170\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.06271\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.16656\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.10571\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.14626\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.04067\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.17338\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 1.00143\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.97556\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.22046\n",
      "\tTrain loss: 0.02922, Accuracy: 4074/6768 (60.00%)\n",
      "\tValidation loss: 0.00058, Accuracy: 976/1692 (57.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 837/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.96547\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.10933\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 0.98894\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.06028\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.28755\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 1.05254\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.00897\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.43510\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.10269\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.21911\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 0.90457\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.14805\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 0.97793\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.10157\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 1.02176\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.21121\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 0.97436\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.10934\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.03043\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 0.99185\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.09627\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.07352\n",
      "\tTrain loss: 0.02875, Accuracy: 4104/6768 (60.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 994/1692 (58.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 873/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.79932\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.14066\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 1.09157\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.18498\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.13919\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 0.95126\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.03344\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.11478\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.25129\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.07592\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 0.97657\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.15078\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 0.97902\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 0.98690\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.08599\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 1.14878\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.05198\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.11833\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 1.04421\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.90100\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.10235\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.11282\n",
      "\tTrain loss: 0.02868, Accuracy: 4100/6768 (60.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 987/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 872/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.83581\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 1.06723\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 0.89341\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.16361\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.19979\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 1.11969\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 1.09808\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 1.30048\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.15714\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.07804\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 1.08751\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.03681\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 1.03091\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 0.91674\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 1.09210\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.10322\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.15489\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.13333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 1.02785\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.89603\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.11378\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.08181\n",
      "\tTrain loss: 0.02824, Accuracy: 4273/6768 (63.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1010/1692 (59.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 870/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.89176\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.23808\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 1.02870\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.18245\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 0.97282\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.95068\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.01556\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.06621\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 1.07967\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.03207\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 1.01319\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 1.05298\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.93504\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.95948\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.03461\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.05893\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.95816\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.27053\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 1.04010\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.99081\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 0.86609\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.29745\n",
      "\tTrain loss: 0.02694, Accuracy: 4344/6768 (64.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1047/1692 (61.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.89410\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 1.11281\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 0.83391\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.16272\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 1.13087\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.93438\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 0.95993\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 0.89505\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.04855\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.10543\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 1.14997\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 1.15953\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 1.00866\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 0.94471\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 1.19550\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 1.13199\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.16269\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.30671\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 1.00892\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 1.02063\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.12077\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 0.96057\n",
      "\tTrain loss: 0.02668, Accuracy: 4415/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1035/1692 (61.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 891/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.72361\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 0.97377\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.89924\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.07103\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.17209\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.90579\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.81762\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 1.11877\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 0.88649\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.03913\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 1.03608\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.04356\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 0.97769\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 1.10835\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 0.94484\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 1.07545\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.85844\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.90146\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 1.01956\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 0.89917\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 1.06485\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.88857\n",
      "\tTrain loss: 0.02524, Accuracy: 4556/6768 (67.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1058/1692 (62.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 916/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.83295\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.08656\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.83294\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 1.09697\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 0.83552\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 0.93634\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.99245\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 1.11322\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 0.98673\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 1.10484\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.93513\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 1.03866\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 0.99090\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 0.93485\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 1.05460\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 1.09154\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.12230\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.19076\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 0.99098\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 0.92216\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 0.99290\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.09590\n",
      "\tTrain loss: 0.02594, Accuracy: 4559/6768 (67.00%)\n",
      "\tValidation loss: 0.00052, Accuracy: 1081/1692 (63.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 860/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.84274\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 1.06698\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.82311\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.17741\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 1.05610\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 0.97451\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.85489\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 1.05849\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 1.06602\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 0.87094\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.93529\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.91778\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 0.96035\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 1.16183\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 1.20099\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 1.01159\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 1.08802\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 0.93814\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.98147\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.87340\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 0.82759\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.96985\n",
      "\tTrain loss: 0.02411, Accuracy: 4731/6768 (69.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00049, Accuracy: 1120/1692 (66.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 906/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.88856\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.18331\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.88486\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 1.10009\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 1.08969\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.84749\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 0.94915\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 1.20870\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 0.93605\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 1.08360\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.94573\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 1.01934\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 1.07411\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 1.05898\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 0.82200\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 1.11047\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 1.08709\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 0.83751\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.99175\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.78070\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 1.09687\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 0.97914\n",
      "\tTrain loss: 0.02358, Accuracy: 4757/6768 (70.00%)\n",
      "\tValidation loss: 0.00049, Accuracy: 1117/1692 (66.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 892/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.70297\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 1.08682\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.81288\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 1.10447\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 0.99834\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.93456\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.95843\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 1.02815\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 0.96413\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.96037\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.85682\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 0.86620\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 0.90783\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 1.01384\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 1.08233\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 0.96809\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 1.03889\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.83121\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 1.06964\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.83022\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.95196\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.95361\n",
      "\tTrain loss: 0.02319, Accuracy: 4865/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1137/1692 (67.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 905/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.87990\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.99439\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.70302\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 1.10592\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 1.26451\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.81205\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.84952\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 0.92721\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 1.13030\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.96399\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.97759\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 1.03132\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.95857\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 0.96802\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 0.98887\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 1.17858\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 1.02532\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 0.87453\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.82563\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.76474\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 1.10028\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.77964\n",
      "\tTrain loss: 0.02308, Accuracy: 4822/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 926/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.65684\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 1.06135\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.88001\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 1.08200\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 1.05129\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.82406\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.90873\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 1.07004\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 1.10730\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.89185\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.82453\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.77730\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 1.11646\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 0.93055\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 0.97512\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 1.00028\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.90721\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.01729\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.82849\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.82242\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 0.93187\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 1.11824\n",
      "\tTrain loss: 0.02299, Accuracy: 4809/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1113/1692 (65.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 927/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.73125\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 1.05987\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.86250\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 1.13100\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.90586\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.84601\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.85256\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 0.95112\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 1.01991\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 0.90079\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 0.85467\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 0.89415\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.91914\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 0.94345\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.98357\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 1.07773\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 1.00575\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 1.16385\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 1.02784\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 0.81943\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 0.86158\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 0.93928\n",
      "\tTrain loss: 0.02224, Accuracy: 4896/6768 (72.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1133/1692 (66.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 908/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.66781\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.54376\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.83282\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 1.01355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.11193\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.79246\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.83169\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 0.78075\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 1.01439\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 1.14409\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.81782\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 1.17323\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.90736\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 1.08501\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.98581\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.93835\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.98190\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.83229\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.88844\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.82265\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.85425\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 1.31885\n",
      "\tTrain loss: 0.02275, Accuracy: 4679/6768 (69.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1081/1692 (63.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 879/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.78363\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 1.02552\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.74629\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 0.94041\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.93775\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 1.01494\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 0.91035\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 0.83812\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.96727\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 1.03274\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.83512\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 1.02738\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 0.91623\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.93356\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.91841\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.99432\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 1.16503\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 1.11354\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.95828\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.78692\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.94583\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 0.80494\n",
      "\tTrain loss: 0.02081, Accuracy: 5073/6768 (74.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1204/1692 (71.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 924/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.71772\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 1.01255\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.86751\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.98698\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.91321\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.67861\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.91192\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 1.10149\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 1.11871\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 0.76021\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.79457\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 1.03493\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 1.00163\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 0.79492\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 0.84933\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 0.97976\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.92843\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 1.24381\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.95920\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.83360\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.89745\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.74447\n",
      "\tTrain loss: 0.02055, Accuracy: 5138/6768 (75.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1201/1692 (70.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 932/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.76915\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 1.05576\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.77220\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.98047\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.78536\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.75968\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.76579\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 0.98031\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.94348\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.90017\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.74736\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.87453\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 1.02009\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.82410\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 1.02880\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.82439\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 1.00016\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 0.80312\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.80246\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.71447\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.92565\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 1.10631\n",
      "\tTrain loss: 0.01949, Accuracy: 5179/6768 (76.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1197/1692 (70.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 956/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.64982\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 1.10240\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.84077\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.90620\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.87966\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 0.65736\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.77021\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 1.00453\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 0.97222\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 0.85537\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.65243\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 0.77798\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 1.00933\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.87647\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.92110\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 0.92168\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.89110\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 0.88180\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 1.01397\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.70475\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.89854\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.92390\n",
      "\tTrain loss: 0.02062, Accuracy: 5159/6768 (76.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1201/1692 (70.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 940/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.84370\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 0.93182\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.67231\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.93685\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.98361\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.87136\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.75361\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 1.02358\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.85303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.89910\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.79607\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 0.87957\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.94699\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 0.99874\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 1.04190\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 1.00613\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 1.03053\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 0.80899\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 1.00050\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.72369\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.89034\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 0.87853\n",
      "\tTrain loss: 0.01879, Accuracy: 5246/6768 (77.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1222/1692 (72.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.67000\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 1.05008\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.69811\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.92259\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.94065\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.69045\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.86206\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 1.11903\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.82905\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.96533\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.79757\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 0.93104\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.88536\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.84307\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 0.97988\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 1.00103\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 1.07952\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.78465\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.90291\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.72566\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 0.93391\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 0.87691\n",
      "\tTrain loss: 0.01859, Accuracy: 5340/6768 (78.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1231/1692 (72.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 959/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.75952\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 1.13424\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.88060\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.91928\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.67842\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.83127\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.76108\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.96045\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.80127\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.76746\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.73524\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.92088\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.91179\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.79853\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 0.80118\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 1.11985\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 0.73524\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 0.84530\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.75440\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.62107\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 1.00388\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 0.86208\n",
      "\tTrain loss: 0.01889, Accuracy: 5302/6768 (78.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1223/1692 (72.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 919/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.60783\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.86916\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.78736\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.81774\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 1.06203\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.69317\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.76220\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.96857\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.87472\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 0.85826\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 1.02779\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 0.88434\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.95241\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.67717\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.90147\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 0.95482\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.86185\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 1.03125\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.91778\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.76789\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.93072\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.97850\n",
      "\tTrain loss: 0.01759, Accuracy: 5383/6768 (79.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1239/1692 (73.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 894/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.68846\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 0.76773\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.84317\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.71356\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.84999\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.81667\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.69346\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.88492\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.88731\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.85657\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.64265\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.88824\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.89985\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.80099\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.85168\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.85567\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 1.17062\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 0.95292\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.81689\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.74992\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.88347\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.95733\n",
      "\tTrain loss: 0.01768, Accuracy: 5400/6768 (79.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1258/1692 (74.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.71322\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.94849\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.84159\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 1.01875\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.95846\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.63771\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.84622\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.88595\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.76245\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 1.03251\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.88948\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.95449\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.76380\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 1.04530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 0.85682\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 0.92138\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.78388\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 0.83954\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 0.92574\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.86607\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 0.76656\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.76616\n",
      "\tTrain loss: 0.01749, Accuracy: 5498/6768 (81.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1269/1692 (75.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 983/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.75824\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 1.15659\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.88942\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.99996\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.89112\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.82311\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.73701\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 1.08186\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 0.71273\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.98835\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.72110\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.88583\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.75013\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.91104\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.71539\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.86046\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.80628\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.90060\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.93702\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.49149\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 1.06927\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.76547\n",
      "\tTrain loss: 0.01699, Accuracy: 5504/6768 (81.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1246/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 928/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.60839\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 1.19859\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.71152\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 1.11638\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 0.97004\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 0.86175\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.88702\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.84652\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.87472\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.85333\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.74593\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.90688\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.86441\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.93401\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.84673\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.91066\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 0.93822\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.81006\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.68997\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.67566\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.79108\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.80937\n",
      "\tTrain loss: 0.01677, Accuracy: 5458/6768 (80.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1258/1692 (74.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 929/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.58491\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 1.20276\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.68710\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.88070\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.94475\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.81596\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.66026\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.93431\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.76853\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.77897\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.64875\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.93014\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.79629\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 0.77353\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.88512\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.96579\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.77998\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 0.83391\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.95927\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.81353\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.86348\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 1.05061\n",
      "\tTrain loss: 0.01684, Accuracy: 5492/6768 (81.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1273/1692 (75.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 946/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.69250\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 1.02134\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.81622\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.83738\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.74085\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.65873\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.72846\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.86378\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.89135\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.83263\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.78210\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.81775\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.80659\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.75666\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.74829\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.74530\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.86196\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.78810\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 1.01244\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.81660\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.75314\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.78400\n",
      "\tTrain loss: 0.01675, Accuracy: 5552/6768 (82.00%)\n",
      "\tValidation loss: 0.00037, Accuracy: 1286/1692 (76.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 909/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.69239\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 1.00234\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.79698\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.84680\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.91407\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.63903\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.77465\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.75246\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.92403\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.77967\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.93004\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.80926\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.72085\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.82243\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.90178\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.78513\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 1.08073\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 0.97225\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.98889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.68210\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.90677\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 1.00228\n",
      "\tTrain loss: 0.01511, Accuracy: 5675/6768 (83.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1314/1692 (77.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 981/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.59082\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 0.96585\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.67807\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.97777\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.72606\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.83993\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.68100\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 0.82647\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 1.07184\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.85644\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.76517\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.81472\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 1.05638\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.70029\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.89510\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 0.86693\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.70507\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 1.05335\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.78618\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.68755\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.74872\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.78393\n",
      "\tTrain loss: 0.01585, Accuracy: 5606/6768 (82.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1297/1692 (76.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 918/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.68770\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.93545\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.83790\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.79738\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.75095\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.69841\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.87189\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.87083\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 1.15625\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.70002\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.62887\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.95879\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.86387\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.86598\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.99621\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.94344\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.96449\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.90408\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.65918\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.60112\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.69183\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.89673\n",
      "\tTrain loss: 0.01571, Accuracy: 5649/6768 (83.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1288/1692 (76.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 925/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.65365\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 1.00780\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.82247\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.88756\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.65062\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.61357\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.68356\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.80319\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.90011\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.80527\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.75439\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.94036\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.94847\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 0.66299\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.80673\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.86636\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.69106\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 0.98379\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.91381\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.58257\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.79766\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.77952\n",
      "\tTrain loss: 0.01379, Accuracy: 5925/6768 (87.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1375/1692 (81.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 955/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.66567\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.83052\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.82346\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 1.15532\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.93067\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.52541\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.65636\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.63722\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 1.11211\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.78454\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.70345\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.92258\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.61646\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.59690\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.95957\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.82800\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.73877\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.86423\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.66488\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.63935\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.71831\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.81804\n",
      "\tTrain loss: 0.01352, Accuracy: 5869/6768 (86.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1359/1692 (80.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 997/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.60521\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.80190\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.52343\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.92196\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.59432\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.62674\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.73234\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.60273\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 1.03128\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.83352\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.59441\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.82328\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.75963\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.74723\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 0.90747\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.81889\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.79265\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 1.04151\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.93060\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.61478\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.81212\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 0.71746\n",
      "\tTrain loss: 0.01306, Accuracy: 5898/6768 (87.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1366/1692 (80.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 968/1772 (54.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.66449\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 1.17755\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.76389\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.64560\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.73866\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.59398\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.97751\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.80857\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.81468\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.79672\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.81200\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.80963\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.92248\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.71200\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.84149\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.82463\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.85034\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 0.95802\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.92529\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.73643\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.98203\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.70755\n",
      "\tTrain loss: 0.01409, Accuracy: 5947/6768 (87.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1381/1692 (81.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.60704\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 0.85961\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.89173\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.65747\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.63124\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.68672\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.75131\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.69413\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.74042\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.61945\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.98342\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.69654\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.68021\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.68543\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.72718\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.83184\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.80622\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 1.11369\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.67095\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.72802\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.84471\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.68591\n",
      "\tTrain loss: 0.01334, Accuracy: 5833/6768 (86.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1333/1692 (78.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 956/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.65050\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 0.92994\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.94872\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.67594\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.66290\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.65490\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.71380\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.63650\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.87435\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.62134\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.87957\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.63889\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.59677\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.89405\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.94059\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.61405\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.89092\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.67033\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.69980\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.58429\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.60543\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.79028\n",
      "\tTrain loss: 0.01352, Accuracy: 5767/6768 (85.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1304/1692 (77.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 913/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.71249\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.80658\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.78579\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.81075\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.82118\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.71632\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.66405\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.98157\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.74046\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.68500\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.65969\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.84332\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.64875\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.64213\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 1.03462\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.86628\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.73049\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.87029\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.63763\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.51199\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.89964\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 0.61969\n",
      "\tTrain loss: 0.01298, Accuracy: 5857/6768 (86.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1332/1692 (78.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 958/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.76043\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.80770\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.82217\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.70212\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.62785\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.67960\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.71244\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.87422\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.88921\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.86181\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.79514\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.73513\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 0.61888\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.86002\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.77023\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.62282\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.69338\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.65224\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.91321\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.51782\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.89527\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.66092\n",
      "\tTrain loss: 0.01233, Accuracy: 6029/6768 (89.00%)\n",
      "\tValidation loss: 0.00030, Accuracy: 1388/1692 (82.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 968/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.59829\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 1.02204\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.84419\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.86312\n",
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.74762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.66140\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.65965\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.76003\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.83429\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 0.54696\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.79027\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.56621\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.68690\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.72862\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.74667\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.60925\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.76150\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.83050\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 1.03057\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.67502\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.91086\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.81065\n",
      "\tTrain loss: 0.01207, Accuracy: 6044/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1400/1692 (82.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 946/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.58889\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.56230\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.68374\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 1.02515\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.82608\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.61304\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.70971\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.84205\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.77278\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.62486\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.76437\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.91809\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.68888\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.67252\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.60336\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.87375\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.90291\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.89536\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.78374\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.57860\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.68944\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.73450\n",
      "\tTrain loss: 0.01072, Accuracy: 6151/6768 (90.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1419/1692 (83.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1012/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.45528\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.86801\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.51257\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.78765\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.73765\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.78615\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.67588\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.93157\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.66186\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 0.82424\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.65242\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.70390\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.66025\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.77645\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.65234\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.88818\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.88190\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.87172\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.67760\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.67882\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.93021\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.72156\n",
      "\tTrain loss: 0.01042, Accuracy: 6191/6768 (91.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1435/1692 (84.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 964/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.53897\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.66119\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.77201\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.92447\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.69380\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.65110\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.73484\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.48830\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.91857\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 0.65759\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.58498\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.64206\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.69549\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 1.07255\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.81712\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.56825\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.84458\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.86209\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.74351\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.63104\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.80332\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.75694\n",
      "\tTrain loss: 0.01033, Accuracy: 6101/6768 (90.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1403/1692 (82.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1011/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.65184\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.82982\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.65134\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.62924\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.89523\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.50649\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.49071\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.65937\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.98516\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.61885\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.62182\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 0.67074\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.69376\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.61275\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.74384\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.68686\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.86822\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.71310\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.60122\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.59264\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.66428\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.76880\n",
      "\tTrain loss: 0.01154, Accuracy: 6064/6768 (89.00%)\n",
      "\tValidation loss: 0.00029, Accuracy: 1391/1692 (82.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 977/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.50909\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.74530\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.77643\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.95004\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.78836\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.76268\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.70210\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.93954\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.98674\n",
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.60754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.78298\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 1.10598\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.79089\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.98262\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.93148\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.83130\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.74794\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.76799\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.58741\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.46465\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.63119\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.55831\n",
      "\tTrain loss: 0.00949, Accuracy: 6272/6768 (92.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1453/1692 (85.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 987/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.62769\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.85705\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.67109\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.62848\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.75039\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.57743\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.71867\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.76928\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.92199\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.82465\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.61625\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.60330\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.87077\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.54821\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.77138\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.69110\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.75975\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.77880\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.79052\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.66465\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.78821\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.50526\n",
      "\tTrain loss: 0.00953, Accuracy: 6258/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1440/1692 (85.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1027/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.55705\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.59539\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.63574\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.78964\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.47282\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.37034\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.60246\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.61620\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.94339\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.62539\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.65647\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.73493\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.75814\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.64071\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.59403\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.65110\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.59496\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.72069\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.38070\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.55670\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.74846\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.66311\n",
      "\tTrain loss: 0.00937, Accuracy: 6253/6768 (92.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1466/1692 (86.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1003/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.65491\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.76939\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.60888\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.70487\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.69812\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.43378\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.81949\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.77758\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.96361\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.59767\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.74128\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.52720\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.72605\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.86164\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.51264\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.82911\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.67869\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.83400\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.96511\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.48897\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.91927\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.93135\n",
      "\tTrain loss: 0.00887, Accuracy: 6299/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1472/1692 (86.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 990/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.71750\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 1.04257\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.65938\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 1.12050\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.75611\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.47379\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.63416\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.70518\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.88876\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.56346\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.59331\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.81317\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.70155\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.47469\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.79273\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.62683\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.49215\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.64416\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.79850\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.40806\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.67342\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.94130\n",
      "\tTrain loss: 0.00907, Accuracy: 6301/6768 (93.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1462/1692 (86.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1026/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.58863\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.96071\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.76858\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.67791\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.50822\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.72214\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.47886\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.71738\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.65510\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.65683\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.55434\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.82854\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.63465\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.87357\n",
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.71850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.75352\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.84350\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.98146\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.75886\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.57937\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.63544\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.79171\n",
      "\tTrain loss: 0.01056, Accuracy: 6199/6768 (91.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1429/1692 (84.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 991/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.58702\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.68986\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.76716\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.67522\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.59012\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.53328\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.49655\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.76111\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.75019\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.71477\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.54484\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 0.71112\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.61617\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.79299\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.66404\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.84749\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.59376\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.87112\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.66612\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.34355\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.40773\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.65160\n",
      "\tTrain loss: 0.00757, Accuracy: 6370/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1478/1692 (87.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1026/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.68250\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.90275\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.82575\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.60321\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.75307\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.75934\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.58037\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.65476\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.70141\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.89080\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.78032\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.83399\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.69402\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.70988\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.62039\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.74163\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.64398\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.73043\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.55108\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.39310\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.65328\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.71333\n",
      "\tTrain loss: 0.00732, Accuracy: 6395/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1484/1692 (87.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1056/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.57083\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.87133\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.69780\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.88673\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.66300\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.49343\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.83881\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.67717\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.69245\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.51607\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.47004\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 1.14575\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.58224\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.78766\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.84690\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.58839\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 1.01527\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.87558\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.56419\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.66800\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.89321\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.69331\n",
      "\tTrain loss: 0.00870, Accuracy: 6362/6768 (94.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1482/1692 (87.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1022/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.58409\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 1.00763\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.46320\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.70566\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.62770\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.32716\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.53160\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.69876\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.67816\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.64417\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.66931\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.61313\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.66130\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.61687\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.62927\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.81537\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.71395\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.82134\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.67604\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.55552\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.50827\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.72327\n",
      "\tTrain loss: 0.00767, Accuracy: 6378/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1474/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1034/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.50144\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.70846\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.59155\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.61196\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.74215\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.67902\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.62590\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.65618\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.61656\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.74816\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.88040\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.64275\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.64200\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.48053\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.66750\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.61369\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.80767\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.76638\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.62673\n",
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.50422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.77811\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.73860\n",
      "\tTrain loss: 0.00796, Accuracy: 6414/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1489/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1046/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.64146\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.68978\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.52477\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.81845\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.75452\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.42254\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.53657\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.81173\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.57744\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.49713\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.56900\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.50752\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.56903\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.53072\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.69706\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.55608\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.48309\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.89425\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.50185\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.35004\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.64287\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.55133\n",
      "\tTrain loss: 0.00730, Accuracy: 6372/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1473/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1029/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.54385\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.63499\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.58933\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.77001\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.74685\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.52276\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.55653\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.74880\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.69855\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.62929\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.62374\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.55239\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.82738\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.64689\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.70372\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.54201\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.62617\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.86204\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.80648\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.61552\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.85379\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.49976\n",
      "\tTrain loss: 0.00750, Accuracy: 6435/6768 (95.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1494/1692 (88.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1049/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.56338\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.77435\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.67412\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.92585\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.54656\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.37623\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.63652\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.54904\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.95374\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.65902\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.62673\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.61905\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.80432\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.58887\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.64175\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.54635\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.84778\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.72913\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.72368\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.45333\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.52151\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.64996\n",
      "\tTrain loss: 0.00702, Accuracy: 6470/6768 (95.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1506/1692 (89.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1055/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.47328\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.69729\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.85263\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.68815\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.67520\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.57525\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.57922\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.49513\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.63381\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.54469\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.63418\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.69042\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.59908\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.65172\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.77935\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.40444\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.41635\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.74564\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.63141\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.40403\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.57064\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.95128\n",
      "\tTrain loss: 0.00792, Accuracy: 6354/6768 (93.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1476/1692 (87.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.35872\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 0.37379\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.56108\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.91886\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.75085\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.46157\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.51290\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.63722\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.87493\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.62470\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.43996\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.61971\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.82854\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.64119\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.51348\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.63630\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.42676\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.80441\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.65631\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.38040\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.78325\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.61863\n",
      "\tTrain loss: 0.00810, Accuracy: 6229/6768 (92.00%)\n",
      "\tValidation loss: 0.00023, Accuracy: 1458/1692 (86.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 992/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.52822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.73271\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.58862\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.80828\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.77820\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.33509\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.46745\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.85668\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.63710\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.43975\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.58377\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.49360\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.79760\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.56786\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.51599\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.59246\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.51830\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.81011\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.62602\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.43916\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.70706\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.78408\n",
      "\tTrain loss: 0.00617, Accuracy: 6501/6768 (96.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1523/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1048/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.38247\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.90846\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.81449\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.61033\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.57398\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.44353\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.41663\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.78180\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.80795\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.51038\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.34483\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.74974\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.67818\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.56204\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.64265\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.60922\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.59752\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.76398\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.72768\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.47028\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.45936\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.49082\n",
      "\tTrain loss: 0.00640, Accuracy: 6467/6768 (95.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1507/1692 (89.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1059/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.60590\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.63896\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.50366\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.85928\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.66333\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.57884\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.63790\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.62585\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.73980\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.53968\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.44950\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.57233\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.56577\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.90582\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.67777\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.72147\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.72150\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.69801\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.46169\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.51976\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.44831\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.56321\n",
      "\tTrain loss: 0.00599, Accuracy: 6490/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1515/1692 (89.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1035/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.68290\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.62400\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.70530\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.57032\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.64739\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.29891\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.45212\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.61734\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.63021\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.52689\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.65810\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.52096\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.57722\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.38651\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.50749\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.54184\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.74669\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.65301\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.66155\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.41102\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.39153\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.56476\n",
      "\tTrain loss: 0.00574, Accuracy: 6537/6768 (96.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1527/1692 (90.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1002/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.31553\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.76739\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.63293\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.86232\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.73635\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.33206\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.29697\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.86471\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.39278\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.51765\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.43319\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.55692\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.60020\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.65078\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.53972\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.47923\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.76325\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.87455\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.53964\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.72599\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.56868\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.44531\n",
      "\tTrain loss: 0.00556, Accuracy: 6549/6768 (96.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1543/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1046/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.62844\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 1.00465\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.55116\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.70244\n",
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.65607\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.51661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.42002\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.49648\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.52092\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.43626\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.43631\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.81621\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.77795\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.64929\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.51296\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.60438\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.52674\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.47327\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.80542\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.53891\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.75760\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.61996\n",
      "\tTrain loss: 0.00516, Accuracy: 6573/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1550/1692 (91.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1084/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.38259\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.48348\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.54128\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.42394\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.60897\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.56203\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.52884\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.79259\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.49879\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.50284\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.65864\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.59347\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.71478\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.53031\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.54198\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.56842\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.77704\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.62074\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.56825\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.56819\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.74221\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.57974\n",
      "\tTrain loss: 0.00539, Accuracy: 6571/6768 (97.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1539/1692 (90.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1049/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.48985\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.77512\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.30995\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.66538\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.58826\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.58353\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.45673\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.56052\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.43516\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.39683\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.59180\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.61453\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.53744\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.61761\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.56900\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.53499\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.72051\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.61487\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.88199\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.46037\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.71383\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.61015\n",
      "\tTrain loss: 0.00465, Accuracy: 6560/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1550/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1051/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.39579\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.53345\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 1.00885\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.71694\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.41067\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.30638\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.67817\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.67331\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.57054\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.58591\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.46600\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.63803\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.61942\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.75738\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.62550\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.66824\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.37457\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.59423\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.62917\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.34605\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.43430\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.64168\n",
      "\tTrain loss: 0.00415, Accuracy: 6600/6768 (97.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1541/1692 (91.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1065/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.57946\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.64106\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.53051\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.60766\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.54200\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.29548\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.53801\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.55583\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.57201\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.57784\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.49350\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.45943\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.36282\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.63955\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.61206\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.54223\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.45568\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.58163\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.37779\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.54513\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.40189\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.80166\n",
      "\tTrain loss: 0.00451, Accuracy: 6560/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1551/1692 (91.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.52000\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.95158\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.83651\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.60124\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.75592\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.35684\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.61571\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.63248\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.57293\n",
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.36071\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.46857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.63053\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.54311\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.75000\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.75882\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.44546\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.75132\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.49625\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.65541\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.54573\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.55641\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.58734\n",
      "\tTrain loss: 0.00445, Accuracy: 6617/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1576/1692 (93.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1054/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.35489\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.86987\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.42443\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.74621\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.27860\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.42136\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.42491\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.61464\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.52471\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.48672\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 0.32383\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.67349\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.62990\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.36468\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.63441\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.71535\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.46671\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.55313\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.65942\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.49426\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.80395\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.48805\n",
      "\tTrain loss: 0.00542, Accuracy: 6617/6768 (97.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1560/1692 (92.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.50628\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.80284\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.67125\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.76032\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.44926\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.57202\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.38895\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.70370\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.62343\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.67513\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.34898\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.67846\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.70581\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.35811\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.58926\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.64345\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.46936\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.63875\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.59475\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.50780\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.86991\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.64754\n",
      "\tTrain loss: 0.00461, Accuracy: 6580/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1550/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1052/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.26975\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.72601\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.58125\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.45810\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.76273\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.53112\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.56865\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.67673\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.62712\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.42559\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.32831\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.72624\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.65814\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.56084\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.50545\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.33185\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.48133\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.49573\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.46436\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.78086\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.55222\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.52513\n",
      "\tTrain loss: 0.00478, Accuracy: 6572/6768 (97.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1541/1692 (91.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.46076\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.56615\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.63371\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.70966\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.81484\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.28805\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.41245\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.74016\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.54124\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.39357\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.48616\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.43275\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.80728\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.42247\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.52023\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.55858\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.49280\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.60482\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.46705\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.51904\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.47216\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.68989\n",
      "\tTrain loss: 0.00436, Accuracy: 6622/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1560/1692 (92.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.36245\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.62569\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.67259\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.48484\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.60248\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.39998\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.46875\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 0.43502\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.75137\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.37450\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.56691\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.50637\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.45406\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.47704\n",
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.51569\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.44772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.85837\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.62277\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.55209\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.67197\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.47776\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.47075\n",
      "\tTrain loss: 0.00407, Accuracy: 6618/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1575/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1082/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.29155\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.64062\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.45277\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.78853\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.50730\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.31652\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.42769\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.54501\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.46070\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.81365\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.44400\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.55617\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.86436\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.73306\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.55130\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.33661\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.32303\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.70742\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.47561\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.66225\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.52014\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.72123\n",
      "\tTrain loss: 0.00426, Accuracy: 6604/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1559/1692 (92.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1046/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.70699\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.35662\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.56799\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.42849\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.75817\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.40747\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.52621\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.47161\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.59360\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.39645\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.36684\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.74523\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.49867\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.76054\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.64974\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.67042\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.54713\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.52287\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.52304\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.58500\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.46698\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.39165\n",
      "\tTrain loss: 0.00409, Accuracy: 6594/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1550/1692 (91.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1017/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.56452\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.41383\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.41114\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.53363\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.58886\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.42895\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.37232\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.53353\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.52823\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.32988\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.46559\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.40232\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.50956\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.64886\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.59289\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.56949\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.69935\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.59347\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.68845\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.26305\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.60334\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.60899\n",
      "\tTrain loss: 0.00376, Accuracy: 6634/6768 (98.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1568/1692 (92.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.49355\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.52873\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.41151\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.72423\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.52584\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.55834\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.77729\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.80385\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.78233\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.31715\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.46594\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.42928\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.83682\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.53050\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.36412\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.40733\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.62820\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.38705\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.38512\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.47421\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.61978\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.90353\n",
      "\tTrain loss: 0.00426, Accuracy: 6617/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1564/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.28242\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.78498\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.43325\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.63260\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.28377\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.52567\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.49497\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.52829\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.34018\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.55469\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.35327\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.65825\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.71493\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.64609\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.57590\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.59105\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.63192\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.96372\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.67350\n",
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.35342\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.56429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.51003\n",
      "\tTrain loss: 0.00346, Accuracy: 6633/6768 (98.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1576/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1072/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.52505\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.63679\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.47891\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.69680\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.65134\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.56890\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.50677\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.54316\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.74397\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.42885\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.36792\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.62797\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.50374\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.41490\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.61294\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.43755\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.62954\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.78386\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.42203\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.36981\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.56564\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.58413\n",
      "\tTrain loss: 0.00508, Accuracy: 6616/6768 (97.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1574/1692 (93.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.53999\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.60160\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.41042\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.64762\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.54280\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.53465\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.56381\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.59130\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.56132\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.35284\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.59977\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.48480\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.40560\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.36803\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.41025\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.48505\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.43780\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.49974\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.46423\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.33576\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.56767\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.62669\n",
      "\tTrain loss: 0.00406, Accuracy: 6638/6768 (98.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1594/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1067/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.25212\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.60357\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.44890\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.69404\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.50799\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.30194\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.58660\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.65267\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.66308\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.52300\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.33253\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.65832\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.44424\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.45265\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.65491\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.68035\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.40242\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.43316\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.62107\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.60434\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.41191\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.64114\n",
      "\tTrain loss: 0.00372, Accuracy: 6666/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1602/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.43470\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.58722\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.53192\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.41294\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.60805\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.42998\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.43418\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.57904\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.62845\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.57372\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.61733\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.53296\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.45760\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.53952\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.79768\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.60558\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.60606\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.59613\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.41495\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.40554\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.43998\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.75890\n",
      "\tTrain loss: 0.00305, Accuracy: 6649/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1597/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1076/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.47246\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.68275\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.54053\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.56797\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.67652\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.34450\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.37061\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.55126\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.33846\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.54588\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.46614\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.51820\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.29822\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.53271\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.58175\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.51875\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.48267\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.80445\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.51927\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.48879\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.51803\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.46005\n",
      "\tTrain loss: 0.00365, Accuracy: 6634/6768 (98.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1573/1692 (92.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1078/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.48658\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.67051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.23440\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.65573\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.52693\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.39174\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.55911\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.50449\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.61130\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.41643\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.28543\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.36753\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.73415\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.58376\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.72532\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.47836\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.47424\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.55440\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.49050\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.50734\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.70606\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.38383\n",
      "\tTrain loss: 0.00338, Accuracy: 6632/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1586/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9468085106382979\n",
      "Best test accuracy:\n",
      "0.6156884875846501\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_lstm, cnn_lstm_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b0813",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5c10b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 1 [0/6768 (0.00%)]\t\tLoss: 1.40381\n",
      "Training Progress: \tEpoch 1 [320/6768 (4.72%)]\t\tLoss: 1.56074\n",
      "Training Progress: \tEpoch 1 [640/6768 (9.43%)]\t\tLoss: 1.41990\n",
      "Training Progress: \tEpoch 1 [960/6768 (14.15%)]\t\tLoss: 1.50231\n",
      "Training Progress: \tEpoch 1 [1280/6768 (18.87%)]\t\tLoss: 1.46568\n",
      "Training Progress: \tEpoch 1 [1600/6768 (23.58%)]\t\tLoss: 1.51253\n",
      "Training Progress: \tEpoch 1 [1920/6768 (28.30%)]\t\tLoss: 1.38714\n",
      "Training Progress: \tEpoch 1 [2240/6768 (33.02%)]\t\tLoss: 1.38503\n",
      "Training Progress: \tEpoch 1 [2560/6768 (37.74%)]\t\tLoss: 1.39417\n",
      "Training Progress: \tEpoch 1 [2880/6768 (42.45%)]\t\tLoss: 1.43328\n",
      "Training Progress: \tEpoch 1 [3200/6768 (47.17%)]\t\tLoss: 1.50875\n",
      "Training Progress: \tEpoch 1 [3520/6768 (51.89%)]\t\tLoss: 1.38872\n",
      "Training Progress: \tEpoch 1 [3840/6768 (56.60%)]\t\tLoss: 1.46835\n",
      "Training Progress: \tEpoch 1 [4160/6768 (61.32%)]\t\tLoss: 1.38387\n",
      "Training Progress: \tEpoch 1 [4480/6768 (66.04%)]\t\tLoss: 1.44480\n",
      "Training Progress: \tEpoch 1 [4800/6768 (70.75%)]\t\tLoss: 1.41339\n",
      "Training Progress: \tEpoch 1 [5120/6768 (75.47%)]\t\tLoss: 1.43538\n",
      "Training Progress: \tEpoch 1 [5440/6768 (80.19%)]\t\tLoss: 1.38055\n",
      "Training Progress: \tEpoch 1 [5760/6768 (84.91%)]\t\tLoss: 1.38491\n",
      "Training Progress: \tEpoch 1 [6080/6768 (89.62%)]\t\tLoss: 1.36532\n",
      "Training Progress: \tEpoch 1 [6400/6768 (94.34%)]\t\tLoss: 1.30580\n",
      "Training Progress: \tEpoch 1 [6720/6768 (99.06%)]\t\tLoss: 1.37265\n",
      "\tTrain loss: 0.04330, Accuracy: 1725/6768 (25.00%)\n",
      "\tValidation loss: 0.00082, Accuracy: 423/1692 (25.00%)\n",
      "\tTest loss: 0.00078, Accuracy: 456/1772 (25.00%)\n",
      "\n",
      "Training Progress: \tEpoch 2 [0/6768 (0.00%)]\t\tLoss: 1.41685\n",
      "Training Progress: \tEpoch 2 [320/6768 (4.72%)]\t\tLoss: 1.38022\n",
      "Training Progress: \tEpoch 2 [640/6768 (9.43%)]\t\tLoss: 1.37804\n",
      "Training Progress: \tEpoch 2 [960/6768 (14.15%)]\t\tLoss: 1.40460\n",
      "Training Progress: \tEpoch 2 [1280/6768 (18.87%)]\t\tLoss: 1.40340\n",
      "Training Progress: \tEpoch 2 [1600/6768 (23.58%)]\t\tLoss: 1.41756\n",
      "Training Progress: \tEpoch 2 [1920/6768 (28.30%)]\t\tLoss: 1.35791\n",
      "Training Progress: \tEpoch 2 [2240/6768 (33.02%)]\t\tLoss: 1.42788\n",
      "Training Progress: \tEpoch 2 [2560/6768 (37.74%)]\t\tLoss: 1.42216\n",
      "Training Progress: \tEpoch 2 [2880/6768 (42.45%)]\t\tLoss: 1.41797\n",
      "Training Progress: \tEpoch 2 [3200/6768 (47.17%)]\t\tLoss: 1.32452\n",
      "Training Progress: \tEpoch 2 [3520/6768 (51.89%)]\t\tLoss: 1.33999\n",
      "Training Progress: \tEpoch 2 [3840/6768 (56.60%)]\t\tLoss: 1.37640\n",
      "Training Progress: \tEpoch 2 [4160/6768 (61.32%)]\t\tLoss: 1.46571\n",
      "Training Progress: \tEpoch 2 [4480/6768 (66.04%)]\t\tLoss: 1.38892\n",
      "Training Progress: \tEpoch 2 [4800/6768 (70.75%)]\t\tLoss: 1.35317\n",
      "Training Progress: \tEpoch 2 [5120/6768 (75.47%)]\t\tLoss: 1.34259\n",
      "Training Progress: \tEpoch 2 [5440/6768 (80.19%)]\t\tLoss: 1.45338\n",
      "Training Progress: \tEpoch 2 [5760/6768 (84.91%)]\t\tLoss: 1.38522\n",
      "Training Progress: \tEpoch 2 [6080/6768 (89.62%)]\t\tLoss: 1.36234\n",
      "Training Progress: \tEpoch 2 [6400/6768 (94.34%)]\t\tLoss: 1.41643\n",
      "Training Progress: \tEpoch 2 [6720/6768 (99.06%)]\t\tLoss: 1.31396\n",
      "\tTrain loss: 0.04154, Accuracy: 2314/6768 (34.00%)\n",
      "\tValidation loss: 0.00079, Accuracy: 554/1692 (32.00%)\n",
      "\tTest loss: 0.00075, Accuracy: 604/1772 (34.00%)\n",
      "\n",
      "Training Progress: \tEpoch 3 [0/6768 (0.00%)]\t\tLoss: 1.32693\n",
      "Training Progress: \tEpoch 3 [320/6768 (4.72%)]\t\tLoss: 1.34592\n",
      "Training Progress: \tEpoch 3 [640/6768 (9.43%)]\t\tLoss: 1.34074\n",
      "Training Progress: \tEpoch 3 [960/6768 (14.15%)]\t\tLoss: 1.41771\n",
      "Training Progress: \tEpoch 3 [1280/6768 (18.87%)]\t\tLoss: 1.28362\n",
      "Training Progress: \tEpoch 3 [1600/6768 (23.58%)]\t\tLoss: 1.27456\n",
      "Training Progress: \tEpoch 3 [1920/6768 (28.30%)]\t\tLoss: 1.26600\n",
      "Training Progress: \tEpoch 3 [2240/6768 (33.02%)]\t\tLoss: 1.29375\n",
      "Training Progress: \tEpoch 3 [2560/6768 (37.74%)]\t\tLoss: 1.37472\n",
      "Training Progress: \tEpoch 3 [2880/6768 (42.45%)]\t\tLoss: 1.42516\n",
      "Training Progress: \tEpoch 3 [3200/6768 (47.17%)]\t\tLoss: 1.20615\n",
      "Training Progress: \tEpoch 3 [3520/6768 (51.89%)]\t\tLoss: 1.31587\n",
      "Training Progress: \tEpoch 3 [3840/6768 (56.60%)]\t\tLoss: 1.24117\n",
      "Training Progress: \tEpoch 3 [4160/6768 (61.32%)]\t\tLoss: 1.32435\n",
      "Training Progress: \tEpoch 3 [4480/6768 (66.04%)]\t\tLoss: 1.39652\n",
      "Training Progress: \tEpoch 3 [4800/6768 (70.75%)]\t\tLoss: 1.44773\n",
      "Training Progress: \tEpoch 3 [5120/6768 (75.47%)]\t\tLoss: 1.14257\n",
      "Training Progress: \tEpoch 3 [5440/6768 (80.19%)]\t\tLoss: 1.42039\n",
      "Training Progress: \tEpoch 3 [5760/6768 (84.91%)]\t\tLoss: 1.31090\n",
      "Training Progress: \tEpoch 3 [6080/6768 (89.62%)]\t\tLoss: 1.36287\n",
      "Training Progress: \tEpoch 3 [6400/6768 (94.34%)]\t\tLoss: 1.30706\n",
      "Training Progress: \tEpoch 3 [6720/6768 (99.06%)]\t\tLoss: 1.26868\n",
      "\tTrain loss: 0.03828, Accuracy: 2982/6768 (44.00%)\n",
      "\tValidation loss: 0.00072, Accuracy: 750/1692 (44.00%)\n",
      "\tTest loss: 0.00071, Accuracy: 721/1772 (40.00%)\n",
      "\n",
      "Training Progress: \tEpoch 4 [0/6768 (0.00%)]\t\tLoss: 1.18380\n",
      "Training Progress: \tEpoch 4 [320/6768 (4.72%)]\t\tLoss: 1.28691\n",
      "Training Progress: \tEpoch 4 [640/6768 (9.43%)]\t\tLoss: 1.23702\n",
      "Training Progress: \tEpoch 4 [960/6768 (14.15%)]\t\tLoss: 1.29007\n",
      "Training Progress: \tEpoch 4 [1280/6768 (18.87%)]\t\tLoss: 1.23641\n",
      "Training Progress: \tEpoch 4 [1600/6768 (23.58%)]\t\tLoss: 1.32662\n",
      "Training Progress: \tEpoch 4 [1920/6768 (28.30%)]\t\tLoss: 1.21096\n",
      "Training Progress: \tEpoch 4 [2240/6768 (33.02%)]\t\tLoss: 1.27280\n",
      "Training Progress: \tEpoch 4 [2560/6768 (37.74%)]\t\tLoss: 1.34209\n",
      "Training Progress: \tEpoch 4 [2880/6768 (42.45%)]\t\tLoss: 1.40186\n",
      "Training Progress: \tEpoch 4 [3200/6768 (47.17%)]\t\tLoss: 1.23786\n",
      "Training Progress: \tEpoch 4 [3520/6768 (51.89%)]\t\tLoss: 1.27677\n",
      "Training Progress: \tEpoch 4 [3840/6768 (56.60%)]\t\tLoss: 1.22030\n",
      "Training Progress: \tEpoch 4 [4160/6768 (61.32%)]\t\tLoss: 1.29052\n",
      "Training Progress: \tEpoch 4 [4480/6768 (66.04%)]\t\tLoss: 1.36542\n",
      "Training Progress: \tEpoch 4 [4800/6768 (70.75%)]\t\tLoss: 1.30400\n",
      "Training Progress: \tEpoch 4 [5120/6768 (75.47%)]\t\tLoss: 1.16309\n",
      "Training Progress: \tEpoch 4 [5440/6768 (80.19%)]\t\tLoss: 1.37345\n",
      "Training Progress: \tEpoch 4 [5760/6768 (84.91%)]\t\tLoss: 1.32721\n",
      "Training Progress: \tEpoch 4 [6080/6768 (89.62%)]\t\tLoss: 1.17118\n",
      "Training Progress: \tEpoch 4 [6400/6768 (94.34%)]\t\tLoss: 1.21945\n",
      "Training Progress: \tEpoch 4 [6720/6768 (99.06%)]\t\tLoss: 1.25762\n",
      "\tTrain loss: 0.03680, Accuracy: 3156/6768 (46.00%)\n",
      "\tValidation loss: 0.00070, Accuracy: 789/1692 (46.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 747/1772 (42.00%)\n",
      "\n",
      "Training Progress: \tEpoch 5 [0/6768 (0.00%)]\t\tLoss: 1.04220\n",
      "Training Progress: \tEpoch 5 [320/6768 (4.72%)]\t\tLoss: 1.25712\n",
      "Training Progress: \tEpoch 5 [640/6768 (9.43%)]\t\tLoss: 1.09915\n",
      "Training Progress: \tEpoch 5 [960/6768 (14.15%)]\t\tLoss: 1.12088\n",
      "Training Progress: \tEpoch 5 [1280/6768 (18.87%)]\t\tLoss: 1.29016\n",
      "Training Progress: \tEpoch 5 [1600/6768 (23.58%)]\t\tLoss: 1.26925\n",
      "Training Progress: \tEpoch 5 [1920/6768 (28.30%)]\t\tLoss: 1.16560\n",
      "Training Progress: \tEpoch 5 [2240/6768 (33.02%)]\t\tLoss: 1.29377\n",
      "Training Progress: \tEpoch 5 [2560/6768 (37.74%)]\t\tLoss: 1.26424\n",
      "Training Progress: \tEpoch 5 [2880/6768 (42.45%)]\t\tLoss: 1.29418\n",
      "Training Progress: \tEpoch 5 [3200/6768 (47.17%)]\t\tLoss: 1.23562\n",
      "Training Progress: \tEpoch 5 [3520/6768 (51.89%)]\t\tLoss: 1.26595\n",
      "Training Progress: \tEpoch 5 [3840/6768 (56.60%)]\t\tLoss: 1.19187\n",
      "Training Progress: \tEpoch 5 [4160/6768 (61.32%)]\t\tLoss: 1.31671\n",
      "Training Progress: \tEpoch 5 [4480/6768 (66.04%)]\t\tLoss: 1.30348\n",
      "Training Progress: \tEpoch 5 [4800/6768 (70.75%)]\t\tLoss: 1.28171\n",
      "Training Progress: \tEpoch 5 [5120/6768 (75.47%)]\t\tLoss: 1.17033\n",
      "Training Progress: \tEpoch 5 [5440/6768 (80.19%)]\t\tLoss: 1.42463\n",
      "Training Progress: \tEpoch 5 [5760/6768 (84.91%)]\t\tLoss: 1.32326\n",
      "Training Progress: \tEpoch 5 [6080/6768 (89.62%)]\t\tLoss: 1.01686\n",
      "Training Progress: \tEpoch 5 [6400/6768 (94.34%)]\t\tLoss: 1.35982\n",
      "Training Progress: \tEpoch 5 [6720/6768 (99.06%)]\t\tLoss: 1.31064\n",
      "\tTrain loss: 0.03629, Accuracy: 3169/6768 (46.00%)\n",
      "\tValidation loss: 0.00069, Accuracy: 814/1692 (48.00%)\n",
      "\tTest loss: 0.00070, Accuracy: 728/1772 (41.00%)\n",
      "\n",
      "Training Progress: \tEpoch 6 [0/6768 (0.00%)]\t\tLoss: 1.12993\n",
      "Training Progress: \tEpoch 6 [320/6768 (4.72%)]\t\tLoss: 1.21792\n",
      "Training Progress: \tEpoch 6 [640/6768 (9.43%)]\t\tLoss: 1.04695\n",
      "Training Progress: \tEpoch 6 [960/6768 (14.15%)]\t\tLoss: 1.21911\n",
      "Training Progress: \tEpoch 6 [1280/6768 (18.87%)]\t\tLoss: 1.50197\n",
      "Training Progress: \tEpoch 6 [1600/6768 (23.58%)]\t\tLoss: 1.27074\n",
      "Training Progress: \tEpoch 6 [1920/6768 (28.30%)]\t\tLoss: 1.18256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 6 [2240/6768 (33.02%)]\t\tLoss: 1.31625\n",
      "Training Progress: \tEpoch 6 [2560/6768 (37.74%)]\t\tLoss: 1.29869\n",
      "Training Progress: \tEpoch 6 [2880/6768 (42.45%)]\t\tLoss: 1.26399\n",
      "Training Progress: \tEpoch 6 [3200/6768 (47.17%)]\t\tLoss: 1.17159\n",
      "Training Progress: \tEpoch 6 [3520/6768 (51.89%)]\t\tLoss: 1.28525\n",
      "Training Progress: \tEpoch 6 [3840/6768 (56.60%)]\t\tLoss: 1.08833\n",
      "Training Progress: \tEpoch 6 [4160/6768 (61.32%)]\t\tLoss: 1.27012\n",
      "Training Progress: \tEpoch 6 [4480/6768 (66.04%)]\t\tLoss: 1.40289\n",
      "Training Progress: \tEpoch 6 [4800/6768 (70.75%)]\t\tLoss: 1.14476\n",
      "Training Progress: \tEpoch 6 [5120/6768 (75.47%)]\t\tLoss: 1.20884\n",
      "Training Progress: \tEpoch 6 [5440/6768 (80.19%)]\t\tLoss: 1.29402\n",
      "Training Progress: \tEpoch 6 [5760/6768 (84.91%)]\t\tLoss: 1.20786\n",
      "Training Progress: \tEpoch 6 [6080/6768 (89.62%)]\t\tLoss: 1.06586\n",
      "Training Progress: \tEpoch 6 [6400/6768 (94.34%)]\t\tLoss: 1.23081\n",
      "Training Progress: \tEpoch 6 [6720/6768 (99.06%)]\t\tLoss: 1.31777\n",
      "\tTrain loss: 0.03492, Accuracy: 3353/6768 (49.00%)\n",
      "\tValidation loss: 0.00067, Accuracy: 844/1692 (49.00%)\n",
      "\tTest loss: 0.00068, Accuracy: 780/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 7 [0/6768 (0.00%)]\t\tLoss: 0.98147\n",
      "Training Progress: \tEpoch 7 [320/6768 (4.72%)]\t\tLoss: 1.16211\n",
      "Training Progress: \tEpoch 7 [640/6768 (9.43%)]\t\tLoss: 1.12725\n",
      "Training Progress: \tEpoch 7 [960/6768 (14.15%)]\t\tLoss: 1.17452\n",
      "Training Progress: \tEpoch 7 [1280/6768 (18.87%)]\t\tLoss: 1.29296\n",
      "Training Progress: \tEpoch 7 [1600/6768 (23.58%)]\t\tLoss: 1.22932\n",
      "Training Progress: \tEpoch 7 [1920/6768 (28.30%)]\t\tLoss: 1.13473\n",
      "Training Progress: \tEpoch 7 [2240/6768 (33.02%)]\t\tLoss: 1.21400\n",
      "Training Progress: \tEpoch 7 [2560/6768 (37.74%)]\t\tLoss: 1.22091\n",
      "Training Progress: \tEpoch 7 [2880/6768 (42.45%)]\t\tLoss: 1.20545\n",
      "Training Progress: \tEpoch 7 [3200/6768 (47.17%)]\t\tLoss: 1.18198\n",
      "Training Progress: \tEpoch 7 [3520/6768 (51.89%)]\t\tLoss: 1.20237\n",
      "Training Progress: \tEpoch 7 [3840/6768 (56.60%)]\t\tLoss: 1.00914\n",
      "Training Progress: \tEpoch 7 [4160/6768 (61.32%)]\t\tLoss: 1.19631\n",
      "Training Progress: \tEpoch 7 [4480/6768 (66.04%)]\t\tLoss: 1.29286\n",
      "Training Progress: \tEpoch 7 [4800/6768 (70.75%)]\t\tLoss: 1.19654\n",
      "Training Progress: \tEpoch 7 [5120/6768 (75.47%)]\t\tLoss: 1.07852\n",
      "Training Progress: \tEpoch 7 [5440/6768 (80.19%)]\t\tLoss: 1.26418\n",
      "Training Progress: \tEpoch 7 [5760/6768 (84.91%)]\t\tLoss: 1.19866\n",
      "Training Progress: \tEpoch 7 [6080/6768 (89.62%)]\t\tLoss: 1.03830\n",
      "Training Progress: \tEpoch 7 [6400/6768 (94.34%)]\t\tLoss: 1.23675\n",
      "Training Progress: \tEpoch 7 [6720/6768 (99.06%)]\t\tLoss: 1.24191\n",
      "\tTrain loss: 0.03424, Accuracy: 3508/6768 (51.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 883/1692 (52.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 810/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 8 [0/6768 (0.00%)]\t\tLoss: 1.03120\n",
      "Training Progress: \tEpoch 8 [320/6768 (4.72%)]\t\tLoss: 1.19806\n",
      "Training Progress: \tEpoch 8 [640/6768 (9.43%)]\t\tLoss: 1.10824\n",
      "Training Progress: \tEpoch 8 [960/6768 (14.15%)]\t\tLoss: 1.12424\n",
      "Training Progress: \tEpoch 8 [1280/6768 (18.87%)]\t\tLoss: 1.29829\n",
      "Training Progress: \tEpoch 8 [1600/6768 (23.58%)]\t\tLoss: 1.07295\n",
      "Training Progress: \tEpoch 8 [1920/6768 (28.30%)]\t\tLoss: 1.07600\n",
      "Training Progress: \tEpoch 8 [2240/6768 (33.02%)]\t\tLoss: 1.16346\n",
      "Training Progress: \tEpoch 8 [2560/6768 (37.74%)]\t\tLoss: 1.18251\n",
      "Training Progress: \tEpoch 8 [2880/6768 (42.45%)]\t\tLoss: 1.29692\n",
      "Training Progress: \tEpoch 8 [3200/6768 (47.17%)]\t\tLoss: 1.06927\n",
      "Training Progress: \tEpoch 8 [3520/6768 (51.89%)]\t\tLoss: 1.09310\n",
      "Training Progress: \tEpoch 8 [3840/6768 (56.60%)]\t\tLoss: 1.09377\n",
      "Training Progress: \tEpoch 8 [4160/6768 (61.32%)]\t\tLoss: 1.17457\n",
      "Training Progress: \tEpoch 8 [4480/6768 (66.04%)]\t\tLoss: 1.29130\n",
      "Training Progress: \tEpoch 8 [4800/6768 (70.75%)]\t\tLoss: 1.20617\n",
      "Training Progress: \tEpoch 8 [5120/6768 (75.47%)]\t\tLoss: 1.15937\n",
      "Training Progress: \tEpoch 8 [5440/6768 (80.19%)]\t\tLoss: 1.47960\n",
      "Training Progress: \tEpoch 8 [5760/6768 (84.91%)]\t\tLoss: 1.16100\n",
      "Training Progress: \tEpoch 8 [6080/6768 (89.62%)]\t\tLoss: 1.10631\n",
      "Training Progress: \tEpoch 8 [6400/6768 (94.34%)]\t\tLoss: 1.26204\n",
      "Training Progress: \tEpoch 8 [6720/6768 (99.06%)]\t\tLoss: 1.34085\n",
      "\tTrain loss: 0.03369, Accuracy: 3579/6768 (52.00%)\n",
      "\tValidation loss: 0.00065, Accuracy: 885/1692 (52.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 783/1772 (44.00%)\n",
      "\n",
      "Training Progress: \tEpoch 9 [0/6768 (0.00%)]\t\tLoss: 0.96047\n",
      "Training Progress: \tEpoch 9 [320/6768 (4.72%)]\t\tLoss: 1.18078\n",
      "Training Progress: \tEpoch 9 [640/6768 (9.43%)]\t\tLoss: 1.03102\n",
      "Training Progress: \tEpoch 9 [960/6768 (14.15%)]\t\tLoss: 1.15508\n",
      "Training Progress: \tEpoch 9 [1280/6768 (18.87%)]\t\tLoss: 1.25515\n",
      "Training Progress: \tEpoch 9 [1600/6768 (23.58%)]\t\tLoss: 1.23100\n",
      "Training Progress: \tEpoch 9 [1920/6768 (28.30%)]\t\tLoss: 1.08981\n",
      "Training Progress: \tEpoch 9 [2240/6768 (33.02%)]\t\tLoss: 1.28327\n",
      "Training Progress: \tEpoch 9 [2560/6768 (37.74%)]\t\tLoss: 1.22552\n",
      "Training Progress: \tEpoch 9 [2880/6768 (42.45%)]\t\tLoss: 1.24336\n",
      "Training Progress: \tEpoch 9 [3200/6768 (47.17%)]\t\tLoss: 1.07779\n",
      "Training Progress: \tEpoch 9 [3520/6768 (51.89%)]\t\tLoss: 1.14526\n",
      "Training Progress: \tEpoch 9 [3840/6768 (56.60%)]\t\tLoss: 0.97006\n",
      "Training Progress: \tEpoch 9 [4160/6768 (61.32%)]\t\tLoss: 1.27002\n",
      "Training Progress: \tEpoch 9 [4480/6768 (66.04%)]\t\tLoss: 1.29385\n",
      "Training Progress: \tEpoch 9 [4800/6768 (70.75%)]\t\tLoss: 1.21962\n",
      "Training Progress: \tEpoch 9 [5120/6768 (75.47%)]\t\tLoss: 1.09318\n",
      "Training Progress: \tEpoch 9 [5440/6768 (80.19%)]\t\tLoss: 1.23400\n",
      "Training Progress: \tEpoch 9 [5760/6768 (84.91%)]\t\tLoss: 1.20970\n",
      "Training Progress: \tEpoch 9 [6080/6768 (89.62%)]\t\tLoss: 0.97504\n",
      "Training Progress: \tEpoch 9 [6400/6768 (94.34%)]\t\tLoss: 1.19484\n",
      "Training Progress: \tEpoch 9 [6720/6768 (99.06%)]\t\tLoss: 1.19812\n",
      "\tTrain loss: 0.03370, Accuracy: 3562/6768 (52.00%)\n",
      "\tValidation loss: 0.00064, Accuracy: 900/1692 (53.00%)\n",
      "\tTest loss: 0.00067, Accuracy: 762/1772 (43.00%)\n",
      "\n",
      "Training Progress: \tEpoch 10 [0/6768 (0.00%)]\t\tLoss: 1.02463\n",
      "Training Progress: \tEpoch 10 [320/6768 (4.72%)]\t\tLoss: 1.11824\n",
      "Training Progress: \tEpoch 10 [640/6768 (9.43%)]\t\tLoss: 1.10472\n",
      "Training Progress: \tEpoch 10 [960/6768 (14.15%)]\t\tLoss: 1.17476\n",
      "Training Progress: \tEpoch 10 [1280/6768 (18.87%)]\t\tLoss: 1.34290\n",
      "Training Progress: \tEpoch 10 [1600/6768 (23.58%)]\t\tLoss: 1.08859\n",
      "Training Progress: \tEpoch 10 [1920/6768 (28.30%)]\t\tLoss: 1.14373\n",
      "Training Progress: \tEpoch 10 [2240/6768 (33.02%)]\t\tLoss: 1.20723\n",
      "Training Progress: \tEpoch 10 [2560/6768 (37.74%)]\t\tLoss: 1.10468\n",
      "Training Progress: \tEpoch 10 [2880/6768 (42.45%)]\t\tLoss: 1.21247\n",
      "Training Progress: \tEpoch 10 [3200/6768 (47.17%)]\t\tLoss: 1.19926\n",
      "Training Progress: \tEpoch 10 [3520/6768 (51.89%)]\t\tLoss: 1.00872\n",
      "Training Progress: \tEpoch 10 [3840/6768 (56.60%)]\t\tLoss: 1.12633\n",
      "Training Progress: \tEpoch 10 [4160/6768 (61.32%)]\t\tLoss: 1.23463\n",
      "Training Progress: \tEpoch 10 [4480/6768 (66.04%)]\t\tLoss: 1.38155\n",
      "Training Progress: \tEpoch 10 [4800/6768 (70.75%)]\t\tLoss: 1.28741\n",
      "Training Progress: \tEpoch 10 [5120/6768 (75.47%)]\t\tLoss: 1.18263\n",
      "Training Progress: \tEpoch 10 [5440/6768 (80.19%)]\t\tLoss: 1.15514\n",
      "Training Progress: \tEpoch 10 [5760/6768 (84.91%)]\t\tLoss: 1.21356\n",
      "Training Progress: \tEpoch 10 [6080/6768 (89.62%)]\t\tLoss: 1.06656\n",
      "Training Progress: \tEpoch 10 [6400/6768 (94.34%)]\t\tLoss: 1.26034\n",
      "Training Progress: \tEpoch 10 [6720/6768 (99.06%)]\t\tLoss: 1.21990\n",
      "\tTrain loss: 0.03180, Accuracy: 3832/6768 (56.00%)\n",
      "\tValidation loss: 0.00062, Accuracy: 947/1692 (55.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 855/1772 (48.00%)\n",
      "\n",
      "Training Progress: \tEpoch 11 [0/6768 (0.00%)]\t\tLoss: 0.86654\n",
      "Training Progress: \tEpoch 11 [320/6768 (4.72%)]\t\tLoss: 1.18688\n",
      "Training Progress: \tEpoch 11 [640/6768 (9.43%)]\t\tLoss: 0.99969\n",
      "Training Progress: \tEpoch 11 [960/6768 (14.15%)]\t\tLoss: 1.08463\n",
      "Training Progress: \tEpoch 11 [1280/6768 (18.87%)]\t\tLoss: 1.27604\n",
      "Training Progress: \tEpoch 11 [1600/6768 (23.58%)]\t\tLoss: 1.07266\n",
      "Training Progress: \tEpoch 11 [1920/6768 (28.30%)]\t\tLoss: 1.16519\n",
      "Training Progress: \tEpoch 11 [2240/6768 (33.02%)]\t\tLoss: 1.16262\n",
      "Training Progress: \tEpoch 11 [2560/6768 (37.74%)]\t\tLoss: 1.19543\n",
      "Training Progress: \tEpoch 11 [2880/6768 (42.45%)]\t\tLoss: 1.19016\n",
      "Training Progress: \tEpoch 11 [3200/6768 (47.17%)]\t\tLoss: 1.18206\n",
      "Training Progress: \tEpoch 11 [3520/6768 (51.89%)]\t\tLoss: 1.25773\n",
      "Training Progress: \tEpoch 11 [3840/6768 (56.60%)]\t\tLoss: 1.08299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 11 [4160/6768 (61.32%)]\t\tLoss: 1.27217\n",
      "Training Progress: \tEpoch 11 [4480/6768 (66.04%)]\t\tLoss: 1.28484\n",
      "Training Progress: \tEpoch 11 [4800/6768 (70.75%)]\t\tLoss: 1.05434\n",
      "Training Progress: \tEpoch 11 [5120/6768 (75.47%)]\t\tLoss: 1.18085\n",
      "Training Progress: \tEpoch 11 [5440/6768 (80.19%)]\t\tLoss: 1.10155\n",
      "Training Progress: \tEpoch 11 [5760/6768 (84.91%)]\t\tLoss: 1.01003\n",
      "Training Progress: \tEpoch 11 [6080/6768 (89.62%)]\t\tLoss: 0.94580\n",
      "Training Progress: \tEpoch 11 [6400/6768 (94.34%)]\t\tLoss: 1.13560\n",
      "Training Progress: \tEpoch 11 [6720/6768 (99.06%)]\t\tLoss: 1.32002\n",
      "\tTrain loss: 0.03121, Accuracy: 3889/6768 (57.00%)\n",
      "\tValidation loss: 0.00061, Accuracy: 978/1692 (57.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 848/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 12 [0/6768 (0.00%)]\t\tLoss: 0.86309\n",
      "Training Progress: \tEpoch 12 [320/6768 (4.72%)]\t\tLoss: 0.98823\n",
      "Training Progress: \tEpoch 12 [640/6768 (9.43%)]\t\tLoss: 1.13335\n",
      "Training Progress: \tEpoch 12 [960/6768 (14.15%)]\t\tLoss: 1.01712\n",
      "Training Progress: \tEpoch 12 [1280/6768 (18.87%)]\t\tLoss: 1.21587\n",
      "Training Progress: \tEpoch 12 [1600/6768 (23.58%)]\t\tLoss: 1.03247\n",
      "Training Progress: \tEpoch 12 [1920/6768 (28.30%)]\t\tLoss: 1.02072\n",
      "Training Progress: \tEpoch 12 [2240/6768 (33.02%)]\t\tLoss: 1.45847\n",
      "Training Progress: \tEpoch 12 [2560/6768 (37.74%)]\t\tLoss: 1.18524\n",
      "Training Progress: \tEpoch 12 [2880/6768 (42.45%)]\t\tLoss: 1.16923\n",
      "Training Progress: \tEpoch 12 [3200/6768 (47.17%)]\t\tLoss: 1.20338\n",
      "Training Progress: \tEpoch 12 [3520/6768 (51.89%)]\t\tLoss: 1.04530\n",
      "Training Progress: \tEpoch 12 [3840/6768 (56.60%)]\t\tLoss: 1.00995\n",
      "Training Progress: \tEpoch 12 [4160/6768 (61.32%)]\t\tLoss: 1.15332\n",
      "Training Progress: \tEpoch 12 [4480/6768 (66.04%)]\t\tLoss: 1.18548\n",
      "Training Progress: \tEpoch 12 [4800/6768 (70.75%)]\t\tLoss: 1.15579\n",
      "Training Progress: \tEpoch 12 [5120/6768 (75.47%)]\t\tLoss: 1.21448\n",
      "Training Progress: \tEpoch 12 [5440/6768 (80.19%)]\t\tLoss: 1.08612\n",
      "Training Progress: \tEpoch 12 [5760/6768 (84.91%)]\t\tLoss: 0.99610\n",
      "Training Progress: \tEpoch 12 [6080/6768 (89.62%)]\t\tLoss: 1.03327\n",
      "Training Progress: \tEpoch 12 [6400/6768 (94.34%)]\t\tLoss: 1.06404\n",
      "Training Progress: \tEpoch 12 [6720/6768 (99.06%)]\t\tLoss: 1.20577\n",
      "\tTrain loss: 0.03081, Accuracy: 3905/6768 (57.00%)\n",
      "\tValidation loss: 0.00060, Accuracy: 976/1692 (57.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 824/1772 (46.00%)\n",
      "\n",
      "Training Progress: \tEpoch 13 [0/6768 (0.00%)]\t\tLoss: 0.89888\n",
      "Training Progress: \tEpoch 13 [320/6768 (4.72%)]\t\tLoss: 1.15351\n",
      "Training Progress: \tEpoch 13 [640/6768 (9.43%)]\t\tLoss: 0.89501\n",
      "Training Progress: \tEpoch 13 [960/6768 (14.15%)]\t\tLoss: 1.06321\n",
      "Training Progress: \tEpoch 13 [1280/6768 (18.87%)]\t\tLoss: 1.17967\n",
      "Training Progress: \tEpoch 13 [1600/6768 (23.58%)]\t\tLoss: 1.05147\n",
      "Training Progress: \tEpoch 13 [1920/6768 (28.30%)]\t\tLoss: 1.08055\n",
      "Training Progress: \tEpoch 13 [2240/6768 (33.02%)]\t\tLoss: 1.34839\n",
      "Training Progress: \tEpoch 13 [2560/6768 (37.74%)]\t\tLoss: 1.15175\n",
      "Training Progress: \tEpoch 13 [2880/6768 (42.45%)]\t\tLoss: 1.27279\n",
      "Training Progress: \tEpoch 13 [3200/6768 (47.17%)]\t\tLoss: 1.12155\n",
      "Training Progress: \tEpoch 13 [3520/6768 (51.89%)]\t\tLoss: 1.00000\n",
      "Training Progress: \tEpoch 13 [3840/6768 (56.60%)]\t\tLoss: 0.92521\n",
      "Training Progress: \tEpoch 13 [4160/6768 (61.32%)]\t\tLoss: 1.08620\n",
      "Training Progress: \tEpoch 13 [4480/6768 (66.04%)]\t\tLoss: 1.26461\n",
      "Training Progress: \tEpoch 13 [4800/6768 (70.75%)]\t\tLoss: 1.33098\n",
      "Training Progress: \tEpoch 13 [5120/6768 (75.47%)]\t\tLoss: 1.11490\n",
      "Training Progress: \tEpoch 13 [5440/6768 (80.19%)]\t\tLoss: 1.29713\n",
      "Training Progress: \tEpoch 13 [5760/6768 (84.91%)]\t\tLoss: 1.03486\n",
      "Training Progress: \tEpoch 13 [6080/6768 (89.62%)]\t\tLoss: 0.82643\n",
      "Training Progress: \tEpoch 13 [6400/6768 (94.34%)]\t\tLoss: 0.98720\n",
      "Training Progress: \tEpoch 13 [6720/6768 (99.06%)]\t\tLoss: 1.16700\n",
      "\tTrain loss: 0.03003, Accuracy: 4009/6768 (59.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 986/1692 (58.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 837/1772 (47.00%)\n",
      "\n",
      "Training Progress: \tEpoch 14 [0/6768 (0.00%)]\t\tLoss: 0.94942\n",
      "Training Progress: \tEpoch 14 [320/6768 (4.72%)]\t\tLoss: 1.06205\n",
      "Training Progress: \tEpoch 14 [640/6768 (9.43%)]\t\tLoss: 0.96979\n",
      "Training Progress: \tEpoch 14 [960/6768 (14.15%)]\t\tLoss: 1.05764\n",
      "Training Progress: \tEpoch 14 [1280/6768 (18.87%)]\t\tLoss: 1.25180\n",
      "Training Progress: \tEpoch 14 [1600/6768 (23.58%)]\t\tLoss: 0.98836\n",
      "Training Progress: \tEpoch 14 [1920/6768 (28.30%)]\t\tLoss: 1.18707\n",
      "Training Progress: \tEpoch 14 [2240/6768 (33.02%)]\t\tLoss: 1.27799\n",
      "Training Progress: \tEpoch 14 [2560/6768 (37.74%)]\t\tLoss: 1.09484\n",
      "Training Progress: \tEpoch 14 [2880/6768 (42.45%)]\t\tLoss: 1.13547\n",
      "Training Progress: \tEpoch 14 [3200/6768 (47.17%)]\t\tLoss: 1.17959\n",
      "Training Progress: \tEpoch 14 [3520/6768 (51.89%)]\t\tLoss: 1.20816\n",
      "Training Progress: \tEpoch 14 [3840/6768 (56.60%)]\t\tLoss: 1.03077\n",
      "Training Progress: \tEpoch 14 [4160/6768 (61.32%)]\t\tLoss: 1.00813\n",
      "Training Progress: \tEpoch 14 [4480/6768 (66.04%)]\t\tLoss: 0.99428\n",
      "Training Progress: \tEpoch 14 [4800/6768 (70.75%)]\t\tLoss: 1.14613\n",
      "Training Progress: \tEpoch 14 [5120/6768 (75.47%)]\t\tLoss: 1.03744\n",
      "Training Progress: \tEpoch 14 [5440/6768 (80.19%)]\t\tLoss: 1.24479\n",
      "Training Progress: \tEpoch 14 [5760/6768 (84.91%)]\t\tLoss: 1.00782\n",
      "Training Progress: \tEpoch 14 [6080/6768 (89.62%)]\t\tLoss: 1.04090\n",
      "Training Progress: \tEpoch 14 [6400/6768 (94.34%)]\t\tLoss: 1.28863\n",
      "Training Progress: \tEpoch 14 [6720/6768 (99.06%)]\t\tLoss: 1.20156\n",
      "\tTrain loss: 0.02997, Accuracy: 4020/6768 (59.00%)\n",
      "\tValidation loss: 0.00059, Accuracy: 978/1692 (57.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 812/1772 (45.00%)\n",
      "\n",
      "Training Progress: \tEpoch 15 [0/6768 (0.00%)]\t\tLoss: 0.98417\n",
      "Training Progress: \tEpoch 15 [320/6768 (4.72%)]\t\tLoss: 1.11297\n",
      "Training Progress: \tEpoch 15 [640/6768 (9.43%)]\t\tLoss: 0.98137\n",
      "Training Progress: \tEpoch 15 [960/6768 (14.15%)]\t\tLoss: 1.08393\n",
      "Training Progress: \tEpoch 15 [1280/6768 (18.87%)]\t\tLoss: 1.09909\n",
      "Training Progress: \tEpoch 15 [1600/6768 (23.58%)]\t\tLoss: 1.08957\n",
      "Training Progress: \tEpoch 15 [1920/6768 (28.30%)]\t\tLoss: 1.12983\n",
      "Training Progress: \tEpoch 15 [2240/6768 (33.02%)]\t\tLoss: 1.15488\n",
      "Training Progress: \tEpoch 15 [2560/6768 (37.74%)]\t\tLoss: 1.25384\n",
      "Training Progress: \tEpoch 15 [2880/6768 (42.45%)]\t\tLoss: 1.08863\n",
      "Training Progress: \tEpoch 15 [3200/6768 (47.17%)]\t\tLoss: 1.13375\n",
      "Training Progress: \tEpoch 15 [3520/6768 (51.89%)]\t\tLoss: 1.31810\n",
      "Training Progress: \tEpoch 15 [3840/6768 (56.60%)]\t\tLoss: 1.00087\n",
      "Training Progress: \tEpoch 15 [4160/6768 (61.32%)]\t\tLoss: 1.04297\n",
      "Training Progress: \tEpoch 15 [4480/6768 (66.04%)]\t\tLoss: 1.18494\n",
      "Training Progress: \tEpoch 15 [4800/6768 (70.75%)]\t\tLoss: 1.11519\n",
      "Training Progress: \tEpoch 15 [5120/6768 (75.47%)]\t\tLoss: 1.13451\n",
      "Training Progress: \tEpoch 15 [5440/6768 (80.19%)]\t\tLoss: 1.14791\n",
      "Training Progress: \tEpoch 15 [5760/6768 (84.91%)]\t\tLoss: 0.88310\n",
      "Training Progress: \tEpoch 15 [6080/6768 (89.62%)]\t\tLoss: 0.86183\n",
      "Training Progress: \tEpoch 15 [6400/6768 (94.34%)]\t\tLoss: 1.13976\n",
      "Training Progress: \tEpoch 15 [6720/6768 (99.06%)]\t\tLoss: 1.06283\n",
      "\tTrain loss: 0.02889, Accuracy: 4236/6768 (62.00%)\n",
      "\tValidation loss: 0.00057, Accuracy: 1025/1692 (60.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 896/1772 (50.00%)\n",
      "\n",
      "Training Progress: \tEpoch 16 [0/6768 (0.00%)]\t\tLoss: 0.89026\n",
      "Training Progress: \tEpoch 16 [320/6768 (4.72%)]\t\tLoss: 0.99289\n",
      "Training Progress: \tEpoch 16 [640/6768 (9.43%)]\t\tLoss: 0.89262\n",
      "Training Progress: \tEpoch 16 [960/6768 (14.15%)]\t\tLoss: 1.16696\n",
      "Training Progress: \tEpoch 16 [1280/6768 (18.87%)]\t\tLoss: 1.14375\n",
      "Training Progress: \tEpoch 16 [1600/6768 (23.58%)]\t\tLoss: 0.99901\n",
      "Training Progress: \tEpoch 16 [1920/6768 (28.30%)]\t\tLoss: 0.98220\n",
      "Training Progress: \tEpoch 16 [2240/6768 (33.02%)]\t\tLoss: 1.25463\n",
      "Training Progress: \tEpoch 16 [2560/6768 (37.74%)]\t\tLoss: 1.18599\n",
      "Training Progress: \tEpoch 16 [2880/6768 (42.45%)]\t\tLoss: 1.11210\n",
      "Training Progress: \tEpoch 16 [3200/6768 (47.17%)]\t\tLoss: 0.89447\n",
      "Training Progress: \tEpoch 16 [3520/6768 (51.89%)]\t\tLoss: 1.12489\n",
      "Training Progress: \tEpoch 16 [3840/6768 (56.60%)]\t\tLoss: 0.82949\n",
      "Training Progress: \tEpoch 16 [4160/6768 (61.32%)]\t\tLoss: 1.10641\n",
      "Training Progress: \tEpoch 16 [4480/6768 (66.04%)]\t\tLoss: 1.14552\n",
      "Training Progress: \tEpoch 16 [4800/6768 (70.75%)]\t\tLoss: 1.13354\n",
      "Training Progress: \tEpoch 16 [5120/6768 (75.47%)]\t\tLoss: 1.07103\n",
      "Training Progress: \tEpoch 16 [5440/6768 (80.19%)]\t\tLoss: 1.25093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 16 [5760/6768 (84.91%)]\t\tLoss: 0.90573\n",
      "Training Progress: \tEpoch 16 [6080/6768 (89.62%)]\t\tLoss: 0.92661\n",
      "Training Progress: \tEpoch 16 [6400/6768 (94.34%)]\t\tLoss: 1.15894\n",
      "Training Progress: \tEpoch 16 [6720/6768 (99.06%)]\t\tLoss: 1.21802\n",
      "\tTrain loss: 0.02782, Accuracy: 4238/6768 (62.00%)\n",
      "\tValidation loss: 0.00056, Accuracy: 1024/1692 (60.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 884/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 17 [0/6768 (0.00%)]\t\tLoss: 0.96745\n",
      "Training Progress: \tEpoch 17 [320/6768 (4.72%)]\t\tLoss: 1.12478\n",
      "Training Progress: \tEpoch 17 [640/6768 (9.43%)]\t\tLoss: 0.98963\n",
      "Training Progress: \tEpoch 17 [960/6768 (14.15%)]\t\tLoss: 1.17505\n",
      "Training Progress: \tEpoch 17 [1280/6768 (18.87%)]\t\tLoss: 1.04683\n",
      "Training Progress: \tEpoch 17 [1600/6768 (23.58%)]\t\tLoss: 0.88390\n",
      "Training Progress: \tEpoch 17 [1920/6768 (28.30%)]\t\tLoss: 1.00185\n",
      "Training Progress: \tEpoch 17 [2240/6768 (33.02%)]\t\tLoss: 1.08823\n",
      "Training Progress: \tEpoch 17 [2560/6768 (37.74%)]\t\tLoss: 1.12705\n",
      "Training Progress: \tEpoch 17 [2880/6768 (42.45%)]\t\tLoss: 1.19999\n",
      "Training Progress: \tEpoch 17 [3200/6768 (47.17%)]\t\tLoss: 0.94451\n",
      "Training Progress: \tEpoch 17 [3520/6768 (51.89%)]\t\tLoss: 0.96619\n",
      "Training Progress: \tEpoch 17 [3840/6768 (56.60%)]\t\tLoss: 0.88679\n",
      "Training Progress: \tEpoch 17 [4160/6768 (61.32%)]\t\tLoss: 0.96038\n",
      "Training Progress: \tEpoch 17 [4480/6768 (66.04%)]\t\tLoss: 1.16853\n",
      "Training Progress: \tEpoch 17 [4800/6768 (70.75%)]\t\tLoss: 1.04240\n",
      "Training Progress: \tEpoch 17 [5120/6768 (75.47%)]\t\tLoss: 0.97266\n",
      "Training Progress: \tEpoch 17 [5440/6768 (80.19%)]\t\tLoss: 1.25681\n",
      "Training Progress: \tEpoch 17 [5760/6768 (84.91%)]\t\tLoss: 0.99224\n",
      "Training Progress: \tEpoch 17 [6080/6768 (89.62%)]\t\tLoss: 0.94399\n",
      "Training Progress: \tEpoch 17 [6400/6768 (94.34%)]\t\tLoss: 1.13576\n",
      "Training Progress: \tEpoch 17 [6720/6768 (99.06%)]\t\tLoss: 1.16216\n",
      "\tTrain loss: 0.02684, Accuracy: 4450/6768 (65.00%)\n",
      "\tValidation loss: 0.00054, Accuracy: 1068/1692 (63.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 878/1772 (49.00%)\n",
      "\n",
      "Training Progress: \tEpoch 18 [0/6768 (0.00%)]\t\tLoss: 0.84404\n",
      "Training Progress: \tEpoch 18 [320/6768 (4.72%)]\t\tLoss: 0.93737\n",
      "Training Progress: \tEpoch 18 [640/6768 (9.43%)]\t\tLoss: 1.06043\n",
      "Training Progress: \tEpoch 18 [960/6768 (14.15%)]\t\tLoss: 1.04114\n",
      "Training Progress: \tEpoch 18 [1280/6768 (18.87%)]\t\tLoss: 0.96433\n",
      "Training Progress: \tEpoch 18 [1600/6768 (23.58%)]\t\tLoss: 0.97948\n",
      "Training Progress: \tEpoch 18 [1920/6768 (28.30%)]\t\tLoss: 1.05053\n",
      "Training Progress: \tEpoch 18 [2240/6768 (33.02%)]\t\tLoss: 1.31905\n",
      "Training Progress: \tEpoch 18 [2560/6768 (37.74%)]\t\tLoss: 1.06894\n",
      "Training Progress: \tEpoch 18 [2880/6768 (42.45%)]\t\tLoss: 1.13004\n",
      "Training Progress: \tEpoch 18 [3200/6768 (47.17%)]\t\tLoss: 1.00226\n",
      "Training Progress: \tEpoch 18 [3520/6768 (51.89%)]\t\tLoss: 0.91819\n",
      "Training Progress: \tEpoch 18 [3840/6768 (56.60%)]\t\tLoss: 1.00040\n",
      "Training Progress: \tEpoch 18 [4160/6768 (61.32%)]\t\tLoss: 1.00631\n",
      "Training Progress: \tEpoch 18 [4480/6768 (66.04%)]\t\tLoss: 0.99166\n",
      "Training Progress: \tEpoch 18 [4800/6768 (70.75%)]\t\tLoss: 0.96381\n",
      "Training Progress: \tEpoch 18 [5120/6768 (75.47%)]\t\tLoss: 1.21170\n",
      "Training Progress: \tEpoch 18 [5440/6768 (80.19%)]\t\tLoss: 1.14007\n",
      "Training Progress: \tEpoch 18 [5760/6768 (84.91%)]\t\tLoss: 0.96041\n",
      "Training Progress: \tEpoch 18 [6080/6768 (89.62%)]\t\tLoss: 0.86993\n",
      "Training Progress: \tEpoch 18 [6400/6768 (94.34%)]\t\tLoss: 1.09748\n",
      "Training Progress: \tEpoch 18 [6720/6768 (99.06%)]\t\tLoss: 1.01408\n",
      "\tTrain loss: 0.02626, Accuracy: 4522/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1084/1692 (64.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 932/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 19 [0/6768 (0.00%)]\t\tLoss: 0.74251\n",
      "Training Progress: \tEpoch 19 [320/6768 (4.72%)]\t\tLoss: 1.00682\n",
      "Training Progress: \tEpoch 19 [640/6768 (9.43%)]\t\tLoss: 0.94011\n",
      "Training Progress: \tEpoch 19 [960/6768 (14.15%)]\t\tLoss: 1.02070\n",
      "Training Progress: \tEpoch 19 [1280/6768 (18.87%)]\t\tLoss: 1.07065\n",
      "Training Progress: \tEpoch 19 [1600/6768 (23.58%)]\t\tLoss: 0.85630\n",
      "Training Progress: \tEpoch 19 [1920/6768 (28.30%)]\t\tLoss: 0.95669\n",
      "Training Progress: \tEpoch 19 [2240/6768 (33.02%)]\t\tLoss: 1.08314\n",
      "Training Progress: \tEpoch 19 [2560/6768 (37.74%)]\t\tLoss: 1.07986\n",
      "Training Progress: \tEpoch 19 [2880/6768 (42.45%)]\t\tLoss: 1.10527\n",
      "Training Progress: \tEpoch 19 [3200/6768 (47.17%)]\t\tLoss: 0.94189\n",
      "Training Progress: \tEpoch 19 [3520/6768 (51.89%)]\t\tLoss: 1.03744\n",
      "Training Progress: \tEpoch 19 [3840/6768 (56.60%)]\t\tLoss: 1.11375\n",
      "Training Progress: \tEpoch 19 [4160/6768 (61.32%)]\t\tLoss: 1.13714\n",
      "Training Progress: \tEpoch 19 [4480/6768 (66.04%)]\t\tLoss: 1.07224\n",
      "Training Progress: \tEpoch 19 [4800/6768 (70.75%)]\t\tLoss: 1.17206\n",
      "Training Progress: \tEpoch 19 [5120/6768 (75.47%)]\t\tLoss: 0.96964\n",
      "Training Progress: \tEpoch 19 [5440/6768 (80.19%)]\t\tLoss: 0.92445\n",
      "Training Progress: \tEpoch 19 [5760/6768 (84.91%)]\t\tLoss: 0.95461\n",
      "Training Progress: \tEpoch 19 [6080/6768 (89.62%)]\t\tLoss: 1.21962\n",
      "Training Progress: \tEpoch 19 [6400/6768 (94.34%)]\t\tLoss: 0.93291\n",
      "Training Progress: \tEpoch 19 [6720/6768 (99.06%)]\t\tLoss: 0.83454\n",
      "\tTrain loss: 0.02602, Accuracy: 4501/6768 (66.00%)\n",
      "\tValidation loss: 0.00053, Accuracy: 1059/1692 (62.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 922/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 20 [0/6768 (0.00%)]\t\tLoss: 0.75040\n",
      "Training Progress: \tEpoch 20 [320/6768 (4.72%)]\t\tLoss: 1.04916\n",
      "Training Progress: \tEpoch 20 [640/6768 (9.43%)]\t\tLoss: 0.86632\n",
      "Training Progress: \tEpoch 20 [960/6768 (14.15%)]\t\tLoss: 0.97955\n",
      "Training Progress: \tEpoch 20 [1280/6768 (18.87%)]\t\tLoss: 1.07365\n",
      "Training Progress: \tEpoch 20 [1600/6768 (23.58%)]\t\tLoss: 1.07561\n",
      "Training Progress: \tEpoch 20 [1920/6768 (28.30%)]\t\tLoss: 0.94962\n",
      "Training Progress: \tEpoch 20 [2240/6768 (33.02%)]\t\tLoss: 1.22074\n",
      "Training Progress: \tEpoch 20 [2560/6768 (37.74%)]\t\tLoss: 1.00089\n",
      "Training Progress: \tEpoch 20 [2880/6768 (42.45%)]\t\tLoss: 0.94122\n",
      "Training Progress: \tEpoch 20 [3200/6768 (47.17%)]\t\tLoss: 0.99117\n",
      "Training Progress: \tEpoch 20 [3520/6768 (51.89%)]\t\tLoss: 0.89777\n",
      "Training Progress: \tEpoch 20 [3840/6768 (56.60%)]\t\tLoss: 1.00946\n",
      "Training Progress: \tEpoch 20 [4160/6768 (61.32%)]\t\tLoss: 1.00061\n",
      "Training Progress: \tEpoch 20 [4480/6768 (66.04%)]\t\tLoss: 0.99453\n",
      "Training Progress: \tEpoch 20 [4800/6768 (70.75%)]\t\tLoss: 0.93127\n",
      "Training Progress: \tEpoch 20 [5120/6768 (75.47%)]\t\tLoss: 1.09809\n",
      "Training Progress: \tEpoch 20 [5440/6768 (80.19%)]\t\tLoss: 1.22224\n",
      "Training Progress: \tEpoch 20 [5760/6768 (84.91%)]\t\tLoss: 1.06772\n",
      "Training Progress: \tEpoch 20 [6080/6768 (89.62%)]\t\tLoss: 1.02135\n",
      "Training Progress: \tEpoch 20 [6400/6768 (94.34%)]\t\tLoss: 1.10314\n",
      "Training Progress: \tEpoch 20 [6720/6768 (99.06%)]\t\tLoss: 1.38931\n",
      "\tTrain loss: 0.02509, Accuracy: 4718/6768 (69.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1116/1692 (65.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 942/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 21 [0/6768 (0.00%)]\t\tLoss: 0.79219\n",
      "Training Progress: \tEpoch 21 [320/6768 (4.72%)]\t\tLoss: 0.99809\n",
      "Training Progress: \tEpoch 21 [640/6768 (9.43%)]\t\tLoss: 0.93071\n",
      "Training Progress: \tEpoch 21 [960/6768 (14.15%)]\t\tLoss: 1.03053\n",
      "Training Progress: \tEpoch 21 [1280/6768 (18.87%)]\t\tLoss: 0.87115\n",
      "Training Progress: \tEpoch 21 [1600/6768 (23.58%)]\t\tLoss: 1.32628\n",
      "Training Progress: \tEpoch 21 [1920/6768 (28.30%)]\t\tLoss: 0.84310\n",
      "Training Progress: \tEpoch 21 [2240/6768 (33.02%)]\t\tLoss: 1.17367\n",
      "Training Progress: \tEpoch 21 [2560/6768 (37.74%)]\t\tLoss: 1.09090\n",
      "Training Progress: \tEpoch 21 [2880/6768 (42.45%)]\t\tLoss: 1.00928\n",
      "Training Progress: \tEpoch 21 [3200/6768 (47.17%)]\t\tLoss: 0.90088\n",
      "Training Progress: \tEpoch 21 [3520/6768 (51.89%)]\t\tLoss: 0.95201\n",
      "Training Progress: \tEpoch 21 [3840/6768 (56.60%)]\t\tLoss: 1.01882\n",
      "Training Progress: \tEpoch 21 [4160/6768 (61.32%)]\t\tLoss: 0.95189\n",
      "Training Progress: \tEpoch 21 [4480/6768 (66.04%)]\t\tLoss: 1.07782\n",
      "Training Progress: \tEpoch 21 [4800/6768 (70.75%)]\t\tLoss: 1.00154\n",
      "Training Progress: \tEpoch 21 [5120/6768 (75.47%)]\t\tLoss: 1.11862\n",
      "Training Progress: \tEpoch 21 [5440/6768 (80.19%)]\t\tLoss: 1.05931\n",
      "Training Progress: \tEpoch 21 [5760/6768 (84.91%)]\t\tLoss: 0.86348\n",
      "Training Progress: \tEpoch 21 [6080/6768 (89.62%)]\t\tLoss: 0.92356\n",
      "Training Progress: \tEpoch 21 [6400/6768 (94.34%)]\t\tLoss: 1.11740\n",
      "Training Progress: \tEpoch 21 [6720/6768 (99.06%)]\t\tLoss: 0.99076\n",
      "\tTrain loss: 0.02506, Accuracy: 4684/6768 (69.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation loss: 0.00051, Accuracy: 1109/1692 (65.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 939/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 22 [0/6768 (0.00%)]\t\tLoss: 0.71147\n",
      "Training Progress: \tEpoch 22 [320/6768 (4.72%)]\t\tLoss: 1.10802\n",
      "Training Progress: \tEpoch 22 [640/6768 (9.43%)]\t\tLoss: 0.88411\n",
      "Training Progress: \tEpoch 22 [960/6768 (14.15%)]\t\tLoss: 0.97061\n",
      "Training Progress: \tEpoch 22 [1280/6768 (18.87%)]\t\tLoss: 1.03774\n",
      "Training Progress: \tEpoch 22 [1600/6768 (23.58%)]\t\tLoss: 0.95147\n",
      "Training Progress: \tEpoch 22 [1920/6768 (28.30%)]\t\tLoss: 1.04899\n",
      "Training Progress: \tEpoch 22 [2240/6768 (33.02%)]\t\tLoss: 1.12529\n",
      "Training Progress: \tEpoch 22 [2560/6768 (37.74%)]\t\tLoss: 1.19236\n",
      "Training Progress: \tEpoch 22 [2880/6768 (42.45%)]\t\tLoss: 0.86298\n",
      "Training Progress: \tEpoch 22 [3200/6768 (47.17%)]\t\tLoss: 0.89848\n",
      "Training Progress: \tEpoch 22 [3520/6768 (51.89%)]\t\tLoss: 1.00380\n",
      "Training Progress: \tEpoch 22 [3840/6768 (56.60%)]\t\tLoss: 1.03304\n",
      "Training Progress: \tEpoch 22 [4160/6768 (61.32%)]\t\tLoss: 0.85028\n",
      "Training Progress: \tEpoch 22 [4480/6768 (66.04%)]\t\tLoss: 1.06908\n",
      "Training Progress: \tEpoch 22 [4800/6768 (70.75%)]\t\tLoss: 1.05644\n",
      "Training Progress: \tEpoch 22 [5120/6768 (75.47%)]\t\tLoss: 1.01262\n",
      "Training Progress: \tEpoch 22 [5440/6768 (80.19%)]\t\tLoss: 1.00214\n",
      "Training Progress: \tEpoch 22 [5760/6768 (84.91%)]\t\tLoss: 0.90862\n",
      "Training Progress: \tEpoch 22 [6080/6768 (89.62%)]\t\tLoss: 0.85052\n",
      "Training Progress: \tEpoch 22 [6400/6768 (94.34%)]\t\tLoss: 0.84242\n",
      "Training Progress: \tEpoch 22 [6720/6768 (99.06%)]\t\tLoss: 1.10040\n",
      "\tTrain loss: 0.02478, Accuracy: 4755/6768 (70.00%)\n",
      "\tValidation loss: 0.00051, Accuracy: 1127/1692 (66.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 911/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 23 [0/6768 (0.00%)]\t\tLoss: 0.89221\n",
      "Training Progress: \tEpoch 23 [320/6768 (4.72%)]\t\tLoss: 0.99744\n",
      "Training Progress: \tEpoch 23 [640/6768 (9.43%)]\t\tLoss: 0.78072\n",
      "Training Progress: \tEpoch 23 [960/6768 (14.15%)]\t\tLoss: 1.09640\n",
      "Training Progress: \tEpoch 23 [1280/6768 (18.87%)]\t\tLoss: 1.06943\n",
      "Training Progress: \tEpoch 23 [1600/6768 (23.58%)]\t\tLoss: 0.85710\n",
      "Training Progress: \tEpoch 23 [1920/6768 (28.30%)]\t\tLoss: 0.88213\n",
      "Training Progress: \tEpoch 23 [2240/6768 (33.02%)]\t\tLoss: 1.17431\n",
      "Training Progress: \tEpoch 23 [2560/6768 (37.74%)]\t\tLoss: 1.07780\n",
      "Training Progress: \tEpoch 23 [2880/6768 (42.45%)]\t\tLoss: 0.99945\n",
      "Training Progress: \tEpoch 23 [3200/6768 (47.17%)]\t\tLoss: 0.95950\n",
      "Training Progress: \tEpoch 23 [3520/6768 (51.89%)]\t\tLoss: 1.10101\n",
      "Training Progress: \tEpoch 23 [3840/6768 (56.60%)]\t\tLoss: 1.05684\n",
      "Training Progress: \tEpoch 23 [4160/6768 (61.32%)]\t\tLoss: 0.86489\n",
      "Training Progress: \tEpoch 23 [4480/6768 (66.04%)]\t\tLoss: 0.99996\n",
      "Training Progress: \tEpoch 23 [4800/6768 (70.75%)]\t\tLoss: 1.11675\n",
      "Training Progress: \tEpoch 23 [5120/6768 (75.47%)]\t\tLoss: 0.96010\n",
      "Training Progress: \tEpoch 23 [5440/6768 (80.19%)]\t\tLoss: 0.93797\n",
      "Training Progress: \tEpoch 23 [5760/6768 (84.91%)]\t\tLoss: 0.86350\n",
      "Training Progress: \tEpoch 23 [6080/6768 (89.62%)]\t\tLoss: 0.80935\n",
      "Training Progress: \tEpoch 23 [6400/6768 (94.34%)]\t\tLoss: 0.94531\n",
      "Training Progress: \tEpoch 23 [6720/6768 (99.06%)]\t\tLoss: 0.90472\n",
      "\tTrain loss: 0.02301, Accuracy: 4853/6768 (71.00%)\n",
      "\tValidation loss: 0.00048, Accuracy: 1139/1692 (67.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 947/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 24 [0/6768 (0.00%)]\t\tLoss: 0.80708\n",
      "Training Progress: \tEpoch 24 [320/6768 (4.72%)]\t\tLoss: 0.96407\n",
      "Training Progress: \tEpoch 24 [640/6768 (9.43%)]\t\tLoss: 0.83737\n",
      "Training Progress: \tEpoch 24 [960/6768 (14.15%)]\t\tLoss: 0.90245\n",
      "Training Progress: \tEpoch 24 [1280/6768 (18.87%)]\t\tLoss: 0.99124\n",
      "Training Progress: \tEpoch 24 [1600/6768 (23.58%)]\t\tLoss: 0.95989\n",
      "Training Progress: \tEpoch 24 [1920/6768 (28.30%)]\t\tLoss: 0.85289\n",
      "Training Progress: \tEpoch 24 [2240/6768 (33.02%)]\t\tLoss: 1.00897\n",
      "Training Progress: \tEpoch 24 [2560/6768 (37.74%)]\t\tLoss: 1.01671\n",
      "Training Progress: \tEpoch 24 [2880/6768 (42.45%)]\t\tLoss: 0.99390\n",
      "Training Progress: \tEpoch 24 [3200/6768 (47.17%)]\t\tLoss: 0.98026\n",
      "Training Progress: \tEpoch 24 [3520/6768 (51.89%)]\t\tLoss: 0.81420\n",
      "Training Progress: \tEpoch 24 [3840/6768 (56.60%)]\t\tLoss: 0.89316\n",
      "Training Progress: \tEpoch 24 [4160/6768 (61.32%)]\t\tLoss: 1.03572\n",
      "Training Progress: \tEpoch 24 [4480/6768 (66.04%)]\t\tLoss: 1.08892\n",
      "Training Progress: \tEpoch 24 [4800/6768 (70.75%)]\t\tLoss: 1.03668\n",
      "Training Progress: \tEpoch 24 [5120/6768 (75.47%)]\t\tLoss: 0.94357\n",
      "Training Progress: \tEpoch 24 [5440/6768 (80.19%)]\t\tLoss: 1.04703\n",
      "Training Progress: \tEpoch 24 [5760/6768 (84.91%)]\t\tLoss: 0.93232\n",
      "Training Progress: \tEpoch 24 [6080/6768 (89.62%)]\t\tLoss: 0.81973\n",
      "Training Progress: \tEpoch 24 [6400/6768 (94.34%)]\t\tLoss: 0.95788\n",
      "Training Progress: \tEpoch 24 [6720/6768 (99.06%)]\t\tLoss: 0.95918\n",
      "\tTrain loss: 0.02231, Accuracy: 4940/6768 (72.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1170/1692 (69.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 920/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 25 [0/6768 (0.00%)]\t\tLoss: 0.80050\n",
      "Training Progress: \tEpoch 25 [320/6768 (4.72%)]\t\tLoss: 0.86321\n",
      "Training Progress: \tEpoch 25 [640/6768 (9.43%)]\t\tLoss: 0.85895\n",
      "Training Progress: \tEpoch 25 [960/6768 (14.15%)]\t\tLoss: 1.11004\n",
      "Training Progress: \tEpoch 25 [1280/6768 (18.87%)]\t\tLoss: 1.08030\n",
      "Training Progress: \tEpoch 25 [1600/6768 (23.58%)]\t\tLoss: 0.86550\n",
      "Training Progress: \tEpoch 25 [1920/6768 (28.30%)]\t\tLoss: 0.99854\n",
      "Training Progress: \tEpoch 25 [2240/6768 (33.02%)]\t\tLoss: 1.14249\n",
      "Training Progress: \tEpoch 25 [2560/6768 (37.74%)]\t\tLoss: 1.06194\n",
      "Training Progress: \tEpoch 25 [2880/6768 (42.45%)]\t\tLoss: 0.96546\n",
      "Training Progress: \tEpoch 25 [3200/6768 (47.17%)]\t\tLoss: 0.90188\n",
      "Training Progress: \tEpoch 25 [3520/6768 (51.89%)]\t\tLoss: 0.91641\n",
      "Training Progress: \tEpoch 25 [3840/6768 (56.60%)]\t\tLoss: 0.97689\n",
      "Training Progress: \tEpoch 25 [4160/6768 (61.32%)]\t\tLoss: 1.04750\n",
      "Training Progress: \tEpoch 25 [4480/6768 (66.04%)]\t\tLoss: 1.09894\n",
      "Training Progress: \tEpoch 25 [4800/6768 (70.75%)]\t\tLoss: 1.06600\n",
      "Training Progress: \tEpoch 25 [5120/6768 (75.47%)]\t\tLoss: 0.98797\n",
      "Training Progress: \tEpoch 25 [5440/6768 (80.19%)]\t\tLoss: 1.17992\n",
      "Training Progress: \tEpoch 25 [5760/6768 (84.91%)]\t\tLoss: 0.76205\n",
      "Training Progress: \tEpoch 25 [6080/6768 (89.62%)]\t\tLoss: 0.85902\n",
      "Training Progress: \tEpoch 25 [6400/6768 (94.34%)]\t\tLoss: 1.00897\n",
      "Training Progress: \tEpoch 25 [6720/6768 (99.06%)]\t\tLoss: 1.36525\n",
      "\tTrain loss: 0.02211, Accuracy: 4946/6768 (73.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1149/1692 (67.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 950/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 26 [0/6768 (0.00%)]\t\tLoss: 0.75368\n",
      "Training Progress: \tEpoch 26 [320/6768 (4.72%)]\t\tLoss: 0.94272\n",
      "Training Progress: \tEpoch 26 [640/6768 (9.43%)]\t\tLoss: 0.88705\n",
      "Training Progress: \tEpoch 26 [960/6768 (14.15%)]\t\tLoss: 1.03457\n",
      "Training Progress: \tEpoch 26 [1280/6768 (18.87%)]\t\tLoss: 0.99828\n",
      "Training Progress: \tEpoch 26 [1600/6768 (23.58%)]\t\tLoss: 0.78651\n",
      "Training Progress: \tEpoch 26 [1920/6768 (28.30%)]\t\tLoss: 0.87467\n",
      "Training Progress: \tEpoch 26 [2240/6768 (33.02%)]\t\tLoss: 1.17217\n",
      "Training Progress: \tEpoch 26 [2560/6768 (37.74%)]\t\tLoss: 1.07187\n",
      "Training Progress: \tEpoch 26 [2880/6768 (42.45%)]\t\tLoss: 1.11895\n",
      "Training Progress: \tEpoch 26 [3200/6768 (47.17%)]\t\tLoss: 1.03255\n",
      "Training Progress: \tEpoch 26 [3520/6768 (51.89%)]\t\tLoss: 1.03712\n",
      "Training Progress: \tEpoch 26 [3840/6768 (56.60%)]\t\tLoss: 0.95614\n",
      "Training Progress: \tEpoch 26 [4160/6768 (61.32%)]\t\tLoss: 1.01244\n",
      "Training Progress: \tEpoch 26 [4480/6768 (66.04%)]\t\tLoss: 0.99399\n",
      "Training Progress: \tEpoch 26 [4800/6768 (70.75%)]\t\tLoss: 0.89256\n",
      "Training Progress: \tEpoch 26 [5120/6768 (75.47%)]\t\tLoss: 0.92497\n",
      "Training Progress: \tEpoch 26 [5440/6768 (80.19%)]\t\tLoss: 0.98062\n",
      "Training Progress: \tEpoch 26 [5760/6768 (84.91%)]\t\tLoss: 0.95136\n",
      "Training Progress: \tEpoch 26 [6080/6768 (89.62%)]\t\tLoss: 1.00684\n",
      "Training Progress: \tEpoch 26 [6400/6768 (94.34%)]\t\tLoss: 1.02485\n",
      "Training Progress: \tEpoch 26 [6720/6768 (99.06%)]\t\tLoss: 1.32757\n",
      "\tTrain loss: 0.02231, Accuracy: 5014/6768 (74.00%)\n",
      "\tValidation loss: 0.00047, Accuracy: 1182/1692 (69.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 927/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 27 [0/6768 (0.00%)]\t\tLoss: 0.80936\n",
      "Training Progress: \tEpoch 27 [320/6768 (4.72%)]\t\tLoss: 1.30683\n",
      "Training Progress: \tEpoch 27 [640/6768 (9.43%)]\t\tLoss: 0.75121\n",
      "Training Progress: \tEpoch 27 [960/6768 (14.15%)]\t\tLoss: 0.88279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 27 [1280/6768 (18.87%)]\t\tLoss: 1.06560\n",
      "Training Progress: \tEpoch 27 [1600/6768 (23.58%)]\t\tLoss: 0.81453\n",
      "Training Progress: \tEpoch 27 [1920/6768 (28.30%)]\t\tLoss: 0.83981\n",
      "Training Progress: \tEpoch 27 [2240/6768 (33.02%)]\t\tLoss: 1.08588\n",
      "Training Progress: \tEpoch 27 [2560/6768 (37.74%)]\t\tLoss: 0.83557\n",
      "Training Progress: \tEpoch 27 [2880/6768 (42.45%)]\t\tLoss: 1.13794\n",
      "Training Progress: \tEpoch 27 [3200/6768 (47.17%)]\t\tLoss: 0.93502\n",
      "Training Progress: \tEpoch 27 [3520/6768 (51.89%)]\t\tLoss: 0.90496\n",
      "Training Progress: \tEpoch 27 [3840/6768 (56.60%)]\t\tLoss: 0.69640\n",
      "Training Progress: \tEpoch 27 [4160/6768 (61.32%)]\t\tLoss: 0.76401\n",
      "Training Progress: \tEpoch 27 [4480/6768 (66.04%)]\t\tLoss: 0.98373\n",
      "Training Progress: \tEpoch 27 [4800/6768 (70.75%)]\t\tLoss: 0.89933\n",
      "Training Progress: \tEpoch 27 [5120/6768 (75.47%)]\t\tLoss: 0.90111\n",
      "Training Progress: \tEpoch 27 [5440/6768 (80.19%)]\t\tLoss: 0.95941\n",
      "Training Progress: \tEpoch 27 [5760/6768 (84.91%)]\t\tLoss: 0.76397\n",
      "Training Progress: \tEpoch 27 [6080/6768 (89.62%)]\t\tLoss: 0.83352\n",
      "Training Progress: \tEpoch 27 [6400/6768 (94.34%)]\t\tLoss: 0.98350\n",
      "Training Progress: \tEpoch 27 [6720/6768 (99.06%)]\t\tLoss: 1.20703\n",
      "\tTrain loss: 0.02085, Accuracy: 5151/6768 (76.00%)\n",
      "\tValidation loss: 0.00045, Accuracy: 1180/1692 (69.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 944/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 28 [0/6768 (0.00%)]\t\tLoss: 0.80229\n",
      "Training Progress: \tEpoch 28 [320/6768 (4.72%)]\t\tLoss: 1.11919\n",
      "Training Progress: \tEpoch 28 [640/6768 (9.43%)]\t\tLoss: 0.90772\n",
      "Training Progress: \tEpoch 28 [960/6768 (14.15%)]\t\tLoss: 1.15516\n",
      "Training Progress: \tEpoch 28 [1280/6768 (18.87%)]\t\tLoss: 0.92112\n",
      "Training Progress: \tEpoch 28 [1600/6768 (23.58%)]\t\tLoss: 0.99894\n",
      "Training Progress: \tEpoch 28 [1920/6768 (28.30%)]\t\tLoss: 1.04983\n",
      "Training Progress: \tEpoch 28 [2240/6768 (33.02%)]\t\tLoss: 1.17447\n",
      "Training Progress: \tEpoch 28 [2560/6768 (37.74%)]\t\tLoss: 0.96991\n",
      "Training Progress: \tEpoch 28 [2880/6768 (42.45%)]\t\tLoss: 0.95307\n",
      "Training Progress: \tEpoch 28 [3200/6768 (47.17%)]\t\tLoss: 0.99186\n",
      "Training Progress: \tEpoch 28 [3520/6768 (51.89%)]\t\tLoss: 0.84982\n",
      "Training Progress: \tEpoch 28 [3840/6768 (56.60%)]\t\tLoss: 1.04348\n",
      "Training Progress: \tEpoch 28 [4160/6768 (61.32%)]\t\tLoss: 0.81230\n",
      "Training Progress: \tEpoch 28 [4480/6768 (66.04%)]\t\tLoss: 0.91250\n",
      "Training Progress: \tEpoch 28 [4800/6768 (70.75%)]\t\tLoss: 0.99901\n",
      "Training Progress: \tEpoch 28 [5120/6768 (75.47%)]\t\tLoss: 1.03260\n",
      "Training Progress: \tEpoch 28 [5440/6768 (80.19%)]\t\tLoss: 0.76313\n",
      "Training Progress: \tEpoch 28 [5760/6768 (84.91%)]\t\tLoss: 0.75795\n",
      "Training Progress: \tEpoch 28 [6080/6768 (89.62%)]\t\tLoss: 0.67176\n",
      "Training Progress: \tEpoch 28 [6400/6768 (94.34%)]\t\tLoss: 0.91400\n",
      "Training Progress: \tEpoch 28 [6720/6768 (99.06%)]\t\tLoss: 1.03159\n",
      "\tTrain loss: 0.02036, Accuracy: 5261/6768 (77.00%)\n",
      "\tValidation loss: 0.00044, Accuracy: 1235/1692 (72.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 972/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 29 [0/6768 (0.00%)]\t\tLoss: 0.77108\n",
      "Training Progress: \tEpoch 29 [320/6768 (4.72%)]\t\tLoss: 0.82277\n",
      "Training Progress: \tEpoch 29 [640/6768 (9.43%)]\t\tLoss: 0.84084\n",
      "Training Progress: \tEpoch 29 [960/6768 (14.15%)]\t\tLoss: 0.85615\n",
      "Training Progress: \tEpoch 29 [1280/6768 (18.87%)]\t\tLoss: 0.88512\n",
      "Training Progress: \tEpoch 29 [1600/6768 (23.58%)]\t\tLoss: 0.87465\n",
      "Training Progress: \tEpoch 29 [1920/6768 (28.30%)]\t\tLoss: 0.86310\n",
      "Training Progress: \tEpoch 29 [2240/6768 (33.02%)]\t\tLoss: 0.99625\n",
      "Training Progress: \tEpoch 29 [2560/6768 (37.74%)]\t\tLoss: 0.94492\n",
      "Training Progress: \tEpoch 29 [2880/6768 (42.45%)]\t\tLoss: 1.17611\n",
      "Training Progress: \tEpoch 29 [3200/6768 (47.17%)]\t\tLoss: 0.79050\n",
      "Training Progress: \tEpoch 29 [3520/6768 (51.89%)]\t\tLoss: 1.06324\n",
      "Training Progress: \tEpoch 29 [3840/6768 (56.60%)]\t\tLoss: 0.78420\n",
      "Training Progress: \tEpoch 29 [4160/6768 (61.32%)]\t\tLoss: 1.01128\n",
      "Training Progress: \tEpoch 29 [4480/6768 (66.04%)]\t\tLoss: 1.13469\n",
      "Training Progress: \tEpoch 29 [4800/6768 (70.75%)]\t\tLoss: 1.09671\n",
      "Training Progress: \tEpoch 29 [5120/6768 (75.47%)]\t\tLoss: 0.91765\n",
      "Training Progress: \tEpoch 29 [5440/6768 (80.19%)]\t\tLoss: 0.81567\n",
      "Training Progress: \tEpoch 29 [5760/6768 (84.91%)]\t\tLoss: 0.74716\n",
      "Training Progress: \tEpoch 29 [6080/6768 (89.62%)]\t\tLoss: 0.75979\n",
      "Training Progress: \tEpoch 29 [6400/6768 (94.34%)]\t\tLoss: 0.79869\n",
      "Training Progress: \tEpoch 29 [6720/6768 (99.06%)]\t\tLoss: 0.99151\n",
      "\tTrain loss: 0.01977, Accuracy: 5352/6768 (79.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1239/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 952/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 30 [0/6768 (0.00%)]\t\tLoss: 0.76783\n",
      "Training Progress: \tEpoch 30 [320/6768 (4.72%)]\t\tLoss: 0.95758\n",
      "Training Progress: \tEpoch 30 [640/6768 (9.43%)]\t\tLoss: 0.73356\n",
      "Training Progress: \tEpoch 30 [960/6768 (14.15%)]\t\tLoss: 0.87363\n",
      "Training Progress: \tEpoch 30 [1280/6768 (18.87%)]\t\tLoss: 0.85087\n",
      "Training Progress: \tEpoch 30 [1600/6768 (23.58%)]\t\tLoss: 0.84809\n",
      "Training Progress: \tEpoch 30 [1920/6768 (28.30%)]\t\tLoss: 0.91290\n",
      "Training Progress: \tEpoch 30 [2240/6768 (33.02%)]\t\tLoss: 1.18050\n",
      "Training Progress: \tEpoch 30 [2560/6768 (37.74%)]\t\tLoss: 0.97614\n",
      "Training Progress: \tEpoch 30 [2880/6768 (42.45%)]\t\tLoss: 0.90407\n",
      "Training Progress: \tEpoch 30 [3200/6768 (47.17%)]\t\tLoss: 0.87414\n",
      "Training Progress: \tEpoch 30 [3520/6768 (51.89%)]\t\tLoss: 0.86441\n",
      "Training Progress: \tEpoch 30 [3840/6768 (56.60%)]\t\tLoss: 0.79300\n",
      "Training Progress: \tEpoch 30 [4160/6768 (61.32%)]\t\tLoss: 0.88395\n",
      "Training Progress: \tEpoch 30 [4480/6768 (66.04%)]\t\tLoss: 0.95107\n",
      "Training Progress: \tEpoch 30 [4800/6768 (70.75%)]\t\tLoss: 0.99436\n",
      "Training Progress: \tEpoch 30 [5120/6768 (75.47%)]\t\tLoss: 0.83912\n",
      "Training Progress: \tEpoch 30 [5440/6768 (80.19%)]\t\tLoss: 1.03495\n",
      "Training Progress: \tEpoch 30 [5760/6768 (84.91%)]\t\tLoss: 0.75217\n",
      "Training Progress: \tEpoch 30 [6080/6768 (89.62%)]\t\tLoss: 0.79107\n",
      "Training Progress: \tEpoch 30 [6400/6768 (94.34%)]\t\tLoss: 0.77920\n",
      "Training Progress: \tEpoch 30 [6720/6768 (99.06%)]\t\tLoss: 1.07220\n",
      "\tTrain loss: 0.02014, Accuracy: 5327/6768 (78.00%)\n",
      "\tValidation loss: 0.00043, Accuracy: 1246/1692 (73.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 942/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 31 [0/6768 (0.00%)]\t\tLoss: 0.72808\n",
      "Training Progress: \tEpoch 31 [320/6768 (4.72%)]\t\tLoss: 0.92624\n",
      "Training Progress: \tEpoch 31 [640/6768 (9.43%)]\t\tLoss: 0.77015\n",
      "Training Progress: \tEpoch 31 [960/6768 (14.15%)]\t\tLoss: 0.97620\n",
      "Training Progress: \tEpoch 31 [1280/6768 (18.87%)]\t\tLoss: 0.88616\n",
      "Training Progress: \tEpoch 31 [1600/6768 (23.58%)]\t\tLoss: 1.00386\n",
      "Training Progress: \tEpoch 31 [1920/6768 (28.30%)]\t\tLoss: 0.81503\n",
      "Training Progress: \tEpoch 31 [2240/6768 (33.02%)]\t\tLoss: 1.09290\n",
      "Training Progress: \tEpoch 31 [2560/6768 (37.74%)]\t\tLoss: 1.01890\n",
      "Training Progress: \tEpoch 31 [2880/6768 (42.45%)]\t\tLoss: 1.02618\n",
      "Training Progress: \tEpoch 31 [3200/6768 (47.17%)]\t\tLoss: 0.62866\n",
      "Training Progress: \tEpoch 31 [3520/6768 (51.89%)]\t\tLoss: 1.07200\n",
      "Training Progress: \tEpoch 31 [3840/6768 (56.60%)]\t\tLoss: 0.81387\n",
      "Training Progress: \tEpoch 31 [4160/6768 (61.32%)]\t\tLoss: 0.91064\n",
      "Training Progress: \tEpoch 31 [4480/6768 (66.04%)]\t\tLoss: 0.94934\n",
      "Training Progress: \tEpoch 31 [4800/6768 (70.75%)]\t\tLoss: 1.07442\n",
      "Training Progress: \tEpoch 31 [5120/6768 (75.47%)]\t\tLoss: 0.77583\n",
      "Training Progress: \tEpoch 31 [5440/6768 (80.19%)]\t\tLoss: 1.01968\n",
      "Training Progress: \tEpoch 31 [5760/6768 (84.91%)]\t\tLoss: 0.91110\n",
      "Training Progress: \tEpoch 31 [6080/6768 (89.62%)]\t\tLoss: 0.81943\n",
      "Training Progress: \tEpoch 31 [6400/6768 (94.34%)]\t\tLoss: 0.89550\n",
      "Training Progress: \tEpoch 31 [6720/6768 (99.06%)]\t\tLoss: 0.98890\n",
      "\tTrain loss: 0.01860, Accuracy: 5411/6768 (79.00%)\n",
      "\tValidation loss: 0.00041, Accuracy: 1264/1692 (74.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 950/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 32 [0/6768 (0.00%)]\t\tLoss: 0.78939\n",
      "Training Progress: \tEpoch 32 [320/6768 (4.72%)]\t\tLoss: 1.11347\n",
      "Training Progress: \tEpoch 32 [640/6768 (9.43%)]\t\tLoss: 0.81882\n",
      "Training Progress: \tEpoch 32 [960/6768 (14.15%)]\t\tLoss: 0.94209\n",
      "Training Progress: \tEpoch 32 [1280/6768 (18.87%)]\t\tLoss: 0.93524\n",
      "Training Progress: \tEpoch 32 [1600/6768 (23.58%)]\t\tLoss: 0.81304\n",
      "Training Progress: \tEpoch 32 [1920/6768 (28.30%)]\t\tLoss: 0.78258\n",
      "Training Progress: \tEpoch 32 [2240/6768 (33.02%)]\t\tLoss: 0.96961\n",
      "Training Progress: \tEpoch 32 [2560/6768 (37.74%)]\t\tLoss: 0.84897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 32 [2880/6768 (42.45%)]\t\tLoss: 0.77901\n",
      "Training Progress: \tEpoch 32 [3200/6768 (47.17%)]\t\tLoss: 0.70830\n",
      "Training Progress: \tEpoch 32 [3520/6768 (51.89%)]\t\tLoss: 1.01335\n",
      "Training Progress: \tEpoch 32 [3840/6768 (56.60%)]\t\tLoss: 0.63041\n",
      "Training Progress: \tEpoch 32 [4160/6768 (61.32%)]\t\tLoss: 1.00838\n",
      "Training Progress: \tEpoch 32 [4480/6768 (66.04%)]\t\tLoss: 0.93742\n",
      "Training Progress: \tEpoch 32 [4800/6768 (70.75%)]\t\tLoss: 0.95734\n",
      "Training Progress: \tEpoch 32 [5120/6768 (75.47%)]\t\tLoss: 0.90533\n",
      "Training Progress: \tEpoch 32 [5440/6768 (80.19%)]\t\tLoss: 1.04489\n",
      "Training Progress: \tEpoch 32 [5760/6768 (84.91%)]\t\tLoss: 0.63519\n",
      "Training Progress: \tEpoch 32 [6080/6768 (89.62%)]\t\tLoss: 0.57479\n",
      "Training Progress: \tEpoch 32 [6400/6768 (94.34%)]\t\tLoss: 0.90540\n",
      "Training Progress: \tEpoch 32 [6720/6768 (99.06%)]\t\tLoss: 1.02692\n",
      "\tTrain loss: 0.01896, Accuracy: 5396/6768 (79.00%)\n",
      "\tValidation loss: 0.00042, Accuracy: 1256/1692 (74.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 905/1772 (51.00%)\n",
      "\n",
      "Training Progress: \tEpoch 33 [0/6768 (0.00%)]\t\tLoss: 0.68933\n",
      "Training Progress: \tEpoch 33 [320/6768 (4.72%)]\t\tLoss: 0.97026\n",
      "Training Progress: \tEpoch 33 [640/6768 (9.43%)]\t\tLoss: 0.78954\n",
      "Training Progress: \tEpoch 33 [960/6768 (14.15%)]\t\tLoss: 0.95301\n",
      "Training Progress: \tEpoch 33 [1280/6768 (18.87%)]\t\tLoss: 0.89554\n",
      "Training Progress: \tEpoch 33 [1600/6768 (23.58%)]\t\tLoss: 0.93146\n",
      "Training Progress: \tEpoch 33 [1920/6768 (28.30%)]\t\tLoss: 0.75560\n",
      "Training Progress: \tEpoch 33 [2240/6768 (33.02%)]\t\tLoss: 1.00143\n",
      "Training Progress: \tEpoch 33 [2560/6768 (37.74%)]\t\tLoss: 0.95901\n",
      "Training Progress: \tEpoch 33 [2880/6768 (42.45%)]\t\tLoss: 0.95671\n",
      "Training Progress: \tEpoch 33 [3200/6768 (47.17%)]\t\tLoss: 0.85637\n",
      "Training Progress: \tEpoch 33 [3520/6768 (51.89%)]\t\tLoss: 1.01809\n",
      "Training Progress: \tEpoch 33 [3840/6768 (56.60%)]\t\tLoss: 0.79512\n",
      "Training Progress: \tEpoch 33 [4160/6768 (61.32%)]\t\tLoss: 0.84324\n",
      "Training Progress: \tEpoch 33 [4480/6768 (66.04%)]\t\tLoss: 1.00856\n",
      "Training Progress: \tEpoch 33 [4800/6768 (70.75%)]\t\tLoss: 0.99084\n",
      "Training Progress: \tEpoch 33 [5120/6768 (75.47%)]\t\tLoss: 0.89512\n",
      "Training Progress: \tEpoch 33 [5440/6768 (80.19%)]\t\tLoss: 0.93368\n",
      "Training Progress: \tEpoch 33 [5760/6768 (84.91%)]\t\tLoss: 0.69572\n",
      "Training Progress: \tEpoch 33 [6080/6768 (89.62%)]\t\tLoss: 0.58069\n",
      "Training Progress: \tEpoch 33 [6400/6768 (94.34%)]\t\tLoss: 1.05009\n",
      "Training Progress: \tEpoch 33 [6720/6768 (99.06%)]\t\tLoss: 1.15992\n",
      "\tTrain loss: 0.01794, Accuracy: 5485/6768 (81.00%)\n",
      "\tValidation loss: 0.00040, Accuracy: 1294/1692 (76.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 956/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 34 [0/6768 (0.00%)]\t\tLoss: 0.87295\n",
      "Training Progress: \tEpoch 34 [320/6768 (4.72%)]\t\tLoss: 0.77579\n",
      "Training Progress: \tEpoch 34 [640/6768 (9.43%)]\t\tLoss: 0.92387\n",
      "Training Progress: \tEpoch 34 [960/6768 (14.15%)]\t\tLoss: 0.91778\n",
      "Training Progress: \tEpoch 34 [1280/6768 (18.87%)]\t\tLoss: 0.92642\n",
      "Training Progress: \tEpoch 34 [1600/6768 (23.58%)]\t\tLoss: 0.69710\n",
      "Training Progress: \tEpoch 34 [1920/6768 (28.30%)]\t\tLoss: 0.83302\n",
      "Training Progress: \tEpoch 34 [2240/6768 (33.02%)]\t\tLoss: 0.81766\n",
      "Training Progress: \tEpoch 34 [2560/6768 (37.74%)]\t\tLoss: 0.78037\n",
      "Training Progress: \tEpoch 34 [2880/6768 (42.45%)]\t\tLoss: 0.71162\n",
      "Training Progress: \tEpoch 34 [3200/6768 (47.17%)]\t\tLoss: 0.85836\n",
      "Training Progress: \tEpoch 34 [3520/6768 (51.89%)]\t\tLoss: 0.82303\n",
      "Training Progress: \tEpoch 34 [3840/6768 (56.60%)]\t\tLoss: 0.83452\n",
      "Training Progress: \tEpoch 34 [4160/6768 (61.32%)]\t\tLoss: 0.70400\n",
      "Training Progress: \tEpoch 34 [4480/6768 (66.04%)]\t\tLoss: 1.00056\n",
      "Training Progress: \tEpoch 34 [4800/6768 (70.75%)]\t\tLoss: 1.10465\n",
      "Training Progress: \tEpoch 34 [5120/6768 (75.47%)]\t\tLoss: 1.04321\n",
      "Training Progress: \tEpoch 34 [5440/6768 (80.19%)]\t\tLoss: 1.02035\n",
      "Training Progress: \tEpoch 34 [5760/6768 (84.91%)]\t\tLoss: 0.87279\n",
      "Training Progress: \tEpoch 34 [6080/6768 (89.62%)]\t\tLoss: 0.70005\n",
      "Training Progress: \tEpoch 34 [6400/6768 (94.34%)]\t\tLoss: 0.89324\n",
      "Training Progress: \tEpoch 34 [6720/6768 (99.06%)]\t\tLoss: 1.08367\n",
      "\tTrain loss: 0.01729, Accuracy: 5529/6768 (81.00%)\n",
      "\tValidation loss: 0.00039, Accuracy: 1288/1692 (76.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 934/1772 (52.00%)\n",
      "\n",
      "Training Progress: \tEpoch 35 [0/6768 (0.00%)]\t\tLoss: 0.87184\n",
      "Training Progress: \tEpoch 35 [320/6768 (4.72%)]\t\tLoss: 0.84493\n",
      "Training Progress: \tEpoch 35 [640/6768 (9.43%)]\t\tLoss: 0.68006\n",
      "Training Progress: \tEpoch 35 [960/6768 (14.15%)]\t\tLoss: 0.79872\n",
      "Training Progress: \tEpoch 35 [1280/6768 (18.87%)]\t\tLoss: 1.11609\n",
      "Training Progress: \tEpoch 35 [1600/6768 (23.58%)]\t\tLoss: 0.63699\n",
      "Training Progress: \tEpoch 35 [1920/6768 (28.30%)]\t\tLoss: 0.83621\n",
      "Training Progress: \tEpoch 35 [2240/6768 (33.02%)]\t\tLoss: 0.99523\n",
      "Training Progress: \tEpoch 35 [2560/6768 (37.74%)]\t\tLoss: 0.91026\n",
      "Training Progress: \tEpoch 35 [2880/6768 (42.45%)]\t\tLoss: 1.00225\n",
      "Training Progress: \tEpoch 35 [3200/6768 (47.17%)]\t\tLoss: 0.96778\n",
      "Training Progress: \tEpoch 35 [3520/6768 (51.89%)]\t\tLoss: 1.00678\n",
      "Training Progress: \tEpoch 35 [3840/6768 (56.60%)]\t\tLoss: 0.79404\n",
      "Training Progress: \tEpoch 35 [4160/6768 (61.32%)]\t\tLoss: 0.76640\n",
      "Training Progress: \tEpoch 35 [4480/6768 (66.04%)]\t\tLoss: 0.97919\n",
      "Training Progress: \tEpoch 35 [4800/6768 (70.75%)]\t\tLoss: 1.11060\n",
      "Training Progress: \tEpoch 35 [5120/6768 (75.47%)]\t\tLoss: 0.78635\n",
      "Training Progress: \tEpoch 35 [5440/6768 (80.19%)]\t\tLoss: 0.89251\n",
      "Training Progress: \tEpoch 35 [5760/6768 (84.91%)]\t\tLoss: 0.81354\n",
      "Training Progress: \tEpoch 35 [6080/6768 (89.62%)]\t\tLoss: 0.54331\n",
      "Training Progress: \tEpoch 35 [6400/6768 (94.34%)]\t\tLoss: 0.80349\n",
      "Training Progress: \tEpoch 35 [6720/6768 (99.06%)]\t\tLoss: 0.92495\n",
      "\tTrain loss: 0.01687, Accuracy: 5605/6768 (82.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1314/1692 (77.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 947/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 36 [0/6768 (0.00%)]\t\tLoss: 0.78897\n",
      "Training Progress: \tEpoch 36 [320/6768 (4.72%)]\t\tLoss: 1.12909\n",
      "Training Progress: \tEpoch 36 [640/6768 (9.43%)]\t\tLoss: 0.73636\n",
      "Training Progress: \tEpoch 36 [960/6768 (14.15%)]\t\tLoss: 0.79795\n",
      "Training Progress: \tEpoch 36 [1280/6768 (18.87%)]\t\tLoss: 0.93571\n",
      "Training Progress: \tEpoch 36 [1600/6768 (23.58%)]\t\tLoss: 0.78883\n",
      "Training Progress: \tEpoch 36 [1920/6768 (28.30%)]\t\tLoss: 0.67530\n",
      "Training Progress: \tEpoch 36 [2240/6768 (33.02%)]\t\tLoss: 0.91081\n",
      "Training Progress: \tEpoch 36 [2560/6768 (37.74%)]\t\tLoss: 0.96143\n",
      "Training Progress: \tEpoch 36 [2880/6768 (42.45%)]\t\tLoss: 0.88630\n",
      "Training Progress: \tEpoch 36 [3200/6768 (47.17%)]\t\tLoss: 0.67652\n",
      "Training Progress: \tEpoch 36 [3520/6768 (51.89%)]\t\tLoss: 0.63426\n",
      "Training Progress: \tEpoch 36 [3840/6768 (56.60%)]\t\tLoss: 0.89114\n",
      "Training Progress: \tEpoch 36 [4160/6768 (61.32%)]\t\tLoss: 0.79183\n",
      "Training Progress: \tEpoch 36 [4480/6768 (66.04%)]\t\tLoss: 0.96336\n",
      "Training Progress: \tEpoch 36 [4800/6768 (70.75%)]\t\tLoss: 0.88829\n",
      "Training Progress: \tEpoch 36 [5120/6768 (75.47%)]\t\tLoss: 0.83052\n",
      "Training Progress: \tEpoch 36 [5440/6768 (80.19%)]\t\tLoss: 1.05362\n",
      "Training Progress: \tEpoch 36 [5760/6768 (84.91%)]\t\tLoss: 0.76964\n",
      "Training Progress: \tEpoch 36 [6080/6768 (89.62%)]\t\tLoss: 0.78387\n",
      "Training Progress: \tEpoch 36 [6400/6768 (94.34%)]\t\tLoss: 0.82300\n",
      "Training Progress: \tEpoch 36 [6720/6768 (99.06%)]\t\tLoss: 0.96659\n",
      "\tTrain loss: 0.01648, Accuracy: 5725/6768 (84.00%)\n",
      "\tValidation loss: 0.00038, Accuracy: 1333/1692 (78.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 965/1772 (54.00%)\n",
      "\n",
      "Training Progress: \tEpoch 37 [0/6768 (0.00%)]\t\tLoss: 0.74100\n",
      "Training Progress: \tEpoch 37 [320/6768 (4.72%)]\t\tLoss: 0.89686\n",
      "Training Progress: \tEpoch 37 [640/6768 (9.43%)]\t\tLoss: 0.68000\n",
      "Training Progress: \tEpoch 37 [960/6768 (14.15%)]\t\tLoss: 0.72496\n",
      "Training Progress: \tEpoch 37 [1280/6768 (18.87%)]\t\tLoss: 0.86076\n",
      "Training Progress: \tEpoch 37 [1600/6768 (23.58%)]\t\tLoss: 0.77774\n",
      "Training Progress: \tEpoch 37 [1920/6768 (28.30%)]\t\tLoss: 0.70558\n",
      "Training Progress: \tEpoch 37 [2240/6768 (33.02%)]\t\tLoss: 0.81789\n",
      "Training Progress: \tEpoch 37 [2560/6768 (37.74%)]\t\tLoss: 0.90637\n",
      "Training Progress: \tEpoch 37 [2880/6768 (42.45%)]\t\tLoss: 0.99983\n",
      "Training Progress: \tEpoch 37 [3200/6768 (47.17%)]\t\tLoss: 0.76882\n",
      "Training Progress: \tEpoch 37 [3520/6768 (51.89%)]\t\tLoss: 0.59002\n",
      "Training Progress: \tEpoch 37 [3840/6768 (56.60%)]\t\tLoss: 0.79874\n",
      "Training Progress: \tEpoch 37 [4160/6768 (61.32%)]\t\tLoss: 0.92890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 37 [4480/6768 (66.04%)]\t\tLoss: 1.13569\n",
      "Training Progress: \tEpoch 37 [4800/6768 (70.75%)]\t\tLoss: 1.02004\n",
      "Training Progress: \tEpoch 37 [5120/6768 (75.47%)]\t\tLoss: 0.80154\n",
      "Training Progress: \tEpoch 37 [5440/6768 (80.19%)]\t\tLoss: 1.09079\n",
      "Training Progress: \tEpoch 37 [5760/6768 (84.91%)]\t\tLoss: 1.10605\n",
      "Training Progress: \tEpoch 37 [6080/6768 (89.62%)]\t\tLoss: 0.70585\n",
      "Training Progress: \tEpoch 37 [6400/6768 (94.34%)]\t\tLoss: 1.02178\n",
      "Training Progress: \tEpoch 37 [6720/6768 (99.06%)]\t\tLoss: 0.90634\n",
      "\tTrain loss: 0.01565, Accuracy: 5707/6768 (84.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1334/1692 (78.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 946/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 38 [0/6768 (0.00%)]\t\tLoss: 0.65656\n",
      "Training Progress: \tEpoch 38 [320/6768 (4.72%)]\t\tLoss: 0.86294\n",
      "Training Progress: \tEpoch 38 [640/6768 (9.43%)]\t\tLoss: 0.55018\n",
      "Training Progress: \tEpoch 38 [960/6768 (14.15%)]\t\tLoss: 0.88426\n",
      "Training Progress: \tEpoch 38 [1280/6768 (18.87%)]\t\tLoss: 0.62320\n",
      "Training Progress: \tEpoch 38 [1600/6768 (23.58%)]\t\tLoss: 0.52092\n",
      "Training Progress: \tEpoch 38 [1920/6768 (28.30%)]\t\tLoss: 0.75762\n",
      "Training Progress: \tEpoch 38 [2240/6768 (33.02%)]\t\tLoss: 0.87398\n",
      "Training Progress: \tEpoch 38 [2560/6768 (37.74%)]\t\tLoss: 1.00720\n",
      "Training Progress: \tEpoch 38 [2880/6768 (42.45%)]\t\tLoss: 0.96112\n",
      "Training Progress: \tEpoch 38 [3200/6768 (47.17%)]\t\tLoss: 0.96187\n",
      "Training Progress: \tEpoch 38 [3520/6768 (51.89%)]\t\tLoss: 0.94889\n",
      "Training Progress: \tEpoch 38 [3840/6768 (56.60%)]\t\tLoss: 0.79429\n",
      "Training Progress: \tEpoch 38 [4160/6768 (61.32%)]\t\tLoss: 0.76519\n",
      "Training Progress: \tEpoch 38 [4480/6768 (66.04%)]\t\tLoss: 0.92970\n",
      "Training Progress: \tEpoch 38 [4800/6768 (70.75%)]\t\tLoss: 0.93414\n",
      "Training Progress: \tEpoch 38 [5120/6768 (75.47%)]\t\tLoss: 0.84330\n",
      "Training Progress: \tEpoch 38 [5440/6768 (80.19%)]\t\tLoss: 0.86184\n",
      "Training Progress: \tEpoch 38 [5760/6768 (84.91%)]\t\tLoss: 0.74649\n",
      "Training Progress: \tEpoch 38 [6080/6768 (89.62%)]\t\tLoss: 0.82487\n",
      "Training Progress: \tEpoch 38 [6400/6768 (94.34%)]\t\tLoss: 0.60789\n",
      "Training Progress: \tEpoch 38 [6720/6768 (99.06%)]\t\tLoss: 0.72816\n",
      "\tTrain loss: 0.01463, Accuracy: 5797/6768 (85.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1337/1692 (79.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 982/1772 (55.00%)\n",
      "\n",
      "Training Progress: \tEpoch 39 [0/6768 (0.00%)]\t\tLoss: 0.64326\n",
      "Training Progress: \tEpoch 39 [320/6768 (4.72%)]\t\tLoss: 1.09126\n",
      "Training Progress: \tEpoch 39 [640/6768 (9.43%)]\t\tLoss: 0.81376\n",
      "Training Progress: \tEpoch 39 [960/6768 (14.15%)]\t\tLoss: 0.73582\n",
      "Training Progress: \tEpoch 39 [1280/6768 (18.87%)]\t\tLoss: 1.07149\n",
      "Training Progress: \tEpoch 39 [1600/6768 (23.58%)]\t\tLoss: 1.00237\n",
      "Training Progress: \tEpoch 39 [1920/6768 (28.30%)]\t\tLoss: 0.67738\n",
      "Training Progress: \tEpoch 39 [2240/6768 (33.02%)]\t\tLoss: 0.88629\n",
      "Training Progress: \tEpoch 39 [2560/6768 (37.74%)]\t\tLoss: 0.82084\n",
      "Training Progress: \tEpoch 39 [2880/6768 (42.45%)]\t\tLoss: 0.96649\n",
      "Training Progress: \tEpoch 39 [3200/6768 (47.17%)]\t\tLoss: 0.81388\n",
      "Training Progress: \tEpoch 39 [3520/6768 (51.89%)]\t\tLoss: 0.82416\n",
      "Training Progress: \tEpoch 39 [3840/6768 (56.60%)]\t\tLoss: 0.70673\n",
      "Training Progress: \tEpoch 39 [4160/6768 (61.32%)]\t\tLoss: 0.75423\n",
      "Training Progress: \tEpoch 39 [4480/6768 (66.04%)]\t\tLoss: 0.94654\n",
      "Training Progress: \tEpoch 39 [4800/6768 (70.75%)]\t\tLoss: 0.90096\n",
      "Training Progress: \tEpoch 39 [5120/6768 (75.47%)]\t\tLoss: 1.01265\n",
      "Training Progress: \tEpoch 39 [5440/6768 (80.19%)]\t\tLoss: 0.78562\n",
      "Training Progress: \tEpoch 39 [5760/6768 (84.91%)]\t\tLoss: 0.86096\n",
      "Training Progress: \tEpoch 39 [6080/6768 (89.62%)]\t\tLoss: 0.56935\n",
      "Training Progress: \tEpoch 39 [6400/6768 (94.34%)]\t\tLoss: 0.94487\n",
      "Training Progress: \tEpoch 39 [6720/6768 (99.06%)]\t\tLoss: 0.81640\n",
      "\tTrain loss: 0.01559, Accuracy: 5686/6768 (84.00%)\n",
      "\tValidation loss: 0.00036, Accuracy: 1306/1692 (77.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 955/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 40 [0/6768 (0.00%)]\t\tLoss: 0.55559\n",
      "Training Progress: \tEpoch 40 [320/6768 (4.72%)]\t\tLoss: 1.28639\n",
      "Training Progress: \tEpoch 40 [640/6768 (9.43%)]\t\tLoss: 0.96284\n",
      "Training Progress: \tEpoch 40 [960/6768 (14.15%)]\t\tLoss: 0.86938\n",
      "Training Progress: \tEpoch 40 [1280/6768 (18.87%)]\t\tLoss: 0.78531\n",
      "Training Progress: \tEpoch 40 [1600/6768 (23.58%)]\t\tLoss: 0.67081\n",
      "Training Progress: \tEpoch 40 [1920/6768 (28.30%)]\t\tLoss: 0.79893\n",
      "Training Progress: \tEpoch 40 [2240/6768 (33.02%)]\t\tLoss: 0.95226\n",
      "Training Progress: \tEpoch 40 [2560/6768 (37.74%)]\t\tLoss: 0.72838\n",
      "Training Progress: \tEpoch 40 [2880/6768 (42.45%)]\t\tLoss: 0.91285\n",
      "Training Progress: \tEpoch 40 [3200/6768 (47.17%)]\t\tLoss: 0.96344\n",
      "Training Progress: \tEpoch 40 [3520/6768 (51.89%)]\t\tLoss: 0.64195\n",
      "Training Progress: \tEpoch 40 [3840/6768 (56.60%)]\t\tLoss: 0.77020\n",
      "Training Progress: \tEpoch 40 [4160/6768 (61.32%)]\t\tLoss: 1.06757\n",
      "Training Progress: \tEpoch 40 [4480/6768 (66.04%)]\t\tLoss: 0.89626\n",
      "Training Progress: \tEpoch 40 [4800/6768 (70.75%)]\t\tLoss: 0.82476\n",
      "Training Progress: \tEpoch 40 [5120/6768 (75.47%)]\t\tLoss: 0.85674\n",
      "Training Progress: \tEpoch 40 [5440/6768 (80.19%)]\t\tLoss: 1.00165\n",
      "Training Progress: \tEpoch 40 [5760/6768 (84.91%)]\t\tLoss: 0.74955\n",
      "Training Progress: \tEpoch 40 [6080/6768 (89.62%)]\t\tLoss: 0.71134\n",
      "Training Progress: \tEpoch 40 [6400/6768 (94.34%)]\t\tLoss: 0.55547\n",
      "Training Progress: \tEpoch 40 [6720/6768 (99.06%)]\t\tLoss: 1.09265\n",
      "\tTrain loss: 0.01476, Accuracy: 5833/6768 (86.00%)\n",
      "\tValidation loss: 0.00035, Accuracy: 1346/1692 (79.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 953/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 41 [0/6768 (0.00%)]\t\tLoss: 0.72371\n",
      "Training Progress: \tEpoch 41 [320/6768 (4.72%)]\t\tLoss: 0.77948\n",
      "Training Progress: \tEpoch 41 [640/6768 (9.43%)]\t\tLoss: 0.60897\n",
      "Training Progress: \tEpoch 41 [960/6768 (14.15%)]\t\tLoss: 0.72451\n",
      "Training Progress: \tEpoch 41 [1280/6768 (18.87%)]\t\tLoss: 0.65145\n",
      "Training Progress: \tEpoch 41 [1600/6768 (23.58%)]\t\tLoss: 0.58779\n",
      "Training Progress: \tEpoch 41 [1920/6768 (28.30%)]\t\tLoss: 0.92680\n",
      "Training Progress: \tEpoch 41 [2240/6768 (33.02%)]\t\tLoss: 0.88419\n",
      "Training Progress: \tEpoch 41 [2560/6768 (37.74%)]\t\tLoss: 0.74530\n",
      "Training Progress: \tEpoch 41 [2880/6768 (42.45%)]\t\tLoss: 0.76057\n",
      "Training Progress: \tEpoch 41 [3200/6768 (47.17%)]\t\tLoss: 0.71411\n",
      "Training Progress: \tEpoch 41 [3520/6768 (51.89%)]\t\tLoss: 0.95930\n",
      "Training Progress: \tEpoch 41 [3840/6768 (56.60%)]\t\tLoss: 0.82295\n",
      "Training Progress: \tEpoch 41 [4160/6768 (61.32%)]\t\tLoss: 0.66656\n",
      "Training Progress: \tEpoch 41 [4480/6768 (66.04%)]\t\tLoss: 0.82796\n",
      "Training Progress: \tEpoch 41 [4800/6768 (70.75%)]\t\tLoss: 0.73535\n",
      "Training Progress: \tEpoch 41 [5120/6768 (75.47%)]\t\tLoss: 0.61571\n",
      "Training Progress: \tEpoch 41 [5440/6768 (80.19%)]\t\tLoss: 0.84316\n",
      "Training Progress: \tEpoch 41 [5760/6768 (84.91%)]\t\tLoss: 0.71651\n",
      "Training Progress: \tEpoch 41 [6080/6768 (89.62%)]\t\tLoss: 0.69065\n",
      "Training Progress: \tEpoch 41 [6400/6768 (94.34%)]\t\tLoss: 0.66194\n",
      "Training Progress: \tEpoch 41 [6720/6768 (99.06%)]\t\tLoss: 0.97385\n",
      "\tTrain loss: 0.01417, Accuracy: 5825/6768 (86.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1358/1692 (80.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 945/1772 (53.00%)\n",
      "\n",
      "Training Progress: \tEpoch 42 [0/6768 (0.00%)]\t\tLoss: 0.69901\n",
      "Training Progress: \tEpoch 42 [320/6768 (4.72%)]\t\tLoss: 1.07180\n",
      "Training Progress: \tEpoch 42 [640/6768 (9.43%)]\t\tLoss: 0.67069\n",
      "Training Progress: \tEpoch 42 [960/6768 (14.15%)]\t\tLoss: 0.88829\n",
      "Training Progress: \tEpoch 42 [1280/6768 (18.87%)]\t\tLoss: 0.74218\n",
      "Training Progress: \tEpoch 42 [1600/6768 (23.58%)]\t\tLoss: 0.54013\n",
      "Training Progress: \tEpoch 42 [1920/6768 (28.30%)]\t\tLoss: 0.72200\n",
      "Training Progress: \tEpoch 42 [2240/6768 (33.02%)]\t\tLoss: 0.95771\n",
      "Training Progress: \tEpoch 42 [2560/6768 (37.74%)]\t\tLoss: 0.71526\n",
      "Training Progress: \tEpoch 42 [2880/6768 (42.45%)]\t\tLoss: 0.71338\n",
      "Training Progress: \tEpoch 42 [3200/6768 (47.17%)]\t\tLoss: 0.67538\n",
      "Training Progress: \tEpoch 42 [3520/6768 (51.89%)]\t\tLoss: 0.77534\n",
      "Training Progress: \tEpoch 42 [3840/6768 (56.60%)]\t\tLoss: 0.63867\n",
      "Training Progress: \tEpoch 42 [4160/6768 (61.32%)]\t\tLoss: 0.75523\n",
      "Training Progress: \tEpoch 42 [4480/6768 (66.04%)]\t\tLoss: 0.58378\n",
      "Training Progress: \tEpoch 42 [4800/6768 (70.75%)]\t\tLoss: 0.85215\n",
      "Training Progress: \tEpoch 42 [5120/6768 (75.47%)]\t\tLoss: 0.67812\n",
      "Training Progress: \tEpoch 42 [5440/6768 (80.19%)]\t\tLoss: 1.03009\n",
      "Training Progress: \tEpoch 42 [5760/6768 (84.91%)]\t\tLoss: 0.86126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 42 [6080/6768 (89.62%)]\t\tLoss: 0.68053\n",
      "Training Progress: \tEpoch 42 [6400/6768 (94.34%)]\t\tLoss: 0.74887\n",
      "Training Progress: \tEpoch 42 [6720/6768 (99.06%)]\t\tLoss: 0.93519\n",
      "\tTrain loss: 0.01439, Accuracy: 5936/6768 (87.00%)\n",
      "\tValidation loss: 0.00034, Accuracy: 1364/1692 (80.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1009/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 43 [0/6768 (0.00%)]\t\tLoss: 0.63179\n",
      "Training Progress: \tEpoch 43 [320/6768 (4.72%)]\t\tLoss: 1.01572\n",
      "Training Progress: \tEpoch 43 [640/6768 (9.43%)]\t\tLoss: 0.81654\n",
      "Training Progress: \tEpoch 43 [960/6768 (14.15%)]\t\tLoss: 0.92902\n",
      "Training Progress: \tEpoch 43 [1280/6768 (18.87%)]\t\tLoss: 0.88359\n",
      "Training Progress: \tEpoch 43 [1600/6768 (23.58%)]\t\tLoss: 0.70176\n",
      "Training Progress: \tEpoch 43 [1920/6768 (28.30%)]\t\tLoss: 0.69130\n",
      "Training Progress: \tEpoch 43 [2240/6768 (33.02%)]\t\tLoss: 1.04701\n",
      "Training Progress: \tEpoch 43 [2560/6768 (37.74%)]\t\tLoss: 0.66496\n",
      "Training Progress: \tEpoch 43 [2880/6768 (42.45%)]\t\tLoss: 0.61626\n",
      "Training Progress: \tEpoch 43 [3200/6768 (47.17%)]\t\tLoss: 0.70253\n",
      "Training Progress: \tEpoch 43 [3520/6768 (51.89%)]\t\tLoss: 0.71911\n",
      "Training Progress: \tEpoch 43 [3840/6768 (56.60%)]\t\tLoss: 0.73846\n",
      "Training Progress: \tEpoch 43 [4160/6768 (61.32%)]\t\tLoss: 0.88949\n",
      "Training Progress: \tEpoch 43 [4480/6768 (66.04%)]\t\tLoss: 0.82882\n",
      "Training Progress: \tEpoch 43 [4800/6768 (70.75%)]\t\tLoss: 1.03986\n",
      "Training Progress: \tEpoch 43 [5120/6768 (75.47%)]\t\tLoss: 0.81846\n",
      "Training Progress: \tEpoch 43 [5440/6768 (80.19%)]\t\tLoss: 0.93355\n",
      "Training Progress: \tEpoch 43 [5760/6768 (84.91%)]\t\tLoss: 0.70754\n",
      "Training Progress: \tEpoch 43 [6080/6768 (89.62%)]\t\tLoss: 0.82207\n",
      "Training Progress: \tEpoch 43 [6400/6768 (94.34%)]\t\tLoss: 0.80682\n",
      "Training Progress: \tEpoch 43 [6720/6768 (99.06%)]\t\tLoss: 0.94384\n",
      "\tTrain loss: 0.01348, Accuracy: 5901/6768 (87.00%)\n",
      "\tValidation loss: 0.00033, Accuracy: 1368/1692 (80.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1016/1772 (57.00%)\n",
      "\n",
      "Training Progress: \tEpoch 44 [0/6768 (0.00%)]\t\tLoss: 0.74204\n",
      "Training Progress: \tEpoch 44 [320/6768 (4.72%)]\t\tLoss: 0.95971\n",
      "Training Progress: \tEpoch 44 [640/6768 (9.43%)]\t\tLoss: 0.57141\n",
      "Training Progress: \tEpoch 44 [960/6768 (14.15%)]\t\tLoss: 0.96210\n",
      "Training Progress: \tEpoch 44 [1280/6768 (18.87%)]\t\tLoss: 0.65602\n",
      "Training Progress: \tEpoch 44 [1600/6768 (23.58%)]\t\tLoss: 0.62432\n",
      "Training Progress: \tEpoch 44 [1920/6768 (28.30%)]\t\tLoss: 0.69093\n",
      "Training Progress: \tEpoch 44 [2240/6768 (33.02%)]\t\tLoss: 0.91943\n",
      "Training Progress: \tEpoch 44 [2560/6768 (37.74%)]\t\tLoss: 0.91562\n",
      "Training Progress: \tEpoch 44 [2880/6768 (42.45%)]\t\tLoss: 0.74800\n",
      "Training Progress: \tEpoch 44 [3200/6768 (47.17%)]\t\tLoss: 0.78634\n",
      "Training Progress: \tEpoch 44 [3520/6768 (51.89%)]\t\tLoss: 0.80350\n",
      "Training Progress: \tEpoch 44 [3840/6768 (56.60%)]\t\tLoss: 0.85031\n",
      "Training Progress: \tEpoch 44 [4160/6768 (61.32%)]\t\tLoss: 0.73694\n",
      "Training Progress: \tEpoch 44 [4480/6768 (66.04%)]\t\tLoss: 0.95317\n",
      "Training Progress: \tEpoch 44 [4800/6768 (70.75%)]\t\tLoss: 0.71098\n",
      "Training Progress: \tEpoch 44 [5120/6768 (75.47%)]\t\tLoss: 0.61493\n",
      "Training Progress: \tEpoch 44 [5440/6768 (80.19%)]\t\tLoss: 0.90544\n",
      "Training Progress: \tEpoch 44 [5760/6768 (84.91%)]\t\tLoss: 0.68943\n",
      "Training Progress: \tEpoch 44 [6080/6768 (89.62%)]\t\tLoss: 0.59826\n",
      "Training Progress: \tEpoch 44 [6400/6768 (94.34%)]\t\tLoss: 0.66603\n",
      "Training Progress: \tEpoch 44 [6720/6768 (99.06%)]\t\tLoss: 0.82467\n",
      "\tTrain loss: 0.01280, Accuracy: 6016/6768 (88.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1385/1692 (81.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1005/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 45 [0/6768 (0.00%)]\t\tLoss: 0.68050\n",
      "Training Progress: \tEpoch 45 [320/6768 (4.72%)]\t\tLoss: 0.85252\n",
      "Training Progress: \tEpoch 45 [640/6768 (9.43%)]\t\tLoss: 0.84149\n",
      "Training Progress: \tEpoch 45 [960/6768 (14.15%)]\t\tLoss: 0.75423\n",
      "Training Progress: \tEpoch 45 [1280/6768 (18.87%)]\t\tLoss: 0.82353\n",
      "Training Progress: \tEpoch 45 [1600/6768 (23.58%)]\t\tLoss: 0.72220\n",
      "Training Progress: \tEpoch 45 [1920/6768 (28.30%)]\t\tLoss: 0.65463\n",
      "Training Progress: \tEpoch 45 [2240/6768 (33.02%)]\t\tLoss: 0.75112\n",
      "Training Progress: \tEpoch 45 [2560/6768 (37.74%)]\t\tLoss: 0.92158\n",
      "Training Progress: \tEpoch 45 [2880/6768 (42.45%)]\t\tLoss: 0.50720\n",
      "Training Progress: \tEpoch 45 [3200/6768 (47.17%)]\t\tLoss: 0.47373\n",
      "Training Progress: \tEpoch 45 [3520/6768 (51.89%)]\t\tLoss: 0.59601\n",
      "Training Progress: \tEpoch 45 [3840/6768 (56.60%)]\t\tLoss: 0.84664\n",
      "Training Progress: \tEpoch 45 [4160/6768 (61.32%)]\t\tLoss: 1.01463\n",
      "Training Progress: \tEpoch 45 [4480/6768 (66.04%)]\t\tLoss: 0.88119\n",
      "Training Progress: \tEpoch 45 [4800/6768 (70.75%)]\t\tLoss: 0.72493\n",
      "Training Progress: \tEpoch 45 [5120/6768 (75.47%)]\t\tLoss: 0.75486\n",
      "Training Progress: \tEpoch 45 [5440/6768 (80.19%)]\t\tLoss: 1.12692\n",
      "Training Progress: \tEpoch 45 [5760/6768 (84.91%)]\t\tLoss: 0.78105\n",
      "Training Progress: \tEpoch 45 [6080/6768 (89.62%)]\t\tLoss: 0.70391\n",
      "Training Progress: \tEpoch 45 [6400/6768 (94.34%)]\t\tLoss: 0.54596\n",
      "Training Progress: \tEpoch 45 [6720/6768 (99.06%)]\t\tLoss: 0.76468\n",
      "\tTrain loss: 0.01315, Accuracy: 5937/6768 (87.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1392/1692 (82.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1008/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 46 [0/6768 (0.00%)]\t\tLoss: 0.66401\n",
      "Training Progress: \tEpoch 46 [320/6768 (4.72%)]\t\tLoss: 0.82709\n",
      "Training Progress: \tEpoch 46 [640/6768 (9.43%)]\t\tLoss: 0.71640\n",
      "Training Progress: \tEpoch 46 [960/6768 (14.15%)]\t\tLoss: 1.00387\n",
      "Training Progress: \tEpoch 46 [1280/6768 (18.87%)]\t\tLoss: 0.97949\n",
      "Training Progress: \tEpoch 46 [1600/6768 (23.58%)]\t\tLoss: 0.52458\n",
      "Training Progress: \tEpoch 46 [1920/6768 (28.30%)]\t\tLoss: 0.58171\n",
      "Training Progress: \tEpoch 46 [2240/6768 (33.02%)]\t\tLoss: 0.79091\n",
      "Training Progress: \tEpoch 46 [2560/6768 (37.74%)]\t\tLoss: 0.98098\n",
      "Training Progress: \tEpoch 46 [2880/6768 (42.45%)]\t\tLoss: 0.76577\n",
      "Training Progress: \tEpoch 46 [3200/6768 (47.17%)]\t\tLoss: 0.58168\n",
      "Training Progress: \tEpoch 46 [3520/6768 (51.89%)]\t\tLoss: 0.75097\n",
      "Training Progress: \tEpoch 46 [3840/6768 (56.60%)]\t\tLoss: 0.78701\n",
      "Training Progress: \tEpoch 46 [4160/6768 (61.32%)]\t\tLoss: 0.76141\n",
      "Training Progress: \tEpoch 46 [4480/6768 (66.04%)]\t\tLoss: 0.77701\n",
      "Training Progress: \tEpoch 46 [4800/6768 (70.75%)]\t\tLoss: 0.53479\n",
      "Training Progress: \tEpoch 46 [5120/6768 (75.47%)]\t\tLoss: 0.71674\n",
      "Training Progress: \tEpoch 46 [5440/6768 (80.19%)]\t\tLoss: 0.72712\n",
      "Training Progress: \tEpoch 46 [5760/6768 (84.91%)]\t\tLoss: 0.48733\n",
      "Training Progress: \tEpoch 46 [6080/6768 (89.62%)]\t\tLoss: 0.59941\n",
      "Training Progress: \tEpoch 46 [6400/6768 (94.34%)]\t\tLoss: 0.66509\n",
      "Training Progress: \tEpoch 46 [6720/6768 (99.06%)]\t\tLoss: 0.77909\n",
      "\tTrain loss: 0.01316, Accuracy: 5986/6768 (88.00%)\n",
      "\tValidation loss: 0.00032, Accuracy: 1401/1692 (82.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1002/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 47 [0/6768 (0.00%)]\t\tLoss: 0.51845\n",
      "Training Progress: \tEpoch 47 [320/6768 (4.72%)]\t\tLoss: 0.99785\n",
      "Training Progress: \tEpoch 47 [640/6768 (9.43%)]\t\tLoss: 0.67633\n",
      "Training Progress: \tEpoch 47 [960/6768 (14.15%)]\t\tLoss: 0.79894\n",
      "Training Progress: \tEpoch 47 [1280/6768 (18.87%)]\t\tLoss: 0.70049\n",
      "Training Progress: \tEpoch 47 [1600/6768 (23.58%)]\t\tLoss: 0.47816\n",
      "Training Progress: \tEpoch 47 [1920/6768 (28.30%)]\t\tLoss: 0.62071\n",
      "Training Progress: \tEpoch 47 [2240/6768 (33.02%)]\t\tLoss: 0.89102\n",
      "Training Progress: \tEpoch 47 [2560/6768 (37.74%)]\t\tLoss: 0.66530\n",
      "Training Progress: \tEpoch 47 [2880/6768 (42.45%)]\t\tLoss: 0.57049\n",
      "Training Progress: \tEpoch 47 [3200/6768 (47.17%)]\t\tLoss: 0.71667\n",
      "Training Progress: \tEpoch 47 [3520/6768 (51.89%)]\t\tLoss: 0.76114\n",
      "Training Progress: \tEpoch 47 [3840/6768 (56.60%)]\t\tLoss: 0.66653\n",
      "Training Progress: \tEpoch 47 [4160/6768 (61.32%)]\t\tLoss: 0.80747\n",
      "Training Progress: \tEpoch 47 [4480/6768 (66.04%)]\t\tLoss: 1.05835\n",
      "Training Progress: \tEpoch 47 [4800/6768 (70.75%)]\t\tLoss: 0.84941\n",
      "Training Progress: \tEpoch 47 [5120/6768 (75.47%)]\t\tLoss: 0.62974\n",
      "Training Progress: \tEpoch 47 [5440/6768 (80.19%)]\t\tLoss: 0.98833\n",
      "Training Progress: \tEpoch 47 [5760/6768 (84.91%)]\t\tLoss: 0.78413\n",
      "Training Progress: \tEpoch 47 [6080/6768 (89.62%)]\t\tLoss: 0.55174\n",
      "Training Progress: \tEpoch 47 [6400/6768 (94.34%)]\t\tLoss: 0.62188\n",
      "Training Progress: \tEpoch 47 [6720/6768 (99.06%)]\t\tLoss: 1.14071\n",
      "\tTrain loss: 0.01075, Accuracy: 6148/6768 (90.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1437/1692 (84.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00062, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 48 [0/6768 (0.00%)]\t\tLoss: 0.67121\n",
      "Training Progress: \tEpoch 48 [320/6768 (4.72%)]\t\tLoss: 0.81453\n",
      "Training Progress: \tEpoch 48 [640/6768 (9.43%)]\t\tLoss: 0.58598\n",
      "Training Progress: \tEpoch 48 [960/6768 (14.15%)]\t\tLoss: 0.76459\n",
      "Training Progress: \tEpoch 48 [1280/6768 (18.87%)]\t\tLoss: 0.92660\n",
      "Training Progress: \tEpoch 48 [1600/6768 (23.58%)]\t\tLoss: 0.62985\n",
      "Training Progress: \tEpoch 48 [1920/6768 (28.30%)]\t\tLoss: 0.81634\n",
      "Training Progress: \tEpoch 48 [2240/6768 (33.02%)]\t\tLoss: 0.86450\n",
      "Training Progress: \tEpoch 48 [2560/6768 (37.74%)]\t\tLoss: 0.70193\n",
      "Training Progress: \tEpoch 48 [2880/6768 (42.45%)]\t\tLoss: 0.95985\n",
      "Training Progress: \tEpoch 48 [3200/6768 (47.17%)]\t\tLoss: 0.72058\n",
      "Training Progress: \tEpoch 48 [3520/6768 (51.89%)]\t\tLoss: 0.80793\n",
      "Training Progress: \tEpoch 48 [3840/6768 (56.60%)]\t\tLoss: 0.70588\n",
      "Training Progress: \tEpoch 48 [4160/6768 (61.32%)]\t\tLoss: 0.82863\n",
      "Training Progress: \tEpoch 48 [4480/6768 (66.04%)]\t\tLoss: 0.78683\n",
      "Training Progress: \tEpoch 48 [4800/6768 (70.75%)]\t\tLoss: 0.63007\n",
      "Training Progress: \tEpoch 48 [5120/6768 (75.47%)]\t\tLoss: 0.53461\n",
      "Training Progress: \tEpoch 48 [5440/6768 (80.19%)]\t\tLoss: 1.02937\n",
      "Training Progress: \tEpoch 48 [5760/6768 (84.91%)]\t\tLoss: 0.59358\n",
      "Training Progress: \tEpoch 48 [6080/6768 (89.62%)]\t\tLoss: 0.74995\n",
      "Training Progress: \tEpoch 48 [6400/6768 (94.34%)]\t\tLoss: 0.69616\n",
      "Training Progress: \tEpoch 48 [6720/6768 (99.06%)]\t\tLoss: 0.73844\n",
      "\tTrain loss: 0.01028, Accuracy: 6195/6768 (91.00%)\n",
      "\tValidation loss: 0.00027, Accuracy: 1456/1692 (86.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1031/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 49 [0/6768 (0.00%)]\t\tLoss: 0.59187\n",
      "Training Progress: \tEpoch 49 [320/6768 (4.72%)]\t\tLoss: 1.09271\n",
      "Training Progress: \tEpoch 49 [640/6768 (9.43%)]\t\tLoss: 0.64832\n",
      "Training Progress: \tEpoch 49 [960/6768 (14.15%)]\t\tLoss: 0.84715\n",
      "Training Progress: \tEpoch 49 [1280/6768 (18.87%)]\t\tLoss: 0.70018\n",
      "Training Progress: \tEpoch 49 [1600/6768 (23.58%)]\t\tLoss: 0.69527\n",
      "Training Progress: \tEpoch 49 [1920/6768 (28.30%)]\t\tLoss: 0.55944\n",
      "Training Progress: \tEpoch 49 [2240/6768 (33.02%)]\t\tLoss: 0.89432\n",
      "Training Progress: \tEpoch 49 [2560/6768 (37.74%)]\t\tLoss: 0.52701\n",
      "Training Progress: \tEpoch 49 [2880/6768 (42.45%)]\t\tLoss: 0.82729\n",
      "Training Progress: \tEpoch 49 [3200/6768 (47.17%)]\t\tLoss: 0.60929\n",
      "Training Progress: \tEpoch 49 [3520/6768 (51.89%)]\t\tLoss: 0.69080\n",
      "Training Progress: \tEpoch 49 [3840/6768 (56.60%)]\t\tLoss: 0.77949\n",
      "Training Progress: \tEpoch 49 [4160/6768 (61.32%)]\t\tLoss: 0.69609\n",
      "Training Progress: \tEpoch 49 [4480/6768 (66.04%)]\t\tLoss: 0.71772\n",
      "Training Progress: \tEpoch 49 [4800/6768 (70.75%)]\t\tLoss: 0.62171\n",
      "Training Progress: \tEpoch 49 [5120/6768 (75.47%)]\t\tLoss: 0.75125\n",
      "Training Progress: \tEpoch 49 [5440/6768 (80.19%)]\t\tLoss: 0.77878\n",
      "Training Progress: \tEpoch 49 [5760/6768 (84.91%)]\t\tLoss: 0.85731\n",
      "Training Progress: \tEpoch 49 [6080/6768 (89.62%)]\t\tLoss: 0.65642\n",
      "Training Progress: \tEpoch 49 [6400/6768 (94.34%)]\t\tLoss: 0.67803\n",
      "Training Progress: \tEpoch 49 [6720/6768 (99.06%)]\t\tLoss: 0.95809\n",
      "\tTrain loss: 0.01067, Accuracy: 6121/6768 (90.00%)\n",
      "\tValidation loss: 0.00028, Accuracy: 1433/1692 (84.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1037/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 50 [0/6768 (0.00%)]\t\tLoss: 0.64773\n",
      "Training Progress: \tEpoch 50 [320/6768 (4.72%)]\t\tLoss: 1.19942\n",
      "Training Progress: \tEpoch 50 [640/6768 (9.43%)]\t\tLoss: 0.56717\n",
      "Training Progress: \tEpoch 50 [960/6768 (14.15%)]\t\tLoss: 0.66667\n",
      "Training Progress: \tEpoch 50 [1280/6768 (18.87%)]\t\tLoss: 0.80371\n",
      "Training Progress: \tEpoch 50 [1600/6768 (23.58%)]\t\tLoss: 0.43505\n",
      "Training Progress: \tEpoch 50 [1920/6768 (28.30%)]\t\tLoss: 0.61867\n",
      "Training Progress: \tEpoch 50 [2240/6768 (33.02%)]\t\tLoss: 0.76406\n",
      "Training Progress: \tEpoch 50 [2560/6768 (37.74%)]\t\tLoss: 0.86859\n",
      "Training Progress: \tEpoch 50 [2880/6768 (42.45%)]\t\tLoss: 0.75947\n",
      "Training Progress: \tEpoch 50 [3200/6768 (47.17%)]\t\tLoss: 0.74769\n",
      "Training Progress: \tEpoch 50 [3520/6768 (51.89%)]\t\tLoss: 0.84236\n",
      "Training Progress: \tEpoch 50 [3840/6768 (56.60%)]\t\tLoss: 0.62695\n",
      "Training Progress: \tEpoch 50 [4160/6768 (61.32%)]\t\tLoss: 0.80951\n",
      "Training Progress: \tEpoch 50 [4480/6768 (66.04%)]\t\tLoss: 0.60136\n",
      "Training Progress: \tEpoch 50 [4800/6768 (70.75%)]\t\tLoss: 0.75760\n",
      "Training Progress: \tEpoch 50 [5120/6768 (75.47%)]\t\tLoss: 0.62847\n",
      "Training Progress: \tEpoch 50 [5440/6768 (80.19%)]\t\tLoss: 0.84040\n",
      "Training Progress: \tEpoch 50 [5760/6768 (84.91%)]\t\tLoss: 0.40725\n",
      "Training Progress: \tEpoch 50 [6080/6768 (89.62%)]\t\tLoss: 0.50551\n",
      "Training Progress: \tEpoch 50 [6400/6768 (94.34%)]\t\tLoss: 0.70941\n",
      "Training Progress: \tEpoch 50 [6720/6768 (99.06%)]\t\tLoss: 0.92254\n",
      "\tTrain loss: 0.00997, Accuracy: 6148/6768 (90.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1439/1692 (85.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1032/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 51 [0/6768 (0.00%)]\t\tLoss: 0.72442\n",
      "Training Progress: \tEpoch 51 [320/6768 (4.72%)]\t\tLoss: 0.76574\n",
      "Training Progress: \tEpoch 51 [640/6768 (9.43%)]\t\tLoss: 0.64289\n",
      "Training Progress: \tEpoch 51 [960/6768 (14.15%)]\t\tLoss: 0.66561\n",
      "Training Progress: \tEpoch 51 [1280/6768 (18.87%)]\t\tLoss: 0.91103\n",
      "Training Progress: \tEpoch 51 [1600/6768 (23.58%)]\t\tLoss: 0.61346\n",
      "Training Progress: \tEpoch 51 [1920/6768 (28.30%)]\t\tLoss: 0.50963\n",
      "Training Progress: \tEpoch 51 [2240/6768 (33.02%)]\t\tLoss: 0.86541\n",
      "Training Progress: \tEpoch 51 [2560/6768 (37.74%)]\t\tLoss: 0.77298\n",
      "Training Progress: \tEpoch 51 [2880/6768 (42.45%)]\t\tLoss: 0.79726\n",
      "Training Progress: \tEpoch 51 [3200/6768 (47.17%)]\t\tLoss: 0.51662\n",
      "Training Progress: \tEpoch 51 [3520/6768 (51.89%)]\t\tLoss: 0.58088\n",
      "Training Progress: \tEpoch 51 [3840/6768 (56.60%)]\t\tLoss: 0.61110\n",
      "Training Progress: \tEpoch 51 [4160/6768 (61.32%)]\t\tLoss: 0.89911\n",
      "Training Progress: \tEpoch 51 [4480/6768 (66.04%)]\t\tLoss: 0.70846\n",
      "Training Progress: \tEpoch 51 [4800/6768 (70.75%)]\t\tLoss: 0.62187\n",
      "Training Progress: \tEpoch 51 [5120/6768 (75.47%)]\t\tLoss: 0.59192\n",
      "Training Progress: \tEpoch 51 [5440/6768 (80.19%)]\t\tLoss: 0.86707\n",
      "Training Progress: \tEpoch 51 [5760/6768 (84.91%)]\t\tLoss: 0.55195\n",
      "Training Progress: \tEpoch 51 [6080/6768 (89.62%)]\t\tLoss: 0.63868\n",
      "Training Progress: \tEpoch 51 [6400/6768 (94.34%)]\t\tLoss: 0.57846\n",
      "Training Progress: \tEpoch 51 [6720/6768 (99.06%)]\t\tLoss: 1.07524\n",
      "\tTrain loss: 0.00972, Accuracy: 6189/6768 (91.00%)\n",
      "\tValidation loss: 0.00026, Accuracy: 1451/1692 (85.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1049/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 52 [0/6768 (0.00%)]\t\tLoss: 0.39843\n",
      "Training Progress: \tEpoch 52 [320/6768 (4.72%)]\t\tLoss: 0.77346\n",
      "Training Progress: \tEpoch 52 [640/6768 (9.43%)]\t\tLoss: 0.44730\n",
      "Training Progress: \tEpoch 52 [960/6768 (14.15%)]\t\tLoss: 0.67189\n",
      "Training Progress: \tEpoch 52 [1280/6768 (18.87%)]\t\tLoss: 0.74324\n",
      "Training Progress: \tEpoch 52 [1600/6768 (23.58%)]\t\tLoss: 0.58370\n",
      "Training Progress: \tEpoch 52 [1920/6768 (28.30%)]\t\tLoss: 0.80393\n",
      "Training Progress: \tEpoch 52 [2240/6768 (33.02%)]\t\tLoss: 0.77126\n",
      "Training Progress: \tEpoch 52 [2560/6768 (37.74%)]\t\tLoss: 0.77183\n",
      "Training Progress: \tEpoch 52 [2880/6768 (42.45%)]\t\tLoss: 0.73376\n",
      "Training Progress: \tEpoch 52 [3200/6768 (47.17%)]\t\tLoss: 0.64624\n",
      "Training Progress: \tEpoch 52 [3520/6768 (51.89%)]\t\tLoss: 0.77885\n",
      "Training Progress: \tEpoch 52 [3840/6768 (56.60%)]\t\tLoss: 1.05634\n",
      "Training Progress: \tEpoch 52 [4160/6768 (61.32%)]\t\tLoss: 0.50176\n",
      "Training Progress: \tEpoch 52 [4480/6768 (66.04%)]\t\tLoss: 0.76154\n",
      "Training Progress: \tEpoch 52 [4800/6768 (70.75%)]\t\tLoss: 0.61644\n",
      "Training Progress: \tEpoch 52 [5120/6768 (75.47%)]\t\tLoss: 0.94750\n",
      "Training Progress: \tEpoch 52 [5440/6768 (80.19%)]\t\tLoss: 0.53206\n",
      "Training Progress: \tEpoch 52 [5760/6768 (84.91%)]\t\tLoss: 0.71960\n",
      "Training Progress: \tEpoch 52 [6080/6768 (89.62%)]\t\tLoss: 0.61134\n",
      "Training Progress: \tEpoch 52 [6400/6768 (94.34%)]\t\tLoss: 0.52052\n",
      "Training Progress: \tEpoch 52 [6720/6768 (99.06%)]\t\tLoss: 0.82947\n",
      "\tTrain loss: 0.00898, Accuracy: 6275/6768 (92.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1470/1692 (86.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1064/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 53 [0/6768 (0.00%)]\t\tLoss: 0.70260\n",
      "Training Progress: \tEpoch 53 [320/6768 (4.72%)]\t\tLoss: 0.84167\n",
      "Training Progress: \tEpoch 53 [640/6768 (9.43%)]\t\tLoss: 0.58845\n",
      "Training Progress: \tEpoch 53 [960/6768 (14.15%)]\t\tLoss: 0.75271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 53 [1280/6768 (18.87%)]\t\tLoss: 0.62171\n",
      "Training Progress: \tEpoch 53 [1600/6768 (23.58%)]\t\tLoss: 0.39821\n",
      "Training Progress: \tEpoch 53 [1920/6768 (28.30%)]\t\tLoss: 0.72747\n",
      "Training Progress: \tEpoch 53 [2240/6768 (33.02%)]\t\tLoss: 0.71340\n",
      "Training Progress: \tEpoch 53 [2560/6768 (37.74%)]\t\tLoss: 0.69837\n",
      "Training Progress: \tEpoch 53 [2880/6768 (42.45%)]\t\tLoss: 1.17767\n",
      "Training Progress: \tEpoch 53 [3200/6768 (47.17%)]\t\tLoss: 0.67326\n",
      "Training Progress: \tEpoch 53 [3520/6768 (51.89%)]\t\tLoss: 0.70095\n",
      "Training Progress: \tEpoch 53 [3840/6768 (56.60%)]\t\tLoss: 0.72648\n",
      "Training Progress: \tEpoch 53 [4160/6768 (61.32%)]\t\tLoss: 0.73818\n",
      "Training Progress: \tEpoch 53 [4480/6768 (66.04%)]\t\tLoss: 0.63736\n",
      "Training Progress: \tEpoch 53 [4800/6768 (70.75%)]\t\tLoss: 0.94533\n",
      "Training Progress: \tEpoch 53 [5120/6768 (75.47%)]\t\tLoss: 0.72520\n",
      "Training Progress: \tEpoch 53 [5440/6768 (80.19%)]\t\tLoss: 0.85435\n",
      "Training Progress: \tEpoch 53 [5760/6768 (84.91%)]\t\tLoss: 0.47571\n",
      "Training Progress: \tEpoch 53 [6080/6768 (89.62%)]\t\tLoss: 0.82745\n",
      "Training Progress: \tEpoch 53 [6400/6768 (94.34%)]\t\tLoss: 0.89870\n",
      "Training Progress: \tEpoch 53 [6720/6768 (99.06%)]\t\tLoss: 0.93009\n",
      "\tTrain loss: 0.00881, Accuracy: 6261/6768 (92.00%)\n",
      "\tValidation loss: 0.00024, Accuracy: 1478/1692 (87.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1031/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 54 [0/6768 (0.00%)]\t\tLoss: 0.39189\n",
      "Training Progress: \tEpoch 54 [320/6768 (4.72%)]\t\tLoss: 0.86926\n",
      "Training Progress: \tEpoch 54 [640/6768 (9.43%)]\t\tLoss: 0.72352\n",
      "Training Progress: \tEpoch 54 [960/6768 (14.15%)]\t\tLoss: 0.78909\n",
      "Training Progress: \tEpoch 54 [1280/6768 (18.87%)]\t\tLoss: 0.83152\n",
      "Training Progress: \tEpoch 54 [1600/6768 (23.58%)]\t\tLoss: 0.54423\n",
      "Training Progress: \tEpoch 54 [1920/6768 (28.30%)]\t\tLoss: 0.43994\n",
      "Training Progress: \tEpoch 54 [2240/6768 (33.02%)]\t\tLoss: 0.71301\n",
      "Training Progress: \tEpoch 54 [2560/6768 (37.74%)]\t\tLoss: 0.67960\n",
      "Training Progress: \tEpoch 54 [2880/6768 (42.45%)]\t\tLoss: 0.53144\n",
      "Training Progress: \tEpoch 54 [3200/6768 (47.17%)]\t\tLoss: 0.53925\n",
      "Training Progress: \tEpoch 54 [3520/6768 (51.89%)]\t\tLoss: 0.59377\n",
      "Training Progress: \tEpoch 54 [3840/6768 (56.60%)]\t\tLoss: 0.82604\n",
      "Training Progress: \tEpoch 54 [4160/6768 (61.32%)]\t\tLoss: 0.59809\n",
      "Training Progress: \tEpoch 54 [4480/6768 (66.04%)]\t\tLoss: 0.64191\n",
      "Training Progress: \tEpoch 54 [4800/6768 (70.75%)]\t\tLoss: 0.76120\n",
      "Training Progress: \tEpoch 54 [5120/6768 (75.47%)]\t\tLoss: 0.54789\n",
      "Training Progress: \tEpoch 54 [5440/6768 (80.19%)]\t\tLoss: 0.64211\n",
      "Training Progress: \tEpoch 54 [5760/6768 (84.91%)]\t\tLoss: 0.57626\n",
      "Training Progress: \tEpoch 54 [6080/6768 (89.62%)]\t\tLoss: 0.72589\n",
      "Training Progress: \tEpoch 54 [6400/6768 (94.34%)]\t\tLoss: 0.83923\n",
      "Training Progress: \tEpoch 54 [6720/6768 (99.06%)]\t\tLoss: 0.70492\n",
      "\tTrain loss: 0.00951, Accuracy: 6168/6768 (91.00%)\n",
      "\tValidation loss: 0.00025, Accuracy: 1459/1692 (86.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1004/1772 (56.00%)\n",
      "\n",
      "Training Progress: \tEpoch 55 [0/6768 (0.00%)]\t\tLoss: 0.51315\n",
      "Training Progress: \tEpoch 55 [320/6768 (4.72%)]\t\tLoss: 0.91333\n",
      "Training Progress: \tEpoch 55 [640/6768 (9.43%)]\t\tLoss: 0.66947\n",
      "Training Progress: \tEpoch 55 [960/6768 (14.15%)]\t\tLoss: 0.72426\n",
      "Training Progress: \tEpoch 55 [1280/6768 (18.87%)]\t\tLoss: 0.46803\n",
      "Training Progress: \tEpoch 55 [1600/6768 (23.58%)]\t\tLoss: 0.51960\n",
      "Training Progress: \tEpoch 55 [1920/6768 (28.30%)]\t\tLoss: 0.60466\n",
      "Training Progress: \tEpoch 55 [2240/6768 (33.02%)]\t\tLoss: 0.83568\n",
      "Training Progress: \tEpoch 55 [2560/6768 (37.74%)]\t\tLoss: 0.73954\n",
      "Training Progress: \tEpoch 55 [2880/6768 (42.45%)]\t\tLoss: 1.08417\n",
      "Training Progress: \tEpoch 55 [3200/6768 (47.17%)]\t\tLoss: 0.47571\n",
      "Training Progress: \tEpoch 55 [3520/6768 (51.89%)]\t\tLoss: 0.68344\n",
      "Training Progress: \tEpoch 55 [3840/6768 (56.60%)]\t\tLoss: 0.60005\n",
      "Training Progress: \tEpoch 55 [4160/6768 (61.32%)]\t\tLoss: 0.73483\n",
      "Training Progress: \tEpoch 55 [4480/6768 (66.04%)]\t\tLoss: 0.67948\n",
      "Training Progress: \tEpoch 55 [4800/6768 (70.75%)]\t\tLoss: 0.80911\n",
      "Training Progress: \tEpoch 55 [5120/6768 (75.47%)]\t\tLoss: 0.69052\n",
      "Training Progress: \tEpoch 55 [5440/6768 (80.19%)]\t\tLoss: 0.71166\n",
      "Training Progress: \tEpoch 55 [5760/6768 (84.91%)]\t\tLoss: 0.72035\n",
      "Training Progress: \tEpoch 55 [6080/6768 (89.62%)]\t\tLoss: 0.68673\n",
      "Training Progress: \tEpoch 55 [6400/6768 (94.34%)]\t\tLoss: 0.48583\n",
      "Training Progress: \tEpoch 55 [6720/6768 (99.06%)]\t\tLoss: 0.67413\n",
      "\tTrain loss: 0.00797, Accuracy: 6327/6768 (93.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1497/1692 (88.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1050/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 56 [0/6768 (0.00%)]\t\tLoss: 0.49924\n",
      "Training Progress: \tEpoch 56 [320/6768 (4.72%)]\t\tLoss: 0.81824\n",
      "Training Progress: \tEpoch 56 [640/6768 (9.43%)]\t\tLoss: 0.57908\n",
      "Training Progress: \tEpoch 56 [960/6768 (14.15%)]\t\tLoss: 0.78757\n",
      "Training Progress: \tEpoch 56 [1280/6768 (18.87%)]\t\tLoss: 0.90027\n",
      "Training Progress: \tEpoch 56 [1600/6768 (23.58%)]\t\tLoss: 0.58826\n",
      "Training Progress: \tEpoch 56 [1920/6768 (28.30%)]\t\tLoss: 0.48749\n",
      "Training Progress: \tEpoch 56 [2240/6768 (33.02%)]\t\tLoss: 0.78772\n",
      "Training Progress: \tEpoch 56 [2560/6768 (37.74%)]\t\tLoss: 0.71846\n",
      "Training Progress: \tEpoch 56 [2880/6768 (42.45%)]\t\tLoss: 1.04618\n",
      "Training Progress: \tEpoch 56 [3200/6768 (47.17%)]\t\tLoss: 0.58100\n",
      "Training Progress: \tEpoch 56 [3520/6768 (51.89%)]\t\tLoss: 0.67764\n",
      "Training Progress: \tEpoch 56 [3840/6768 (56.60%)]\t\tLoss: 0.72261\n",
      "Training Progress: \tEpoch 56 [4160/6768 (61.32%)]\t\tLoss: 0.78682\n",
      "Training Progress: \tEpoch 56 [4480/6768 (66.04%)]\t\tLoss: 0.66427\n",
      "Training Progress: \tEpoch 56 [4800/6768 (70.75%)]\t\tLoss: 0.83336\n",
      "Training Progress: \tEpoch 56 [5120/6768 (75.47%)]\t\tLoss: 0.57761\n",
      "Training Progress: \tEpoch 56 [5440/6768 (80.19%)]\t\tLoss: 0.59594\n",
      "Training Progress: \tEpoch 56 [5760/6768 (84.91%)]\t\tLoss: 0.46257\n",
      "Training Progress: \tEpoch 56 [6080/6768 (89.62%)]\t\tLoss: 0.50509\n",
      "Training Progress: \tEpoch 56 [6400/6768 (94.34%)]\t\tLoss: 0.64092\n",
      "Training Progress: \tEpoch 56 [6720/6768 (99.06%)]\t\tLoss: 0.78009\n",
      "\tTrain loss: 0.00736, Accuracy: 6367/6768 (94.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1490/1692 (88.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1058/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 57 [0/6768 (0.00%)]\t\tLoss: 0.45877\n",
      "Training Progress: \tEpoch 57 [320/6768 (4.72%)]\t\tLoss: 0.82184\n",
      "Training Progress: \tEpoch 57 [640/6768 (9.43%)]\t\tLoss: 0.61027\n",
      "Training Progress: \tEpoch 57 [960/6768 (14.15%)]\t\tLoss: 0.68235\n",
      "Training Progress: \tEpoch 57 [1280/6768 (18.87%)]\t\tLoss: 0.49986\n",
      "Training Progress: \tEpoch 57 [1600/6768 (23.58%)]\t\tLoss: 0.34277\n",
      "Training Progress: \tEpoch 57 [1920/6768 (28.30%)]\t\tLoss: 0.56005\n",
      "Training Progress: \tEpoch 57 [2240/6768 (33.02%)]\t\tLoss: 0.79956\n",
      "Training Progress: \tEpoch 57 [2560/6768 (37.74%)]\t\tLoss: 0.66523\n",
      "Training Progress: \tEpoch 57 [2880/6768 (42.45%)]\t\tLoss: 0.59913\n",
      "Training Progress: \tEpoch 57 [3200/6768 (47.17%)]\t\tLoss: 0.76939\n",
      "Training Progress: \tEpoch 57 [3520/6768 (51.89%)]\t\tLoss: 1.04180\n",
      "Training Progress: \tEpoch 57 [3840/6768 (56.60%)]\t\tLoss: 0.70305\n",
      "Training Progress: \tEpoch 57 [4160/6768 (61.32%)]\t\tLoss: 0.69407\n",
      "Training Progress: \tEpoch 57 [4480/6768 (66.04%)]\t\tLoss: 0.89013\n",
      "Training Progress: \tEpoch 57 [4800/6768 (70.75%)]\t\tLoss: 0.66270\n",
      "Training Progress: \tEpoch 57 [5120/6768 (75.47%)]\t\tLoss: 0.47034\n",
      "Training Progress: \tEpoch 57 [5440/6768 (80.19%)]\t\tLoss: 0.71147\n",
      "Training Progress: \tEpoch 57 [5760/6768 (84.91%)]\t\tLoss: 0.57393\n",
      "Training Progress: \tEpoch 57 [6080/6768 (89.62%)]\t\tLoss: 0.56251\n",
      "Training Progress: \tEpoch 57 [6400/6768 (94.34%)]\t\tLoss: 0.66753\n",
      "Training Progress: \tEpoch 57 [6720/6768 (99.06%)]\t\tLoss: 0.68077\n",
      "\tTrain loss: 0.00763, Accuracy: 6361/6768 (93.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1506/1692 (89.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 58 [0/6768 (0.00%)]\t\tLoss: 0.54316\n",
      "Training Progress: \tEpoch 58 [320/6768 (4.72%)]\t\tLoss: 0.85980\n",
      "Training Progress: \tEpoch 58 [640/6768 (9.43%)]\t\tLoss: 0.57182\n",
      "Training Progress: \tEpoch 58 [960/6768 (14.15%)]\t\tLoss: 0.76920\n",
      "Training Progress: \tEpoch 58 [1280/6768 (18.87%)]\t\tLoss: 0.74590\n",
      "Training Progress: \tEpoch 58 [1600/6768 (23.58%)]\t\tLoss: 0.69108\n",
      "Training Progress: \tEpoch 58 [1920/6768 (28.30%)]\t\tLoss: 0.75935\n",
      "Training Progress: \tEpoch 58 [2240/6768 (33.02%)]\t\tLoss: 0.80763\n",
      "Training Progress: \tEpoch 58 [2560/6768 (37.74%)]\t\tLoss: 0.56555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 58 [2880/6768 (42.45%)]\t\tLoss: 0.77019\n",
      "Training Progress: \tEpoch 58 [3200/6768 (47.17%)]\t\tLoss: 0.50633\n",
      "Training Progress: \tEpoch 58 [3520/6768 (51.89%)]\t\tLoss: 0.70312\n",
      "Training Progress: \tEpoch 58 [3840/6768 (56.60%)]\t\tLoss: 0.74930\n",
      "Training Progress: \tEpoch 58 [4160/6768 (61.32%)]\t\tLoss: 0.76765\n",
      "Training Progress: \tEpoch 58 [4480/6768 (66.04%)]\t\tLoss: 0.64978\n",
      "Training Progress: \tEpoch 58 [4800/6768 (70.75%)]\t\tLoss: 0.68393\n",
      "Training Progress: \tEpoch 58 [5120/6768 (75.47%)]\t\tLoss: 0.44204\n",
      "Training Progress: \tEpoch 58 [5440/6768 (80.19%)]\t\tLoss: 0.68484\n",
      "Training Progress: \tEpoch 58 [5760/6768 (84.91%)]\t\tLoss: 0.53400\n",
      "Training Progress: \tEpoch 58 [6080/6768 (89.62%)]\t\tLoss: 0.60551\n",
      "Training Progress: \tEpoch 58 [6400/6768 (94.34%)]\t\tLoss: 0.79889\n",
      "Training Progress: \tEpoch 58 [6720/6768 (99.06%)]\t\tLoss: 0.61824\n",
      "\tTrain loss: 0.00761, Accuracy: 6379/6768 (94.00%)\n",
      "\tValidation loss: 0.00021, Accuracy: 1500/1692 (88.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1053/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 59 [0/6768 (0.00%)]\t\tLoss: 0.51476\n",
      "Training Progress: \tEpoch 59 [320/6768 (4.72%)]\t\tLoss: 0.88447\n",
      "Training Progress: \tEpoch 59 [640/6768 (9.43%)]\t\tLoss: 0.76648\n",
      "Training Progress: \tEpoch 59 [960/6768 (14.15%)]\t\tLoss: 0.63161\n",
      "Training Progress: \tEpoch 59 [1280/6768 (18.87%)]\t\tLoss: 0.74006\n",
      "Training Progress: \tEpoch 59 [1600/6768 (23.58%)]\t\tLoss: 0.61245\n",
      "Training Progress: \tEpoch 59 [1920/6768 (28.30%)]\t\tLoss: 0.96131\n",
      "Training Progress: \tEpoch 59 [2240/6768 (33.02%)]\t\tLoss: 0.77070\n",
      "Training Progress: \tEpoch 59 [2560/6768 (37.74%)]\t\tLoss: 0.56604\n",
      "Training Progress: \tEpoch 59 [2880/6768 (42.45%)]\t\tLoss: 0.55881\n",
      "Training Progress: \tEpoch 59 [3200/6768 (47.17%)]\t\tLoss: 0.55078\n",
      "Training Progress: \tEpoch 59 [3520/6768 (51.89%)]\t\tLoss: 0.94760\n",
      "Training Progress: \tEpoch 59 [3840/6768 (56.60%)]\t\tLoss: 0.78097\n",
      "Training Progress: \tEpoch 59 [4160/6768 (61.32%)]\t\tLoss: 0.99146\n",
      "Training Progress: \tEpoch 59 [4480/6768 (66.04%)]\t\tLoss: 0.52552\n",
      "Training Progress: \tEpoch 59 [4800/6768 (70.75%)]\t\tLoss: 0.58950\n",
      "Training Progress: \tEpoch 59 [5120/6768 (75.47%)]\t\tLoss: 0.68600\n",
      "Training Progress: \tEpoch 59 [5440/6768 (80.19%)]\t\tLoss: 0.72840\n",
      "Training Progress: \tEpoch 59 [5760/6768 (84.91%)]\t\tLoss: 0.51274\n",
      "Training Progress: \tEpoch 59 [6080/6768 (89.62%)]\t\tLoss: 0.61533\n",
      "Training Progress: \tEpoch 59 [6400/6768 (94.34%)]\t\tLoss: 0.81171\n",
      "Training Progress: \tEpoch 59 [6720/6768 (99.06%)]\t\tLoss: 0.67353\n",
      "\tTrain loss: 0.00816, Accuracy: 6388/6768 (94.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1512/1692 (89.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1070/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 60 [0/6768 (0.00%)]\t\tLoss: 0.70815\n",
      "Training Progress: \tEpoch 60 [320/6768 (4.72%)]\t\tLoss: 0.74347\n",
      "Training Progress: \tEpoch 60 [640/6768 (9.43%)]\t\tLoss: 0.68180\n",
      "Training Progress: \tEpoch 60 [960/6768 (14.15%)]\t\tLoss: 0.85964\n",
      "Training Progress: \tEpoch 60 [1280/6768 (18.87%)]\t\tLoss: 0.40409\n",
      "Training Progress: \tEpoch 60 [1600/6768 (23.58%)]\t\tLoss: 0.44739\n",
      "Training Progress: \tEpoch 60 [1920/6768 (28.30%)]\t\tLoss: 0.71575\n",
      "Training Progress: \tEpoch 60 [2240/6768 (33.02%)]\t\tLoss: 0.88254\n",
      "Training Progress: \tEpoch 60 [2560/6768 (37.74%)]\t\tLoss: 0.73251\n",
      "Training Progress: \tEpoch 60 [2880/6768 (42.45%)]\t\tLoss: 0.79694\n",
      "Training Progress: \tEpoch 60 [3200/6768 (47.17%)]\t\tLoss: 0.66037\n",
      "Training Progress: \tEpoch 60 [3520/6768 (51.89%)]\t\tLoss: 0.90973\n",
      "Training Progress: \tEpoch 60 [3840/6768 (56.60%)]\t\tLoss: 0.77976\n",
      "Training Progress: \tEpoch 60 [4160/6768 (61.32%)]\t\tLoss: 0.85118\n",
      "Training Progress: \tEpoch 60 [4480/6768 (66.04%)]\t\tLoss: 0.72587\n",
      "Training Progress: \tEpoch 60 [4800/6768 (70.75%)]\t\tLoss: 0.45502\n",
      "Training Progress: \tEpoch 60 [5120/6768 (75.47%)]\t\tLoss: 0.60551\n",
      "Training Progress: \tEpoch 60 [5440/6768 (80.19%)]\t\tLoss: 0.86550\n",
      "Training Progress: \tEpoch 60 [5760/6768 (84.91%)]\t\tLoss: 0.59036\n",
      "Training Progress: \tEpoch 60 [6080/6768 (89.62%)]\t\tLoss: 0.39199\n",
      "Training Progress: \tEpoch 60 [6400/6768 (94.34%)]\t\tLoss: 0.59302\n",
      "Training Progress: \tEpoch 60 [6720/6768 (99.06%)]\t\tLoss: 0.53510\n",
      "\tTrain loss: 0.00675, Accuracy: 6427/6768 (94.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1518/1692 (89.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1085/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 61 [0/6768 (0.00%)]\t\tLoss: 0.70390\n",
      "Training Progress: \tEpoch 61 [320/6768 (4.72%)]\t\tLoss: 0.78589\n",
      "Training Progress: \tEpoch 61 [640/6768 (9.43%)]\t\tLoss: 0.51821\n",
      "Training Progress: \tEpoch 61 [960/6768 (14.15%)]\t\tLoss: 0.79821\n",
      "Training Progress: \tEpoch 61 [1280/6768 (18.87%)]\t\tLoss: 0.47208\n",
      "Training Progress: \tEpoch 61 [1600/6768 (23.58%)]\t\tLoss: 0.52251\n",
      "Training Progress: \tEpoch 61 [1920/6768 (28.30%)]\t\tLoss: 0.56828\n",
      "Training Progress: \tEpoch 61 [2240/6768 (33.02%)]\t\tLoss: 0.56419\n",
      "Training Progress: \tEpoch 61 [2560/6768 (37.74%)]\t\tLoss: 0.78486\n",
      "Training Progress: \tEpoch 61 [2880/6768 (42.45%)]\t\tLoss: 0.65275\n",
      "Training Progress: \tEpoch 61 [3200/6768 (47.17%)]\t\tLoss: 0.41934\n",
      "Training Progress: \tEpoch 61 [3520/6768 (51.89%)]\t\tLoss: 0.65876\n",
      "Training Progress: \tEpoch 61 [3840/6768 (56.60%)]\t\tLoss: 0.69229\n",
      "Training Progress: \tEpoch 61 [4160/6768 (61.32%)]\t\tLoss: 0.92242\n",
      "Training Progress: \tEpoch 61 [4480/6768 (66.04%)]\t\tLoss: 0.57631\n",
      "Training Progress: \tEpoch 61 [4800/6768 (70.75%)]\t\tLoss: 0.53610\n",
      "Training Progress: \tEpoch 61 [5120/6768 (75.47%)]\t\tLoss: 0.75413\n",
      "Training Progress: \tEpoch 61 [5440/6768 (80.19%)]\t\tLoss: 0.96157\n",
      "Training Progress: \tEpoch 61 [5760/6768 (84.91%)]\t\tLoss: 0.45838\n",
      "Training Progress: \tEpoch 61 [6080/6768 (89.62%)]\t\tLoss: 0.39555\n",
      "Training Progress: \tEpoch 61 [6400/6768 (94.34%)]\t\tLoss: 0.84874\n",
      "Training Progress: \tEpoch 61 [6720/6768 (99.06%)]\t\tLoss: 0.67226\n",
      "\tTrain loss: 0.00781, Accuracy: 6356/6768 (93.00%)\n",
      "\tValidation loss: 0.00022, Accuracy: 1480/1692 (87.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1057/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 62 [0/6768 (0.00%)]\t\tLoss: 0.57172\n",
      "Training Progress: \tEpoch 62 [320/6768 (4.72%)]\t\tLoss: 0.76348\n",
      "Training Progress: \tEpoch 62 [640/6768 (9.43%)]\t\tLoss: 0.60407\n",
      "Training Progress: \tEpoch 62 [960/6768 (14.15%)]\t\tLoss: 0.58909\n",
      "Training Progress: \tEpoch 62 [1280/6768 (18.87%)]\t\tLoss: 0.64912\n",
      "Training Progress: \tEpoch 62 [1600/6768 (23.58%)]\t\tLoss: 0.49404\n",
      "Training Progress: \tEpoch 62 [1920/6768 (28.30%)]\t\tLoss: 0.45674\n",
      "Training Progress: \tEpoch 62 [2240/6768 (33.02%)]\t\tLoss: 0.75822\n",
      "Training Progress: \tEpoch 62 [2560/6768 (37.74%)]\t\tLoss: 0.52426\n",
      "Training Progress: \tEpoch 62 [2880/6768 (42.45%)]\t\tLoss: 0.82464\n",
      "Training Progress: \tEpoch 62 [3200/6768 (47.17%)]\t\tLoss: 0.74276\n",
      "Training Progress: \tEpoch 62 [3520/6768 (51.89%)]\t\tLoss: 0.62366\n",
      "Training Progress: \tEpoch 62 [3840/6768 (56.60%)]\t\tLoss: 0.70087\n",
      "Training Progress: \tEpoch 62 [4160/6768 (61.32%)]\t\tLoss: 0.57275\n",
      "Training Progress: \tEpoch 62 [4480/6768 (66.04%)]\t\tLoss: 0.66145\n",
      "Training Progress: \tEpoch 62 [4800/6768 (70.75%)]\t\tLoss: 0.78084\n",
      "Training Progress: \tEpoch 62 [5120/6768 (75.47%)]\t\tLoss: 0.71021\n",
      "Training Progress: \tEpoch 62 [5440/6768 (80.19%)]\t\tLoss: 0.50387\n",
      "Training Progress: \tEpoch 62 [5760/6768 (84.91%)]\t\tLoss: 0.65312\n",
      "Training Progress: \tEpoch 62 [6080/6768 (89.62%)]\t\tLoss: 0.48750\n",
      "Training Progress: \tEpoch 62 [6400/6768 (94.34%)]\t\tLoss: 0.45297\n",
      "Training Progress: \tEpoch 62 [6720/6768 (99.06%)]\t\tLoss: 0.50552\n",
      "\tTrain loss: 0.00634, Accuracy: 6428/6768 (94.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1515/1692 (89.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 63 [0/6768 (0.00%)]\t\tLoss: 0.53465\n",
      "Training Progress: \tEpoch 63 [320/6768 (4.72%)]\t\tLoss: 0.87027\n",
      "Training Progress: \tEpoch 63 [640/6768 (9.43%)]\t\tLoss: 0.48295\n",
      "Training Progress: \tEpoch 63 [960/6768 (14.15%)]\t\tLoss: 0.65347\n",
      "Training Progress: \tEpoch 63 [1280/6768 (18.87%)]\t\tLoss: 0.70115\n",
      "Training Progress: \tEpoch 63 [1600/6768 (23.58%)]\t\tLoss: 0.47057\n",
      "Training Progress: \tEpoch 63 [1920/6768 (28.30%)]\t\tLoss: 0.72567\n",
      "Training Progress: \tEpoch 63 [2240/6768 (33.02%)]\t\tLoss: 0.83273\n",
      "Training Progress: \tEpoch 63 [2560/6768 (37.74%)]\t\tLoss: 0.65329\n",
      "Training Progress: \tEpoch 63 [2880/6768 (42.45%)]\t\tLoss: 0.39127\n",
      "Training Progress: \tEpoch 63 [3200/6768 (47.17%)]\t\tLoss: 0.68933\n",
      "Training Progress: \tEpoch 63 [3520/6768 (51.89%)]\t\tLoss: 0.73403\n",
      "Training Progress: \tEpoch 63 [3840/6768 (56.60%)]\t\tLoss: 0.59796\n",
      "Training Progress: \tEpoch 63 [4160/6768 (61.32%)]\t\tLoss: 0.53977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 63 [4480/6768 (66.04%)]\t\tLoss: 0.73253\n",
      "Training Progress: \tEpoch 63 [4800/6768 (70.75%)]\t\tLoss: 0.52984\n",
      "Training Progress: \tEpoch 63 [5120/6768 (75.47%)]\t\tLoss: 0.84695\n",
      "Training Progress: \tEpoch 63 [5440/6768 (80.19%)]\t\tLoss: 0.70142\n",
      "Training Progress: \tEpoch 63 [5760/6768 (84.91%)]\t\tLoss: 0.48786\n",
      "Training Progress: \tEpoch 63 [6080/6768 (89.62%)]\t\tLoss: 0.37498\n",
      "Training Progress: \tEpoch 63 [6400/6768 (94.34%)]\t\tLoss: 0.73759\n",
      "Training Progress: \tEpoch 63 [6720/6768 (99.06%)]\t\tLoss: 0.60737\n",
      "\tTrain loss: 0.00693, Accuracy: 6463/6768 (95.00%)\n",
      "\tValidation loss: 0.00020, Accuracy: 1515/1692 (89.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1061/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 64 [0/6768 (0.00%)]\t\tLoss: 0.51067\n",
      "Training Progress: \tEpoch 64 [320/6768 (4.72%)]\t\tLoss: 0.98130\n",
      "Training Progress: \tEpoch 64 [640/6768 (9.43%)]\t\tLoss: 0.59966\n",
      "Training Progress: \tEpoch 64 [960/6768 (14.15%)]\t\tLoss: 0.52030\n",
      "Training Progress: \tEpoch 64 [1280/6768 (18.87%)]\t\tLoss: 0.57610\n",
      "Training Progress: \tEpoch 64 [1600/6768 (23.58%)]\t\tLoss: 0.40742\n",
      "Training Progress: \tEpoch 64 [1920/6768 (28.30%)]\t\tLoss: 0.47814\n",
      "Training Progress: \tEpoch 64 [2240/6768 (33.02%)]\t\tLoss: 0.64685\n",
      "Training Progress: \tEpoch 64 [2560/6768 (37.74%)]\t\tLoss: 0.67945\n",
      "Training Progress: \tEpoch 64 [2880/6768 (42.45%)]\t\tLoss: 0.66228\n",
      "Training Progress: \tEpoch 64 [3200/6768 (47.17%)]\t\tLoss: 0.70242\n",
      "Training Progress: \tEpoch 64 [3520/6768 (51.89%)]\t\tLoss: 1.00885\n",
      "Training Progress: \tEpoch 64 [3840/6768 (56.60%)]\t\tLoss: 0.69046\n",
      "Training Progress: \tEpoch 64 [4160/6768 (61.32%)]\t\tLoss: 0.62734\n",
      "Training Progress: \tEpoch 64 [4480/6768 (66.04%)]\t\tLoss: 0.55985\n",
      "Training Progress: \tEpoch 64 [4800/6768 (70.75%)]\t\tLoss: 0.54397\n",
      "Training Progress: \tEpoch 64 [5120/6768 (75.47%)]\t\tLoss: 0.62558\n",
      "Training Progress: \tEpoch 64 [5440/6768 (80.19%)]\t\tLoss: 0.80814\n",
      "Training Progress: \tEpoch 64 [5760/6768 (84.91%)]\t\tLoss: 0.59789\n",
      "Training Progress: \tEpoch 64 [6080/6768 (89.62%)]\t\tLoss: 0.43952\n",
      "Training Progress: \tEpoch 64 [6400/6768 (94.34%)]\t\tLoss: 0.80827\n",
      "Training Progress: \tEpoch 64 [6720/6768 (99.06%)]\t\tLoss: 0.68438\n",
      "\tTrain loss: 0.00677, Accuracy: 6487/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1531/1692 (90.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1069/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 65 [0/6768 (0.00%)]\t\tLoss: 0.42929\n",
      "Training Progress: \tEpoch 65 [320/6768 (4.72%)]\t\tLoss: 0.79270\n",
      "Training Progress: \tEpoch 65 [640/6768 (9.43%)]\t\tLoss: 0.49769\n",
      "Training Progress: \tEpoch 65 [960/6768 (14.15%)]\t\tLoss: 0.67200\n",
      "Training Progress: \tEpoch 65 [1280/6768 (18.87%)]\t\tLoss: 0.57053\n",
      "Training Progress: \tEpoch 65 [1600/6768 (23.58%)]\t\tLoss: 0.58482\n",
      "Training Progress: \tEpoch 65 [1920/6768 (28.30%)]\t\tLoss: 0.65206\n",
      "Training Progress: \tEpoch 65 [2240/6768 (33.02%)]\t\tLoss: 0.62186\n",
      "Training Progress: \tEpoch 65 [2560/6768 (37.74%)]\t\tLoss: 0.60778\n",
      "Training Progress: \tEpoch 65 [2880/6768 (42.45%)]\t\tLoss: 0.56176\n",
      "Training Progress: \tEpoch 65 [3200/6768 (47.17%)]\t\tLoss: 0.62765\n",
      "Training Progress: \tEpoch 65 [3520/6768 (51.89%)]\t\tLoss: 0.78969\n",
      "Training Progress: \tEpoch 65 [3840/6768 (56.60%)]\t\tLoss: 0.76465\n",
      "Training Progress: \tEpoch 65 [4160/6768 (61.32%)]\t\tLoss: 0.72622\n",
      "Training Progress: \tEpoch 65 [4480/6768 (66.04%)]\t\tLoss: 0.90998\n",
      "Training Progress: \tEpoch 65 [4800/6768 (70.75%)]\t\tLoss: 0.62900\n",
      "Training Progress: \tEpoch 65 [5120/6768 (75.47%)]\t\tLoss: 0.33840\n",
      "Training Progress: \tEpoch 65 [5440/6768 (80.19%)]\t\tLoss: 0.65508\n",
      "Training Progress: \tEpoch 65 [5760/6768 (84.91%)]\t\tLoss: 0.69575\n",
      "Training Progress: \tEpoch 65 [6080/6768 (89.62%)]\t\tLoss: 0.69468\n",
      "Training Progress: \tEpoch 65 [6400/6768 (94.34%)]\t\tLoss: 0.58414\n",
      "Training Progress: \tEpoch 65 [6720/6768 (99.06%)]\t\tLoss: 0.70640\n",
      "\tTrain loss: 0.00608, Accuracy: 6463/6768 (95.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1537/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1045/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 66 [0/6768 (0.00%)]\t\tLoss: 0.49636\n",
      "Training Progress: \tEpoch 66 [320/6768 (4.72%)]\t\tLoss: 0.71392\n",
      "Training Progress: \tEpoch 66 [640/6768 (9.43%)]\t\tLoss: 0.48524\n",
      "Training Progress: \tEpoch 66 [960/6768 (14.15%)]\t\tLoss: 0.70077\n",
      "Training Progress: \tEpoch 66 [1280/6768 (18.87%)]\t\tLoss: 0.41563\n",
      "Training Progress: \tEpoch 66 [1600/6768 (23.58%)]\t\tLoss: 0.49271\n",
      "Training Progress: \tEpoch 66 [1920/6768 (28.30%)]\t\tLoss: 0.60640\n",
      "Training Progress: \tEpoch 66 [2240/6768 (33.02%)]\t\tLoss: 0.62972\n",
      "Training Progress: \tEpoch 66 [2560/6768 (37.74%)]\t\tLoss: 0.68124\n",
      "Training Progress: \tEpoch 66 [2880/6768 (42.45%)]\t\tLoss: 0.56924\n",
      "Training Progress: \tEpoch 66 [3200/6768 (47.17%)]\t\tLoss: 0.52305\n",
      "Training Progress: \tEpoch 66 [3520/6768 (51.89%)]\t\tLoss: 0.53013\n",
      "Training Progress: \tEpoch 66 [3840/6768 (56.60%)]\t\tLoss: 0.61053\n",
      "Training Progress: \tEpoch 66 [4160/6768 (61.32%)]\t\tLoss: 0.79705\n",
      "Training Progress: \tEpoch 66 [4480/6768 (66.04%)]\t\tLoss: 0.50736\n",
      "Training Progress: \tEpoch 66 [4800/6768 (70.75%)]\t\tLoss: 0.61421\n",
      "Training Progress: \tEpoch 66 [5120/6768 (75.47%)]\t\tLoss: 0.52664\n",
      "Training Progress: \tEpoch 66 [5440/6768 (80.19%)]\t\tLoss: 0.81469\n",
      "Training Progress: \tEpoch 66 [5760/6768 (84.91%)]\t\tLoss: 0.49684\n",
      "Training Progress: \tEpoch 66 [6080/6768 (89.62%)]\t\tLoss: 0.66996\n",
      "Training Progress: \tEpoch 66 [6400/6768 (94.34%)]\t\tLoss: 0.49001\n",
      "Training Progress: \tEpoch 66 [6720/6768 (99.06%)]\t\tLoss: 0.51459\n",
      "\tTrain loss: 0.00582, Accuracy: 6483/6768 (95.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1530/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 67 [0/6768 (0.00%)]\t\tLoss: 0.66614\n",
      "Training Progress: \tEpoch 67 [320/6768 (4.72%)]\t\tLoss: 0.85934\n",
      "Training Progress: \tEpoch 67 [640/6768 (9.43%)]\t\tLoss: 0.55873\n",
      "Training Progress: \tEpoch 67 [960/6768 (14.15%)]\t\tLoss: 0.77828\n",
      "Training Progress: \tEpoch 67 [1280/6768 (18.87%)]\t\tLoss: 0.50887\n",
      "Training Progress: \tEpoch 67 [1600/6768 (23.58%)]\t\tLoss: 0.64509\n",
      "Training Progress: \tEpoch 67 [1920/6768 (28.30%)]\t\tLoss: 0.55622\n",
      "Training Progress: \tEpoch 67 [2240/6768 (33.02%)]\t\tLoss: 0.62211\n",
      "Training Progress: \tEpoch 67 [2560/6768 (37.74%)]\t\tLoss: 0.60133\n",
      "Training Progress: \tEpoch 67 [2880/6768 (42.45%)]\t\tLoss: 0.87352\n",
      "Training Progress: \tEpoch 67 [3200/6768 (47.17%)]\t\tLoss: 0.37793\n",
      "Training Progress: \tEpoch 67 [3520/6768 (51.89%)]\t\tLoss: 0.40510\n",
      "Training Progress: \tEpoch 67 [3840/6768 (56.60%)]\t\tLoss: 0.59771\n",
      "Training Progress: \tEpoch 67 [4160/6768 (61.32%)]\t\tLoss: 0.57464\n",
      "Training Progress: \tEpoch 67 [4480/6768 (66.04%)]\t\tLoss: 0.67226\n",
      "Training Progress: \tEpoch 67 [4800/6768 (70.75%)]\t\tLoss: 0.76267\n",
      "Training Progress: \tEpoch 67 [5120/6768 (75.47%)]\t\tLoss: 0.54615\n",
      "Training Progress: \tEpoch 67 [5440/6768 (80.19%)]\t\tLoss: 0.62515\n",
      "Training Progress: \tEpoch 67 [5760/6768 (84.91%)]\t\tLoss: 0.66454\n",
      "Training Progress: \tEpoch 67 [6080/6768 (89.62%)]\t\tLoss: 0.61180\n",
      "Training Progress: \tEpoch 67 [6400/6768 (94.34%)]\t\tLoss: 0.79607\n",
      "Training Progress: \tEpoch 67 [6720/6768 (99.06%)]\t\tLoss: 0.75416\n",
      "\tTrain loss: 0.00550, Accuracy: 6510/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1539/1692 (90.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 68 [0/6768 (0.00%)]\t\tLoss: 0.39924\n",
      "Training Progress: \tEpoch 68 [320/6768 (4.72%)]\t\tLoss: 0.63893\n",
      "Training Progress: \tEpoch 68 [640/6768 (9.43%)]\t\tLoss: 0.82243\n",
      "Training Progress: \tEpoch 68 [960/6768 (14.15%)]\t\tLoss: 0.84532\n",
      "Training Progress: \tEpoch 68 [1280/6768 (18.87%)]\t\tLoss: 0.67401\n",
      "Training Progress: \tEpoch 68 [1600/6768 (23.58%)]\t\tLoss: 0.43276\n",
      "Training Progress: \tEpoch 68 [1920/6768 (28.30%)]\t\tLoss: 0.57794\n",
      "Training Progress: \tEpoch 68 [2240/6768 (33.02%)]\t\tLoss: 0.60715\n",
      "Training Progress: \tEpoch 68 [2560/6768 (37.74%)]\t\tLoss: 0.85421\n",
      "Training Progress: \tEpoch 68 [2880/6768 (42.45%)]\t\tLoss: 0.61294\n",
      "Training Progress: \tEpoch 68 [3200/6768 (47.17%)]\t\tLoss: 0.53637\n",
      "Training Progress: \tEpoch 68 [3520/6768 (51.89%)]\t\tLoss: 0.48947\n",
      "Training Progress: \tEpoch 68 [3840/6768 (56.60%)]\t\tLoss: 0.49135\n",
      "Training Progress: \tEpoch 68 [4160/6768 (61.32%)]\t\tLoss: 0.46265\n",
      "Training Progress: \tEpoch 68 [4480/6768 (66.04%)]\t\tLoss: 0.71773\n",
      "Training Progress: \tEpoch 68 [4800/6768 (70.75%)]\t\tLoss: 0.74955\n",
      "Training Progress: \tEpoch 68 [5120/6768 (75.47%)]\t\tLoss: 0.53218\n",
      "Training Progress: \tEpoch 68 [5440/6768 (80.19%)]\t\tLoss: 0.67722\n",
      "Training Progress: \tEpoch 68 [5760/6768 (84.91%)]\t\tLoss: 0.48860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 68 [6080/6768 (89.62%)]\t\tLoss: 0.38616\n",
      "Training Progress: \tEpoch 68 [6400/6768 (94.34%)]\t\tLoss: 0.54430\n",
      "Training Progress: \tEpoch 68 [6720/6768 (99.06%)]\t\tLoss: 0.53608\n",
      "\tTrain loss: 0.00596, Accuracy: 6530/6768 (96.00%)\n",
      "\tValidation loss: 0.00018, Accuracy: 1551/1692 (91.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1073/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 69 [0/6768 (0.00%)]\t\tLoss: 0.60877\n",
      "Training Progress: \tEpoch 69 [320/6768 (4.72%)]\t\tLoss: 0.94660\n",
      "Training Progress: \tEpoch 69 [640/6768 (9.43%)]\t\tLoss: 0.39460\n",
      "Training Progress: \tEpoch 69 [960/6768 (14.15%)]\t\tLoss: 0.71986\n",
      "Training Progress: \tEpoch 69 [1280/6768 (18.87%)]\t\tLoss: 0.77257\n",
      "Training Progress: \tEpoch 69 [1600/6768 (23.58%)]\t\tLoss: 0.51055\n",
      "Training Progress: \tEpoch 69 [1920/6768 (28.30%)]\t\tLoss: 0.66863\n",
      "Training Progress: \tEpoch 69 [2240/6768 (33.02%)]\t\tLoss: 0.87635\n",
      "Training Progress: \tEpoch 69 [2560/6768 (37.74%)]\t\tLoss: 0.53753\n",
      "Training Progress: \tEpoch 69 [2880/6768 (42.45%)]\t\tLoss: 0.52367\n",
      "Training Progress: \tEpoch 69 [3200/6768 (47.17%)]\t\tLoss: 0.41656\n",
      "Training Progress: \tEpoch 69 [3520/6768 (51.89%)]\t\tLoss: 0.42980\n",
      "Training Progress: \tEpoch 69 [3840/6768 (56.60%)]\t\tLoss: 0.61598\n",
      "Training Progress: \tEpoch 69 [4160/6768 (61.32%)]\t\tLoss: 0.49309\n",
      "Training Progress: \tEpoch 69 [4480/6768 (66.04%)]\t\tLoss: 0.75459\n",
      "Training Progress: \tEpoch 69 [4800/6768 (70.75%)]\t\tLoss: 0.71905\n",
      "Training Progress: \tEpoch 69 [5120/6768 (75.47%)]\t\tLoss: 0.52578\n",
      "Training Progress: \tEpoch 69 [5440/6768 (80.19%)]\t\tLoss: 0.58372\n",
      "Training Progress: \tEpoch 69 [5760/6768 (84.91%)]\t\tLoss: 0.71717\n",
      "Training Progress: \tEpoch 69 [6080/6768 (89.62%)]\t\tLoss: 0.38789\n",
      "Training Progress: \tEpoch 69 [6400/6768 (94.34%)]\t\tLoss: 0.58819\n",
      "Training Progress: \tEpoch 69 [6720/6768 (99.06%)]\t\tLoss: 0.51104\n",
      "\tTrain loss: 0.00646, Accuracy: 6430/6768 (95.00%)\n",
      "\tValidation loss: 0.00019, Accuracy: 1519/1692 (89.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1030/1772 (58.00%)\n",
      "\n",
      "Training Progress: \tEpoch 70 [0/6768 (0.00%)]\t\tLoss: 0.58868\n",
      "Training Progress: \tEpoch 70 [320/6768 (4.72%)]\t\tLoss: 0.59086\n",
      "Training Progress: \tEpoch 70 [640/6768 (9.43%)]\t\tLoss: 0.36216\n",
      "Training Progress: \tEpoch 70 [960/6768 (14.15%)]\t\tLoss: 0.38300\n",
      "Training Progress: \tEpoch 70 [1280/6768 (18.87%)]\t\tLoss: 0.86205\n",
      "Training Progress: \tEpoch 70 [1600/6768 (23.58%)]\t\tLoss: 0.39335\n",
      "Training Progress: \tEpoch 70 [1920/6768 (28.30%)]\t\tLoss: 0.44045\n",
      "Training Progress: \tEpoch 70 [2240/6768 (33.02%)]\t\tLoss: 0.84047\n",
      "Training Progress: \tEpoch 70 [2560/6768 (37.74%)]\t\tLoss: 0.66103\n",
      "Training Progress: \tEpoch 70 [2880/6768 (42.45%)]\t\tLoss: 0.82731\n",
      "Training Progress: \tEpoch 70 [3200/6768 (47.17%)]\t\tLoss: 0.46602\n",
      "Training Progress: \tEpoch 70 [3520/6768 (51.89%)]\t\tLoss: 0.55925\n",
      "Training Progress: \tEpoch 70 [3840/6768 (56.60%)]\t\tLoss: 0.83809\n",
      "Training Progress: \tEpoch 70 [4160/6768 (61.32%)]\t\tLoss: 0.61491\n",
      "Training Progress: \tEpoch 70 [4480/6768 (66.04%)]\t\tLoss: 0.61283\n",
      "Training Progress: \tEpoch 70 [4800/6768 (70.75%)]\t\tLoss: 0.56721\n",
      "Training Progress: \tEpoch 70 [5120/6768 (75.47%)]\t\tLoss: 0.61570\n",
      "Training Progress: \tEpoch 70 [5440/6768 (80.19%)]\t\tLoss: 0.73206\n",
      "Training Progress: \tEpoch 70 [5760/6768 (84.91%)]\t\tLoss: 0.56136\n",
      "Training Progress: \tEpoch 70 [6080/6768 (89.62%)]\t\tLoss: 0.57676\n",
      "Training Progress: \tEpoch 70 [6400/6768 (94.34%)]\t\tLoss: 0.69410\n",
      "Training Progress: \tEpoch 70 [6720/6768 (99.06%)]\t\tLoss: 0.57125\n",
      "\tTrain loss: 0.00577, Accuracy: 6516/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1542/1692 (91.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1060/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 71 [0/6768 (0.00%)]\t\tLoss: 0.41979\n",
      "Training Progress: \tEpoch 71 [320/6768 (4.72%)]\t\tLoss: 0.70486\n",
      "Training Progress: \tEpoch 71 [640/6768 (9.43%)]\t\tLoss: 0.40720\n",
      "Training Progress: \tEpoch 71 [960/6768 (14.15%)]\t\tLoss: 0.65981\n",
      "Training Progress: \tEpoch 71 [1280/6768 (18.87%)]\t\tLoss: 0.57452\n",
      "Training Progress: \tEpoch 71 [1600/6768 (23.58%)]\t\tLoss: 0.45551\n",
      "Training Progress: \tEpoch 71 [1920/6768 (28.30%)]\t\tLoss: 0.41500\n",
      "Training Progress: \tEpoch 71 [2240/6768 (33.02%)]\t\tLoss: 0.76830\n",
      "Training Progress: \tEpoch 71 [2560/6768 (37.74%)]\t\tLoss: 0.85173\n",
      "Training Progress: \tEpoch 71 [2880/6768 (42.45%)]\t\tLoss: 0.55981\n",
      "Training Progress: \tEpoch 71 [3200/6768 (47.17%)]\t\tLoss: 0.90294\n",
      "Training Progress: \tEpoch 71 [3520/6768 (51.89%)]\t\tLoss: 0.61331\n",
      "Training Progress: \tEpoch 71 [3840/6768 (56.60%)]\t\tLoss: 0.72626\n",
      "Training Progress: \tEpoch 71 [4160/6768 (61.32%)]\t\tLoss: 0.63661\n",
      "Training Progress: \tEpoch 71 [4480/6768 (66.04%)]\t\tLoss: 0.79832\n",
      "Training Progress: \tEpoch 71 [4800/6768 (70.75%)]\t\tLoss: 0.43459\n",
      "Training Progress: \tEpoch 71 [5120/6768 (75.47%)]\t\tLoss: 0.56546\n",
      "Training Progress: \tEpoch 71 [5440/6768 (80.19%)]\t\tLoss: 0.70835\n",
      "Training Progress: \tEpoch 71 [5760/6768 (84.91%)]\t\tLoss: 0.47487\n",
      "Training Progress: \tEpoch 71 [6080/6768 (89.62%)]\t\tLoss: 0.47349\n",
      "Training Progress: \tEpoch 71 [6400/6768 (94.34%)]\t\tLoss: 0.45907\n",
      "Training Progress: \tEpoch 71 [6720/6768 (99.06%)]\t\tLoss: 0.55567\n",
      "\tTrain loss: 0.00574, Accuracy: 6515/6768 (96.00%)\n",
      "\tValidation loss: 0.00017, Accuracy: 1546/1692 (91.00%)\n",
      "\tTest loss: 0.00058, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 72 [0/6768 (0.00%)]\t\tLoss: 0.44572\n",
      "Training Progress: \tEpoch 72 [320/6768 (4.72%)]\t\tLoss: 0.74293\n",
      "Training Progress: \tEpoch 72 [640/6768 (9.43%)]\t\tLoss: 0.41179\n",
      "Training Progress: \tEpoch 72 [960/6768 (14.15%)]\t\tLoss: 0.61187\n",
      "Training Progress: \tEpoch 72 [1280/6768 (18.87%)]\t\tLoss: 0.57467\n",
      "Training Progress: \tEpoch 72 [1600/6768 (23.58%)]\t\tLoss: 0.59147\n",
      "Training Progress: \tEpoch 72 [1920/6768 (28.30%)]\t\tLoss: 0.44274\n",
      "Training Progress: \tEpoch 72 [2240/6768 (33.02%)]\t\tLoss: 0.55547\n",
      "Training Progress: \tEpoch 72 [2560/6768 (37.74%)]\t\tLoss: 0.83130\n",
      "Training Progress: \tEpoch 72 [2880/6768 (42.45%)]\t\tLoss: 0.59187\n",
      "Training Progress: \tEpoch 72 [3200/6768 (47.17%)]\t\tLoss: 0.47162\n",
      "Training Progress: \tEpoch 72 [3520/6768 (51.89%)]\t\tLoss: 0.75718\n",
      "Training Progress: \tEpoch 72 [3840/6768 (56.60%)]\t\tLoss: 0.55490\n",
      "Training Progress: \tEpoch 72 [4160/6768 (61.32%)]\t\tLoss: 0.82994\n",
      "Training Progress: \tEpoch 72 [4480/6768 (66.04%)]\t\tLoss: 0.59096\n",
      "Training Progress: \tEpoch 72 [4800/6768 (70.75%)]\t\tLoss: 0.44049\n",
      "Training Progress: \tEpoch 72 [5120/6768 (75.47%)]\t\tLoss: 0.43124\n",
      "Training Progress: \tEpoch 72 [5440/6768 (80.19%)]\t\tLoss: 0.59787\n",
      "Training Progress: \tEpoch 72 [5760/6768 (84.91%)]\t\tLoss: 0.57507\n",
      "Training Progress: \tEpoch 72 [6080/6768 (89.62%)]\t\tLoss: 0.55770\n",
      "Training Progress: \tEpoch 72 [6400/6768 (94.34%)]\t\tLoss: 0.60462\n",
      "Training Progress: \tEpoch 72 [6720/6768 (99.06%)]\t\tLoss: 0.40248\n",
      "\tTrain loss: 0.00535, Accuracy: 6549/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1565/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1049/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 73 [0/6768 (0.00%)]\t\tLoss: 0.53088\n",
      "Training Progress: \tEpoch 73 [320/6768 (4.72%)]\t\tLoss: 1.07950\n",
      "Training Progress: \tEpoch 73 [640/6768 (9.43%)]\t\tLoss: 0.44846\n",
      "Training Progress: \tEpoch 73 [960/6768 (14.15%)]\t\tLoss: 0.48424\n",
      "Training Progress: \tEpoch 73 [1280/6768 (18.87%)]\t\tLoss: 0.53140\n",
      "Training Progress: \tEpoch 73 [1600/6768 (23.58%)]\t\tLoss: 0.44445\n",
      "Training Progress: \tEpoch 73 [1920/6768 (28.30%)]\t\tLoss: 0.53008\n",
      "Training Progress: \tEpoch 73 [2240/6768 (33.02%)]\t\tLoss: 0.65793\n",
      "Training Progress: \tEpoch 73 [2560/6768 (37.74%)]\t\tLoss: 0.41001\n",
      "Training Progress: \tEpoch 73 [2880/6768 (42.45%)]\t\tLoss: 0.55492\n",
      "Training Progress: \tEpoch 73 [3200/6768 (47.17%)]\t\tLoss: 0.42571\n",
      "Training Progress: \tEpoch 73 [3520/6768 (51.89%)]\t\tLoss: 0.71846\n",
      "Training Progress: \tEpoch 73 [3840/6768 (56.60%)]\t\tLoss: 0.73878\n",
      "Training Progress: \tEpoch 73 [4160/6768 (61.32%)]\t\tLoss: 0.60308\n",
      "Training Progress: \tEpoch 73 [4480/6768 (66.04%)]\t\tLoss: 0.56932\n",
      "Training Progress: \tEpoch 73 [4800/6768 (70.75%)]\t\tLoss: 0.68110\n",
      "Training Progress: \tEpoch 73 [5120/6768 (75.47%)]\t\tLoss: 0.76132\n",
      "Training Progress: \tEpoch 73 [5440/6768 (80.19%)]\t\tLoss: 0.59166\n",
      "Training Progress: \tEpoch 73 [5760/6768 (84.91%)]\t\tLoss: 0.53136\n",
      "Training Progress: \tEpoch 73 [6080/6768 (89.62%)]\t\tLoss: 0.43338\n",
      "Training Progress: \tEpoch 73 [6400/6768 (94.34%)]\t\tLoss: 0.64854\n",
      "Training Progress: \tEpoch 73 [6720/6768 (99.06%)]\t\tLoss: 0.60193\n",
      "\tTrain loss: 0.00528, Accuracy: 6561/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1567/1692 (92.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00059, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 74 [0/6768 (0.00%)]\t\tLoss: 0.46181\n",
      "Training Progress: \tEpoch 74 [320/6768 (4.72%)]\t\tLoss: 0.73164\n",
      "Training Progress: \tEpoch 74 [640/6768 (9.43%)]\t\tLoss: 0.46240\n",
      "Training Progress: \tEpoch 74 [960/6768 (14.15%)]\t\tLoss: 0.67816\n",
      "Training Progress: \tEpoch 74 [1280/6768 (18.87%)]\t\tLoss: 0.59355\n",
      "Training Progress: \tEpoch 74 [1600/6768 (23.58%)]\t\tLoss: 0.37089\n",
      "Training Progress: \tEpoch 74 [1920/6768 (28.30%)]\t\tLoss: 0.62654\n",
      "Training Progress: \tEpoch 74 [2240/6768 (33.02%)]\t\tLoss: 0.50837\n",
      "Training Progress: \tEpoch 74 [2560/6768 (37.74%)]\t\tLoss: 0.69123\n",
      "Training Progress: \tEpoch 74 [2880/6768 (42.45%)]\t\tLoss: 0.47999\n",
      "Training Progress: \tEpoch 74 [3200/6768 (47.17%)]\t\tLoss: 0.47971\n",
      "Training Progress: \tEpoch 74 [3520/6768 (51.89%)]\t\tLoss: 0.55185\n",
      "Training Progress: \tEpoch 74 [3840/6768 (56.60%)]\t\tLoss: 0.57376\n",
      "Training Progress: \tEpoch 74 [4160/6768 (61.32%)]\t\tLoss: 0.53283\n",
      "Training Progress: \tEpoch 74 [4480/6768 (66.04%)]\t\tLoss: 0.73863\n",
      "Training Progress: \tEpoch 74 [4800/6768 (70.75%)]\t\tLoss: 0.75080\n",
      "Training Progress: \tEpoch 74 [5120/6768 (75.47%)]\t\tLoss: 0.74518\n",
      "Training Progress: \tEpoch 74 [5440/6768 (80.19%)]\t\tLoss: 0.56468\n",
      "Training Progress: \tEpoch 74 [5760/6768 (84.91%)]\t\tLoss: 0.46160\n",
      "Training Progress: \tEpoch 74 [6080/6768 (89.62%)]\t\tLoss: 0.64277\n",
      "Training Progress: \tEpoch 74 [6400/6768 (94.34%)]\t\tLoss: 0.69271\n",
      "Training Progress: \tEpoch 74 [6720/6768 (99.06%)]\t\tLoss: 0.57692\n",
      "\tTrain loss: 0.00512, Accuracy: 6553/6768 (96.00%)\n",
      "\tValidation loss: 0.00016, Accuracy: 1562/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1094/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 75 [0/6768 (0.00%)]\t\tLoss: 0.42657\n",
      "Training Progress: \tEpoch 75 [320/6768 (4.72%)]\t\tLoss: 0.90048\n",
      "Training Progress: \tEpoch 75 [640/6768 (9.43%)]\t\tLoss: 0.63911\n",
      "Training Progress: \tEpoch 75 [960/6768 (14.15%)]\t\tLoss: 0.54992\n",
      "Training Progress: \tEpoch 75 [1280/6768 (18.87%)]\t\tLoss: 0.47214\n",
      "Training Progress: \tEpoch 75 [1600/6768 (23.58%)]\t\tLoss: 0.31601\n",
      "Training Progress: \tEpoch 75 [1920/6768 (28.30%)]\t\tLoss: 0.28860\n",
      "Training Progress: \tEpoch 75 [2240/6768 (33.02%)]\t\tLoss: 0.66987\n",
      "Training Progress: \tEpoch 75 [2560/6768 (37.74%)]\t\tLoss: 0.71516\n",
      "Training Progress: \tEpoch 75 [2880/6768 (42.45%)]\t\tLoss: 0.43802\n",
      "Training Progress: \tEpoch 75 [3200/6768 (47.17%)]\t\tLoss: 0.49566\n",
      "Training Progress: \tEpoch 75 [3520/6768 (51.89%)]\t\tLoss: 0.55144\n",
      "Training Progress: \tEpoch 75 [3840/6768 (56.60%)]\t\tLoss: 0.59290\n",
      "Training Progress: \tEpoch 75 [4160/6768 (61.32%)]\t\tLoss: 0.46564\n",
      "Training Progress: \tEpoch 75 [4480/6768 (66.04%)]\t\tLoss: 0.74164\n",
      "Training Progress: \tEpoch 75 [4800/6768 (70.75%)]\t\tLoss: 0.45713\n",
      "Training Progress: \tEpoch 75 [5120/6768 (75.47%)]\t\tLoss: 0.39092\n",
      "Training Progress: \tEpoch 75 [5440/6768 (80.19%)]\t\tLoss: 0.62805\n",
      "Training Progress: \tEpoch 75 [5760/6768 (84.91%)]\t\tLoss: 0.53175\n",
      "Training Progress: \tEpoch 75 [6080/6768 (89.62%)]\t\tLoss: 0.46087\n",
      "Training Progress: \tEpoch 75 [6400/6768 (94.34%)]\t\tLoss: 0.39910\n",
      "Training Progress: \tEpoch 75 [6720/6768 (99.06%)]\t\tLoss: 0.51028\n",
      "\tTrain loss: 0.00488, Accuracy: 6545/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1559/1692 (92.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1099/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 76 [0/6768 (0.00%)]\t\tLoss: 0.42106\n",
      "Training Progress: \tEpoch 76 [320/6768 (4.72%)]\t\tLoss: 0.86208\n",
      "Training Progress: \tEpoch 76 [640/6768 (9.43%)]\t\tLoss: 0.37024\n",
      "Training Progress: \tEpoch 76 [960/6768 (14.15%)]\t\tLoss: 0.95527\n",
      "Training Progress: \tEpoch 76 [1280/6768 (18.87%)]\t\tLoss: 0.45146\n",
      "Training Progress: \tEpoch 76 [1600/6768 (23.58%)]\t\tLoss: 0.40754\n",
      "Training Progress: \tEpoch 76 [1920/6768 (28.30%)]\t\tLoss: 0.63247\n",
      "Training Progress: \tEpoch 76 [2240/6768 (33.02%)]\t\tLoss: 0.64205\n",
      "Training Progress: \tEpoch 76 [2560/6768 (37.74%)]\t\tLoss: 0.50378\n",
      "Training Progress: \tEpoch 76 [2880/6768 (42.45%)]\t\tLoss: 0.33224\n",
      "Training Progress: \tEpoch 76 [3200/6768 (47.17%)]\t\tLoss: 0.39241\n",
      "Training Progress: \tEpoch 76 [3520/6768 (51.89%)]\t\tLoss: 0.52230\n",
      "Training Progress: \tEpoch 76 [3840/6768 (56.60%)]\t\tLoss: 0.58336\n",
      "Training Progress: \tEpoch 76 [4160/6768 (61.32%)]\t\tLoss: 0.37659\n",
      "Training Progress: \tEpoch 76 [4480/6768 (66.04%)]\t\tLoss: 0.49623\n",
      "Training Progress: \tEpoch 76 [4800/6768 (70.75%)]\t\tLoss: 0.58800\n",
      "Training Progress: \tEpoch 76 [5120/6768 (75.47%)]\t\tLoss: 0.54899\n",
      "Training Progress: \tEpoch 76 [5440/6768 (80.19%)]\t\tLoss: 0.75766\n",
      "Training Progress: \tEpoch 76 [5760/6768 (84.91%)]\t\tLoss: 0.42144\n",
      "Training Progress: \tEpoch 76 [6080/6768 (89.62%)]\t\tLoss: 0.35106\n",
      "Training Progress: \tEpoch 76 [6400/6768 (94.34%)]\t\tLoss: 0.85133\n",
      "Training Progress: \tEpoch 76 [6720/6768 (99.06%)]\t\tLoss: 0.79759\n",
      "\tTrain loss: 0.00442, Accuracy: 6595/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1579/1692 (93.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1083/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 77 [0/6768 (0.00%)]\t\tLoss: 0.55520\n",
      "Training Progress: \tEpoch 77 [320/6768 (4.72%)]\t\tLoss: 0.83719\n",
      "Training Progress: \tEpoch 77 [640/6768 (9.43%)]\t\tLoss: 0.27186\n",
      "Training Progress: \tEpoch 77 [960/6768 (14.15%)]\t\tLoss: 0.75949\n",
      "Training Progress: \tEpoch 77 [1280/6768 (18.87%)]\t\tLoss: 0.46627\n",
      "Training Progress: \tEpoch 77 [1600/6768 (23.58%)]\t\tLoss: 0.33386\n",
      "Training Progress: \tEpoch 77 [1920/6768 (28.30%)]\t\tLoss: 0.59135\n",
      "Training Progress: \tEpoch 77 [2240/6768 (33.02%)]\t\tLoss: 0.63534\n",
      "Training Progress: \tEpoch 77 [2560/6768 (37.74%)]\t\tLoss: 0.55424\n",
      "Training Progress: \tEpoch 77 [2880/6768 (42.45%)]\t\tLoss: 0.57972\n",
      "Training Progress: \tEpoch 77 [3200/6768 (47.17%)]\t\tLoss: 0.66032\n",
      "Training Progress: \tEpoch 77 [3520/6768 (51.89%)]\t\tLoss: 0.48328\n",
      "Training Progress: \tEpoch 77 [3840/6768 (56.60%)]\t\tLoss: 0.62062\n",
      "Training Progress: \tEpoch 77 [4160/6768 (61.32%)]\t\tLoss: 0.68706\n",
      "Training Progress: \tEpoch 77 [4480/6768 (66.04%)]\t\tLoss: 0.87349\n",
      "Training Progress: \tEpoch 77 [4800/6768 (70.75%)]\t\tLoss: 0.47268\n",
      "Training Progress: \tEpoch 77 [5120/6768 (75.47%)]\t\tLoss: 0.39722\n",
      "Training Progress: \tEpoch 77 [5440/6768 (80.19%)]\t\tLoss: 0.60174\n",
      "Training Progress: \tEpoch 77 [5760/6768 (84.91%)]\t\tLoss: 0.48895\n",
      "Training Progress: \tEpoch 77 [6080/6768 (89.62%)]\t\tLoss: 0.62246\n",
      "Training Progress: \tEpoch 77 [6400/6768 (94.34%)]\t\tLoss: 0.53680\n",
      "Training Progress: \tEpoch 77 [6720/6768 (99.06%)]\t\tLoss: 0.85076\n",
      "\tTrain loss: 0.00499, Accuracy: 6550/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1560/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1077/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 78 [0/6768 (0.00%)]\t\tLoss: 0.44089\n",
      "Training Progress: \tEpoch 78 [320/6768 (4.72%)]\t\tLoss: 0.56031\n",
      "Training Progress: \tEpoch 78 [640/6768 (9.43%)]\t\tLoss: 0.43995\n",
      "Training Progress: \tEpoch 78 [960/6768 (14.15%)]\t\tLoss: 0.69133\n",
      "Training Progress: \tEpoch 78 [1280/6768 (18.87%)]\t\tLoss: 0.51638\n",
      "Training Progress: \tEpoch 78 [1600/6768 (23.58%)]\t\tLoss: 0.50475\n",
      "Training Progress: \tEpoch 78 [1920/6768 (28.30%)]\t\tLoss: 0.70717\n",
      "Training Progress: \tEpoch 78 [2240/6768 (33.02%)]\t\tLoss: 0.47876\n",
      "Training Progress: \tEpoch 78 [2560/6768 (37.74%)]\t\tLoss: 0.54046\n",
      "Training Progress: \tEpoch 78 [2880/6768 (42.45%)]\t\tLoss: 0.58844\n",
      "Training Progress: \tEpoch 78 [3200/6768 (47.17%)]\t\tLoss: 0.76554\n",
      "Training Progress: \tEpoch 78 [3520/6768 (51.89%)]\t\tLoss: 0.54088\n",
      "Training Progress: \tEpoch 78 [3840/6768 (56.60%)]\t\tLoss: 0.45977\n",
      "Training Progress: \tEpoch 78 [4160/6768 (61.32%)]\t\tLoss: 0.71046\n",
      "Training Progress: \tEpoch 78 [4480/6768 (66.04%)]\t\tLoss: 0.62065\n",
      "Training Progress: \tEpoch 78 [4800/6768 (70.75%)]\t\tLoss: 0.46053\n",
      "Training Progress: \tEpoch 78 [5120/6768 (75.47%)]\t\tLoss: 0.82922\n",
      "Training Progress: \tEpoch 78 [5440/6768 (80.19%)]\t\tLoss: 0.60403\n",
      "Training Progress: \tEpoch 78 [5760/6768 (84.91%)]\t\tLoss: 0.54833\n",
      "Training Progress: \tEpoch 78 [6080/6768 (89.62%)]\t\tLoss: 0.41479\n",
      "Training Progress: \tEpoch 78 [6400/6768 (94.34%)]\t\tLoss: 0.51541\n",
      "Training Progress: \tEpoch 78 [6720/6768 (99.06%)]\t\tLoss: 0.66551\n",
      "\tTrain loss: 0.00461, Accuracy: 6559/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1572/1692 (92.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1084/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 79 [0/6768 (0.00%)]\t\tLoss: 0.47365\n",
      "Training Progress: \tEpoch 79 [320/6768 (4.72%)]\t\tLoss: 0.58255\n",
      "Training Progress: \tEpoch 79 [640/6768 (9.43%)]\t\tLoss: 0.60815\n",
      "Training Progress: \tEpoch 79 [960/6768 (14.15%)]\t\tLoss: 0.70167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 79 [1280/6768 (18.87%)]\t\tLoss: 0.69026\n",
      "Training Progress: \tEpoch 79 [1600/6768 (23.58%)]\t\tLoss: 0.40283\n",
      "Training Progress: \tEpoch 79 [1920/6768 (28.30%)]\t\tLoss: 0.64746\n",
      "Training Progress: \tEpoch 79 [2240/6768 (33.02%)]\t\tLoss: 0.87857\n",
      "Training Progress: \tEpoch 79 [2560/6768 (37.74%)]\t\tLoss: 0.46098\n",
      "Training Progress: \tEpoch 79 [2880/6768 (42.45%)]\t\tLoss: 0.32234\n",
      "Training Progress: \tEpoch 79 [3200/6768 (47.17%)]\t\tLoss: 0.68934\n",
      "Training Progress: \tEpoch 79 [3520/6768 (51.89%)]\t\tLoss: 0.40987\n",
      "Training Progress: \tEpoch 79 [3840/6768 (56.60%)]\t\tLoss: 0.55330\n",
      "Training Progress: \tEpoch 79 [4160/6768 (61.32%)]\t\tLoss: 0.43039\n",
      "Training Progress: \tEpoch 79 [4480/6768 (66.04%)]\t\tLoss: 0.50487\n",
      "Training Progress: \tEpoch 79 [4800/6768 (70.75%)]\t\tLoss: 0.42877\n",
      "Training Progress: \tEpoch 79 [5120/6768 (75.47%)]\t\tLoss: 0.66293\n",
      "Training Progress: \tEpoch 79 [5440/6768 (80.19%)]\t\tLoss: 0.48737\n",
      "Training Progress: \tEpoch 79 [5760/6768 (84.91%)]\t\tLoss: 0.56094\n",
      "Training Progress: \tEpoch 79 [6080/6768 (89.62%)]\t\tLoss: 0.63781\n",
      "Training Progress: \tEpoch 79 [6400/6768 (94.34%)]\t\tLoss: 0.71714\n",
      "Training Progress: \tEpoch 79 [6720/6768 (99.06%)]\t\tLoss: 0.42716\n",
      "\tTrain loss: 0.00471, Accuracy: 6557/6768 (96.00%)\n",
      "\tValidation loss: 0.00015, Accuracy: 1566/1692 (92.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1087/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 80 [0/6768 (0.00%)]\t\tLoss: 0.52108\n",
      "Training Progress: \tEpoch 80 [320/6768 (4.72%)]\t\tLoss: 0.68601\n",
      "Training Progress: \tEpoch 80 [640/6768 (9.43%)]\t\tLoss: 0.49986\n",
      "Training Progress: \tEpoch 80 [960/6768 (14.15%)]\t\tLoss: 0.68320\n",
      "Training Progress: \tEpoch 80 [1280/6768 (18.87%)]\t\tLoss: 0.69694\n",
      "Training Progress: \tEpoch 80 [1600/6768 (23.58%)]\t\tLoss: 0.66933\n",
      "Training Progress: \tEpoch 80 [1920/6768 (28.30%)]\t\tLoss: 0.56057\n",
      "Training Progress: \tEpoch 80 [2240/6768 (33.02%)]\t\tLoss: 0.73535\n",
      "Training Progress: \tEpoch 80 [2560/6768 (37.74%)]\t\tLoss: 0.63398\n",
      "Training Progress: \tEpoch 80 [2880/6768 (42.45%)]\t\tLoss: 0.48241\n",
      "Training Progress: \tEpoch 80 [3200/6768 (47.17%)]\t\tLoss: 0.49533\n",
      "Training Progress: \tEpoch 80 [3520/6768 (51.89%)]\t\tLoss: 0.59044\n",
      "Training Progress: \tEpoch 80 [3840/6768 (56.60%)]\t\tLoss: 0.56919\n",
      "Training Progress: \tEpoch 80 [4160/6768 (61.32%)]\t\tLoss: 0.46993\n",
      "Training Progress: \tEpoch 80 [4480/6768 (66.04%)]\t\tLoss: 0.76188\n",
      "Training Progress: \tEpoch 80 [4800/6768 (70.75%)]\t\tLoss: 0.46679\n",
      "Training Progress: \tEpoch 80 [5120/6768 (75.47%)]\t\tLoss: 0.73850\n",
      "Training Progress: \tEpoch 80 [5440/6768 (80.19%)]\t\tLoss: 0.63969\n",
      "Training Progress: \tEpoch 80 [5760/6768 (84.91%)]\t\tLoss: 0.40044\n",
      "Training Progress: \tEpoch 80 [6080/6768 (89.62%)]\t\tLoss: 0.29740\n",
      "Training Progress: \tEpoch 80 [6400/6768 (94.34%)]\t\tLoss: 0.69634\n",
      "Training Progress: \tEpoch 80 [6720/6768 (99.06%)]\t\tLoss: 0.55853\n",
      "\tTrain loss: 0.00429, Accuracy: 6613/6768 (97.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1586/1692 (93.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 81 [0/6768 (0.00%)]\t\tLoss: 0.53802\n",
      "Training Progress: \tEpoch 81 [320/6768 (4.72%)]\t\tLoss: 0.65094\n",
      "Training Progress: \tEpoch 81 [640/6768 (9.43%)]\t\tLoss: 0.36894\n",
      "Training Progress: \tEpoch 81 [960/6768 (14.15%)]\t\tLoss: 0.57407\n",
      "Training Progress: \tEpoch 81 [1280/6768 (18.87%)]\t\tLoss: 0.47339\n",
      "Training Progress: \tEpoch 81 [1600/6768 (23.58%)]\t\tLoss: 0.57365\n",
      "Training Progress: \tEpoch 81 [1920/6768 (28.30%)]\t\tLoss: 0.39626\n",
      "Training Progress: \tEpoch 81 [2240/6768 (33.02%)]\t\tLoss: 0.28473\n",
      "Training Progress: \tEpoch 81 [2560/6768 (37.74%)]\t\tLoss: 0.58755\n",
      "Training Progress: \tEpoch 81 [2880/6768 (42.45%)]\t\tLoss: 0.28631\n",
      "Training Progress: \tEpoch 81 [3200/6768 (47.17%)]\t\tLoss: 0.47148\n",
      "Training Progress: \tEpoch 81 [3520/6768 (51.89%)]\t\tLoss: 0.54108\n",
      "Training Progress: \tEpoch 81 [3840/6768 (56.60%)]\t\tLoss: 0.61026\n",
      "Training Progress: \tEpoch 81 [4160/6768 (61.32%)]\t\tLoss: 0.50667\n",
      "Training Progress: \tEpoch 81 [4480/6768 (66.04%)]\t\tLoss: 0.52830\n",
      "Training Progress: \tEpoch 81 [4800/6768 (70.75%)]\t\tLoss: 0.57741\n",
      "Training Progress: \tEpoch 81 [5120/6768 (75.47%)]\t\tLoss: 0.54678\n",
      "Training Progress: \tEpoch 81 [5440/6768 (80.19%)]\t\tLoss: 0.61684\n",
      "Training Progress: \tEpoch 81 [5760/6768 (84.91%)]\t\tLoss: 0.61415\n",
      "Training Progress: \tEpoch 81 [6080/6768 (89.62%)]\t\tLoss: 0.48832\n",
      "Training Progress: \tEpoch 81 [6400/6768 (94.34%)]\t\tLoss: 0.43707\n",
      "Training Progress: \tEpoch 81 [6720/6768 (99.06%)]\t\tLoss: 0.84371\n",
      "\tTrain loss: 0.00475, Accuracy: 6549/6768 (96.00%)\n",
      "\tValidation loss: 0.00014, Accuracy: 1574/1692 (93.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1084/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 82 [0/6768 (0.00%)]\t\tLoss: 0.42233\n",
      "Training Progress: \tEpoch 82 [320/6768 (4.72%)]\t\tLoss: 0.60891\n",
      "Training Progress: \tEpoch 82 [640/6768 (9.43%)]\t\tLoss: 0.58587\n",
      "Training Progress: \tEpoch 82 [960/6768 (14.15%)]\t\tLoss: 0.44417\n",
      "Training Progress: \tEpoch 82 [1280/6768 (18.87%)]\t\tLoss: 0.67476\n",
      "Training Progress: \tEpoch 82 [1600/6768 (23.58%)]\t\tLoss: 0.42291\n",
      "Training Progress: \tEpoch 82 [1920/6768 (28.30%)]\t\tLoss: 0.53055\n",
      "Training Progress: \tEpoch 82 [2240/6768 (33.02%)]\t\tLoss: 0.61348\n",
      "Training Progress: \tEpoch 82 [2560/6768 (37.74%)]\t\tLoss: 0.51057\n",
      "Training Progress: \tEpoch 82 [2880/6768 (42.45%)]\t\tLoss: 0.67821\n",
      "Training Progress: \tEpoch 82 [3200/6768 (47.17%)]\t\tLoss: 0.54635\n",
      "Training Progress: \tEpoch 82 [3520/6768 (51.89%)]\t\tLoss: 0.60456\n",
      "Training Progress: \tEpoch 82 [3840/6768 (56.60%)]\t\tLoss: 0.37474\n",
      "Training Progress: \tEpoch 82 [4160/6768 (61.32%)]\t\tLoss: 0.61499\n",
      "Training Progress: \tEpoch 82 [4480/6768 (66.04%)]\t\tLoss: 0.36280\n",
      "Training Progress: \tEpoch 82 [4800/6768 (70.75%)]\t\tLoss: 0.48118\n",
      "Training Progress: \tEpoch 82 [5120/6768 (75.47%)]\t\tLoss: 0.72756\n",
      "Training Progress: \tEpoch 82 [5440/6768 (80.19%)]\t\tLoss: 0.76662\n",
      "Training Progress: \tEpoch 82 [5760/6768 (84.91%)]\t\tLoss: 0.74911\n",
      "Training Progress: \tEpoch 82 [6080/6768 (89.62%)]\t\tLoss: 0.25096\n",
      "Training Progress: \tEpoch 82 [6400/6768 (94.34%)]\t\tLoss: 0.39727\n",
      "Training Progress: \tEpoch 82 [6720/6768 (99.06%)]\t\tLoss: 0.82132\n",
      "\tTrain loss: 0.00383, Accuracy: 6616/6768 (97.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1590/1692 (93.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1061/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 83 [0/6768 (0.00%)]\t\tLoss: 0.33967\n",
      "Training Progress: \tEpoch 83 [320/6768 (4.72%)]\t\tLoss: 0.61162\n",
      "Training Progress: \tEpoch 83 [640/6768 (9.43%)]\t\tLoss: 0.47860\n",
      "Training Progress: \tEpoch 83 [960/6768 (14.15%)]\t\tLoss: 0.57601\n",
      "Training Progress: \tEpoch 83 [1280/6768 (18.87%)]\t\tLoss: 0.48554\n",
      "Training Progress: \tEpoch 83 [1600/6768 (23.58%)]\t\tLoss: 0.28250\n",
      "Training Progress: \tEpoch 83 [1920/6768 (28.30%)]\t\tLoss: 0.41362\n",
      "Training Progress: \tEpoch 83 [2240/6768 (33.02%)]\t\tLoss: 0.58064\n",
      "Training Progress: \tEpoch 83 [2560/6768 (37.74%)]\t\tLoss: 0.76133\n",
      "Training Progress: \tEpoch 83 [2880/6768 (42.45%)]\t\tLoss: 0.44623\n",
      "Training Progress: \tEpoch 83 [3200/6768 (47.17%)]\t\tLoss: 0.30894\n",
      "Training Progress: \tEpoch 83 [3520/6768 (51.89%)]\t\tLoss: 0.65637\n",
      "Training Progress: \tEpoch 83 [3840/6768 (56.60%)]\t\tLoss: 0.74756\n",
      "Training Progress: \tEpoch 83 [4160/6768 (61.32%)]\t\tLoss: 0.40016\n",
      "Training Progress: \tEpoch 83 [4480/6768 (66.04%)]\t\tLoss: 0.56413\n",
      "Training Progress: \tEpoch 83 [4800/6768 (70.75%)]\t\tLoss: 0.44541\n",
      "Training Progress: \tEpoch 83 [5120/6768 (75.47%)]\t\tLoss: 0.85423\n",
      "Training Progress: \tEpoch 83 [5440/6768 (80.19%)]\t\tLoss: 0.73502\n",
      "Training Progress: \tEpoch 83 [5760/6768 (84.91%)]\t\tLoss: 0.39821\n",
      "Training Progress: \tEpoch 83 [6080/6768 (89.62%)]\t\tLoss: 0.47169\n",
      "Training Progress: \tEpoch 83 [6400/6768 (94.34%)]\t\tLoss: 0.68263\n",
      "Training Progress: \tEpoch 83 [6720/6768 (99.06%)]\t\tLoss: 0.45684\n",
      "\tTrain loss: 0.00417, Accuracy: 6596/6768 (97.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1592/1692 (94.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1059/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 84 [0/6768 (0.00%)]\t\tLoss: 0.53971\n",
      "Training Progress: \tEpoch 84 [320/6768 (4.72%)]\t\tLoss: 0.51484\n",
      "Training Progress: \tEpoch 84 [640/6768 (9.43%)]\t\tLoss: 0.32673\n",
      "Training Progress: \tEpoch 84 [960/6768 (14.15%)]\t\tLoss: 0.89868\n",
      "Training Progress: \tEpoch 84 [1280/6768 (18.87%)]\t\tLoss: 0.63760\n",
      "Training Progress: \tEpoch 84 [1600/6768 (23.58%)]\t\tLoss: 0.65379\n",
      "Training Progress: \tEpoch 84 [1920/6768 (28.30%)]\t\tLoss: 0.53877\n",
      "Training Progress: \tEpoch 84 [2240/6768 (33.02%)]\t\tLoss: 0.33648\n",
      "Training Progress: \tEpoch 84 [2560/6768 (37.74%)]\t\tLoss: 0.98978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 84 [2880/6768 (42.45%)]\t\tLoss: 0.45132\n",
      "Training Progress: \tEpoch 84 [3200/6768 (47.17%)]\t\tLoss: 0.51531\n",
      "Training Progress: \tEpoch 84 [3520/6768 (51.89%)]\t\tLoss: 0.36921\n",
      "Training Progress: \tEpoch 84 [3840/6768 (56.60%)]\t\tLoss: 0.47951\n",
      "Training Progress: \tEpoch 84 [4160/6768 (61.32%)]\t\tLoss: 0.49902\n",
      "Training Progress: \tEpoch 84 [4480/6768 (66.04%)]\t\tLoss: 0.61684\n",
      "Training Progress: \tEpoch 84 [4800/6768 (70.75%)]\t\tLoss: 0.51367\n",
      "Training Progress: \tEpoch 84 [5120/6768 (75.47%)]\t\tLoss: 0.61821\n",
      "Training Progress: \tEpoch 84 [5440/6768 (80.19%)]\t\tLoss: 0.57565\n",
      "Training Progress: \tEpoch 84 [5760/6768 (84.91%)]\t\tLoss: 0.63693\n",
      "Training Progress: \tEpoch 84 [6080/6768 (89.62%)]\t\tLoss: 0.39461\n",
      "Training Progress: \tEpoch 84 [6400/6768 (94.34%)]\t\tLoss: 0.60182\n",
      "Training Progress: \tEpoch 84 [6720/6768 (99.06%)]\t\tLoss: 0.56004\n",
      "\tTrain loss: 0.00363, Accuracy: 6635/6768 (98.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1601/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1093/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 85 [0/6768 (0.00%)]\t\tLoss: 0.60796\n",
      "Training Progress: \tEpoch 85 [320/6768 (4.72%)]\t\tLoss: 0.67159\n",
      "Training Progress: \tEpoch 85 [640/6768 (9.43%)]\t\tLoss: 0.46421\n",
      "Training Progress: \tEpoch 85 [960/6768 (14.15%)]\t\tLoss: 0.63659\n",
      "Training Progress: \tEpoch 85 [1280/6768 (18.87%)]\t\tLoss: 0.36453\n",
      "Training Progress: \tEpoch 85 [1600/6768 (23.58%)]\t\tLoss: 0.55510\n",
      "Training Progress: \tEpoch 85 [1920/6768 (28.30%)]\t\tLoss: 0.46060\n",
      "Training Progress: \tEpoch 85 [2240/6768 (33.02%)]\t\tLoss: 0.66792\n",
      "Training Progress: \tEpoch 85 [2560/6768 (37.74%)]\t\tLoss: 0.59733\n",
      "Training Progress: \tEpoch 85 [2880/6768 (42.45%)]\t\tLoss: 0.54711\n",
      "Training Progress: \tEpoch 85 [3200/6768 (47.17%)]\t\tLoss: 1.04912\n",
      "Training Progress: \tEpoch 85 [3520/6768 (51.89%)]\t\tLoss: 0.46478\n",
      "Training Progress: \tEpoch 85 [3840/6768 (56.60%)]\t\tLoss: 0.47212\n",
      "Training Progress: \tEpoch 85 [4160/6768 (61.32%)]\t\tLoss: 0.52224\n",
      "Training Progress: \tEpoch 85 [4480/6768 (66.04%)]\t\tLoss: 0.70376\n",
      "Training Progress: \tEpoch 85 [4800/6768 (70.75%)]\t\tLoss: 0.57100\n",
      "Training Progress: \tEpoch 85 [5120/6768 (75.47%)]\t\tLoss: 0.71075\n",
      "Training Progress: \tEpoch 85 [5440/6768 (80.19%)]\t\tLoss: 0.69701\n",
      "Training Progress: \tEpoch 85 [5760/6768 (84.91%)]\t\tLoss: 0.55630\n",
      "Training Progress: \tEpoch 85 [6080/6768 (89.62%)]\t\tLoss: 0.51862\n",
      "Training Progress: \tEpoch 85 [6400/6768 (94.34%)]\t\tLoss: 0.63929\n",
      "Training Progress: \tEpoch 85 [6720/6768 (99.06%)]\t\tLoss: 0.67079\n",
      "\tTrain loss: 0.00400, Accuracy: 6622/6768 (97.00%)\n",
      "\tValidation loss: 0.00013, Accuracy: 1600/1692 (94.00%)\n",
      "\tTest loss: 0.00059, Accuracy: 1089/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 86 [0/6768 (0.00%)]\t\tLoss: 0.51591\n",
      "Training Progress: \tEpoch 86 [320/6768 (4.72%)]\t\tLoss: 0.38818\n",
      "Training Progress: \tEpoch 86 [640/6768 (9.43%)]\t\tLoss: 0.34654\n",
      "Training Progress: \tEpoch 86 [960/6768 (14.15%)]\t\tLoss: 0.65026\n",
      "Training Progress: \tEpoch 86 [1280/6768 (18.87%)]\t\tLoss: 0.35265\n",
      "Training Progress: \tEpoch 86 [1600/6768 (23.58%)]\t\tLoss: 0.50822\n",
      "Training Progress: \tEpoch 86 [1920/6768 (28.30%)]\t\tLoss: 0.79010\n",
      "Training Progress: \tEpoch 86 [2240/6768 (33.02%)]\t\tLoss: 0.63698\n",
      "Training Progress: \tEpoch 86 [2560/6768 (37.74%)]\t\tLoss: 0.41102\n",
      "Training Progress: \tEpoch 86 [2880/6768 (42.45%)]\t\tLoss: 0.87876\n",
      "Training Progress: \tEpoch 86 [3200/6768 (47.17%)]\t\tLoss: 0.38864\n",
      "Training Progress: \tEpoch 86 [3520/6768 (51.89%)]\t\tLoss: 0.57375\n",
      "Training Progress: \tEpoch 86 [3840/6768 (56.60%)]\t\tLoss: 0.73506\n",
      "Training Progress: \tEpoch 86 [4160/6768 (61.32%)]\t\tLoss: 0.71187\n",
      "Training Progress: \tEpoch 86 [4480/6768 (66.04%)]\t\tLoss: 0.44048\n",
      "Training Progress: \tEpoch 86 [4800/6768 (70.75%)]\t\tLoss: 0.21962\n",
      "Training Progress: \tEpoch 86 [5120/6768 (75.47%)]\t\tLoss: 0.55201\n",
      "Training Progress: \tEpoch 86 [5440/6768 (80.19%)]\t\tLoss: 0.52223\n",
      "Training Progress: \tEpoch 86 [5760/6768 (84.91%)]\t\tLoss: 0.42223\n",
      "Training Progress: \tEpoch 86 [6080/6768 (89.62%)]\t\tLoss: 0.43086\n",
      "Training Progress: \tEpoch 86 [6400/6768 (94.34%)]\t\tLoss: 0.47033\n",
      "Training Progress: \tEpoch 86 [6720/6768 (99.06%)]\t\tLoss: 0.89077\n",
      "\tTrain loss: 0.00322, Accuracy: 6643/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1599/1692 (94.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1098/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 87 [0/6768 (0.00%)]\t\tLoss: 0.30007\n",
      "Training Progress: \tEpoch 87 [320/6768 (4.72%)]\t\tLoss: 0.60469\n",
      "Training Progress: \tEpoch 87 [640/6768 (9.43%)]\t\tLoss: 0.75640\n",
      "Training Progress: \tEpoch 87 [960/6768 (14.15%)]\t\tLoss: 0.55520\n",
      "Training Progress: \tEpoch 87 [1280/6768 (18.87%)]\t\tLoss: 0.51262\n",
      "Training Progress: \tEpoch 87 [1600/6768 (23.58%)]\t\tLoss: 0.45428\n",
      "Training Progress: \tEpoch 87 [1920/6768 (28.30%)]\t\tLoss: 0.41990\n",
      "Training Progress: \tEpoch 87 [2240/6768 (33.02%)]\t\tLoss: 0.47525\n",
      "Training Progress: \tEpoch 87 [2560/6768 (37.74%)]\t\tLoss: 0.43660\n",
      "Training Progress: \tEpoch 87 [2880/6768 (42.45%)]\t\tLoss: 0.75031\n",
      "Training Progress: \tEpoch 87 [3200/6768 (47.17%)]\t\tLoss: 0.32196\n",
      "Training Progress: \tEpoch 87 [3520/6768 (51.89%)]\t\tLoss: 0.68214\n",
      "Training Progress: \tEpoch 87 [3840/6768 (56.60%)]\t\tLoss: 0.49806\n",
      "Training Progress: \tEpoch 87 [4160/6768 (61.32%)]\t\tLoss: 0.53561\n",
      "Training Progress: \tEpoch 87 [4480/6768 (66.04%)]\t\tLoss: 0.63247\n",
      "Training Progress: \tEpoch 87 [4800/6768 (70.75%)]\t\tLoss: 0.61299\n",
      "Training Progress: \tEpoch 87 [5120/6768 (75.47%)]\t\tLoss: 0.52185\n",
      "Training Progress: \tEpoch 87 [5440/6768 (80.19%)]\t\tLoss: 0.73992\n",
      "Training Progress: \tEpoch 87 [5760/6768 (84.91%)]\t\tLoss: 0.50744\n",
      "Training Progress: \tEpoch 87 [6080/6768 (89.62%)]\t\tLoss: 0.38397\n",
      "Training Progress: \tEpoch 87 [6400/6768 (94.34%)]\t\tLoss: 0.67577\n",
      "Training Progress: \tEpoch 87 [6720/6768 (99.06%)]\t\tLoss: 0.64062\n",
      "\tTrain loss: 0.00331, Accuracy: 6633/6768 (98.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1597/1692 (94.00%)\n",
      "\tTest loss: 0.00063, Accuracy: 1098/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 88 [0/6768 (0.00%)]\t\tLoss: 0.35284\n",
      "Training Progress: \tEpoch 88 [320/6768 (4.72%)]\t\tLoss: 0.48374\n",
      "Training Progress: \tEpoch 88 [640/6768 (9.43%)]\t\tLoss: 0.52997\n",
      "Training Progress: \tEpoch 88 [960/6768 (14.15%)]\t\tLoss: 0.72209\n",
      "Training Progress: \tEpoch 88 [1280/6768 (18.87%)]\t\tLoss: 0.75303\n",
      "Training Progress: \tEpoch 88 [1600/6768 (23.58%)]\t\tLoss: 0.31159\n",
      "Training Progress: \tEpoch 88 [1920/6768 (28.30%)]\t\tLoss: 0.47116\n",
      "Training Progress: \tEpoch 88 [2240/6768 (33.02%)]\t\tLoss: 0.61251\n",
      "Training Progress: \tEpoch 88 [2560/6768 (37.74%)]\t\tLoss: 0.56911\n",
      "Training Progress: \tEpoch 88 [2880/6768 (42.45%)]\t\tLoss: 0.55508\n",
      "Training Progress: \tEpoch 88 [3200/6768 (47.17%)]\t\tLoss: 0.40253\n",
      "Training Progress: \tEpoch 88 [3520/6768 (51.89%)]\t\tLoss: 0.59358\n",
      "Training Progress: \tEpoch 88 [3840/6768 (56.60%)]\t\tLoss: 0.64081\n",
      "Training Progress: \tEpoch 88 [4160/6768 (61.32%)]\t\tLoss: 0.42537\n",
      "Training Progress: \tEpoch 88 [4480/6768 (66.04%)]\t\tLoss: 0.39320\n",
      "Training Progress: \tEpoch 88 [4800/6768 (70.75%)]\t\tLoss: 0.32851\n",
      "Training Progress: \tEpoch 88 [5120/6768 (75.47%)]\t\tLoss: 0.49950\n",
      "Training Progress: \tEpoch 88 [5440/6768 (80.19%)]\t\tLoss: 0.34801\n",
      "Training Progress: \tEpoch 88 [5760/6768 (84.91%)]\t\tLoss: 0.43146\n",
      "Training Progress: \tEpoch 88 [6080/6768 (89.62%)]\t\tLoss: 0.36093\n",
      "Training Progress: \tEpoch 88 [6400/6768 (94.34%)]\t\tLoss: 0.59454\n",
      "Training Progress: \tEpoch 88 [6720/6768 (99.06%)]\t\tLoss: 0.60096\n",
      "\tTrain loss: 0.00344, Accuracy: 6631/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1590/1692 (93.00%)\n",
      "\tTest loss: 0.00065, Accuracy: 1070/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 89 [0/6768 (0.00%)]\t\tLoss: 0.40549\n",
      "Training Progress: \tEpoch 89 [320/6768 (4.72%)]\t\tLoss: 0.57086\n",
      "Training Progress: \tEpoch 89 [640/6768 (9.43%)]\t\tLoss: 0.47510\n",
      "Training Progress: \tEpoch 89 [960/6768 (14.15%)]\t\tLoss: 0.40358\n",
      "Training Progress: \tEpoch 89 [1280/6768 (18.87%)]\t\tLoss: 0.54484\n",
      "Training Progress: \tEpoch 89 [1600/6768 (23.58%)]\t\tLoss: 0.42679\n",
      "Training Progress: \tEpoch 89 [1920/6768 (28.30%)]\t\tLoss: 0.56203\n",
      "Training Progress: \tEpoch 89 [2240/6768 (33.02%)]\t\tLoss: 1.04979\n",
      "Training Progress: \tEpoch 89 [2560/6768 (37.74%)]\t\tLoss: 0.66610\n",
      "Training Progress: \tEpoch 89 [2880/6768 (42.45%)]\t\tLoss: 0.45391\n",
      "Training Progress: \tEpoch 89 [3200/6768 (47.17%)]\t\tLoss: 0.44554\n",
      "Training Progress: \tEpoch 89 [3520/6768 (51.89%)]\t\tLoss: 0.42565\n",
      "Training Progress: \tEpoch 89 [3840/6768 (56.60%)]\t\tLoss: 0.53285\n",
      "Training Progress: \tEpoch 89 [4160/6768 (61.32%)]\t\tLoss: 0.57583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 89 [4480/6768 (66.04%)]\t\tLoss: 0.55449\n",
      "Training Progress: \tEpoch 89 [4800/6768 (70.75%)]\t\tLoss: 0.61474\n",
      "Training Progress: \tEpoch 89 [5120/6768 (75.47%)]\t\tLoss: 0.39223\n",
      "Training Progress: \tEpoch 89 [5440/6768 (80.19%)]\t\tLoss: 0.44002\n",
      "Training Progress: \tEpoch 89 [5760/6768 (84.91%)]\t\tLoss: 0.58517\n",
      "Training Progress: \tEpoch 89 [6080/6768 (89.62%)]\t\tLoss: 0.49531\n",
      "Training Progress: \tEpoch 89 [6400/6768 (94.34%)]\t\tLoss: 0.50247\n",
      "Training Progress: \tEpoch 89 [6720/6768 (99.06%)]\t\tLoss: 0.73391\n",
      "\tTrain loss: 0.00321, Accuracy: 6624/6768 (97.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1595/1692 (94.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1099/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 90 [0/6768 (0.00%)]\t\tLoss: 0.35739\n",
      "Training Progress: \tEpoch 90 [320/6768 (4.72%)]\t\tLoss: 0.48877\n",
      "Training Progress: \tEpoch 90 [640/6768 (9.43%)]\t\tLoss: 0.34562\n",
      "Training Progress: \tEpoch 90 [960/6768 (14.15%)]\t\tLoss: 0.82512\n",
      "Training Progress: \tEpoch 90 [1280/6768 (18.87%)]\t\tLoss: 0.56321\n",
      "Training Progress: \tEpoch 90 [1600/6768 (23.58%)]\t\tLoss: 0.37343\n",
      "Training Progress: \tEpoch 90 [1920/6768 (28.30%)]\t\tLoss: 0.45639\n",
      "Training Progress: \tEpoch 90 [2240/6768 (33.02%)]\t\tLoss: 0.71579\n",
      "Training Progress: \tEpoch 90 [2560/6768 (37.74%)]\t\tLoss: 0.32892\n",
      "Training Progress: \tEpoch 90 [2880/6768 (42.45%)]\t\tLoss: 0.47557\n",
      "Training Progress: \tEpoch 90 [3200/6768 (47.17%)]\t\tLoss: 0.41721\n",
      "Training Progress: \tEpoch 90 [3520/6768 (51.89%)]\t\tLoss: 0.45537\n",
      "Training Progress: \tEpoch 90 [3840/6768 (56.60%)]\t\tLoss: 0.74103\n",
      "Training Progress: \tEpoch 90 [4160/6768 (61.32%)]\t\tLoss: 0.63887\n",
      "Training Progress: \tEpoch 90 [4480/6768 (66.04%)]\t\tLoss: 0.53819\n",
      "Training Progress: \tEpoch 90 [4800/6768 (70.75%)]\t\tLoss: 0.41186\n",
      "Training Progress: \tEpoch 90 [5120/6768 (75.47%)]\t\tLoss: 0.48988\n",
      "Training Progress: \tEpoch 90 [5440/6768 (80.19%)]\t\tLoss: 0.58865\n",
      "Training Progress: \tEpoch 90 [5760/6768 (84.91%)]\t\tLoss: 0.43058\n",
      "Training Progress: \tEpoch 90 [6080/6768 (89.62%)]\t\tLoss: 0.50659\n",
      "Training Progress: \tEpoch 90 [6400/6768 (94.34%)]\t\tLoss: 0.48916\n",
      "Training Progress: \tEpoch 90 [6720/6768 (99.06%)]\t\tLoss: 0.25523\n",
      "\tTrain loss: 0.00369, Accuracy: 6607/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1593/1692 (94.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1051/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 91 [0/6768 (0.00%)]\t\tLoss: 0.46904\n",
      "Training Progress: \tEpoch 91 [320/6768 (4.72%)]\t\tLoss: 0.87967\n",
      "Training Progress: \tEpoch 91 [640/6768 (9.43%)]\t\tLoss: 0.43123\n",
      "Training Progress: \tEpoch 91 [960/6768 (14.15%)]\t\tLoss: 0.67599\n",
      "Training Progress: \tEpoch 91 [1280/6768 (18.87%)]\t\tLoss: 0.34356\n",
      "Training Progress: \tEpoch 91 [1600/6768 (23.58%)]\t\tLoss: 0.32042\n",
      "Training Progress: \tEpoch 91 [1920/6768 (28.30%)]\t\tLoss: 0.36094\n",
      "Training Progress: \tEpoch 91 [2240/6768 (33.02%)]\t\tLoss: 0.35231\n",
      "Training Progress: \tEpoch 91 [2560/6768 (37.74%)]\t\tLoss: 0.77557\n",
      "Training Progress: \tEpoch 91 [2880/6768 (42.45%)]\t\tLoss: 0.49855\n",
      "Training Progress: \tEpoch 91 [3200/6768 (47.17%)]\t\tLoss: 0.69997\n",
      "Training Progress: \tEpoch 91 [3520/6768 (51.89%)]\t\tLoss: 0.47295\n",
      "Training Progress: \tEpoch 91 [3840/6768 (56.60%)]\t\tLoss: 0.49575\n",
      "Training Progress: \tEpoch 91 [4160/6768 (61.32%)]\t\tLoss: 0.44956\n",
      "Training Progress: \tEpoch 91 [4480/6768 (66.04%)]\t\tLoss: 0.72936\n",
      "Training Progress: \tEpoch 91 [4800/6768 (70.75%)]\t\tLoss: 0.30236\n",
      "Training Progress: \tEpoch 91 [5120/6768 (75.47%)]\t\tLoss: 0.41080\n",
      "Training Progress: \tEpoch 91 [5440/6768 (80.19%)]\t\tLoss: 0.51118\n",
      "Training Progress: \tEpoch 91 [5760/6768 (84.91%)]\t\tLoss: 0.64868\n",
      "Training Progress: \tEpoch 91 [6080/6768 (89.62%)]\t\tLoss: 0.28381\n",
      "Training Progress: \tEpoch 91 [6400/6768 (94.34%)]\t\tLoss: 0.58684\n",
      "Training Progress: \tEpoch 91 [6720/6768 (99.06%)]\t\tLoss: 0.71249\n",
      "\tTrain loss: 0.00326, Accuracy: 6619/6768 (97.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1599/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1113/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 92 [0/6768 (0.00%)]\t\tLoss: 0.47215\n",
      "Training Progress: \tEpoch 92 [320/6768 (4.72%)]\t\tLoss: 0.71347\n",
      "Training Progress: \tEpoch 92 [640/6768 (9.43%)]\t\tLoss: 0.42294\n",
      "Training Progress: \tEpoch 92 [960/6768 (14.15%)]\t\tLoss: 0.65117\n",
      "Training Progress: \tEpoch 92 [1280/6768 (18.87%)]\t\tLoss: 0.47922\n",
      "Training Progress: \tEpoch 92 [1600/6768 (23.58%)]\t\tLoss: 0.39200\n",
      "Training Progress: \tEpoch 92 [1920/6768 (28.30%)]\t\tLoss: 0.55656\n",
      "Training Progress: \tEpoch 92 [2240/6768 (33.02%)]\t\tLoss: 0.56156\n",
      "Training Progress: \tEpoch 92 [2560/6768 (37.74%)]\t\tLoss: 0.38742\n",
      "Training Progress: \tEpoch 92 [2880/6768 (42.45%)]\t\tLoss: 0.43933\n",
      "Training Progress: \tEpoch 92 [3200/6768 (47.17%)]\t\tLoss: 0.37242\n",
      "Training Progress: \tEpoch 92 [3520/6768 (51.89%)]\t\tLoss: 0.71929\n",
      "Training Progress: \tEpoch 92 [3840/6768 (56.60%)]\t\tLoss: 0.47886\n",
      "Training Progress: \tEpoch 92 [4160/6768 (61.32%)]\t\tLoss: 0.55956\n",
      "Training Progress: \tEpoch 92 [4480/6768 (66.04%)]\t\tLoss: 0.53696\n",
      "Training Progress: \tEpoch 92 [4800/6768 (70.75%)]\t\tLoss: 0.38748\n",
      "Training Progress: \tEpoch 92 [5120/6768 (75.47%)]\t\tLoss: 0.22465\n",
      "Training Progress: \tEpoch 92 [5440/6768 (80.19%)]\t\tLoss: 0.57129\n",
      "Training Progress: \tEpoch 92 [5760/6768 (84.91%)]\t\tLoss: 0.51563\n",
      "Training Progress: \tEpoch 92 [6080/6768 (89.62%)]\t\tLoss: 0.30144\n",
      "Training Progress: \tEpoch 92 [6400/6768 (94.34%)]\t\tLoss: 0.33095\n",
      "Training Progress: \tEpoch 92 [6720/6768 (99.06%)]\t\tLoss: 0.58044\n",
      "\tTrain loss: 0.00333, Accuracy: 6636/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1607/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1118/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 93 [0/6768 (0.00%)]\t\tLoss: 0.18332\n",
      "Training Progress: \tEpoch 93 [320/6768 (4.72%)]\t\tLoss: 0.46104\n",
      "Training Progress: \tEpoch 93 [640/6768 (9.43%)]\t\tLoss: 0.61511\n",
      "Training Progress: \tEpoch 93 [960/6768 (14.15%)]\t\tLoss: 0.71917\n",
      "Training Progress: \tEpoch 93 [1280/6768 (18.87%)]\t\tLoss: 0.58514\n",
      "Training Progress: \tEpoch 93 [1600/6768 (23.58%)]\t\tLoss: 0.23067\n",
      "Training Progress: \tEpoch 93 [1920/6768 (28.30%)]\t\tLoss: 0.33485\n",
      "Training Progress: \tEpoch 93 [2240/6768 (33.02%)]\t\tLoss: 0.82145\n",
      "Training Progress: \tEpoch 93 [2560/6768 (37.74%)]\t\tLoss: 0.41865\n",
      "Training Progress: \tEpoch 93 [2880/6768 (42.45%)]\t\tLoss: 0.29047\n",
      "Training Progress: \tEpoch 93 [3200/6768 (47.17%)]\t\tLoss: 0.48474\n",
      "Training Progress: \tEpoch 93 [3520/6768 (51.89%)]\t\tLoss: 0.62534\n",
      "Training Progress: \tEpoch 93 [3840/6768 (56.60%)]\t\tLoss: 0.54970\n",
      "Training Progress: \tEpoch 93 [4160/6768 (61.32%)]\t\tLoss: 0.35866\n",
      "Training Progress: \tEpoch 93 [4480/6768 (66.04%)]\t\tLoss: 0.81895\n",
      "Training Progress: \tEpoch 93 [4800/6768 (70.75%)]\t\tLoss: 0.40400\n",
      "Training Progress: \tEpoch 93 [5120/6768 (75.47%)]\t\tLoss: 0.63432\n",
      "Training Progress: \tEpoch 93 [5440/6768 (80.19%)]\t\tLoss: 0.55242\n",
      "Training Progress: \tEpoch 93 [5760/6768 (84.91%)]\t\tLoss: 0.51029\n",
      "Training Progress: \tEpoch 93 [6080/6768 (89.62%)]\t\tLoss: 0.48630\n",
      "Training Progress: \tEpoch 93 [6400/6768 (94.34%)]\t\tLoss: 0.53663\n",
      "Training Progress: \tEpoch 93 [6720/6768 (99.06%)]\t\tLoss: 0.52843\n",
      "\tTrain loss: 0.00306, Accuracy: 6659/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1614/1692 (95.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1088/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 94 [0/6768 (0.00%)]\t\tLoss: 0.34181\n",
      "Training Progress: \tEpoch 94 [320/6768 (4.72%)]\t\tLoss: 0.90176\n",
      "Training Progress: \tEpoch 94 [640/6768 (9.43%)]\t\tLoss: 0.53677\n",
      "Training Progress: \tEpoch 94 [960/6768 (14.15%)]\t\tLoss: 0.40148\n",
      "Training Progress: \tEpoch 94 [1280/6768 (18.87%)]\t\tLoss: 0.78306\n",
      "Training Progress: \tEpoch 94 [1600/6768 (23.58%)]\t\tLoss: 0.25982\n",
      "Training Progress: \tEpoch 94 [1920/6768 (28.30%)]\t\tLoss: 0.42422\n",
      "Training Progress: \tEpoch 94 [2240/6768 (33.02%)]\t\tLoss: 0.79687\n",
      "Training Progress: \tEpoch 94 [2560/6768 (37.74%)]\t\tLoss: 0.43541\n",
      "Training Progress: \tEpoch 94 [2880/6768 (42.45%)]\t\tLoss: 0.49538\n",
      "Training Progress: \tEpoch 94 [3200/6768 (47.17%)]\t\tLoss: 0.54376\n",
      "Training Progress: \tEpoch 94 [3520/6768 (51.89%)]\t\tLoss: 0.48918\n",
      "Training Progress: \tEpoch 94 [3840/6768 (56.60%)]\t\tLoss: 0.57664\n",
      "Training Progress: \tEpoch 94 [4160/6768 (61.32%)]\t\tLoss: 0.52254\n",
      "Training Progress: \tEpoch 94 [4480/6768 (66.04%)]\t\tLoss: 0.45067\n",
      "Training Progress: \tEpoch 94 [4800/6768 (70.75%)]\t\tLoss: 0.47488\n",
      "Training Progress: \tEpoch 94 [5120/6768 (75.47%)]\t\tLoss: 0.45341\n",
      "Training Progress: \tEpoch 94 [5440/6768 (80.19%)]\t\tLoss: 0.56091\n",
      "Training Progress: \tEpoch 94 [5760/6768 (84.91%)]\t\tLoss: 0.45729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: \tEpoch 94 [6080/6768 (89.62%)]\t\tLoss: 0.29494\n",
      "Training Progress: \tEpoch 94 [6400/6768 (94.34%)]\t\tLoss: 0.52735\n",
      "Training Progress: \tEpoch 94 [6720/6768 (99.06%)]\t\tLoss: 0.57930\n",
      "\tTrain loss: 0.00340, Accuracy: 6602/6768 (97.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1581/1692 (93.00%)\n",
      "\tTest loss: 0.00066, Accuracy: 1058/1772 (59.00%)\n",
      "\n",
      "Training Progress: \tEpoch 95 [0/6768 (0.00%)]\t\tLoss: 0.50214\n",
      "Training Progress: \tEpoch 95 [320/6768 (4.72%)]\t\tLoss: 0.72671\n",
      "Training Progress: \tEpoch 95 [640/6768 (9.43%)]\t\tLoss: 0.29510\n",
      "Training Progress: \tEpoch 95 [960/6768 (14.15%)]\t\tLoss: 0.90903\n",
      "Training Progress: \tEpoch 95 [1280/6768 (18.87%)]\t\tLoss: 0.29186\n",
      "Training Progress: \tEpoch 95 [1600/6768 (23.58%)]\t\tLoss: 0.26699\n",
      "Training Progress: \tEpoch 95 [1920/6768 (28.30%)]\t\tLoss: 0.46162\n",
      "Training Progress: \tEpoch 95 [2240/6768 (33.02%)]\t\tLoss: 0.65780\n",
      "Training Progress: \tEpoch 95 [2560/6768 (37.74%)]\t\tLoss: 0.60022\n",
      "Training Progress: \tEpoch 95 [2880/6768 (42.45%)]\t\tLoss: 0.58467\n",
      "Training Progress: \tEpoch 95 [3200/6768 (47.17%)]\t\tLoss: 0.72831\n",
      "Training Progress: \tEpoch 95 [3520/6768 (51.89%)]\t\tLoss: 0.43294\n",
      "Training Progress: \tEpoch 95 [3840/6768 (56.60%)]\t\tLoss: 0.58973\n",
      "Training Progress: \tEpoch 95 [4160/6768 (61.32%)]\t\tLoss: 0.41504\n",
      "Training Progress: \tEpoch 95 [4480/6768 (66.04%)]\t\tLoss: 0.67195\n",
      "Training Progress: \tEpoch 95 [4800/6768 (70.75%)]\t\tLoss: 0.59342\n",
      "Training Progress: \tEpoch 95 [5120/6768 (75.47%)]\t\tLoss: 0.51931\n",
      "Training Progress: \tEpoch 95 [5440/6768 (80.19%)]\t\tLoss: 0.57308\n",
      "Training Progress: \tEpoch 95 [5760/6768 (84.91%)]\t\tLoss: 0.43747\n",
      "Training Progress: \tEpoch 95 [6080/6768 (89.62%)]\t\tLoss: 0.55905\n",
      "Training Progress: \tEpoch 95 [6400/6768 (94.34%)]\t\tLoss: 0.36675\n",
      "Training Progress: \tEpoch 95 [6720/6768 (99.06%)]\t\tLoss: 0.41332\n",
      "\tTrain loss: 0.00320, Accuracy: 6644/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1597/1692 (94.00%)\n",
      "\tTest loss: 0.00060, Accuracy: 1105/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 96 [0/6768 (0.00%)]\t\tLoss: 0.47701\n",
      "Training Progress: \tEpoch 96 [320/6768 (4.72%)]\t\tLoss: 0.61874\n",
      "Training Progress: \tEpoch 96 [640/6768 (9.43%)]\t\tLoss: 0.50621\n",
      "Training Progress: \tEpoch 96 [960/6768 (14.15%)]\t\tLoss: 0.46674\n",
      "Training Progress: \tEpoch 96 [1280/6768 (18.87%)]\t\tLoss: 0.76364\n",
      "Training Progress: \tEpoch 96 [1600/6768 (23.58%)]\t\tLoss: 0.39626\n",
      "Training Progress: \tEpoch 96 [1920/6768 (28.30%)]\t\tLoss: 0.41870\n",
      "Training Progress: \tEpoch 96 [2240/6768 (33.02%)]\t\tLoss: 0.52958\n",
      "Training Progress: \tEpoch 96 [2560/6768 (37.74%)]\t\tLoss: 0.43151\n",
      "Training Progress: \tEpoch 96 [2880/6768 (42.45%)]\t\tLoss: 0.26839\n",
      "Training Progress: \tEpoch 96 [3200/6768 (47.17%)]\t\tLoss: 0.53734\n",
      "Training Progress: \tEpoch 96 [3520/6768 (51.89%)]\t\tLoss: 0.58275\n",
      "Training Progress: \tEpoch 96 [3840/6768 (56.60%)]\t\tLoss: 0.76999\n",
      "Training Progress: \tEpoch 96 [4160/6768 (61.32%)]\t\tLoss: 0.33919\n",
      "Training Progress: \tEpoch 96 [4480/6768 (66.04%)]\t\tLoss: 0.45653\n",
      "Training Progress: \tEpoch 96 [4800/6768 (70.75%)]\t\tLoss: 0.26129\n",
      "Training Progress: \tEpoch 96 [5120/6768 (75.47%)]\t\tLoss: 0.35755\n",
      "Training Progress: \tEpoch 96 [5440/6768 (80.19%)]\t\tLoss: 0.59037\n",
      "Training Progress: \tEpoch 96 [5760/6768 (84.91%)]\t\tLoss: 0.38916\n",
      "Training Progress: \tEpoch 96 [6080/6768 (89.62%)]\t\tLoss: 0.38430\n",
      "Training Progress: \tEpoch 96 [6400/6768 (94.34%)]\t\tLoss: 0.45306\n",
      "Training Progress: \tEpoch 96 [6720/6768 (99.06%)]\t\tLoss: 0.49613\n",
      "\tTrain loss: 0.00322, Accuracy: 6611/6768 (97.00%)\n",
      "\tValidation loss: 0.00012, Accuracy: 1590/1692 (93.00%)\n",
      "\tTest loss: 0.00062, Accuracy: 1074/1772 (60.00%)\n",
      "\n",
      "Training Progress: \tEpoch 97 [0/6768 (0.00%)]\t\tLoss: 0.34619\n",
      "Training Progress: \tEpoch 97 [320/6768 (4.72%)]\t\tLoss: 0.68579\n",
      "Training Progress: \tEpoch 97 [640/6768 (9.43%)]\t\tLoss: 0.30288\n",
      "Training Progress: \tEpoch 97 [960/6768 (14.15%)]\t\tLoss: 0.44130\n",
      "Training Progress: \tEpoch 97 [1280/6768 (18.87%)]\t\tLoss: 0.35885\n",
      "Training Progress: \tEpoch 97 [1600/6768 (23.58%)]\t\tLoss: 0.23487\n",
      "Training Progress: \tEpoch 97 [1920/6768 (28.30%)]\t\tLoss: 0.38535\n",
      "Training Progress: \tEpoch 97 [2240/6768 (33.02%)]\t\tLoss: 0.40203\n",
      "Training Progress: \tEpoch 97 [2560/6768 (37.74%)]\t\tLoss: 0.31984\n",
      "Training Progress: \tEpoch 97 [2880/6768 (42.45%)]\t\tLoss: 0.63378\n",
      "Training Progress: \tEpoch 97 [3200/6768 (47.17%)]\t\tLoss: 0.44301\n",
      "Training Progress: \tEpoch 97 [3520/6768 (51.89%)]\t\tLoss: 0.69153\n",
      "Training Progress: \tEpoch 97 [3840/6768 (56.60%)]\t\tLoss: 0.57816\n",
      "Training Progress: \tEpoch 97 [4160/6768 (61.32%)]\t\tLoss: 0.47028\n",
      "Training Progress: \tEpoch 97 [4480/6768 (66.04%)]\t\tLoss: 0.62884\n",
      "Training Progress: \tEpoch 97 [4800/6768 (70.75%)]\t\tLoss: 0.46898\n",
      "Training Progress: \tEpoch 97 [5120/6768 (75.47%)]\t\tLoss: 0.50500\n",
      "Training Progress: \tEpoch 97 [5440/6768 (80.19%)]\t\tLoss: 0.57762\n",
      "Training Progress: \tEpoch 97 [5760/6768 (84.91%)]\t\tLoss: 0.29892\n",
      "Training Progress: \tEpoch 97 [6080/6768 (89.62%)]\t\tLoss: 0.62643\n",
      "Training Progress: \tEpoch 97 [6400/6768 (94.34%)]\t\tLoss: 0.63830\n",
      "Training Progress: \tEpoch 97 [6720/6768 (99.06%)]\t\tLoss: 0.74997\n",
      "\tTrain loss: 0.00307, Accuracy: 6632/6768 (97.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1599/1692 (94.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1105/1772 (62.00%)\n",
      "\n",
      "Training Progress: \tEpoch 98 [0/6768 (0.00%)]\t\tLoss: 0.29180\n",
      "Training Progress: \tEpoch 98 [320/6768 (4.72%)]\t\tLoss: 0.55581\n",
      "Training Progress: \tEpoch 98 [640/6768 (9.43%)]\t\tLoss: 0.54726\n",
      "Training Progress: \tEpoch 98 [960/6768 (14.15%)]\t\tLoss: 0.78243\n",
      "Training Progress: \tEpoch 98 [1280/6768 (18.87%)]\t\tLoss: 0.49830\n",
      "Training Progress: \tEpoch 98 [1600/6768 (23.58%)]\t\tLoss: 0.44464\n",
      "Training Progress: \tEpoch 98 [1920/6768 (28.30%)]\t\tLoss: 0.63147\n",
      "Training Progress: \tEpoch 98 [2240/6768 (33.02%)]\t\tLoss: 0.52307\n",
      "Training Progress: \tEpoch 98 [2560/6768 (37.74%)]\t\tLoss: 0.40901\n",
      "Training Progress: \tEpoch 98 [2880/6768 (42.45%)]\t\tLoss: 0.35467\n",
      "Training Progress: \tEpoch 98 [3200/6768 (47.17%)]\t\tLoss: 0.43030\n",
      "Training Progress: \tEpoch 98 [3520/6768 (51.89%)]\t\tLoss: 0.53597\n",
      "Training Progress: \tEpoch 98 [3840/6768 (56.60%)]\t\tLoss: 0.72607\n",
      "Training Progress: \tEpoch 98 [4160/6768 (61.32%)]\t\tLoss: 0.61933\n",
      "Training Progress: \tEpoch 98 [4480/6768 (66.04%)]\t\tLoss: 0.47970\n",
      "Training Progress: \tEpoch 98 [4800/6768 (70.75%)]\t\tLoss: 0.48814\n",
      "Training Progress: \tEpoch 98 [5120/6768 (75.47%)]\t\tLoss: 0.34627\n",
      "Training Progress: \tEpoch 98 [5440/6768 (80.19%)]\t\tLoss: 0.63022\n",
      "Training Progress: \tEpoch 98 [5760/6768 (84.91%)]\t\tLoss: 0.40864\n",
      "Training Progress: \tEpoch 98 [6080/6768 (89.62%)]\t\tLoss: 0.48851\n",
      "Training Progress: \tEpoch 98 [6400/6768 (94.34%)]\t\tLoss: 0.36685\n",
      "Training Progress: \tEpoch 98 [6720/6768 (99.06%)]\t\tLoss: 0.69272\n",
      "\tTrain loss: 0.00271, Accuracy: 6669/6768 (98.00%)\n",
      "\tValidation loss: 0.00010, Accuracy: 1613/1692 (95.00%)\n",
      "\tTest loss: 0.00061, Accuracy: 1133/1772 (63.00%)\n",
      "\n",
      "Training Progress: \tEpoch 99 [0/6768 (0.00%)]\t\tLoss: 0.55007\n",
      "Training Progress: \tEpoch 99 [320/6768 (4.72%)]\t\tLoss: 0.45842\n",
      "Training Progress: \tEpoch 99 [640/6768 (9.43%)]\t\tLoss: 0.49944\n",
      "Training Progress: \tEpoch 99 [960/6768 (14.15%)]\t\tLoss: 0.63768\n",
      "Training Progress: \tEpoch 99 [1280/6768 (18.87%)]\t\tLoss: 0.54641\n",
      "Training Progress: \tEpoch 99 [1600/6768 (23.58%)]\t\tLoss: 0.35632\n",
      "Training Progress: \tEpoch 99 [1920/6768 (28.30%)]\t\tLoss: 0.90836\n",
      "Training Progress: \tEpoch 99 [2240/6768 (33.02%)]\t\tLoss: 0.57948\n",
      "Training Progress: \tEpoch 99 [2560/6768 (37.74%)]\t\tLoss: 0.41091\n",
      "Training Progress: \tEpoch 99 [2880/6768 (42.45%)]\t\tLoss: 0.28755\n",
      "Training Progress: \tEpoch 99 [3200/6768 (47.17%)]\t\tLoss: 0.46814\n",
      "Training Progress: \tEpoch 99 [3520/6768 (51.89%)]\t\tLoss: 0.57874\n",
      "Training Progress: \tEpoch 99 [3840/6768 (56.60%)]\t\tLoss: 0.39345\n",
      "Training Progress: \tEpoch 99 [4160/6768 (61.32%)]\t\tLoss: 0.40713\n",
      "Training Progress: \tEpoch 99 [4480/6768 (66.04%)]\t\tLoss: 0.38082\n",
      "Training Progress: \tEpoch 99 [4800/6768 (70.75%)]\t\tLoss: 0.39372\n",
      "Training Progress: \tEpoch 99 [5120/6768 (75.47%)]\t\tLoss: 0.69905\n",
      "Training Progress: \tEpoch 99 [5440/6768 (80.19%)]\t\tLoss: 0.64210\n",
      "Training Progress: \tEpoch 99 [5760/6768 (84.91%)]\t\tLoss: 0.45773\n",
      "Training Progress: \tEpoch 99 [6080/6768 (89.62%)]\t\tLoss: 0.35585\n",
      "Training Progress: \tEpoch 99 [6400/6768 (94.34%)]\t\tLoss: 0.28127\n",
      "Training Progress: \tEpoch 99 [6720/6768 (99.06%)]\t\tLoss: 0.58192\n",
      "\tTrain loss: 0.00253, Accuracy: 6664/6768 (98.00%)\n",
      "\tValidation loss: 0.00009, Accuracy: 1615/1692 (95.00%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest loss: 0.00063, Accuracy: 1091/1772 (61.00%)\n",
      "\n",
      "Training Progress: \tEpoch 100 [0/6768 (0.00%)]\t\tLoss: 0.20872\n",
      "Training Progress: \tEpoch 100 [320/6768 (4.72%)]\t\tLoss: 0.49007\n",
      "Training Progress: \tEpoch 100 [640/6768 (9.43%)]\t\tLoss: 0.29785\n",
      "Training Progress: \tEpoch 100 [960/6768 (14.15%)]\t\tLoss: 0.55326\n",
      "Training Progress: \tEpoch 100 [1280/6768 (18.87%)]\t\tLoss: 0.41574\n",
      "Training Progress: \tEpoch 100 [1600/6768 (23.58%)]\t\tLoss: 0.57971\n",
      "Training Progress: \tEpoch 100 [1920/6768 (28.30%)]\t\tLoss: 0.40500\n",
      "Training Progress: \tEpoch 100 [2240/6768 (33.02%)]\t\tLoss: 0.52476\n",
      "Training Progress: \tEpoch 100 [2560/6768 (37.74%)]\t\tLoss: 0.47743\n",
      "Training Progress: \tEpoch 100 [2880/6768 (42.45%)]\t\tLoss: 0.27496\n",
      "Training Progress: \tEpoch 100 [3200/6768 (47.17%)]\t\tLoss: 0.68290\n",
      "Training Progress: \tEpoch 100 [3520/6768 (51.89%)]\t\tLoss: 0.46199\n",
      "Training Progress: \tEpoch 100 [3840/6768 (56.60%)]\t\tLoss: 0.53693\n",
      "Training Progress: \tEpoch 100 [4160/6768 (61.32%)]\t\tLoss: 0.51132\n",
      "Training Progress: \tEpoch 100 [4480/6768 (66.04%)]\t\tLoss: 0.54421\n",
      "Training Progress: \tEpoch 100 [4800/6768 (70.75%)]\t\tLoss: 0.46101\n",
      "Training Progress: \tEpoch 100 [5120/6768 (75.47%)]\t\tLoss: 0.43947\n",
      "Training Progress: \tEpoch 100 [5440/6768 (80.19%)]\t\tLoss: 0.59695\n",
      "Training Progress: \tEpoch 100 [5760/6768 (84.91%)]\t\tLoss: 0.31981\n",
      "Training Progress: \tEpoch 100 [6080/6768 (89.62%)]\t\tLoss: 0.26291\n",
      "Training Progress: \tEpoch 100 [6400/6768 (94.34%)]\t\tLoss: 0.43744\n",
      "Training Progress: \tEpoch 100 [6720/6768 (99.06%)]\t\tLoss: 0.54548\n",
      "\tTrain loss: 0.00303, Accuracy: 6673/6768 (98.00%)\n",
      "\tValidation loss: 0.00011, Accuracy: 1621/1692 (95.00%)\n",
      "\tTest loss: 0.00064, Accuracy: 1082/1772 (61.00%)\n",
      "\n",
      "Best validation accuracy:\n",
      "0.9580378250591016\n",
      "Best test accuracy:\n",
      "0.6393905191873589\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(cnn_gru, cnn_gru_optimizer, data_loaders_acgan, num_epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
